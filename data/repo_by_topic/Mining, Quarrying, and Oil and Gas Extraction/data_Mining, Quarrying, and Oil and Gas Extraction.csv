Unnamed: 0,repo,user,organization,url (HTML),url (API),description,readme,stargazer count,watcher count,subscriber count,open issue count,topic (search),topics,NAICS Code
0,yohanesnuwara,reservoir-engineering,,https://github.com/yohanesnuwara/reservoir-engineering,https://api.github.com/repos/reservoir-engineering/yohanesnuwara,"Python worked examples and problems from Reservoir Engineering textbooks (Brian Towler SPE Textbook Vol. 8, etc.)","# Python Worked Examples and Problems of Worldwide Reservoir Engineering Textbooks

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## SPE Textbook Vol. 8 *Fundamental Principles of Reservoir Engineering* by Brian F. Towler

![brian towler book](https://user-images.githubusercontent.com/51282928/74505368-89a88e80-4f29-11ea-80a6-e563b6237729.jpg)

Original copy of this book can be purchased from [SPE Bookstore](https://store.spe.org/Fundamental-Principles-of-Reservoir-Engineering-P27.aspx)

All examples and problems from the book are worked on Python language.

The book contains **14 units** and **1 Appendix**. In this repo, each unit has 3 folders: 

* `data`: compilation of data in CSV tables for examples and problems
* `functions`: compilation of functions from the Equations in the book to execute certain purposed calculations
* `notebooks`: Jupyter notebooks of worked examples and practice problems

|**No.**|**Chapter Name**|**Link**|
|:--:|:--:|:--:|
|2|Review of Rock and Fluid Properties|[Folder](https://github.com/yohanesnuwara/reservoir-engineering/tree/master/Unit%202%20Review%20of%20Rock%20and%20Fluid%20Properties)|
|3|Reservoir Statics|[Folder](https://github.com/yohanesnuwara/reservoir-engineering/tree/master/Unit%203%20Reservoir%20Statics)|
|4|Volumetrics|[Folder](https://github.com/yohanesnuwara/reservoir-engineering/tree/master/Unit%204%20Volumetrics)|
|5|Material Balance|[Folder](https://github.com/yohanesnuwara/reservoir-engineering/tree/master/Unit%205%20Material%20Balance/notebook)|
|6|Single-Phase-Fluid Flow in Porous Media|[Folder](https://github.com/yohanesnuwara/reservoir-engineering/tree/master/Unit%206%20Single-Phase-Fluid%20Flow%20in%20Porous%20Media)|
|7|Introduction to Well-Test Analysis|[Folder](https://github.com/yohanesnuwara/reservoir-engineering/tree/master/Unit%207%20Introduction%20to%20Well-Test%20Analysis)|
|8|Aquifer Influx|[Folder](https://github.com/yohanesnuwara/reservoir-engineering/tree/master/Unit%208%20Aquifer%20Influx)|
|9|Dry-Gas Reservoirs|[Folder](https://github.com/yohanesnuwara/reservoir-engineering/tree/master/Unit%209%20Dry-Gas%20Reservoirs)|
|10|Gas-Condensate Reservoirs|[Folder](https://github.com/yohanesnuwara/reservoir-engineering/tree/master/Unit%2010%20Gas-Condensate%20Reservoirs/notebook)|
|11|Undersaturated-Oil Reservoirs|[Folder](https://github.com/yohanesnuwara/reservoir-engineering/tree/master/Unit%2011%20Undersaturated-Oil%20Reservoirs/notebook)|
|12|Saturated-Oil Reservoirs|Soon|
|13|Fluid Distribution and Displacement|Soon|
|14|Decline-Curve Analysis|Soon|

**Appendix A** contains values and tabulations for solving certain equations in the book. 

## External Tools that I Use

* [**Online OCR**](https://www.onlineocr.net/): Optical Character Recognition (OCR) online to convert paper Tables from the book (printed) to a digital Excel spreadsheet 
* **Microsoft Excel**: to save the digitized Table as a CSV format 

## Citation BibTex

If you use this repository for a certain purpose, please make this citation

> Y. Nuwara, reservoir-engineering, (2020), GitHub repository, https://github.com/yohanesnuwara/reservoir-engineering

BibTex registry:

```
@misc{Nuwara2020,
  author = {Nuwara, Y.},
  title = {reservoir-engineering},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/yohanesnuwara/reservoir-engineering}},
  commit = {b1a8a2e3de8a9f53febf5ca8452e564bc34c9c76}
}
```

## Friends who Helped Completing the Missing Puzzles

Thanks ... for ...

* **Mark Burgoyne**, (Principal Reservoir Engineer in Santos), helping with solution to solve numerically the transient flow problems (Chapter 5) based on [Klins et al (1988) SPE-15433-PA paper](https://www.onepetro.org/journal-paper/SPE-15433-PA). The code to calculate the aquifer flow [here](https://github.com/yohanesnuwara/reservoir-engineering/blob/master/Unit%206%20Single-Phase-Fluid%20Flow%20in%20Porous%20Media/functions/aquifer_flow.py). Mark's [Monograph-20-Examples](https://github.com/vinomarkus/Monograph-20-Examples) repo has inspired me to create this repo. 

## License

<a rel=""license"" href=""http://creativecommons.org/licenses/by/4.0/""><img alt=""Creative Commons License"" style=""border-width:0"" src=""https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png"" /></a><br />This work is licensed under a <a rel=""license"" href=""http://creativecommons.org/licenses/by/4.0/"">Creative Commons Attribution 4.0 International License</a>.
",161,161,23,3,oil-and-gas,"[numerical-computation, oil-and-gas, reservoir-engineering]",0.0
1,dudley-fitzgerald,AutomatedWellLogCorrelation,,https://github.com/dudley-fitzgerald/AutomatedWellLogCorrelation,https://api.github.com/repos/AutomatedWellLogCorrelation/dudley-fitzgerald,Framework for correlating two or more well logs using feature vectors generated from CNN's in Pytorch,"# Automated well log correlation using Python

This repository contains a preliminary (and admittely incomplete) set of scripts that can be used to correlate two or more well logs automatically without any user input.

## Background

Well log correlation is the process of identifying equivalent geological units / features between two or more well logs.  This is commonly done by identifying corresponding patterns between well logs from different wells and assigning them to a particular marker.  This process allows us to develop an understanding of the subsurface geology and answer a variety of different questions.

Correlating well logs is a time consuming, tedious task.  Depending on the number of wells,  number of formations that are being interpreted, quality of the data, and the complexity of the geology this process can take days to months to complete.  Additionally, this process is susceptible to bias as we each bring our own individual background and experiences, as well as prone to error as there is ambiguity in the well logs and general complexity of correlation.

## Overview of process

The process captured in this repository leverages a variety of different capabilities from the computer vision and image processing community.  Essentially, the approach outlined here is a 1D adaptation of panoramic stiching using feature vectors generated from a 1D Convolutional Autoencoder.  The basic idea being to identify a series of matching points between a pair of well logs from locally minimal values in a cost matrix that was generated from two sets of feature vectors.

### Training a 1D Convolutional Autoencoder

Ultimately we want to train a model that can learn higher dimensional representations of well log expressions that we can use to assess similarities between well logs.  In order to achieve this a very simple 1D Convolutional Autoencorder was put together using PyTorch.  This model contains a 5 layer enconding method consiting of 1D convolutional layers that are combined with ReLu activation and Max Pooling functions, as well as a 5 layer decoding method consisting of 1D transpose convolutional layers that are combined with ReLu activation functions.  **There is definitely room to improve on the architecture of this model.**

#### Create training data

![Create training data workflow](/images/createTrainingDataWorkflow.png)

Aggregate as many well logs of the same type as possible and put them in the same directory.  The **createTrainingData.py** script accepts a series of command line arguements that will process the well log data and for each well take a series of windowed extractions and save them to disk as a 2D array.  Explaination of the arguements can be found in the script.  An example would be:

python createTrainingData.py --data-name autoWell --data-dir /path/to/LAS/files --output-dir /path/to/save/to --log-name GR

#### Train model

![Train model](/images/trainModelWorkflow.png)

Training the model is fairly simple and consistent with how most CNN's are trained.  The **trainModel.py** script accepts a series of command line arguements that will being the training process.  Explaination of the arguements can be found in the script.  An example would be:

python trainModel.py --model-name myNewModel --data-dir /path/to/trainingData --output-dir /location/to/save/model 

**Note: The model doesn't always initialize well.  If the loss remains fairly consistent after a few epochs, kill the script and run again**

**Note: Currently the script is hard coded in terms of optimizer, learning rate, and criterion**

Here is an example of the prediction on a windowedd extraction from the test data set along with the loss curves after nearly 250 epochs.
![Loss and validation](/images/trainValidation.png)

### Correlating well logs

#### Creating a project

In order to correlate well logs using these scripts you will need to create a project file.  This file is a HDF5 data store that contains the well header and all of the well logs as Pandas DataFrames.

![Create project](/images/createDataStructureWorkflow.png)

Creating the project is a bit clunky, but using the **createAutoWellProject.py** script along with the appropriate command line arguements you should be able to successfully construct a project.  In the header file you will need to have at least 3 columns, 1) UWI / API, 2) X coorinate, and 3) Y coordinate.  In the terminal you will need to specify the column name that corresponds to the appropriate value so the script will use the right columns.  Additionally, the LAS files must be named the same value as can be found in the UWI column of the header to match the well log data to the appropriate coordinate.  Explaination of the arguements can be found in the script.  An example would be:

python createAutoWellProject.py --proj-name autoWell --proj-dir /path/to/project/location --header-dir /path/to/headerFile --las-dir /las/directory --uwi-col APINo --x-col Longitude --y-col Latitude

#### Creating the AutoWellCorrelation class and building a connectivity graph

The correlation process is an adaptation of panormic stiching for 1D well logs.  The scripts provided here are somewhat incomplete.  Please see the end of the README file for additional work.

![Create AutoWellCorrelation](/images/autoWellCorrWorkflow.png)

The first two steps are relatively straight forward.  We first need to load the data and process the well logs to be usable by our 1D Convolutional Autoenconder.  Additionally, we identify well pairs based on spatial proximity to each other.

An example of what the windowed well log data looks like:
![windowed well log data](/images/windowExtractions.png)

To automatically correlate our well logs we iterate over all of the pairs of wells and process the pairs individually.

First, we compute the feature vectors by passing the well log values through encoding portion of our autoencoder:
![feature vectors](/images/featureVectors.png)

Second, we compute the Difference of Gaussians and identify key points for each well log:
![key points](/images/keyPointsDoG.png)

Third, we compute a cost and a lag (depth) distance matrix using the feature vectors and depths at the location of the key points identified in the previous step.  The cost matrix is then analyzed for local minima in both the I and J directions to find matching points which represent the indices of correlations between well logs  

Example of cost map and matching points:
![cost map #1](/images/costMap1.png)
![cost map #2](/images/costMap2.png)
![cost map #2](/images/costMap3.png)

Each of the matching points is added to a NetworkX graph as an edge.  The purpose of generating this graph is to enable automated picking of individual markers or for generating a global interpretation across all wells and depths.  The results here are limited, but could be extended to encompass the global solution.",117,117,10,2,oil-and-gas,"[automated-interpretation, cnn, feature-vector, geology, interpretation, joblib, networkx, oil-and-gas, oil-wells, python, pytorch, sift, well-correlation, well-logs]",0.0
2,Skoltech-CHR,DeepField,Skoltech-CHR,https://github.com/Skoltech-CHR/DeepField,https://api.github.com/repos/DeepField/Skoltech-CHR,Machine learning framework for reservoir simulation,"[![Python](https://img.shields.io/badge/python-3-blue.svg)](https://python.org)
[![Pytorch](https://img.shields.io/badge/PyTorch-orange.svg)](https://pytorch.org)
![](https://github.com/Skoltech-CHR/DeepField/workflows/pylint-check/badge.svg)


# DeepField

Machine learning framework for reservoir simulation.

![img](static/3d_basic.PNG)

## Features

* reservoir representation with Grid, Rock, States, Wells, Aquifer and PVT-tables components
* interactive 3D visualization with some advanced options
* common reservoir preprocessing tools
* working with arbitrary large datasets of field simulations
* constructor of neural network models
* generative models for field augmentation
* various model training scenarios for arbitrary long simulation periods
* detailed [documentation](https://Skoltech-CHR.github.io/DeepField) and step-by-step [tutorials](/tutorials)
* complete [pipelines](/pipelines) of the reservoir simulation steps


![img](static/framework.PNG)

## Installation

Clone the repository:

    git clone https://github.com/Skoltech-CHR/DeepField.git

Working with a remote server, it is recommended to install
VNC for remote rendering of 3D graphics (follow this [instruction](./vnc/README.md))

Another option is to build the docker image with DeepField inside.
Instructions and dockerfile are provided in the [docker](./docker) directory.

```
Note: the project is in developement. We welcome contributions and collaborations.
```

## Quick start

Load a reservoir model from `.DATA` file:

```python

  from deepfield import Field

  model = Field('model.data').load()
```

See the [tutorials](./tutorials) to explore the framework step-by-step
and the [documentation](https://Skoltech-CHR.github.io/DeepField) for more details.


## Model formats

Initial reservoir model can be given in a mixture of ECLIPSE, MORE, PETREL, tNavigator formats.
However, there is no guarantee that any mixture will be understood.
Main file should be in .DATA file. Dependencies can be text and binary files including common formats:

* .GRDECL
* .INC
* .RSM
* .UNRST
* .RSSPEC
* .UNSMRY
* .SMSPEC
* .EGRID
* .INIT

Within the `DeepField` framework it is recommended to use the HDF5 format
to speed up data load and dump in Python-friendly data formats. In this
case all data are contained in a single .HDF5 file. At any point the model
can be exported back into .DATA text and binary files to ensure a compatibility
with conventional software.

## Citing

Plain text
```
E. Illarionov, P. Temirchev, D. Voloskov, R. Kostoev, M. Simonov, D. Pissarenko, D. Orlov, D. Koroteev, 2022. End-to-end neural network approach to 3D reservoir simulation and adaptation. J. Pet. Sci. Eng. 208, 109332. https://doi.org/10.1016/j.petrol.2021.109332
```

BibTex
```
@article{ILLARIONOV2022109332,
author = {E. Illarionov and P. Temirchev and D. Voloskov and R. Kostoev and M. Simonov and D. Pissarenko and D. Orlov and D. Koroteev},
title = {End-to-end neural network approach to 3D reservoir simulation and adaptation},
journal = {Journal of Petroleum Science and Engineering},
volume = {208},
pages = {109332},
year = {2022},
issn = {0920-4105},
doi = {https://doi.org/10.1016/j.petrol.2021.109332},
url = {https://www.sciencedirect.com/science/article/pii/S0920410521009827}
}
```
",88,88,4,0,oil-and-gas,"[machine-learning, neural-network, oil-and-gas, petroleum-engineering, reservoir-engineering, reservoir-simulation]",0.0
3,yohanesnuwara,pyresim,,https://github.com/yohanesnuwara/pyresim,https://api.github.com/repos/pyresim/yohanesnuwara,Reservoir simulator in Python language,"# PyReSim*

#### Python reservoir simulation from single-phase simple reservoir to multi-phase complex reservoir 

*) still on work. Progress bar ![40%](https://progress-bar.dev/40)

<p align=""center"">
  <img width=""400"" height=""250"" src=""https://user-images.githubusercontent.com/51282928/91322826-60c35900-e7ea-11ea-8e5e-102a0f1a0f79.png"">
</p>

> This repository is still worked on. However, each week or so, there will be a teaser posted in my LinkedIn, about one reservoir simulation case and how PyReSim is used to solve. It will be scheduled for launch once all simulators have been set up and complete (scheduled in 2021). See my progress bar to keep updated, stay tuned!

## Aspects simulated in *PyReSim*

|Aspects|Availability|
|:--:|:--|
|Reservoir geometry|Regular 1D and 2D; 2D cylindrical well simulation; 2D reservoir with different elevations; 2D reservoir with irregular boundaries; 3D reservoir (*)|
|Reservoir property|Homogeneous (both isotropic & anisotropic permeability); heterogeneous (*)|
|Boundary conditions|Specified flow rate; Specified pressure; Specified pressure gradient; No flow|
|Well details|Diameter; skin factor; location at the grid block (center, edge, or corner)|
|Well-operating conditions|Specified flow rate (producer/injector well); Specified flowing borehole pressure (FBHP); Specified pressure gradient; Shut-in|
|Reservoir fluid types|Single-phase incompressible fluid (brine), slightly-compressible fluid (undersaturated oil), compressible fluid (gas); Multi-phase reservoir (brine-oil-gas)|
|Solver methods|Matrix inversion (very 3x basic method); explicit; implicit; Crank-Nicholson; many more. |

> (*) These aspects are still not available on the first launch (in 2021). It will be available on the second launch (afterwards). 

## Teasers

|Teaser No.|Picture|Description|Input data|Simulator|
|:--:|:--:|:--:|:--:|:--:|
|1|<div><img src=""https://user-images.githubusercontent.com/51282928/90217017-50929d80-de2a-11ea-8bb1-560b2ff2365c.png"" width=""200""/></div><br>[Click here for case description](https://github.com/yohanesnuwara/pyresim/blob/master/docs/teaser.md#teaser-1)|A gas-free oil in a 2D reservoir with uniform<br> grid dimension. Reservoir boundary in the west<br> has constant pressure, in the east is sealed (no flow),<br> in the south has pressure gradient, and in the<br> north has constant rate. Five wells penetrates<br> the reservoir, with various wellbore radius,<br> skin, and operating conditions.|[`input file`](https://github.com/yohanesnuwara/pyresim/blob/master/input/teasers/teaser1_input.txt)|[`source code`](https://github.com/yohanesnuwara/pyresim/blob/master/simulators/teasers/PyReSim_teaser1.ipynb)|
|2|<div><img src=""https://user-images.githubusercontent.com/51282928/90431371-21fb1800-e0f3-11ea-9afc-2921fa94e196.png"" width=""200""/></div><br>[Click here for case description](https://github.com/yohanesnuwara/pyresim/blob/master/docs/teaser.md#teaser-2)|A 2D reservoir with irregular boundaries<br> hosting a volatile oil. The reservoir is bounded<br> by a constant pressure. 2 wells penetrate into the<br> reservoir, and the flow rate as well as FBHP will be<br> reported after 50 days.<br>|[`input file`](https://github.com/yohanesnuwara/pyresim/blob/master/input/teasers/teaser2_input.txt)|[`source code`](https://github.com/yohanesnuwara/pyresim/blob/master/simulators/teasers/PyReSim_teaser2.ipynb)|
|3|<div><img src=""https://user-images.githubusercontent.com/51282928/89118863-7b4c3000-d4d3-11ea-918d-8432b110b475.png"" width=""200""/></div>|The same 2D reservoir in Teaser 2, has now<br> elevations (Pseudo-3D). The reservoir hosts gas.<br> The reservoir is bounded by a constant pressure.<br> 2 wells penetrate into the reservoir, and the flow<br> rate as well as FBHP will be reported after 50 days.|Coming soon|Coming soon|

## Challenging Case

PyReSim will be performed to a more challenging case. This case is obtained from a ""Chapter Project"" in *Basic Applied Reservoir Simulation* (Ertekin, Abou-Kassem, King; 2001). In this case, the reservoir geometry is complex (irregular boundary, varied grid size) and reservoir property is heterogeneous. [See more details of this case](https://github.com/yohanesnuwara/pyresim/blob/master/docs/challenge_description.md)

<div><img src=""https://user-images.githubusercontent.com/51282928/89013581-54a8c080-d33e-11ea-8f96-704e8b263c5c.png"" width=""300""/>  <img src=""https://user-images.githubusercontent.com/51282928/89118272-894b8200-d4ce-11ea-9e02-6d18d3e48583.png"" width=""650""/></div>

## Open for Contribution!

These is a list contains several options for contributions:
* Help writing and translating a reservoir data into Schlumberger ECLIPSE format, or JSON format
* (Updated more soon)

Let's make PyReSim better together. If you're confident to contribute, please let me know and [mail me](ign.nuwara97@gmail.com)

### List of our collaborators:

* Mohammed Saif ([@mohammedsaif38](https://github.com/mohammedsaif38))

## License

The author chooses Creative Commons BY-NC-ND 4.0 International to license this work. Please read what's permitted and what's not permitted [here](https://github.com/yohanesnuwara/pyresim/blob/master/LICENSE.md)

<a rel=""license"" href=""http://creativecommons.org/licenses/by-nc-nd/4.0/""><img alt=""Creative Commons License"" style=""border-width:0"" src=""https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png"" /></a><br />This work is licensed under a <a rel=""license"" href=""http://creativecommons.org/licenses/by-nc-nd/4.0/"">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License</a>.

<!--
**yohanesnuwara/yohanesnuwara** is a ✨ _special_ ✨ repository because its `README.md` (this file) appears on your GitHub profile.

## Simulation Cases

### Basic

|Case No.|Picture|Description|Input data|Simulator|
|:--:|:--:|:--:|:--:|:--:|
|1|<div><img src=""https://user-images.githubusercontent.com/51282928/90313743-2bd91b80-df39-11ea-9153-3e138a951508.png"" width=""300""/></div>|A 1D non-elevated reservoir bounded by constant<br> rate in the west side and no flow in the east side. Three wells<br> penetrates the reservoir, with contant rate, similar<br> wellbore size, and no skin.|[`input file`](https://github.com/yohanesnuwara/pyresim/tree/master/input/basic/basic1d_input.txt)<br><br>[`depth file`](https://github.com/yohanesnuwara/pyresim/tree/master/input/basic/basic1d_depth.txt)|[`source code`](https://github.com/yohanesnuwara/pyresim/blob/master/simulators/basic/PyReSim_basic1d_case1.ipynb)|

### Intermediate 

> Varying boundary conditions in different sides of the reservoir, and varying well operating conditions, configurations, wellbore radius, and skin.

|Case No.|Picture|Description|Input data|Simulator|
|:--:|:--:|:--:|:--:|:--:|
|1|<div><img src=""https://user-images.githubusercontent.com/51282928/90242937-235ce400-de58-11ea-9903-40ce8ef8feae.png"" width=""300""/></div>|A 1D non-elevated reservoir bounded by constant<br> pressure boundary in the west side and constant<br> pressure gradient in the east side. Five wells<br> penetrates the reservoir, with various wellbore<br> radius, skin, and operating conditions.|[`input file`](https://github.com/yohanesnuwara/pyresim/tree/master/input/intermediate/intermediate1d_input.txt)<br><br>[`depth file`](https://github.com/yohanesnuwara/pyresim/tree/master/input/intermediate/intermediate1d_depth.txt)|[`source code`](https://github.com/yohanesnuwara/pyresim/blob/master/simulators/intermediate/PyReSim_intermediate1d_case1.ipynb)|

### Advanced 

> Giving uneven distributions of multiple boundary conditions, and giving elevation to the grid blocks. 

|Case No.|Picture|Description|Input data|Simulator|
|:--:|:--:|:--:|:--:|:--:|
|1|<div><img src=""https://user-images.githubusercontent.com/51282928/90243270-c7468f80-de58-11ea-8491-a91612ae20c7.png"" width=""300""/></div>|A 1D elevated reservoir bounded by constant<br> pressure boundary in the west side and constant<br> pressure gradient in the east side. Five wells<br> penetrates the reservoir, with various wellbore<br> radius, skin, and operating conditions.|[`input file`](https://github.com/yohanesnuwara/pyresim/tree/master/input/advanced/advanced1d_input.txt)<br><br>[`depth file`](https://github.com/yohanesnuwara/pyresim/tree/master/input/advanced/advanced1d_depth.txt)|[`source code`](https://github.com/yohanesnuwara/pyresim/blob/master/simulators/advanced/PyReSim_advanced1d_case1.ipynb)|

## Cases

|Case No.|Picture|Description|Simulator|
|:--:|:--:|:--:|:--:|
|1|<div><img src=""https://user-images.githubusercontent.com/51282928/88264056-526ab480-ccf5-11ea-9cd0-622b6a57af6b.png"" width=""300""/></div><br>[Click here for case description](https://github.com/yohanesnuwara/pyresim/blob/master/docs/case_descriptions.md#case-1-1d-rectangular-reservoir-homogeneous-single-phase)|1D reservoir, same size rectangular<br> grid, homogeneous, single-phase|[`floweq_1d`](https://github.com/yohanesnuwara/pyresim/blob/master/simulators/floweq_1d.py)|
|2|<div><img src=""https://user-images.githubusercontent.com/51282928/88287885-28c58380-cd1d-11ea-915a-80a7bae7df72.png"" width=""250""/></div><br>[Click here for case description](https://github.com/yohanesnuwara/pyresim/blob/master/docs/case_descriptions.md#case-2-2d-rectangular-reservoir-homogeneous-single-phase)|2D reservoir, same size rectangular<br> grid, homogeneous, single-phase|[`floweq_2d`](https://github.com/yohanesnuwara/pyresim/blob/master/simulators/floweq_2d.py)|
|3|<div><img src=""https://user-images.githubusercontent.com/51282928/88464930-d638c280-cee8-11ea-8014-59c010afd95b.png"" width=""300""/></div><br>[Click here for case description](https://github.com/yohanesnuwara/pyresim/blob/master/docs/case_descriptions.md#case-3-3d-rectangular-reservoir-homogeneous-single-phase)|3D reservoir, same size rectangular<br> grid, homogeneous, single-phase|[`floweq_3d`](https://github.com/yohanesnuwara/pyresim/blob/master/simulators/floweq_3d.py)|
|4|<div><img src=""https://user-images.githubusercontent.com/51282928/88837303-4d7c9800-d202-11ea-8ee4-5221e8e1e298.png"" width=""200""/></div><br>[Click here for case description](https://github.com/yohanesnuwara/pyresim/blob/master/docs/case_descriptions.md#case-4-2d-cylindrical-reservoir-well-in-the-middle-homogeneous-varying-size-in-radial-direction)|2D reservoir, varying size in radial<br> direction, homogeneous, single-phase|[`floweq_well`](https://github.com/yohanesnuwara/pyresim/blob/master/simulators/floweq_well.py)|

## Using PyReSim for Academic or Industry Use

For any specific need that is unique to your case, and want to use PyReSim for your specific need, you are encouraged to discuss with us. We could provide a tutorial for your need. Presented here are the simulator codes applied to the cases presented above. We hope PyReSim can help to simulate most of your needs.

**Contact**<br>
📧 e-mail: ign.nuwara97@gmail.com

## Contributing to this Work

## Donation

We tirelessly spent days and nights to continuously develop, fine-tune, and improve PyReSim as an open-source Python reservoir simulator program, with the hope that everyone get benefits from. This work is done without direct financial support. If you like (and trust) this work, we would be very glad if you would consider to give us a little gift. Your donation will allow us to spend even more time improving this simulator.

## License

The author chooses Creative Commons BY-NC-ND 4.0 International to license this work. 

<a rel=""license"" href=""http://creativecommons.org/licenses/by-nc-nd/4.0/""><img alt=""Creative Commons License"" style=""border-width:0"" src=""https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png"" /></a><br />This work is licensed under a <a rel=""license"" href=""http://creativecommons.org/licenses/by-nc-nd/4.0/"">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License</a>.
",77,77,11,1,oil-and-gas,"[oil-and-gas, reservoir-engineering, reservoir-simulation]",0.0
4,yohanesnuwara,python-bootcamp-for-geoengineers,,https://github.com/yohanesnuwara/python-bootcamp-for-geoengineers,https://api.github.com/repos/python-bootcamp-for-geoengineers/yohanesnuwara,"Python source codes and notebooks from my courses I've given to SPEs and in Marietta College, Ohio, US","![python-bootcamp-for-geoengineers-logo](https://user-images.githubusercontent.com/51282928/83759266-80d8f580-a69d-11ea-9149-9c2eed8b025f.png)

This repository contains all Python source codes, notebooks, and data that I use for my course I've given to SPE student chapters and in PioPetro (Marietta College, Ohio, US). Topics of these courses revolve around basics of Python programming and applications in exploration and production.

## SPE Trisakti SC Course (13 September 2020)

This course consisted of 3 sessions in 1 day; first session about basic Python programming, second session about Python application in exploration (basic well-log analysis); and third session about Python application in production (basic decline curve analysis of production data). 

|Session|Topic|Video|Source code|
|:--:|:--:|:--:|:--:|
|1|NumPy, Matplotlib, Pandas, and Scipy|[Lecture video](https://youtu.be/hob5Hilj8sM?list=PLuHj14O65bBAOIdS5AYAKU0Fz_G7tMz73&t=1074) (Play until 38:21)|[Notebook](https://github.com/yohanesnuwara/python-bootcamp-for-geoengineers/blob/master/petroweek_notebooks/petroweek2020_unit1.ipynb)|
|2|Basic well-log data analysis|[Lecture video](https://youtu.be/fLGX92Doiw4?list=PLuHj14O65bBAOIdS5AYAKU0Fz_G7tMz73&t=6561) (Play until 2:30:29)|[Notebook](https://github.com/yohanesnuwara/python-bootcamp-for-geoengineers/blob/master/petroweek_notebooks/petroweek2020_unit2.ipynb)|
|3|Basic decline curve analysis|[Lecture video](https://youtu.be/fLGX92Doiw4?list=PLuHj14O65bBAOIdS5AYAKU0Fz_G7tMz73&t=9842) (Play until 3:13:35)|[Notebook](https://github.com/yohanesnuwara/python-bootcamp-for-geoengineers/blob/master/petroweek_notebooks/petroweek2020_unit3.ipynb)|

## SPE Port Harcourt Section Course in Nigeria (November 2020)

This course consisted of 3 sessions in 1 month; first session about basic Python programming and well-log analysis, second session about material balance analysis; and third session about well-test analysis. There were also coding assignments after this course. 

|Session|Topic|Video|Source code|Assignment|Solution
|:--:|:--:|:--:|:--:|:--:|:--:|
|1a|NumPy, Matplotlib, and Pandas|[Lecture video](https://youtu.be/fLGX92Doiw4?list=PLuHj14O65bBAOIdS5AYAKU0Fz_G7tMz73&t=1701) (Play until 1:35:20)|[Notebook](https://github.com/yohanesnuwara/python-bootcamp-for-geoengineers/blob/master/EnP_training/very_brief_intro_to_python.ipynb)|||
|1b|Advanced well-log data analysis|[Lecture video](https://youtu.be/hob5Hilj8sM?list=PLuHj14O65bBAOIdS5AYAKU0Fz_G7tMz73&t=2602) |[Notebook](https://github.com/yohanesnuwara/python-bootcamp-for-geoengineers/blob/master/EnP_training/session3_formation_evaluation_training.ipynb)|[Notebook](https://github.com/yohanesnuwara/python-bootcamp-for-geoengineers/blob/master/EnP_training/assignment2_SPE_PortHarcourt.ipynb)|[Notebook](https://github.com/yohanesnuwara/python-bootcamp-for-geoengineers/blob/master/EnP_training/SPE_assignment2_solution.ipynb)|
|2|Material balance analysis|[Lecture video](https://youtu.be/7AoExt4Ju1M?list=PLuHj14O65bBAOIdS5AYAKU0Fz_G7tMz73&t=383)|[Notebook](https://github.com/yohanesnuwara/python-bootcamp-for-geoengineers/blob/master/EnP_training/session4_mbal.ipynb)|||
|3|Well-test modeling and analysis|[Lecture video](https://youtu.be/8SujEmdoj0U?t=1102)|[Notebook](https://github.com/yohanesnuwara/python-bootcamp-for-geoengineers/blob/master/EnP_training/session5_welltest_instructor.ipynb)|[Notebook](https://github.com/yohanesnuwara/python-bootcamp-for-geoengineers/blob/master/EnP_training/assignment3_SPE.ipynb)|[Notebook](https://github.com/yohanesnuwara/python-bootcamp-for-geoengineers/blob/master/EnP_training/SPE_assignment3_notebook.ipynb)|

## PioPetro Course in Marietta College, Ohio, US (February 2021)

This course consisted of 4 sessions in 1 month; first session about basic Python programming, second session about production data analysis; and third session about exploration data visualization of well-log data, wellbore trajectory, and natural fractures; and fourth session about decline curve analysis and production forecast. This course was held as part of the Summer Internship Program. 

|Session|Topic|Video|Source code|
|:--:|:--:|:--:|:--:|
|1|NumPy, Matplotlib, Pandas, and Scipy|[Lecture video](https://youtu.be/pun_03d_i4Q)|[Notebook](https://github.com/yohanesnuwara/python-bootcamp-for-geoengineers/blob/master/PioPetro/piopetro_session1_introduction_participant.ipynb)|
|2|Production data analysis|[Lecture video](https://youtu.be/gLHDcwHFN50)|[Notebook](https://github.com/yohanesnuwara/python-bootcamp-for-geoengineers/blob/master/PioPetro/piopetro_session2_production_data_analysis.ipynb)|
|3|Exploration data visualization: </br> well-log, trajectory, natural fractures|[Lecture video](https://youtu.be/-BGiI_5CrwQ)|[Notebook](https://github.com/yohanesnuwara/python-bootcamp-for-geoengineers/blob/master/PioPetro/piopetro_session3_exploration_data.ipynb)|
|4|Basic decline curve analysis and forecast|[Lecture video](https://youtu.be/Fjetnt9Bp6s)|[Notebook](https://github.com/yohanesnuwara/python-bootcamp-for-geoengineers/blob/master/PioPetro/piopetro_session4_decline_curve_analysis.ipynb)|

<!--
Python Bootcamp for Geoengineers is a GitHub repository that stores all of available materials to start learning and working with Python for needs in oil and gas exploration and production, energy sector, and geoscience. This repo also stores all Jupyter notebooks that I give training to several SPEs student and professional sections. Some materials also linked to other repos and packages that I created (e.g. *PyReservoir* and *PyReSim*) where they're used.

## Contents:
* **Workshops and Training**

  * [Introduction to Python for Exploration and Production Course](https://github.com/yohanesnuwara/python-bootcamp-for-geoengineers#1-introduction-to-python-for-exploration-and-production-course-notebook) 
  * [Geoscience, Exploration, and Production Automation with Python](https://github.com/yohanesnuwara/python-bootcamp-for-geoengineers#2-geoscience-exploration-and-production-automation-with-python)
 
* **Talks**

  * [Python Awareness in Exploration and Production for Students and Professionals](https://github.com/yohanesnuwara/python-bootcamp-for-geoengineers#1-python-awareness-in-exploration-and-production-for-students-and-professionals-notebook)


## Workshops and Training

### 1. Introduction to Python for Exploration and Production Course ([Notebook](https://colab.research.google.com/drive/1NKjTuP16JeX8a1lvS2bRaqSCEnhGzFZD?usp=sharing))

This is a 1-day (3-hour) workshop I gave with SPE Trisakti Student Chapter, Indonesia, in September 13rd, 2020. Around 450 participants registered for this training session. 

[<img src=""https://user-images.githubusercontent.com/51282928/91654109-81343180-ead0-11ea-898d-4c43c199fa14.png"">](https://colab.research.google.com/drive/1NKjTuP16JeX8a1lvS2bRaqSCEnhGzFZD?usp=sharing)

**Topics discussed:**

*To start learning the following curriculum, see inside the notebook link provided above.* 

* Intro to Numpy, Matplotlib, Pandas, and Scipy
* Python for Exploration
  * Streaming well log and seismic data from open geoscience data
  * Visualize well log data
  * Basic exploratory data analysis (crossplot and histogram) using Seaborn
  * Basic petrophysics processing (computation of porosity, Vclay)
  * Demo: Read and display 3D seismic data
* Python for Production
  * Streaming production data (borehole pressure, production rate)
  * Simple well-test analysis
  * Simple decline curve analysis

### 2. Automate Geosciences and Reservoir Engineering with Python (Go to [Folder](https://github.com/yohanesnuwara/python-bootcamp-for-geoengineers/tree/master/EnP_training))

This is a 5-day (2-hour) training I (and Destiny Otto) gave with SPE Port Harcourt Section in Nigeria, from 6 to 21 November 2020. Surprisingly, 1,000+ participants from academia and industry from all around the world registered for this training session. 

In the folder, you will find 5 notebooks (3 notebooks for training + 2 for assignments; answer key).

**Topics discussed:**

* Formation evaluation with Python: Dataset used is well 15/9-F-11 A in Volve field dataset
  * Visualize well log data
  * Visualize triple combo
  * Visualize Neutron-Density plot
  * Compute petrophysical variables (formation porosity PHIF, shale volume VSH, and permeability K)
  
* Material balance analysis with Python: Using [`PyReservoir`](https://github.com/yohanesnuwara/pyreservoir). Datasets used are from `PyReservoir` tutorial notebooks, and in Volve field dataset
  * Produce MBAL plot to calculate OOIP and OGIP in gas and oil reservoirs
  * Calculate water (aquifer) influx
  
* Well-test analysis with Python: Using [`PyReservoir`](https://github.com/yohanesnuwara/pyreservoir). Datasets used is well 15/9-F-1 C in Volve field dataset
  * Simulation of constant rate and pressure test
  * Analysis of BHP drawdown and build-up 

## Talks

### 1. Python Awareness in Exploration and Production for Students and Professionals ([Notebook](https://github.com/yohanesnuwara/python-bootcamp-for-geoengineers/blob/master/demo_starting_python_E%26P_1hour.ipynb))

This is my 1-1.5 hour talk to tell reasons why geoscientists and petroleum engineers, students and professionals, should consider starting to learn Python. I gave this talk in a joint webinar by SPE Asia Pacific University (Malaysia) and SPE Northern Emirates Section (UAE) in 21 October, 2020. 

I gave a Python demo as a trigger material. See inside the notebook.

* Python and its effectiveness - *comparing use vs. non-use of list comprehension*
* Numpy, Matplotlib, and Pandas - *quick tour of the libraries*
* Access exploration open dataset - *stream exploration data (well log) from Kansas Geological Survey*
* Access production open dataset - *stream production data from Volve field in a Zenodo repository*

### 2. Machine Learning Application in the Volve Field Dataset

## License

<a rel=""license"" href=""http://creativecommons.org/licenses/by-nc-sa/4.0/""><img alt=""Creative Commons License"" style=""border-width:0"" src=""https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png"" /></a><br />This work is licensed under a <a rel=""license"" href=""http://creativecommons.org/licenses/by-nc-sa/4.0/"">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.

<!--
**yohanesnuwara/yohanesnuwara** is a ✨ _special_ ✨ repository because its `README.md` (this file) appears on your GitHub profile.

**Python Bootcamp for Geoengineers** was created in June 2020, seeing lots of geoengineers and geoscientists are interested to start programming in Python. This Bootcamp is structured into **4 Courses** and **1 Demo Room** (so far), each of the courses contains several modules that can be accessed using **Google Colab**, a web-cloud Python IDE. 

It is always recommended to start with the first 2 courses (**Intro to Python I and II**). Then, you could choose either to take the **Python Course for Oil and Gas** (ideal for geoengineers, such as petroleum engineers or reservoir engineers) or to take the **Python Course for Geoscience** (ideal for geoscientists, such as geologists or geophysicists). 

At the end of each course, there is a dummy **Exam** to test how far you already master each course!

Enjoy!


## Intro to Python I (Absolute Beginner)

1. Intro to Google Colab
2. [Intro to Numpy](https://colab.research.google.com/drive/1C2RCLJCQcyjw3pdfjWpQgOwCQWvHbqJs?usp=sharing)
3. Intro to Visualization with Matplotlib
4. Intro to Data with Pandas
5. Exam

## Intro to Python II (Next Level)

5. Intro to Scientific Computing with Scipy
6. Intro to Exploratory Data Analysis with Seaborn
7. Exam

## Python Course for Oil and Gas

1. Exploring Production Data with Pandas
2. Plotting Production with Matplotlib
3. Decline Curve Analysis with Scipy
4. Volumetric Calculation in Python
5. Exam

## Python Course for Geoscience

1. Accesing Open Geoscience Data
2. Well-log Data Processing and Petrophysics
3. Exploring Seismic Data
4. Exam
",65,65,5,1,oil-and-gas,"[courses, geoscience, oil-and-gas, python, tutorials]",0.0
5,equinor,neqsim,equinor,https://github.com/equinor/neqsim,https://api.github.com/repos/neqsim/equinor,"NeqSim is a library for calculation of fluid behavior, phase equilibrium and process simulation","# NeqSim

[![Build Status](https://neqsim.visualstudio.com/neqsim_cicd/_apis/build/status/neqsim_build?branchName=master)](https://neqsim.visualstudio.com/neqsim_cicd/_build/latest?definitionId=1&branchName=master)
![Build maven](https://github.com/equinor/neqsim/workflows/Build%20maven/badge.svg?branch=master)
[![Known Vulnerabilities](https://snyk.io/test/github/equinor/neqsim/badge.svg)](https://snyk.io/test/github/equinor/neqsim)
[![codecov](https://codecov.io/gh/equinor/neqsim/branch/master/graph/badge.svg?token=IRnbAwRDtc)](https://codecov.io/gh/equinor/neqsim)

NeqSim is the main part of the [NeqSim project](https://equinor.github.io/neqsimhome/). NeqSim (Non-Equilibrium Simulator) is a Java library for estimating fluid properties and process design.
The basis for NeqSim is a library of fundamental mathematical models related to phase behavior and physical properties of fluids.  NeqSim is easilly extended with new models. NeqSim development was initiated at the [Norwegian University of Science and Technology (NTNU)](https://www.ntnu.edu/employees/even.solbraa).

## Releases

[NeqSim releases](https://github.com/equinor/neqsim/releases) are available as a packaged jar file and as source code. NeqSim can be used in a third party application by adding NeqSim jar to the classpath.

## Getting started as a NeqSim Java user

NeqSim can be used in a Java application by adding the neqsim-x.x.x.jar found in [NeqSim releases](https://github.com/equinor/neqsim/releases) to the classpath. A demonstration of downloading the library and running a TPflash  benchmark is illustrated in this [NeqSim Colab demo](https://colab.research.google.com/drive/1XkQ_CrVj2gLTtJvXhFQMWALzXii522CL). Learn and ask questions in [Discussions for use and development of NeqSim](https://github.com/equinor/neqsim/discussions). Also see the [NeqSim JavaDoc](https://htmlpreview.github.io/?https://github.com/equinor/neqsimhome/blob/master/javadoc/site/apidocs/index.html).

## Use of the NeqSim package
NeqSim can be set up as a dependency in a Java project via the [NeqSim GitHub package distribution](https://github.com/equinor/neqsim/packages/42822).

## Getting Started as a NeqSim Java developer

See the [NeqSim Java Wiki](https://github.com/equinor/neqsim/wiki) for how to use the NeqSim API.
NeqSim can be built using the Maven build system (https://maven.apache.org/). All NeqSim build dependencies are given in the pom.xml file. Learn and ask questions in [Discussions for use and development of NeqSim](https://github.com/equinor/neqsim/discussions).

### Initial setup

The NeqSim source code is downloaded by cloning the library to your local computer (alternatively fork it to your private reprository). The following commands are dependent on a local installation of [GIT](https://git-scm.com/) and [Maven](https://maven.apache.org/).

```bash
git clone https://github.com/equinor/neqsim.git
cd neqsim
./mvnw install
```
> **Note**
> The maven wrapper command is dependend on your OS, for Unix use: ```./mvnw```
> Windows:
> ```mvnw.cmd ```

An interactive demonstration of how to get started as a NeqSim developer is presented in this [NeqSim Colab demo](https://colab.research.google.com/drive/1JiszeCxfpcJZT2vejVWuNWGmd9SJdNC7).  

## Running the tests

The test files are written in JUnit5 and placed in the [test directory](https://github.com/equinor/neqsim/tree/master/src/test). Test code shuld be written for all new code added to the project, and all tests have to pass before merging into the master branch.  

Test coverage can be examined using [jacoco](https://www.eclemma.org/jacoco/) from maven.  
Generate a coverage report using `./mvnw jacoco:prepare-agent test install jacoco:report` and see results in target/site/jacoco/index.html.
> **Note**
> The maven wrapper command is dependend on your OS, for Unix use: ```./mvnw```
> Windows:
> ```mvnw.cmd ```


## Deployment

The NeqSim source code is compiled and distributed as a Java library. [NeqSim releases](https://github.com/equinor/neqsim/releases) are available for download from the release pages.

## Built With

[Maven](https://maven.apache.org/) - Dependency Management

## Contributing

See the [getting started as a NeqSim developer](https://github.com/equinor/neqsim/wiki/Getting-started-as-a-NeqSim-developer) documentation. Please read [CONTRIBUTING.md](CONTRIBUTING.md) for details on our code of conduct, and the process for submitting pull requests. An interactive demonstration of how to get started as a NeqSim developer is presented in this [NeqSim Colab demo](https://colab.research.google.com/drive/1JiszeCxfpcJZT2vejVWuNWGmd9SJdNC7).  

## Discussion forum

Questions related to neqsim can be posted in the [github discussion pages](https://github.com/equinor/neqsim/discussions).

## Versioning

NeqSim use [SemVer](https://semver.org/) for versioning.

## Authors and contact persons

Even Solbraa (esolbraa@gmail.com),  Marlene Louise Lund

## Licence

NeqSim is distributed under the [Apache-2.0](https://github.com/equinor/neqsim/blob/master/LICENSE) licence.

## Acknowledgments

A number of master and PhD students at NTNU have contributed to development of NeqSim. We greatly acknowledge their contributions.

## NeqSim modules

NeqSim is built upon six base modules:

1. Thermodynamic Routines
2. Physical Properties Routines
3. Fluid Mechanic Routines
4. Unit Operations
5. Chemical Reactions Routines
6. Parameter Fitting Routines
7. Process simulation routines

## File System

>neqsim/: main library with all modules
>
>neqsim/thermo/: Main path for thermodynamic routines
>neqsim/thermo/util/examples/: examples of use of Thermodynamic Models and Routines
>
>neqsim/thermodynamicOperation: Main path for flash routines (TPflash, phase envelopes, etc.)
>neqsim/thermodynamicOperation/util/example/: examples of use of thermodynamic operations (eg. flash calculations etc.)
>
>neqsim/physicalProperties: Main path for Physical Property methods
>neqsim/physicalProperties/util/examples/: Examples of use of physical properties calculations
>
>neqsim/physicalProperties: Main path for Physical Property methods
>neqsim/physicalProperties/util/examples/: Examples of use of physical properties calculations
>
>neqsim/processSimulation: Main path for Process Simulation Calculations
>neqsim/processSimulation/util/examples/: Examples of use of Process Simulation calculations
>
>changelog.txt : History of what changed between each version.
>license.txt: license document

## Toolboxes

See [NeqSim homepage](https://equinor.github.io/neqsimhome/). NeqSim toolboxes are avalable via GitHub for alternative programming languages.

* [Matlab](https://github.com/equinor/neqsimmatlab)
* [Python](https://github.com/equinor/neqsimpython)
* [.NET (C#)](https://github.com/equinor/neqsimcapeopen)

## Related open source projects

[NeqSim Python/Colab](https://github.com/EvenSol/NeqSim-Colab)
",46,46,7,48,oil-and-gas,"[equation-of-state, fluid-properties, gas-production, gas-transport, java, oil-and-gas, phase-equilibrium, physical-properties, process-simulation, processing, pvt, thermodynamics]",0.0
6,equinor,neqsimpython,equinor,https://github.com/equinor/neqsimpython,https://api.github.com/repos/neqsimpython/equinor,"NeqSim is a library for calculation of fluid behavior, phase equilibrium and process simulation. This project is a Python interface to NeqSim.","# NeqSim Python

NeqSim Python is part of the [NeqSim project](https://equinor.github.io/neqsimhome/). NeqSim Python is a Python interface to the [NeqSim Java library](https://github.com/equinor/neqsim) for estimation of fluid behavior and process design for oil and gas production. NeqSim Python toolboxes (eg. [thermoTools](https://github.com/equinor/neqsimpython/blob/master/neqsim/thermo/thermoTools.py) and [processTools](https://github.com/equinor/neqsimpython/blob/master/neqsim/process/processTools.py)) are implemented to streamline use of neqsim in Python. Examples of use are given in jupyter workbooks.

## Releases

NeqSim Python is distributed as a pip package.  

End-users should install neqsim python with some additional packages by running
```
pip install neqsim[interactive]
```

## Getting Started

See the [NeqSim Python Wiki](https://github.com/equinor/neqsimpython/wiki) for how to use NeqSim Python via Python or in Jupyter notebooks. Also see [examples of use of NeqSim for Gas Processing in Colab](https://colab.research.google.com/github/EvenSol/NeqSim-Colab/blob/master/notebooks/examples_of_NeqSim_in_Colab.ipynb#scrollTo=kHt6u-utpvYf). Learn and ask questions in [Discussions for use and development of NeqSim](https://github.com/equinor/neqsim/discussions).

### Prerequisites

Java version 11 or higher ([Java JDK](https://adoptium.net/)) needs to be installed. The Python package [JPype](https://github.com/jpype-project/jpype) is used to connect Python and Java. Read the [installation requirements for Jpype](https://jpype.readthedocs.io/en/latest/install.html). Be aware that mixing 64 bit Python with 32 bit Java and vice versa crashes on import of the jpype module. The needed Python packages are listed in the [NeqSim Python dependencies page](https://github.com/equinor/neqsimpython/network/dependencies).


## Contributing

Please read [CONTRIBUTING.md](CONTRIBUTING.md) for details on our code of conduct, and the process for submitting pull requests.


## Discussion forum

Questions related to neqsim can be posted in the [github discussion pages](https://github.com/equinor/neqsim/discussions).

## Versioning

NeqSim use [SemVer](https://semver.org/) for versioning.

## Licence

NeqSim is distributed under the [Apache-2.0](https://github.com/equinor/neqsimsource/blob/master/LICENSE) licence.

## Acknowledgments

A number of master and PhD students at NTNU have contributed to development of NeqSim. We greatly acknowledge their contributions.
",35,35,10,1,oil-and-gas,"[equation-of-state, fluid-dynamics, fluid-properties, gas-production, gas-transport, oil-and-gas, oil-production, process-simulation, pvt, python, thermodynamic-properties, thermodynamics]",0.0
7,beatrixparis,connectivity-modeling-system,,https://github.com/beatrixparis/connectivity-modeling-system,https://api.github.com/repos/connectivity-modeling-system/beatrixparis,"The CMS is a multiscale stochastic Lagrangian framework developed by Paris' Lab at the Rosenstiel School of Marine, Atmospheric & Earth Science to study complex behaviors, giving probabilistic estimates of dispersion, connectivity, fate of pollutants, and other Lagrangian phenomena. This repository facilitates community contributions to CMS modules","# connectivity-modeling-system (CMS), for more information please read and cite Paris et al. 2013EMS

  The CMS is a multiscale stochastic Lagrangian framework developed by Professor Claire B. Paris at the Paris' Lab of the University of Miami's Rosenstiel School of Marine, Atmospheric, and Earth Science to trace complex planktonic migrations and give probabilistic estimates of dispersion and migration, marine population and oceanographic connectivity, transport and fate of oil and gas, plastics, and other pollutants, and other Lagrangian phenomena. This repository FACILITATES & ENCOURAGES COMMUNITY CONTRIBUTIONS TO CMS MODULES.

The CMS is an open-source Fortran toolbox for the multi-scale tracking of biotic and abiotic particles in the ocean. The tool is inherently multiscale, allowing for the seamless moving of particles between grids at different resolutions.

The CMS has been used on velocity fields from OFES, HYCOM, NEMO, MITgcm, UVic, ECCO2, SOSE, MOM and many other ocean models to computation dispersion, connectivity and fate in applications including large scale oceanography, marine reserve planning, and movement of marine biota all across the world.

The CMS uses RK4 timestepping and tricubic interpolation and is designed to be modular, meaning that it is relatively easy to add additional `behaviours' on the particles. Modules distributed with the code include random walk diffusion, mortality, diel migration, and mixed layer mixing. 

CMS Google FORUM
https://groups.google.com/forum/#!forum/connectivity-modeling-system-club
",26,26,13,20,oil-and-gas,"[connectivity, deepsea, deepsea-spill, dispersion, lagrangian, larvae, migration, oil-and-gas, orientation, plastic, pollutants, transport]",0.0
8,bbcho,risktools-dev,,https://github.com/bbcho/risktools-dev,https://api.github.com/repos/risktools-dev/bbcho,Risk tools for commodities trading and finance,"# risktools

Python implementation of the R package RTL.  

See CRAN for original R version

https://cran.r-project.org/web/packages/RTL/index.html

## Purpose

    Purposely designed functions for trading, trading analytics and risk practitioners in Commodities and Finance.
    Build to support delivery of Finance classes from one of the co-authors of RTL at the Alberta School of Business.

## Version Notes

    Note that the latest version will require compilation on Windows for python version 3.11 due to the 
    dependency on arch. Arch does not come with binaries and must be compiled on Windows which can be
    avoided by installing numba, however numba is not yet available for python 3.11.

## Features

    Historical forward curves charting.

    Calendars and expiry dates data objects for a wide range of commodity futures contracts.

    roll_adjust to adjust continuous contracts returns for roll adjustments using expiries above.

    Morningstar Marketplace API functions getPrice(), getPrices() and getCurve() using your own Morningstar credentials. Current feeds included:
        ICE_EuroFutures
        ICE_EuroFutures_continuous
        CME_NymexFutures_EOD
        CME_NymexFutures_EOD_continuous
        CME_NymexOptions_EOD
        CME_CbotFuturesEOD
        CME_CbotFuturesEOD_continuous
        CME_Comex_FuturesSettlement_EOD
        CME_Comex_FuturesSettlement_EOD_continuous
        LME_AskBidPrices_Delayed
        SHFE_FuturesSettlement_RT
        CME_CmeFutures_EOD
        CME_CmeFutures_EOD_continuous
        CME_STLCPC_Futures
        CFTC_CommitmentsOfTradersCombined
        ICE_NybotCoffeeSugarCocoaFutures
        ICE_NybotCoffeeSugarCocoaFutures_continuous
        Morningstar_FX_Forwards
        ERCOT_LmpsByResourceNodeAndElectricalBus
        PJM_Rt_Hourly_Lmp
        AESO_ForecastAndActualPoolPrice
        LME_MonthlyDelayed_Derived
        … see ?getPrice for up to date selection and examples.

    chart_zscore() supports seasonality adjusted analysis of residuals, particularly useful when dealing with commodity stocks and/or days demand time series with trends as well as non-constant variance across seasonal periods.

    chart_eia_steo() and chart_eia_sd() return either a chart or dataframe of supply demand balances from the EIA.

    chart_spreads() to generate specific contract spreads across years e.g. ULSD March/April. Requires Morningstar credentials.

    swapInfo() returns all information required to price first line futures contract averaging swap or CMA physical trade, including a current month instrument with prior settlements.

## Data Sets

Accessible via risktools.data.open_data(datsetname). Also use risktools.data.get_names() to get list of available data.

    expiry_table: Historical and forward futures contract metadata.
    holidaysOil: Holiday calendars for ICE and NYMEX.
    tickers_eia: Mapping of EIA tickers to crude and refined products markets for building supply demand balances.
    usSwapIRDef: Data frame of definitions for instruments to build a curve for use with RQuantlib. Use getIRswapCurve() to extract the latest data from FRED and Morningstar.
    usSwapIR: Sample data set output of getIRswapCurve.
    usSwapCurves: Sample data set output of RQuantlib::DiscountCurve().
    cancrudeassays contains historical Canadian crude assays by batch from Crudemonitor. cancrudeassayssum is a summarised average assays version.
    crudeassaysXOM for all publicly available complete assays in Excel format from ExxonMobil
    crudeassaysBP for all publicly available complete assays in Excel format from BP
    eiaStocks: Sample data set of EIA.gov stocks for key commodiities.
    eiaStorageCap: EIA crude storage capacity by PADD.
    dflong and dfwide contain continuous futures prices sample data sets for Nymex (CL, HO, RB and NG contracts) and ICE Brent.
    crudepipelines and refineries contain GIS information in the North American crude space.
    ...

Usernames and password for API services are required.

## Changelog

### Version 0.2.0

New functions added:
- get_curves
- get_gis
- get_ir_swap_curve
- refineryLP
- swap_fut_weight
- swap_info
- dist_desc_plot

New feeds for get_prices:
- ERCOT_LmpsByResourceNodeAndElectricalBus
- PJM_Rt_Hourly_Lmp
- AESO_ForecastAndActualPoolPrice
- LME_MonthlyDelayed_Derived
- SHFE_FuturesSettlement_RT
- CFTC_CommitmentsOfTradersCombined

New datasets for risktools.data.open_data function
- Replicated/refreshed RTL datsets

removed geojson files and dependencies to geopandas - replaced with get_gis function
removed support for Python 3.6. Supports >= 3.7

Companion package to https://github.com/bbcho/finoptions-dev

",20,20,5,1,oil-and-gas,"[cme, commodities, crude, eia, finance, morningstar, nymex, oil, oil-and-gas, python, risk, risk-management, swaps]",0.0
9,Philliec459,NEW-Carbonate-Characterization-Workflow-Jupiter-Notebook-Modules-with-Clerke-Arab-D-Calibration-Data,,https://github.com/Philliec459/NEW-Carbonate-Characterization-Workflow-Jupiter-Notebook-Modules-with-Clerke-Arab-D-Calibration-Data,https://api.github.com/repos/NEW-Carbonate-Characterization-Workflow-Jupiter-Notebook-Modules-with-Clerke-Arab-D-Calibration-Data/Philliec459,Carbonate Reservoir Characterization workflow using Clerke’s carbonate Arab D Rosetta Stone calibration data to provide for a full pore system characterization with modeled saturations using Thomeer Capillary Pressure parameters for an Arab D complex carbonate reservoir,"# Carbonate Reservoir Characterization workflow using Clerke’s carbonate Arab D Rosetta Stone calibration data to provide for a full pore system characterization with modeled saturations using Thomeer Capillary Pressure parameters for an Arab D complex carbonate reservoir.
## New Scipy Optimization for Lithology including Illite Clay. This also has the FWL Search routine too.
### INSTALL NOTE: Please delete any previous installations and re-install this repositiry in full and keep the intire project together in one subirectory since there are relative paths.

This repository contains 4 python Jupyter Notebooks to use as help files for our Geolog Carbonate Reservoir Characterization Workflow to demonstrate a tried and proven workflow employing the techniques as described by Phillips et al. (1) used in the characterization of most Arab D reservoirs in Saudi Arabia. Permeability, Petrophysical Rock Types (PRT), Capillary Pressure and modeled saturations are all estimated or calculated within this workflow in order to characterize this complex carbonate reservoirs, and Clerke’s (2) Arab D Rosetta Stone core analysis database is used as the calibration data. 

>![Geolog_Image](Results.png)

The first Jupyter Notebook is called:

        1_Workflow_Intro_with_Altair_Interrogation_Log_and_RosettaStone_Data.ipynb

and it demonstrates how we interrogate our well log data as well as our Rosetta Stone calibration data using python's Altair. 

The second notebook is called: 

        2_Chartbook_Porosity_Optimized-Lithology_Perm_Thomeer_Saturations_ver3-Lasio_with_FWL_search-implement_Optimization-Illite.ipynb
        
and this notebook reads in the well log data from a .las file using Lasio and then performs all of the workflow steps 2 through 5 as discussed below. 

The third notebook called:

        6_PRT Classification_carbonate_AMMY.ipynb

and this notebook provides a secondary technique to estimate Petrophysical Rock Types (PRTs) employing python’s Sklearn as published by Hall(3). We could have estimated Depositions of Environment or other types of categoric geologic facies too used in this Sklearn prediction process. 

The fourth notebook is used to process your High Pressure Mercury Injection (HPMI) Capillary Pressure data and fit the Thomeer hyperbola to these data to determine the Thomeer Capillary Pressure parameters. You would use this program to create your own Thomeer Parameter database and modify this workflow to include your Thomeer Parameer database for your reservoir.

        Thomeer_from_Pc-curvefit.ipynb

The image below shows how this interactive process works:

>![HPMI_Image](Thomeer_Parameter_fitting.gif)

This program uses Scipy Optimize Curve_fit to estimate the appropriate Thomeer parameters necessary to model the HPMI data. The points selected from the GUIs are used to estimate boundary conditions for these estimations, and the estimations for this example are shown below:

            Thomeer Parameters Estimated from Imported HPMI Data:
            Pd1 = 8.67  ,  G1 = 0.54 , BV1 = 10.13
            Pd2 = 389.1 ,  G2 = 0.24 , BV2 = 4.8



We also have a complete Geolog project in a GitHub repository with Geolog python loglan at the following link:

        https://github.com/Philliec459/Geolog-Used-to-Automate-the-Characterization-Workflow-using-Clerkes-Rosetta-Stone-calibration-data

This repository has one well to use with our workflow to interrogate and characterize this typical Arab D carbonate reservoir. This example will serve as the basis for a full-field reservoir characterization for all wells throughout the entire field. In this example we are showing the results for just one well, but in the full-field reservoir characterization we would follow the same workflow and apply the same process to all wells in the field. The final objective would be to use these results to create a 3D static model of porosity, permeability, Petrophysical Rock Types (PRT), capillary pressure parameters and saturations. Typically, this static model would then be used to initialize the dynamic model for reservoir simulation. 

We are using Ed Clerke’s Rosetta Stone, Arab-D carbonate dataset from Ghawar field in Saudi Arabia as the calibration data. This is a very special carbonate dataset. Clerke randomly selected the final calibration samples from 1,000’s of initial core plugs for the final dataset.  The Rosetta Stone data cover the full range in poro-perm space and Petrophysical Rock Types (PRTs) observed in the Arab D reservoir. For each sample Clerke acquired High Pressure Mercury Injection (HPMI) data, and then fit the HPMI capillary pressure curve using the Thomeer hyperbola (see Altair Plot of Capillary Pressure curves) created from the Initial Displacement Pressure (Pdi), Pc curvature term Gi that relates to the variability of pore throats and Bulk Volume Occupied (BVocci) that is related to the Pore Volume for each pore system 'i'.  From these results (primarily from Pd) Clerke defined his Petrophysical Rock Types (PRT). For this Arab D reservoir, most PRTs have a dual-porosity system, and some PRTs have up to 3 pore systems. 

# Suggested Arab D Carbonate Workflow:
The following workflow and processing is suggested to interrogate, process, interpret and model the petrophysical properties of a typical Arab D carbonate reservoir using Clerke’s Arab D Rosetta Stone Carbonate database as calibration. The workflow consists of the following steps:

1) Interrogate the Well Log data and Rosetta Stone calibration data using standard Geolog layouts, cross plots and histograms and then use a python loglan featuring Altair, which is interactive software driven from a Geolog Module Launcher.


### Altair used to Interrogate the Rosetta Stone Thomeer Capillary Pressure curves and Petrophysical Rock Types (PRTs):
>![Geolog_Image](Thomeer_Pc_and_Thomeer_Parameters2.gif)

### Altair Used to Interrogate the Well log data in Geolog:
>![Geolog_Image](Geolog20_ArabD.gif)

### Petrophysical Rock Typing (PRT):
Most macro rock typically has a dual porosity system where the Pore Throat Distribution (PTD) will have two modes as shown below. 

>![Mode.png](Mode.png)

The macro portion of the rock will have a mode greater than 2 microns with a second (or third) mode less than 2 microns. Probably the most abundant PRT is the M_1. This is a macro-porous rock with a mode in the macro portion of the PTD and a second mode in the meso-porosity range. In this PRT both the macro pores and meso-porous grains can have oil saturations once the capillary pressure is great enough to drive out the water. The M_2 PRT is also a macro rock, but the second pore system is micro-porous and is too tight to have hydrocarbon saturations. The Table below shows Clerke's description of his PRT's. 

The following are some example results using Altair where the data in cross plots can be selected and then the appropriate data for those selected samples are shown in the bar charts below the cross plots. 

### Altair used to Compare Clerke's Petrophysical Rock Types (PRTs) to Winland r35 and Amaefule FZI and RQI:
>![Geolog_Image](rock_typing_hist.gif)

One of the benefits of working with Thomeer parameters is that the exact mode of the PTD (radius) can be calculated for each sample using the Buiting Mode equation as shown below:

        Mode(microns) = (exp(-1.15 * G) * (214/Pd))/2
        
Again, this equation gives us the mode of the pore system, and we normally only calculate the mode of the largest pore system in the sample. 

In the Altair plots below the first row of poro-perm xplot show the PRT and actual Mode of Pore Throats on the Z color axis. The second row of xplots show the Winland r35 on the color axis, Amaefule FZI and RQI on the color axis.

The actual mode of the Pore Throat Distribution is shown in the first row of the poro-perm xplots. In Rock Typing Winlands' r35 and Amaefule's FZI or RQI attempt to predict the most dominant pore throat. In dual-porosity carbonates Winland's r35 is usually a close proximity, but FZI does not do well. RQI appears to be off, but captures the range of the pore throats a bit better. 

Windland's r35 is calculated:

        r35(microns) = 10**(0.732 + 0.588*log(Permeability) - 0.864*log(Porosity*100))


Amaefule-Kersey FZI is calculated:

        RQI(microns) = 0.0314 * sqrt(Permeability/Porosity)
        
        Phiz = Porosity / (1 - Porosity)

        FZI(microns) = RQI / Phiz
    
where Porosity is a volume fraction and not percent. 

On this dataset it appears that r35 has the best match to the actual mode of the Pore Throat Distribution. The bi-modality in the macro-porous rock has FZI falling between the two modes since it is the mean pore throat radius. Windland's r35 does a fairly good job of estimating the most dominant pore throat. 

2) Run MultiMin for a solid log analysis model using the typical minerals found in the Arab D reservoir; Limestone, Dolomite, Anhydrite and Illite. With MultiMin we always use environmentally corrected log data and use the calculated uncertainties for each log curve employed in the analysis. 

To serve as an example we have included this processing in our second Jupyter Notebook. The notebook first uses digitized chartbook data as the basis for our kNN Porosity (PHIT) and Rho Matrix density calculations used in the analysis. Once PHIT is estimated, then we then use Scipy Optimize (minimize) to estimate our carbonate lithology. 

Our primary function is the following in Scipy trying to estimate volumes of Calcite and Dolomite.

        fun = lambda x: (RHOB2 - (2.52*vol_illite + 2.71*x[0]+2.847*x[1]+PHIT*FD)) + (TNPH - (0.247*vol_illite + 0*x[0]+0.005*x[1]+PHIT*1))

Which is using two log response functions that are to be minimized:

        RHOB_theoretical =  2.52*vol_illite + 2.71*x[0]+2.847*x[1]+PHIT*FD

        TNPH_theoretical =  0.247*vol_illite + 0*x[0]+0.005*x[1]+PHIT*1

where res.x[0] = VOL_CALCITE and res.x[1] = VOL_DOLO. The objective is to minimize the difference between RHOB - RHOB_theoretical and TNPH - TNPH_theoretical while solving for our lithology. We have calculated that error from normalized curves, and this error is plotted on the optimized lithology plot in the last track. 

We would like to thank Andy McDonald and his Petrophysics Python Series GitHub repository for his examples and ideas. We used his methods to use Lasio to read in the las file and are using his hatch fill example in our notebook depth plots. Thank you Andy. 

This is all still considered work in progress, but please find below an example of the output. 

>![Geolog_Image](optimized_lith.png)

3) Use available core data from the representative reservoir/field to build a petrophysical model to estimate permeability for all wells in field using our python loglan of kNN using normalized input data and weighted by Euclidean distances for each of the nearest neighbors. 

The next step is to estimate Petrophysical Rock Types (PRT) as defined by Clerke and Thomeer Capillary Pressure parameters. The following example shows the actualy Thomeer parameters for a narrow range of selected porosity and permeability samples from the cross plot. The narrow range of selected samples in poro-perm space shows just how similar the Thomeer parameters are for the selected samples. Step 4 below using kNN will actually improve on this estimation.

4) Using the kNN estimated permeability from step 3 with calculated Total Porosity (PHIT) from MultiMin to query Clerke’s Rosetta Stone core database to predict the following Petrophysical data using kNN:
    - Petrophysical Rock Types (PRT) as defined by Clerke (M_1 Macro/Meso, M_2 Macro/Micro, M_1_2 Macro/Meso/Micro, Type1 Meso, Type 1_1 Meso/Micro and Type 2 Micro PRTs.
    - Thomeer Capillary Pressure parameters (Pdi, Gi and BVocci) for each pore system i over the reservoir interval

>![Geolog_Image](knn_results.png)

5) Use the Thomeer parameters from step 4 to model Capillary Pressure saturations based on reservoir Capillary Pressure (buoyancy) due height above the Free Water Level (FWL) and the fluid density differences in the reservoir. In this instance we compare the Bulk Volume Oil (BVO) from our log analysis vs. BVO from Thomeer-based capillary pressure saturations. BVO is pore volume weighted.

>![Geolog_Image](workflow_examples.gif)

#### Free Water Level Search:
We have provided a FWL Search technique in python too to estimate the FWL elevation (TVDss) in each key well to be used to create a FWL plane for the field. To model Capillary Pressure saturations, it is essential to have a proper Free Water Level (FWL). Reservoir Capillary Pressure or buoyancy is dependent upon the height above the FWL. 

On new discoveries the FWL is usually determined from Formation Test data plotting the pressure data vs. TVDss to find the intersection of the water gradient vs. hydrocarbon gradient. The elevation of this intersection is the FWL or zero Capillary Pressure. However, on older fields, this type of data is typically not available prior to pressure depletion and/or fluid contact movements in the field. Therefore, we need another way to estimate the FWL for the field. In the python software used in our Notebook we offer a FWL search technique that has been shown to work very well in numerous fields.

We perform this well-by-well FWL search by varying the FWL elevation from an estimated highest FWL to the lowest expected FWL (spill point...) for the reservoir.  We then calculate the error difference between the Bulk Volume Oil (BVO) from logs vs. BVO from Thomeer Capillary Pressure at each new FWL estimate for that well. The final fwl_est is the FWL estimated with the lowest Bulk Volume Oil (BVO) error for that well. This fwl_est is then used in our final Thomeer BVO Oil calculations.

The FWL search is usually run on all wells with a fwl_est for each well. In many instances in fields with large hydrocarbon columns, the wells near the crest will be too high above the FWL to give valid results. We have found that the wells near the edge usually give the best estimation.  However, those wells affected by water encroachment will also not give valid results. In the end it is usually a small percentage of wells near the edge of the field that will give valid FWL estimates that are consistent. The search results from these wells are then typically used to construct a plane in the 3D fine-grid model to represent the FWL for the field.

It should be noted, that not all FWL surfaces are flat. Structural tilting, subduction, and dynamic aquifers... can result in a tilted FWL elevations with the possibility of residual oil below the FWL, depending in the situation. 

>![Geolog_Image](FWLSearch.png)

6) As a secondary technique to estimate PRTs, we also implemented another applications in Geolog employing python’s Sklearn as published by Hall(3). We could have estimated Depositions of Environment or other types of categoric geologic facies too used in this Sklearn prediction process. 


### RESOURCES:
https://www.pdgm.com/products/geolog/

https://github.com/Philliec459?tab=repositories


1.	Phillips, E. C., Buiting, J. M., Clerke, E. A, “Full Pore System Petrophysical Characterization Technology for Complex Carbonate Reservoirs – Results from Saudi Arabia”, AAPG, 2009 Extended Abstract.
2.	Clerke, E. A., Mueller III, H. W., Phillips, E. C., Eyvazzadeh, R. Y., Jones, D. H., Ramamoorthy, R., Srivastava, A., (2008) “Application of Thomeer Hyperbolas to decode the pore systems, facies and reservoir properties of the Upper Jurassic Arab D Limestone, Ghawar field, Saudi Arabia: A Rosetta Stone approach”, GeoArabia, Vol. 13, No. 4, p. 113-160, October, 2008. 
3.	Hall, Brendon, “Facies classification using Machine Learning”, The Leading Edge, 2016, Volume 35, Issue 10
 
",17,17,4,0,oil-and-gas,"[geology, logging, oil-and-gas, petroleum-engineering, petrophysics]",0.0
10,Philliec459,Estimate-Core-Permeability-from-NMR-data-using-either-Map-Inversion-or-kNN,,https://github.com/Philliec459/Estimate-Core-Permeability-from-NMR-data-using-either-Map-Inversion-or-kNN,https://api.github.com/repos/Estimate-Core-Permeability-from-NMR-data-using-either-Map-Inversion-or-kNN/Philliec459,Estimate Core-based Permeability from NMR well log data,"# Estimate-Core-Permeability-from-NMR-data
Estimate Core-based Permeability from NMR well log data

## Objective:
The objective of this project is to use either the Map Inversion (inverse distance**4) or kNN to estimate core-based Permeability from NMR data. The following Cross Plot is made from the NMR Effective Porosity (CMRP_3MS, x-axis) vs. the NMR Free Fluid (CMFF, y-axis).  On the z-axis there are the available core Permeability measurements shown as colored dots. This method uses the distribution of these core permeability measurements in our process to estimate core calibrated Permeability.  The NMR data in this instance is being used as a road map in making our permeability estimation.

![TS_Image](NMR.png)

## Map Inversion:
The Map Inversion program used in this instance is inv_dist_perm.py. In this program we read in all of the NMR data from a particular well, read in the reference core data with associated NMR values and then estimate core-based Permeability from the NMR Effective Porosity and Free Fluid over the entire well.  

## kNN:
The kNN program used in this instance is inv_dist_perm_linux_kNN2.py. Again, we read in all of the NMR data from our well, read in the reference core data with associated NMR values and then estimate core-based Permeability from the NMR Effective Porosity and Free Fluid over the entire well.  

The blue curve below shows the estimated permeability for our subject well and the red dots represent the core permeability measurements. 

![Summary_Image](PermEstimate.png) 
",15,15,3,0,oil-and-gas,"[geoscience, oil-and-gas, permeability, petroleum-engineering, petrophysics, reservoir-characterization, reservoir-modeling]",0.0
11,bpamos,welltrajconvert,,https://github.com/bpamos/welltrajconvert,https://api.github.com/repos/welltrajconvert/bpamos,Calculate directional survey metadata points along the wellbore.,"# welltrajconvert

The `welltrajconvert` is a Python package that allows a user to take the bare minimum required
wellbore survey information and return it's directional survey points including its latitude, longitude, TVD, XY offset,
and UTM points.

`welltrajconvert` requires only the wellId, measured depth, inclination angle, azimuth degrees, surface latitude,
and surface longitude points or surface UTM XY and associated CRS to calculate its survey points using a minimum curvature algorithm.

`welltrajconvert` calculates the following points along the wellbore: latitude_points, longitude_points, x_points, y_points, surface_x, surface_y, dogleg severity, tvd, e_w_deviaiton, n_s_deviation, zone_number, zone_letter, and isHorizontal, a categorical array defining if the well is horizontal or not.


The package can take a variety of data inputs ranging from csv, df, and json.


see https://welltrajconvert.readthedocs.io/en/latest/ for complete documentation.



# Installation

Prerequisites: Python 3.7 or later.

It is recommended to install the most recent **stable** release of welltrajconvert from PyPI.


    $ pip install welltrajconvert


Alternatively, you could install from source code. This will give you the **latest**, but unstable, version of welltrajconvert.


    $ git clone https://github.com/bpamos/welltrajconvert.git
    $ cd welltrajconvert/
    $ pip install ./

	
## Overview


---


1. Create (and activate) a new environment, named `welltrajconvert` with Python 3.7. If prompted to proceed with the install `(Proceed [y]/n)` type y.

	- __Linux__ or __Mac__: 
	```
	conda create -n welltrajconvert python=3.7
	source activate welltrajconvert
	```
	- __Windows__: 
	```
	conda create --name welltrajconvert python=3.7
	activate welltrajconvert
	```
	
	At this point your command line should look something like: `(welltrajconvert) <User>:welltrajconvert <user>$`. The `(welltrajconvert)` indicates that your environment has been activated, and you can proceed with further package installations.
",14,14,1,0,oil-and-gas,"[crs-transform, directional-survey, drilling, latitude, longitude-points, minimum-curvature-algorithm, oil-and-gas, trajectory, wellbore]",0.0
12,Philliec459,Simple-Neural-Network-to-Estimate-Carbonate-Rock-Types,,https://github.com/Philliec459/Simple-Neural-Network-to-Estimate-Carbonate-Rock-Types,https://api.github.com/repos/Simple-Neural-Network-to-Estimate-Carbonate-Rock-Types/Philliec459,"We have developed a simple, 1 layer neural network to estimate Rock Types","# Simple-Neural-Network-to-Estimate-Rock-Types-Rock-Types
We have developed a simple, 1 layer neural network to estimate Macro, Meso and Micro Rock Types
### Introduction
#### Predict Macro, Meso and Micro Rock Types (RT)
The objective of this project is to estimate Macro, Meso and Micro Rock Types (RTs). In this repository we are using a simple this single layer neural network to predict our RT. Therefore, we modified the Petrophysical Rock Types (PRT) as defined by Clerke(1) for the Arab-D carbonate reservoir. The objective is to use just Porosity and Permeability to estimate our Macro, Meso or Micro RT.

The Arab D data set published by Clerke is quite distinctive. Clerke acquired nearly 450 High Pressure Mercury Injection Capillary Pressure (HPMI) measurements in the Arab D reservoir; however, Clerke's final samples were randomly selected from 1,000's of pre-qualified core samples ensuring a broad distribution and representation of all Petrophysical properties. This created one of the best Core Analysis datasets every collected in our industry. 

Clerke began evaluating this dataset by fitting a Thomeer hyperbolas for each pore system in each sample to generate the published Thomeer Capillary Pressure parameters. From these data Clerke established his Petrophysical Rock Types (PRT) based on the Initial Displacement Pressures for each pore system and the number of pore systems present in each sample. From the figure below it is rather evident that Clerke's PRTs are Petrophysically well-defined in poro-perm space where each color represents a different PRT.  The Capillary Pressure curves and Pore Throat Distributions (PTD) shown on the right hand side of the figure illustrate the unique characteristics of each PRT. 

![TS_Image](PRT.png)

###### The characterization of Clerke's PRTs are shown below:

![TS_Image](Rock-Types.png)

As can be seen in the first figure above, the PRTs are rather well segregated in the Porosity vs. Permeability Cross Plot as they fall in distinct regions or clusters on the Cross Plot.For modeling purposes it is important to take advantage of the excellent correlations between the PRTs. 

The first part of this notebook develops our single layer neural network as inspired by the video series from giant_neural_network on YouTube. 

https://www.youtube.com/watch?v=LSr96IZQknc

We have used Clerke's Rosetta Stone data and his PRTs as our training set, except that we combined all the macros PRTs into one RT that had a value of 2. We combined all the Type 1 Meso PRT into a RT with a value of 1 and all the Micro PRT compose our third RT with a value of 0. The following is the Sigmoid s-curve that we are using

![TS_Image](sigmoid.png)

We have expanded our Sigmoid curve for values from 0 to 2 to accomodate our 3 RTs at 0, 1 and 2.
 
        def sigmoid(x):
            return 2/(1 + np.exp(-x))

We first calculate a variable z which is a function if weights and bias:
        
        z = Porosity * weight1 + permeability * weight2 + bias

Our prediction (pred) is then a function of sigmoid:        
        
        pred = sigmoid(z) 

where sigmoid is defined by:
        
        def sigmoid(x):
            return 2/(1 + np.exp(-x))

The term z is actually the x-axis on the above sigmoid curve, and y is a function of sigmoid(z). 

For training we make about 1,000 iterations to optimize on the weights (w1 and w2) and bias for our single layer. These weights can be saved and then used in future projects to estimate RT from the user's input of Porosity and log10 of Permeability. We can calculates the most probable Rock Type and even provide Capillary Pressure curves for each Rock Type.

The following is the training loop. 

     # Start the training loop
     learning_rate = 0.1
     costs = []
     cost_all = []

     w1 = np.random.randn()
     w2 = np.random.randn()
     b  = np.random.randn()

     # Save weights from previous run. These can be loaded to continue from there
     #w1 = 21.5
     #w2 = 10.5
     #b  = -77

     for i in range(1000):
         ri = np.random.randint(len(data))
         point = data[ri]

         z = point[0]*w1 + point[1]*w2 + b
         pred = sigmoid(z)

         target = point[2]
         cost = np.square(pred - target)

         cost_all.append(cost)

         ###costs.append(cost)

         #derivatives
         dcost_pred = 2 * (pred - target)
         dpred_dz   = sigmoid_p(z)
         dz_dw1 = point[0]
         dz_dw2 = point[1]
         dz_db  = 1

         dcost_dw1 = dcost_pred * dpred_dz * dz_dw1
         dcost_dw2 = dcost_pred * dpred_dz * dz_dw2
         dcost_db  = dcost_pred * dpred_dz * dz_db

         w1 = w1 - learning_rate * dcost_dw1
         w2 = w2 - learning_rate * dcost_dw2
         b  = b  - learning_rate * dcost_db

         if i % 100==0:
             cost_sum = 0
             for j in range(len(data)):
                 point = data[j]

                 z = point[0]*w1 + point[1]*w2 + b
                 pred = sigmoid(z)

                 target=point[2]
                 cost_sum += np.square(pred - target)

             costs.append(cost_sum/len(data))
             #costs.append(cost_sum)


To make a prediction we would use code similar to the following where we first compute z based on the product of our inputs and weights:

     for i in range(len(data)):
         point = data[i]
         z = point[0] * w1 + point[1]* w2 + b
         pred = sigmoid(z)
         print(""PRT:"", point[2],"","", ""pred: {}"".format(pred))

and then we predict where we are in the sigmoid curve scaled from 0 to 2 (pred = sigmoid(z).  

The final RT are then defined as shown below:
        
        if pred > 1.56:
            RT=2
        elif pred < 0.25:
            RT=0
        else:
            RT=1
            
![TS_Image](pred.png)

For nearly 300 samples, we have accurately predicted the correct RT for all by 6-7 samples using the RT cutoffs shown above. The cutoffs can be varied a bit to improve upon our RT estimations. As they say, simplicity is the best design. 


1 Clerke, E. A., Mueller III, H. W., Phillips, E. C., Eyvazzadeh, R. Y., Jones, D. H., Ramamoorthy, R., Srivastava, A., (2008) “Application of Thomeer Hyperbolas to decode the pore systems, facies and reservoir properties of the Upper Jurassic Arab D Limestone, Ghawar field, Saudi Arabia: A Rosetta Stone approach”, GeoArabia, Vol. 13, No. 4, p. 113-160, October, 2008. 

",14,14,2,0,oil-and-gas,"[geoscience, oil-and-gas, petrophysical-properties, petrophysical-rock-types, petrophysics]",0.0
13,FracThePermian,Reservoir-Simulation-Part-B,,https://github.com/FracThePermian/Reservoir-Simulation-Part-B,https://api.github.com/repos/Reservoir-Simulation-Part-B/FracThePermian,3D Graphical Output of well producers and injectors in oil reservoir.,"# Reservoir-Simulation-Part-B

Compares analytical/numerical results for the drainage of multiple wells w/n the dimensions that you define (drainage area and depth). The Nechelik (.dat) files provide porosity and permeability for each unit block of reservoir. Modify the following variables to your liking.

Control the placement of wells and whether they are injecting and/or producing.

It is written in Matlab, is self-contained, and no external dependencys. 

### Assumptions
  * Incompressible Fluid
  * Homogeneous/Heterogeneous Reservoir
  * Neumann boundary (Reservoir side)
  * Dirichlet boundary (Well side)
### Modify Parameters
 * Reservoir Depth, Area, and Volume (ft ; ft^2 ; ft^3)
 * Reservoir Pressure (psi)
 * Time and/or Iterations (days)
 * Porosity (phi)
 * Permeability (k)
 * Compressibility (ct)
 * Flow Rate (+/- Q)
 * Damage (hk)

## Method and Materials
* The Finite Element Method (FEM) is an essential numerical tool for solving boundary value problems of PDE's. 

**Snippet of module that generates sparsity matrix**
```matlab
for i = 1:N  		%Generate sparsity matrix
    if i+NX <= N 
        T_diag = T_frac(i,i+NX,Ay,mu,Bw,dy);
        T(i,i+NX) = T(i,i+NX)-T_diag;
        T(i+NX,i) = T(i+NX,i)-T_diag;
    end
    
    % This is for diagonals toward the inside of sparsity
    if (mod(i,NX) ~= 0) && (i+1 <= N)  
        T_diag = T_frac(i,i+1,Ax,mu,Bw,dx);
        T(i,i+1) = T(i,i+1)-T_diag;
        T(i+1,i) = T(i+1,i)-T_diag;
    end
    T(i,i) = abs(sum(T(i,:)));%Sum up every row on every iteration
    if BC(i) ~= 0 %if and only if the boundary condition exists
        T(i,i) = T(i,i)+BC(i)*2*T_frac(i,i,Ax,mu,Bw,dx);
    end
```

**Analytical Solution to Pressure Profile**
```matlab
function P = P_analytical(r,t)
Pi = 1000;q = 1000/5.615;mu = 1;k = 25;h = 10;phi = 0.2;ct = 1e-6
P = Pi -70.6*q*mu/(h*k)*expint((39.516*phi*mu*ct*(r).^2)/(k*t));
```

## Installation

Download package:
```git
git clone https://github.com/FracThePermian/Reservoir-Simulation-Part-B
```

Navigate to folder from Matlab (~/Reservoir-Simulation-Part-B/) and run the Project_1_Main.m script.


## Results

**3D Pressure Profile**

![Pressure Visualization](https://github.com/FracThePermian/Reservoir-Simulation-Part-B/blob/master/Graphical-Output/3d_contour.png?raw=true ""Pressure Gain/Reduction Profile"")

**Pressure topology**

![Pressure Contours](https://github.com/FracThePermian/Reservoir-Simulation-Part-B/blob/master/Graphical-Output/Pcontour1.png?raw=true ""Pressure Contours"")

**3D Pressure Profile**

![Pressure Contour 2](https://github.com/FracThePermian/Reservoir-Simulation-Part-B/blob/master/Graphical-Output/Pcontour2.png?raw=true ""Pressure Contour 2"")

**Flow Rate vs. Time**

![Rate vs. Time](https://github.com/FracThePermian/Reservoir-Simulation-Part-B/blob/master/Graphical-Output/RateVTime.png?raw=true ""Q vs. Time"")

**Cumulative Oil Production**

![Cumulative Oil Production](https://github.com/FracThePermian/Reservoir-Simulation-Part-B/blob/master/Graphical-Output/CumulativeOilProduction.png?raw=true ""Cumulative Oil Production"")


### License
[MIT](https://opensource.org/licenses/MIT ""MIT"")
",13,13,2,0,oil-and-gas,"[3d, 3d-simulation, discrete-mathematics, fem, finite-element-methods, gas, matlab, oil, oil-and-gas, petroleum-engineering, production, reservoir-engineering, reservoir-simulation, simulation]",0.0
14,aegis4048,GasCompressibility-py,,https://github.com/aegis4048/GasCompressibility-py,https://api.github.com/repos/GasCompressibility-py/aegis4048,Gas compressibility z-factor calculator package in Python,"![Alt text](docs/_static/intro_image.png)

# GasCompressibility-py

GasCompressibility-py is a Python library for calculating the gas compressibility factor, $Z$, based on real gas law. It is designed with practical oil field application in mind, in which the required inputs ($T$, $P$, and $\gamma_{g}$) can be readily obtained from the surface facility.

If you like this package, please consider giving a star :star: on the top right corner!

## 0. Documentation

Have you ever had an experience of finding the package that piques your interest, only to give up a few moments later after finding out that it has an insane learning curve because the documentation sucks? Worry not. **The package comes with extensive documentation support.** If you have any questions or looking for tutorials, just skim through this README introduction or check out the [official documentation](https://aegis4048.github.io/GasCompressibility-py/calc_z.html). 

<a href=""https://aegis4048.github.io/GasCompressibility-py/theories.html"" target=""_blank"">
  <img align=""center"" src=""misc/documentation_gallery.png"" alt=""GasCompressibility-py Documentation"" />
</a>

&nbsp;

Each page in the documentation have a Disqus plugin section where you can leave comments for any questions. Check the 
[Discussion Forum](https://aegis4048.github.io/GasCompressibility-py/discussion.html) page to post your questions!

## 1. Installation

The package is hosted on the [PyPi](https://pypi.org/project/gascompressibility/) page. You can remotely install it with the `pip` command:
```
pip install gascompressibility
```
To download the most recent version:
```
pip install gascompressibility --upgrade
```

If you are a chemical or petroleum engineer who doesn't know what `pip` is, read [below](#pip).

## 2. Quickstart

Check [gascompressibility.quickstart](https://aegis4048.github.io/GasCompressibility-py/quickstart.html) for more examples.

```python
>>> import gascompressibility as gc
>>> import matplotlib.pyplot as plt
>>>
>>> results, fig, ax = gc.quickstart(prmin=0.2, prmax=15, zmodel='DAK')
>>> plt.show()
```
![Alt text](docs/_static/quickstart_index.png)

## 3. Usage

The package allows for quick calculation of the z-factor from the gas specific gravity ($\gamma_{g}$), pressure ($P$), and temperature ($T$). Check [gascompressibility.calc_z](https://aegis4048.github.io/GasCompressibility-py/calc_z.html) for more examples.

**Basic (most common) usage:**
```python
>>> import gascompressibility as gc
>>>
>>> gc.calc_z(sg=0.7, T=75, P=2010)
0.7366562810878984
```
**In presence of significant non-hydrocarbon impurities:**
```python
>>> gc.calc_z(sg=0.7, T=75, P=2010, CO2=0.1, H2S=0.07, N2=0.05)
0.7765149771306533
```

**When pseudo-critical properties are known (not common):**

```python
>>> gc.calc_z(Pr=1.5, Tr=1.5)
0.859314380561347
```

**Picking correlation models of your choice**
```python
>>> gc.calc_z(sg=0.7, T=75, P=2010, zmodel='kareem', pmodel='sutton')
0.7150183342641309
```

**Returning all associated pseudo-critical properties computed**

```python
>>> gc.calc_z(sg=0.7, T=75, P=2010, ps_props=True)
{'z': 0.7366562810878984, 'Tpc': 371.4335560823552, 'Ppc': 660.6569792741872, 'J': 0.56221847, 'K': 14.450840999999999, 'Tr': 1.4394768357478496, 'Pr': 3.0646766226921294}
```
---------------

The package additionally supports calculation of pseudo-critical properties. Check
[gascompressibility.pseudocritical.Sutton](https://aegis4048.github.io/GasCompressibility-py/sutton.html) and
[gascompressibility.pseudocritical.Piper](https://aegis4048.github.io/GasCompressibility-py/piper.html) for more information.

**Reduced pressure calculation from specific gravity:**

```python
>>> from gascompressibility.pseudocritical import Piper
>>>
>>> Piper().calc_Pr(sg=0.7, N2=0.1, CO2=0.1, H2S=0.05, P=2010)
2.7950877932259734
```

**Pseudo-critical pressure calculation from specific gravity:**

```python
>>> Piper().calc_Ppc(sg=0.7, N2=0.1, CO2=0.1, H2S=0.05)
724.3779622618493
```

**Reduced pressure calculation from pseudo-critical pressure**
```python
>>> Piper().calc_Pr(Ppc=724.37, P=2010)
2.7951185167800987
```

## 4. Models Implemented

<ins><i>Pseudo-critical models</i></ins>

-   **Sutton (1985):** Makes corrections for acid fractions: $H_2S$ and
    $CO_2$
-   **Piper (1993):** Improved version of Sutton. Additionally supports
    corrections for $N_2$ along with $H_2S$ and $CO_2$
- [Theories 1: Pseudo-Critical Property Models](https://aegis4048.github.io/GasCompressibility-py/theories.html#pseudo-critical-property-models)

<ins><i>Z-factor models</i></ins>

-   **DAK (1975):** The most widely used z-factor model in the oil and
    gas industry for the past 40 years. You can\'t go wrong with this
    model
-   **Hall-Yarborough (1973):** Not recommended.
-   **Londono (2005):** Improved version of DAK. Math is exactly the
    same, but regression coefficients are fitted with 4x more data
    points.
-   **Kareem (2016):** Fast, but have shorter working ranges
    ($P_r < 15$)
-   [Theories 2: Z-Factor Correlation Models](https://aegis4048.github.io/GasCompressibility-py/theories.html#z-factor-correlation-models)

Not sure which model to use? You don\'t need to worry about it - default models (Piper + DAK) are more than good enough for real life applications. However, if computation speed is a big concern, use Kareem's method for z-factor correlation for $P_r < 15$. Check
[Theories 3: What models should I use?](https://aegis4048.github.io/GasCompressibility-py/theories.html#what-models-should-i-use) for more information.

## 5. Working Ranges

The below table summarizes the working $P_r$ and $T_r$ ranges of each
model, according to it\'s own original paper.


| Model           | $P_r$     | $T_r$     |
| --------------- | --------- | --------- |
| DAK             | [1, 3]    | [0.2, 30] |
| Hall-Yarborough | [1.15, 3] | (0, 20.5] |
| Londono         | [1, 3]    | [0.2, 30] |
| Kareem          | [1.15, 3] | [0.2, 15] |

However, normally we don't know the $P_r$ and $T_r$ values of a given mixture. The below figure summarizes the corresponding $P_r$ and $T_r$ (computed with Sutton's method) for each of specific gravity, temperature, and pressure ranges. For example, assuming $\gamma_{g}$ = 0.9 (green lines), z-factor correlation can't be used for extreme conditions like $P$ \> 19,000 psia, or $T$ \> 800 °F. If Kareem's method (`zmodel='kareem'`) is used for speed, you can't use it for $P$ \> 11,500 psia.

![Alt text](docs/_static/zmodels_working_ranges.png)

<details><summary>Figure source code</summary>

``` python
import matplotlib.pyplot as plt
import numpy as np
from gascompressibility.pseudocritical import Sutton

pmin = 0
pmax = 25000
Ps = np.linspace(pmin, pmax, 100)
Ps = np.array([round(P, 1) for P in Ps])

tmin = -459
tmax = 1500
Ts = np.linspace(tmin, tmax, 100)
Ts = np.array([round(T, 1) for T in Ts])

sgs = np.arange(0.1, 2.6, 0.4)
sgs = np.array([round(sg, 1) for sg in sgs])

results = {sg: {
    'Pr': np.array([]),
    'P': np.array([]),
    'Tr': np.array([]),
    'T': np.array([]),
} for sg in sgs}

for sg in sgs:
    for P in Ps:
        Pr = Sutton().calc_Pr(sg=sg, P=P)
        results[sg]['P'] = np.append(results[sg]['P'], [P], axis=0)
        results[sg]['Pr'] = np.append(results[sg]['Pr'], [Pr], axis=0)
    for T in Ts:
        Tr = Sutton().calc_Tr(sg=sg, T=T)
        results[sg]['T'] = np.append(results[sg]['T'], [T], axis=0)
        results[sg]['Tr'] = np.append(results[sg]['Tr'], [Tr], axis=0)

fig, axes = plt.subplots(1, 2, figsize=(9, 4))
for i, ax in enumerate(axes):
    if i == 0:
        for sg in sgs:
            Prs = results[sg]['Pr']
            Ps = results[sg]['P']

            p = ax.plot(Ps, Prs, label=sg)

            t = ax.text(Ps[-10], max(Prs) - 3, 'sg = ' + str(sg), color=p[0].get_color())
            t.set_bbox(dict(facecolor='white', alpha=0.7, edgecolor='white', pad=1))

        ax.text(0.06, 0.9, '$P_{r}$  approximation', fontsize=9, transform=ax.transAxes,
            bbox=dict(facecolor='white'))
        ax.set_ylabel('$P_r$', fontsize=11)
        ax.set_xlabel('Pressure (psia)')
        ymax = 60
        ax.hlines(y=30, xmin=pmin, xmax=pmax, color='k', linestyle='--', linewidth=0.8, alpha=0.7)
        ax.text(100, 31.3, '$P_r$ = 30.0', alpha=0.7)
        ax.hlines(y=15, xmin=pmin, xmax=pmax, color='k', linestyle='--', linewidth=0.8, alpha=0.7)
        ax.text(100, 16, '$P_r$ = 15.0', alpha=0.7)
        ax.hlines(y=1, xmin=pmin, xmax=pmax, color='k', linestyle='--', linewidth=0.8, alpha=0.7)
        ax.text(100, 2, '$P_r$ = 1.0', alpha=0.7)
        ax.fill_between(x=Ps, y1=1, y2=30, color='green', interpolate=True, alpha=0.1, zorder=-99)

    else:
        for sg in sgs:
            Trs = results[sg]['Tr']
            Ts = results[sg]['T']

            p = ax.plot(Ts, Trs, label=sg)

            t = ax.text(Ts[-1], max(Trs), 'sg = ' + str(sg), color=p[0].get_color())
            t.set_bbox(dict(facecolor='white', alpha=0.7, edgecolor='white', pad=1))

        ax.text(0.06, 0.9, '$T_{r}$  approximation', fontsize=9, transform=ax.transAxes,
            bbox=dict(facecolor='white'))
        ax.set_ylabel('$T_r$', fontsize=11)
        ax.set_xlabel('Temperature (°F)')
        ymax = 10
        ax.hlines(y=3, xmin=tmin, xmax=tmax, color='k', linestyle='--', linewidth=0.8, alpha=0.7)
        ax.text(tmin, 3.2, '$T_r$ = 3.0', alpha=0.7)
        ax.hlines(y=0.2, xmin=tmin, xmax=tmax, color='k', linestyle='--', linewidth=0.8, alpha=0.7)
        ax.text(tmin, 0.5, '$T_r$ = 0.2', alpha=0.7)
        ax.fill_between(x=Ts, y1=0.2, y2=3, color='green', interpolate=True, alpha=0.1, zorder=-99)


    ymin = 0 - 0.05 * ymax
    ax.set_ylim(ymin, ymax)

    ax.minorticks_on()
    ax.grid(alpha=0.5)
    ax.grid(visible=True, which='minor', alpha=0.1)
    ax.spines.top.set_visible(False)
    ax.spines.right.set_visible(False)


    def setbold(txt):
        return ' '.join([r""$\bf{"" + item + ""}$"" for item in txt.split(' ')])

    bold_txt = setbold('Working Ranges of Z Models')
    plain_txt = ',  for each of specific gravity, pressure, and  temperature ranges'

    fig.suptitle(bold_txt + plain_txt,
                 verticalalignment='top', x=0, horizontalalignment='left', fontsize=11)
    yloc = 0.9
    ax.annotate('', xy=(0.01, yloc), xycoords='figure fraction', xytext=(1.02, yloc),
                arrowprops=dict(arrowstyle=""-"", color='k', lw=0.7))
    ax.text(0.95, 0.1, 'GasCompressibility-Py', fontsize=9, ha='right', va='center',
            transform=ax.transAxes, color='grey', alpha=0.5)

fig.tight_layout()
```
</details>

## 6. Tips

<details><summary><b>What does 'pip' mean?</b></summary>

If you are asking this question, you are probably a petroleum or chemical engineer with minimal programming knowledge. It's basically a command-line program that helps you download & install any open-source library with minimal hassle. Here I offer some practical tips for engineers not proficient in Python (yet...!):

1.  Download Anaconda from [here](https://www.anaconda.com/download).
2.  Run the downloaded file. It's filename will look something like
    this: ""Anaconda3-2023.03-1-Windows-x86_64.exe""
3.  It will ask you to select destination folder. By default, its
    `C:\ProgramData\Anaconda3`
4.  If it asks you to check if you want to add Codna to the environtment
    PATH variable, check yes. It will say that it is not recommended,
    but trust me - this will make your life easier if you don't know
    what you are doing.
5.  Once installation is finished, go to windows search tab (bottom left
    corner of your screen). Type ""cmd"" and launch.
6.  If installation is done correctly, you should have `(base)` next to
    your current directory, like this: `(base) C:\Users\EricKim>`. If
    you don't see `(base)`, go to windows search tab again. Type
    ""Anaconda"". Click ""Anaconda Prompt (Anaconda 3)"" and launch it. If
    you still don't see `(base)`, you are about to do dive into some
    painful troubleshooting. Ask your friends who are good at Python to
    help you with it.
7.  If the Anaconda installation is done correctly, make your command
    line look like the following and press enter:
    `(base) C:\Users\EricKim>pip install gascompressibility`.
8.  Congrats! Installation is finished
9.  Type `(base) C:\Users\EricKim>Jupyter Notebook` and try the package
    on Jupyter Notebook.

</details>

## 7. References

<details><summary><b>Expand</b></summary>

\[1\] Sutton, R.P.: ""Compressibility Factor for High-Molecular Weight
Reservoir Gases,"" paper SPE 14265 (1985).
[(link)](https://onepetro.org/SPEATCE/proceedings-abstract/85SPE/All-85SPE/SPE-14265-MS/61651)

\[2\] Piper, L.D., McCain Jr., W.D., and Corredor J.H.: ""Compressibility
Factors for Naturally Occurring Petroleum Gases,"" paper SPE 26668
(1993).
[(link)](https://onepetro.org/SPEATCE/proceedings/93SPE/All-93SPE/SPE-26668-MS/55401)

\[3\] Dranchuk, P.M., and Abou-Kassem, J.H.: ""Calculation of z-Factors
for Natural Gases Using Equations of State,"" *Journal of Canadian
Petroleum Technology* (1975).
[(link)](https://onepetro.org/JCPT/article-abstract/doi/10.2118/75-03-03)

\[4\] Kay, W.B: \""Density of Hydrocarbon Gases and Vapors at High
Temperature and Pressure,\"" Industrial Engineering Chemistry (1936)

\[5\] Wichert, E.: ""Compressibility Factor of Sour Natural Gases,"" MEng
Thesis, The University of Calgary, Alberta (1970)

\[6\] Stewart, W.F., Burkhardt, S.F., and Voo, D.: \""Prediction of
Pseudocritical Parameters for Mixtures,\"" paper presented at the AIChE
Meeting, Kansas City, MO (May 18, 1959).

\[7\] Hall, K.R., and Yarborough, L.: ""A new equation of state for
Z-factor calculations,"" *Oil and Gas Journal* (1973).
[(link)](https://www.researchgate.net/publication/284299884_A_new_equation_of_state_for_Z-factor_calculations)

\[8\] Londono, F.E., Archer, R.A., and Blasingame, T.A.: ""Simplified
Correlations for Hydrocarbon Gas Viscosity and Gas Density ---
Validation and Correlation of Behavior Using a Large-Scale Database,""
paper SPE 75721 (2005).
[(link)](https://onepetro.org/SPEGTS/proceedings/02GTS/All-02GTS/SPE-75721-MS/135705)

\[9\] Kareem, L.A., Iwalewa, T.M., and Marhoun, M.al-.: ""New explicit
correlation for the compressibility factor of natural gas: linearized
z-factor isotherms,"" *Journal of Petroleum Exploration and Production
Technology* (2016).
[(link)](https://link.springer.com/article/10.1007/s13202-015-0209-3)

\[10\] Elsharkawy, A.M., Aladwani, F., Alostad, N.: ""Uncertainty in sour
gas viscosity estimation and its impact on inflow performance and
production forecasting,"" *Journal of Natural Gas Science and
Engineering* (2015).
[(link)](https://link.springer.com/article/10.1007/s13202-015-0209-3)

\[11\] Elsharkawy, A.M.: ""Predicting the Properties of Sour Gases and
Condensates: Equations of State and Empirical Correlations,"" paper SPE
74369 (2002).
[(link)](https://onepetro.org/SPEIOCEM/proceedings-abstract/02IPCEM/All-02IPCEM/SPE-74369-MS/136841)

\[12\] Elsharkawy, A.M., and Elsharkawy, L.: ""Predicting the
compressibility factor of natural gases containing various amounts of
CO2 at high temperatures and pressures,"" *Journal of Petroleum and Gas
Engineering* (2020).
[(link)](https://www.researchgate.net/publication/343309900_Predicting_the_compressibility_factor_of_natural_gases_containing_various_amounts_of_CO2_at_high_temperatures_and_pressures)

</details>

## 8. Requirements

Python =\> 3.8

1.  Numpy
2.  Scipy
3.  Matplotlib

## 9. Authors

-   [Eric 'Soobin' Kim](https://github.com/aegis4048) - Petroleum engineer with the gas compressor company, [Flogistix](https://flogistix.com/). Primary author of the package. (Contact | <aegis4048@gmail.com>, Website | [PythonicExcursions](https://aegis4048.github.io/))

## 10. License
The packages is under [MIT License](https://github.com/aegis4048/GasCompressibiltiy-py/blob/main/LICENSE) (no restrictions whatsoever). 

## 11. Cite As

Eric \""Soobin\"" Kim, 2021, GasCompressibility-py Python Package, PyPI, Python Package Index, <https://pypi.org/project/gascompressibility/>.
",12,12,1,1,oil-and-gas,"[chemical-engineering, oil-and-gas, petroleum, petroleum-engineering, python]",0.0
15,Accenture,OSDU-Ontology,Accenture,https://github.com/Accenture/OSDU-Ontology,https://api.github.com/repos/OSDU-Ontology/Accenture,"An ontology designed for oil and gas, and subsurface energy data based on the industry standards.","# OSDU Ontology 
This is an open source ontology for the subsurface energy data based on [3rd release of the schema files and standards ](https://community.opengroup.org/osdu/platform/data-flow/data-loading/open-test-data/-/tree/master/rc--3.0.0/3-schema) specified by the [Open Subsurface Data Universe](https://osduforum.org/) community.
Please see the [documentation](./docs) for more information about OSDU and how the ontology is designed.

# License
[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
The OSDU Ontology is licensed under the Apache License 2.0 - see [License](./License).

#Ontology Files
Load the [.ttl](./ttl/OSDU.ttl) in your favorite ontology editor, e.g. [Protege](https://protege.stanford.edu/products.php#desktop-protege).

# Ontology Generator
A tool to convert the OSDU data loading schema to an OWL3-based ontology, in .ttl format.

# Dependecies
Python3, with libraries numpy and regex.

# Installation
~~~
git clone https://github.com/Accenture/OSDU-Ontology.git
~~~

# Usage
Download the latest OSDU schema from [this location.] (https://community.opengroup.org/osdu/platform/data-flow/data-loading/open-test-data/-/tree/master/rc--3.0.0/3-schema)

From a terminal in the osdu-ontology-generator folder:
~~~
python3 -m create_ontology --src path_to_full_schema/
~~~

To run metric calculation (with reporting in terminal):
~~~
python3 -m create_ontology --src path_to_full_schema/ --report_metrics
~~~
",11,11,7,14,oil-and-gas,"[knowledge-graph, oil-and-gas, ontology, owl, rdf]",0.0
16,OlegRyzhkov2020,oil-project,,https://github.com/OlegRyzhkov2020/oil-project,https://api.github.com/repos/oil-project/OlegRyzhkov2020,Oil Data Analytics,"# Oil Data Analytics



![GitHub last commit](https://img.shields.io/github/last-commit/OlegRyzhkov2020/oil-project)
![GitHub top language](https://img.shields.io/github/languages/top/OlegRyzhkov2020/oil-project)
[![made-with-Markdown](https://img.shields.io/badge/Made%20with-Markdown-1f425f.svg)](http://commonmark.org)
[![HitCount](http://hits.dwyl.com/OlegRyzhkov2020/oil-project.svg)](http://hits.dwyl.com/OlegRyzhkov2020/oil-project)
![GitHub watchers](https://img.shields.io/github/watchers/OlegRyzhkov2020/sql-challenge?label=Watch&style=social)
![GitHub followers](https://img.shields.io/github/followers/OlegRyzhkov2020?label=Follow&style=social)
[![](https://data.jsdelivr.com/v1/package/npm/chart.js/badge)](https://www.jsdelivr.com/package/npm/chart.js)


![dashboard_slide](images/project_proposal.png)

## Home Page


![dashboard_slide](images/home_page.png)

## USA Oil Infrastructure Map

![dashboard_slide](images/map_1.png)

## World Oil Statistics

![dashboard_slide](images/map_2.png)

## Data Structure

```python
# US Energy Information Administration API Query Browser, Open Data Source
class EIA_Client():
    api_key=None
    def __init__(self, api_key=creds.eai_key, data='category', *args, **kwargs):
        super().__init__(*args, **kwargs)
        if api_key == None:
            raise Exception('Api key is required')
        self.api_key = api_key
        self. eia_url = f""http://api.eia.gov/{data}/""
    def category(self, cat = 1293027):
        params = {'api_key': self.api_key, 'category_id':cat}
        params_url = urlencode(params)
        url = f""{self.eia_url}?{params_url}""
        response = requests.get(url).json()
        return response
```

* PostgreSQL Data Import


```sql
SET search_path = countries
--Select two countries data by years (self join)
SELECT p1.year, SUM(p1.oil_production) as USA,
				SUM(p2.oil_production) as RUSSIA
FROM production as p1
INNER JOIN production as p2 ON p1.year=p2.year
WHERE p1.country = 'United States of America' AND p2.country = 'Russia'
GROUP BY p1.year
ORDER BY p1.year DESC
```
## Machine Learning Models

![dashboard_slide](images/analysis_1.png)

![dashboard_slide](images/analysis_2.png)

![dashboard_slide](images/mlrmodel.png)

![dashboard_slide](images/export_import.png)

 ![dashboard_slide](images/rnnmodel.png)

 ![dashboard_slide](images/rfmodel.png)

## Key Findings and Future Research Questions

![dashboard_slide](images/findings.png)


## Contacts
[Find Me on
LinkedIn](https://www.linkedin.com/in/oleg-n-ryzhkov/)
",10,10,1,1,oil-and-gas,"[dashboard, flask-application, html-css-javascript, interactive-maps, oil-and-gas, python, r, regression-analysis, sqlalchemy, time-series-analysis]",0.0
17,yohanesnuwara,petroleum-data-analytics-series,,https://github.com/yohanesnuwara/petroleum-data-analytics-series,https://api.github.com/repos/petroleum-data-analytics-series/yohanesnuwara,Series of tutorials and codes for Petroleum Data Analytics & ML,"# Petroleum Data Analytics (PDA) Series
",9,9,1,0,oil-and-gas,"[data-analytics, machine-learning, oil-and-gas]",0.0
18,pro-well-plan,opensource_apps,pro-well-plan,https://github.com/pro-well-plan/opensource_apps,https://api.github.com/repos/opensource_apps/pro-well-plan,Streamlit app covering our open source projects. Try it here:  https://lnkd.in/g2Yr5B9,,8,8,3,0,oil-and-gas,"[oil-and-gas, open-source, streamlit]",0.0
19,imayachita,wireline-log-plotting,,https://github.com/imayachita/wireline-log-plotting,https://api.github.com/repos/wireline-log-plotting/imayachita,Wireline Log Visualization/Plotting with Matplotlib in Python,"# wireline-log-plotting
Wireline Log Visualization/Plotting with Matplotlib in Python with LAS file as input

Written in Jupyter Notebook

Requirements:

```lasio==0.23```

```matplotlib==3.0.2```

```numpy==1.17.2```


Output:
![image1](wireline_log.jpg)


Credit to https://github.com/petroGG/Basic-Well-Log-Interpretation
",7,7,1,0,oil-and-gas,"[geoscience, oil-and-gas, visualization]",0.0
20,fraclad,wellArchitectureDesign,,https://github.com/fraclad/wellArchitectureDesign,https://api.github.com/repos/wellArchitectureDesign/fraclad,Leveraging matplotlib to visualize well drilling and construction,"# Well Design & Architecture

A repository for a set of codes to vizualize a wellbore with pertinent information. 

The codes allow users to add various well construction elements (such as casings and cements) with simple object definitions and calls. Thus, a well is simply a collection of element objects. More well construction elements to come (depends if I have free time or not  😅). 

Current result looks like something below (the dimensions are totally made up lol). See `execute.py` for example on how to create various well elements. 

![wellbore](https://raw.githubusercontent.com/fraclad/wellArchitectureDesign/main/plots/result2022Apr10.svg)

Possible development in the future will most likely revolve around adding more well construction elements such as hanging liners, expandable tubulars, perforation zone, etc. 

Do consider helping!! Always open for a pull request. 
",7,7,1,0,oil-and-gas,"[drilling, drilling-engineering, oil-and-gas, well, well-construction]",0.0
21,rickdelpino,Oil_Finder_Rick_improved_2021,,https://github.com/rickdelpino/Oil_Finder_Rick_improved_2021,https://api.github.com/repos/Oil_Finder_Rick_improved_2021/rickdelpino,"Open source Windows Application in Python for Geosciences, Artificial Intelligence and Data Sciences - Initial stages, 2021","# Oil_Finder_Rick_improved_2021
Open source Windows Application in Python for Geosciences, Artificial Intelligence and Data Sciences - 1st Framework Design 2020

A brief explanation about what this aplication can do so far is here: https://www.youtube.com/watch?v=ABjevE5O3go

- 2D Seismic Loading:
![image](https://user-images.githubusercontent.com/96155532/146419443-cc8ea65d-764e-4c7a-a1ef-aea0526e6e05.png)

- 3D Seismic Loading:
![image](https://user-images.githubusercontent.com/96155532/146419879-4f0ccef3-6e47-4d4c-a2e0-e08feafd6695.png)

- Data Analysis Plotting:
![image](https://user-images.githubusercontent.com/96155532/146420822-aa976fe1-eacf-4e21-adce-25febb097c85.png)



**Hello World!**
I am Edgar Del Pino, an exploration and production geophysicist/geoscientist which have a lot of projects in mind related with
geology, geophysics, programming, AI and Data Sciences.

This is my first attempt to create a software platform where I can implement Machine Learning, AI and data Sciences to a variety
of problems that I have faced during my different jobs and assignations inside the oil industry, in different settings and departments, 
where the lack of time and resources led me to choose a practical solution, but, I always kept some doubts about what kind of output 
could be obtained if we would had apply a more scientific approach using Machine Learning...

The software is incomplete because this is only the design stage of the User Interface and early implemmentations of segy reading, 
plotting and dataframes implementation. There are a lot of fixes to do, rewrite, recoding, debugging and obviously, changes
to be made. This is because for me, the most important thing in an app or analysis software, is the UI. The most popular
geological interpretation software has a very intuitive and straightforward UI, however the closest competitor's software 
has a very serious UI problem: it is not intuitive. So I chose the most user friendly framework construction software: PyQT5.
That is why the following are the first focus of this early software version: the fundations construction and data reading, 
the purposes of each module, the plotting of the data and the framework design. This is only the begining... 


**Let's start with the project!:**

- Programming language: Python 3.8 - because it has a lot of documentation and a very active community.
- Windows IDE (Integrated Development Environment): Visual Studio Code. It works very well. In Linux, Spyder was my choice.
- Framework: PyQT5 - For me, it has the best QT designer, and it's extremely easy to use (on Windows).
- SEGY library: SEGYIO - a very interesting option for reading and manipulation of SEGY files, but I am strugling with its 
                coordinates management or vectorized applications. It has pros and cons.
- Python libraries:
  - numpy: for a very easy array manipulation and best performance.
  - pandas: for Data Science forms or massive ascii and number manipulation. These are the famous Dataframes.
  - matplotlib: a library to plot almost everything with python. I am struggling with its PyQT5 integration.
  - xarray: segyoi has a better integration with it and has matplotlib plotting routines.
  - csv: for readign CSV  files.


What the software do so far:
1) Can load a no-referenced 2D seismic lines and plot them in the Seismic 2D View Tab.
2) Can load a 3D Seismic an plot a line in the Seismic 3D View Tab.
3) Can load sigle trace seismic in the single trace VSP Tab.
4) Can load and display the content of a Dataframe in form of Bar charts and Pie charts, in the Data Science Tab.
5) Most of the loading will be done in the Import Button and the Data Sience Button.


Inside the Data folder, there is a csv file ""Exploratory_Wells_Data.csv"" that is used in the Data Sciences module for
testing purposes.
",7,7,1,0,oil-and-gas,"[data-science, dataframe, geosciences, input-output, machine-learning, matplotlib, oil-and-gas, pyqt5, python, segy, seismic-data]",0.0
22,FracThePermian,FEM-Reservoir-Drainage-3D,,https://github.com/FracThePermian/FEM-Reservoir-Drainage-3D,https://api.github.com/repos/FEM-Reservoir-Drainage-3D/FracThePermian,Compares analytical/numerical results for the drainage of a single well.,,6,6,0,2,oil-and-gas,"[3d, 3d-simulation, finite-element-methods, geology, matlab, oil-and-gas, petroleum, petroleum-engineering, physics-simulation, simulation]",0.0
23,jshumway0475,Petroleum,,https://github.com/jshumway0475/Petroleum,https://api.github.com/repos/Petroleum/jshumway0475,Discounted cash flow analysis for oil and gas,"# Petroleum Discounted Cash Flow Analysis  
This repository contains a Jupyter Notebook called **`Multi-Well DCF.ipynb`** that takes as input oil and natural gas price forecasts (example file provided `STR-071521.csv`) and a list of properties that could be existing wells or future wells (example file provided `property_table.csv`) with several fields needed to create forecasts for oil and gas production, expenses, and capital. The Jupyter Notebook generates the forecasts for the time frame specified, discounts the projected cash flow stream, and provides output of key economic metrics such as net present value, rate of return, etc.

Also included is a Jupyter Notebook called **`DCA Calcs.ipynb`** that is a simple calculator to forecast oil and gas volumes from Arps equations. This is a primary component of the Muti-Well DCF.ipynb program and is provided for reference.  

It is important to note that all properties in the property_table.csv file must have forecast parameters anchored to a common start month ('Base Date' in cell 10 of the Multi-Well DCF.ipynb program). In the example file, the base date (start month) is set to July 2021.
",5,5,1,0,oil-and-gas,"[decline-curve-analysis, discounted-cash-flows, forecasting, oil-and-gas, petroleum, petroleum-engineering]",0.0
24,Yous3ry,Python_Automated_DCA,,https://github.com/Yous3ry/Python_Automated_DCA,https://api.github.com/repos/Python_Automated_DCA/Yous3ry,Python Automatic Decline Curve Analysis (DCA) For Petroleum Producing wells,"# Python Automatic Decline Curve Analysis (DCA) For Petroleum Producing wells

## Table of Contents
1. [About the Project](#about-the-project)
2. [Workflow](#workflow)
3. [Dependencies](#dependencies)
4. [References](#references)

## About The Project
Decline Curve Analysis (DCA) is a widely accepted method for analyzing declining production rates and forecasting future performance of oil and gas wells. [[1]](#ref-1) For oil fields with hundreds or even thousands of wells updating the production forecast for each well based on DCA could be very time consuimg. Here's why:
1. Typically curve fitting to estimate the decline rate and initial rate is done graphically which could be very time consuming, and
2. Over the life of a conventional oil producing well, decline rates may change over time with changes in operating conditions, e.g. start of water injection, aritifical lift methodlogy or effciency changes, etc. Thus the history of the well could be divided into different distinct regions and DCA parameters should only be estimated based on the last stable producing conditions rather than average fitting of all the historical performance.

## Workflow
1. Connect to production database to read well data ([Python File here](https://github.com/Yous3ry/Python_Automated_DCA/blob/main/DB_Connect.py))
2. Use R to detect change points assuming a piecewise linear trend using EnvCpt Package[[2]](#ref-2) as shown in example below ([R File here](https://github.com/Yous3ry/Python_Automated_DCA/blob/main/Change_Detection.R))
![alt text](https://github.com/Yous3ry/Python_Automated_DCA/blob/main/Sample_Well_1_ChangeDetection.png)
3. Finally an integrated Python file ([Final Python File here](https://github.com/Yous3ry/Python_Automated_DCA/blob/main/Python_DCA.py)) is used to a) Load Well Data b) detect change points using R via Rpy2 c) fit exponential or Hyperbolic decline based on user input on the last stable period d) Forecast production rate. Note forecast is limited by user minimum rate and forecast period.
![alt_text](https://github.com/Yous3ry/Python_Automated_DCA/blob/main/Sample_Well_1_Forecast_Results.png)

*NOTE* A compiled Python Script integrates all previous steps and loops through all the wells in the database and exports the total field/company forecast. ([Compiled Python File here](https://github.com/Yous3ry/Python_Automated_DCA/blob/main/Compiled_Script.py))

## Dependencies
Python Packages <- sqlalchemy, scipy, matplotlib, pandas, numpy, json, rpy2\
R Packages <- EnvCpt

<br>

## References
<a id=""ref-1"">[1]</a> 
https://petrowiki.spe.org/Production_forecasting_decline_curve_analysis#:~:text=Decline%20curve%20analysis%20(DCA)%20is,fluids%2C%20are%20usually%20the%20cause.
<br>
<a id=""ref-1"">[2]</a> 
https://rdrr.io/cran/EnvCpt/man/envcpt.html
",5,5,1,0,oil-and-gas,"[change-detection, curve-fitting, decline-curve-analysis, oil-and-gas, petroleum, petroleum-engineering, reservoir, reservoir-engineering, sql]",0.0
25,brunomsantiago,subsea_inpainting,,https://github.com/brunomsantiago/subsea_inpainting,https://api.github.com/repos/subsea_inpainting/brunomsantiago,Removing overlays from subsea inspection videos,,4,4,1,0,oil-and-gas,"[computer-vision, deep-neural-networks, image-dataset, image-inpainting, offshore, oil-and-gas, rov, subsea, underwater, video-inpainting]",0.0
26,victoreronmosele,oil_and_gas_unit_converter,,https://github.com/victoreronmosele/oil_and_gas_unit_converter,https://api.github.com/repos/oil_and_gas_unit_converter/victoreronmosele,Oil and Gas Unit Converter is a mobile app that helps make various unit conversions in the oil and gas industry. ,"


<h1 align=""center"">
 Oil and Gas Converter
</h1>

<h4 align=""center"">This app is a mobile app built with <a href=""https://flutter.dev"" target=""_blank"">Flutter</a> that helps make various unit conversions in the oil and gas industry. It is currently under development.</h4>

<p align=""center"">
  <a href=""#how-to-use"">How To Use</a> •
  <a href=""#credits"">Credits</a> •
  <a href=""#license"">License</a>
</p>

<p align=""center"">
<img src=""https://raw.githubusercontent.com/herovickers/oil_and_gas_unit_converter/assets/oil_and_gas_converter_android_screenshot.png"" alt=""Oil and Gas Unit Converter Mobile App Screenshot"" >
</p>


## How To Use
     
From your terminal:

```bash
# Clone this repository
$ git clone https://github.com/herovickers/oil_and_gas_unit_converter.git

# Go into the repository
$ cd oil_and_gas_unit_converter
```
Then
```bash
# Run the app
$ flutter run
```


## Credits

This app uses the following:

- [Flutter](https://flutter.dev)
- [Rig Zone's Oil and Gas Conversion Calculator](https://www.rigzone.com/calculator/default.asp#calc)'s data for the unit conversions.
- UI inspired by [Laude Pirera Ardi](https://dribbble.com/laudepirera)'s [Currency Calculator](https://dribbble.com/shots/5601649-Currency-Calculator) design.

## License

MIT

---

> [victoreronmosele.com](https://victoreronmosele.com) &nbsp;&middot;&nbsp;
> GitHub [@victoreronmosele](https://github.com/victoreronmosele) &nbsp;&middot;&nbsp;

",4,4,0,0,oil-and-gas,"[android, flutter, oil-and-gas]",0.0
27,james-roden,arcgis-ghsz,,https://github.com/james-roden/arcgis-ghsz,https://api.github.com/repos/arcgis-ghsz/james-roden,Calculates the base of the Gas Hydrate Stability Zone given a sea floor raster using Joides formula.,"# arcgis-ghsz
An ArcGIS toolbox for caculating the base of the Gas Hydrate Stability Zone given a sea floor raster. Derived from the [Joides formula](http://www.odplegacy.org/PDF/Admin/JOIDES_Journal/JJ_1992_V18_No7.pdf).

*James M Roden*

*Version==1.0.0 // Apr 2020*

## Index
1. [Background](https://github.com/james-roden/ArcGIS-GHSZ-Tool/blob/master/README.md#background)
2. [How to Use](https://github.com/james-roden/ArcGIS-GHSZ-Tool/blob/master/README.md#how-to-use)

## Background

Gas hydrate stability zone, abbreviated GHSZ refers to a zone and depth of the marine environment at which methane clathrates (large amount of methane trapped within a crystal structure of water, forming a solid similar to ice) naturally exist in the Earth's crust. 

Stability primarily depends upon temperature and pressure, however other variables such as gas composition and ionic impurities in water influence stability boundaries.

The upper limit of the GHSZ occures at a depth of approximately 150 meters below mud line (BML) in polar regions; approximately 500 meters BML along continental regions; approximately 300 meters in oceanic sediments when the bottom water temperature is at or near 0 degrees celsius. The maximal depth (the base) of the GHSZ is limited by the geothermal gradient. Generally the maximum resides around 2000 meters BML. 

Joides algorithm for calculating the GHSZ given the water depth (meters), bottom water temperature (degrees celsius), and geothermal gradient (degrees c/km):

![JoidesFormula](https://github.com/james-roden/ArcGIS-GHSZ-Tool/blob/master/joides_alg.PNG)

## How to Use
- Ensure the sea floor raster has only positive values. 
",4,4,2,0,oil-and-gas,"[arcgis, environment, ghsz, hydrocarbons, methane, oil-and-gas, python, python2, sea-floor-raster]",0.0
28,devfabiosilva,witsml21parser,,https://github.com/devfabiosilva/witsml21parser,https://api.github.com/repos/witsml21parser/devfabiosilva,"A fast, robust and portable Witsml to BSON parser","# witsml21parser

A fast, robust and portable [Witsml 2.1](https://energistics.org/witsml-data-standards) to [BSON parser](https://bsonspec.org/)

## Features

- Fast
- Robust
- Portable ([NodeJS](https://github.com/devfabiosilva/witsml21parser/tree/master/NodeJS/) | [Java/Kotlin](https://github.com/devfabiosilva/witsml21parser/tree/master/Java) | PHP (in development) | [Python 3](https://github.com/devfabiosilva/witsml21parser/tree/master/Python))
- Low dependency libraries.
- No Garbage Collector on parsing
- Low memory allocation/reallocation
- Optimized for C/C++ applications

# Before you install

Before you install you need to check these tools:

```sh
A Linux based OS
Python >= 3.8 (For Python 3 application)
Python 3 development API (for Python 3 application)
gcc >= 9.4.0
cmake >= 3.16.3
make >= 4.2.1
git >= 2.25.1
execstack >= 1.0 (for Java application)
Java SDK development API (for Java application)
Node JS >= v16.20.2 API (for Node JS)
```

# Downloading

```sh
git clone https://github.com/devfabiosilva/witsml21parser.git
```

# Compiling

**_witsml21parser_** needs _libbson_ library to be compiled. For first time compiling you need to download and compile dependencies first.

There are two static dependencies:

- libbson
- WITSML 2.1 - ETP validation schemas C code

## - Compiling libbson third-party library

```sh
make install_bson
```

For multi-threading fast compiling use _-jN_ option e.g:

```sh
make -j12 install_bson
```
## - WITSML 2.1 - ETP validation schemas C code

```sh
make pre pre_shared
```

For multi-threading fast compiling use _-jN_ option e.g:

```sh
make -j12 pre pre_shared
```

## - Compiling WITSML 2.1 to BSON parser

After compiling _libbson_ third-party library and _WITSML 2.1 - ETP validation schemas C code_ just type:

```sh
make
```

For multi-threading fast compiling use _-jN_ option e.g:

```sh
make -j12
```

### Executing

Just type:

```sh
./cws <FILE_NAME>
```

E.g:

```sh
./cws BhaRun.xml
```

It will convert BhaRun.xml to BhaRun.bson file

## - Compiling WITSML 2.1 to BSON parser for Java/Kotlin

To compile native library for Java/Kotlin just type:

```sh
make jni
```

For multi-threading fast compiling use _-jN_ option e.g:

```sh
make -j12 jni
```

### Executing Java WITSML 2.1 to BSON parser

Before you run your code you MUST set native library environment.

- Go to Java sources [folder](https://github.com/devfabiosilva/witsml21parser/tree/master/Java/library)
- Execute shell script:

```sh
source env.sh
```
- Open with your favorite IDE and run Java code [here](https://github.com/devfabiosilva/witsml21parser/tree/master/Java/library)
- Run Tests and Aplication

# SAMPLE APP jwitsml21cmd-1.0.jar

This app shows how to use WITSML 2.1 BSON parser in Java/Kotlin

## Before use

You need to compile JNI

```sh
cd <WITSML 2.1 BSON PARSER PROJECT FOLDER>/
make jni
```

File _libjwitsmlparser21.so_ must be in root project folder.

Compile Java library with _maven_

```
cd <WITSML 2.1 BSON PARSER PROJECT FOLDER>/Java/library
mvn -U clean install
```

## Running

```sh
cd <WITSML 2.1 BSON PARSER PROJECT FOLDER>/Java/sampleApp
source env.sh
java -jar jwitsml21cmd-1.0.jar <WITSML 2.1 XML FILES>
```

### Examples

```sh
java -jar jwitsml21cmd-1.0.jar ../../examples/xmls/OpsReport.xml

```

```sh
Welcome to WITSML 2.1 parser

========================
Opening ../../examples/xmls/OpsReport.xml
Instance name: jWITSMLParser 2.1 - (0x7f599422ab40)
Saving to file OpsReport.json
Saving to file OpsReport.bson

Statistics for ""../../examples/xmls/OpsReport.xml"":
        {arrays=60, booleans=22, costs=9, dateTime=73, doubles=28, enums=55, long64s=47, measures=382, memoryUsed=3160, strings=463, total=1139}
```

```sh
java -jar jwitsml21cmd-1.0.jar ../../examples/xmls/OpsReport.xml ../../examples/xmls/Risk.xml
```

```sh
Welcome to WITSML 2.1 parser

========================
Opening ../../examples/xmls/OpsReport.xml
Instance name: jWITSMLParser 2.1 - (0x7fe87424aba0)
Saving to file OpsReport.json
Saving to file OpsReport.bson

Statistics for ""../../examples/xmls/OpsReport.xml"":
        {arrays=60, booleans=22, costs=9, dateTime=73, doubles=28, enums=55, long64s=47, measures=382, memoryUsed=4532, strings=463, total=1139}

========================
Opening ../../examples/xmls/Risk.xml
Instance name: jWITSMLParser 2.1 - (0x7fe8742a8b30)
Saving to file Risk.json
Saving to file Risk.bson

Statistics for ""../../examples/xmls/Risk.xml"":
        {arrays=12, dateTime=17, enums=14, long64s=9, measures=18, strings=106, total=176}
```

# Executing NodeJS (>=v16.20.2) WITSML 2.1 to BSON parser

## Before use (first time only)

You need to compile JS WITSML 2.1 to BSON parser

In your root _witsml21parser_ folder

```sh
make nodejs && cd $(pwd)/NodeJS && source env.sh
```

## Running

In your [NodeJS](https://github.com/devfabiosilva/witsml21parser/tree/master/NodeJS/) folder:

```sh
node app.js
```

# Executing Python 3 WITSML 2.1 to BSON parser

## Before use (first time only)

You need to compile PyWITSML 2.1 to BSON parser

In your root _witsml21parser_ folder

```sh
make py && cd $(pwd)/Python && source env.sh
```

## Running

In your [Python](https://github.com/devfabiosilva/witsml21parser/tree/master/Python/) folder:

```sh
python3 witsml21parser.py
```

# BENCHMARKS

Primary benchmarks in Java had been shown that WITSML 1.4.1.1 parsing objects are 56 % faster than JAXB in Java application.

JAXB only parses XML objects and store their values in respective Java objects. In other hand, WITSML BSON parser not only parses XML objects in C structs but it creates BSON objects, adds objects to BSON, serializes it and parses it to Java native byte object.

But wait! How is WITSML BSON parser faster than JAXB doing more stuffs?

Answer is simple.

- Less allocation/reallocation in memory
- Referencing objects instead create and copy
- gSoap optimization on parsing XML to C structs. See some interesting [GENIVIA](https://www.genivia.com/) articles [here](https://www.genivia.com/ugrep.html)
- Recycled alloc'd memory
- Low memory usage
- Only two library dependency ([gSoap](https://www.genivia.com/products.html) and [libbson](https://github.com/mongodb/mongo-c-driver/tree/master/src/libbson))
- No garbage collector on parsing objects

# SIGNING KEY

_Keys for signature_:

**[fabioegel@gmail.com](mailto:fabioegel@gmail.com)**
**fingerprint**: 6E21 A6B6 E1A4 6580 CA00 FA54 1077 26E9 14B9 16A2

# LICENSES

This project is fully open source and MIT license.

## WARNING

WITSML 2.1 BSON parser needs library with different licenses.

See [version.json](https://github.com/devfabiosilva/witsml21parser/blob/master/version.json) for details.

# DONATIONS

Any donation is welcome.

Consider any amount of donation in _BITCOIN_: *1JUzcSh3vsBCRji5n5rJsbHQfW3hYrNAW4*

Thank you :)

",3,3,1,0,oil-and-gas,"[bson, c, cplusplus, energistics, etp, java, nodejs, oil-and-gas, osdu, python, soap, witsml, xml]",0.0
29,Jun-Tam,article_analysis_word2vec,,https://github.com/Jun-Tam/article_analysis_word2vec,https://api.github.com/repos/article_analysis_word2vec/Jun-Tam,NLP Article Trend Analysis (Oil & Gas),"# Article trend Analysis by word2vec

### Summary
This Jupyter Notebook demonstrates application example of NLP to energy-industry articles in PDF. <br>

### Part1: Preprocessing
Preprocessing part is described: conversion from PDF to text, tokenizer, duplicate file deletion. <br>
About 600 articles were collected and converted into text files. <br>
https://github.com/Jun-Tam/article_analysis_word2vec/blob/master/NLP_Articles_Preprocess.ipynb


### Part2: Trend Analysis
Analysis part is described: BoW, IDF/TF, word2vec, WordCloud <br>
https://github.com/Jun-Tam/article_analysis_word2vec/blob/master/NLP_Articles_Word2Vec.ipynb

Word count from all the articles is as shown below. <br>
<br>
![demo](https://github.com/Jun-Tam/article_analysis_word2vec/raw/master/images/word_count.png)
<br>
 
Word Cloud is a usefull tool to visualize what were people's interests in each year. <br>
<br>
![demo](https://github.com/Jun-Tam/article_analysis_word2vec/raw/master/images/word_cloud.png)
<br>
 
Using word2vec ""ness"" vectors are defined, and each article is converted into ness vectors. <br>
The time-series plots below show recent article trends for individual ness vectors along with oil price history. <br>
<br>
![demo](https://github.com/Jun-Tam/article_analysis_word2vec/raw/master/images/article_trend.png)
<br>

### Reference
Natural Language Processing In Action, Undestanding, analyzing, and generating text with Python, Manning <br>
Hobson Lane, Cole Howard, Hannes Max Hapke <br>
",3,3,1,0,oil-and-gas,"[energy, nlp, oil-and-gas, word2vec, wordcloud]",0.0
30,gwallison,KS_injection_wells_formatting,,https://github.com/gwallison/KS_injection_wells_formatting,https://api.github.com/repos/KS_injection_wells_formatting/gwallison,Code to convert Kansas's injection well data to a different format,,3,3,2,0,oil-and-gas,"[chemicals, fracking, oil-and-gas, pollution]",0.0
31,bysarmad,A-Tour-of-Oil-Industry-ML,,https://github.com/bysarmad/A-Tour-of-Oil-Industry-ML,https://api.github.com/repos/A-Tour-of-Oil-Industry-ML/bysarmad,"Machine Learning to predict share prices in the Oil & Gas Industry. In a nutshell, this is a quick introduction to understand the potential of data science and machine learning used in the oil industry. I have chosen to work with the stock price of a few oil companies (or oil service company) and the oil price data set as an example.","Machine Learning to predict share prices in the Oil & Gas Industry





In a nutshell, this is a quick introduction to understand the potential of data science and machine learning used in the oil industry. I have chosen to work with the stock price of a few oil companies (or oil service company) and the oil price dataset as an example. Just to be clear, the intention is not to provide an analysis of the current situation in the oil industry. The main objectives here are to show the potential and give you the tools to create your own view of the markets and apply it to other problems or even industries.

In the low price world, reducing costs, saving time and improving safety are crucial outcomes that can be benefited from using machine learning in oil and gas operations. Find below a quick list with a few of the applications of data analysis and machine learning in the Oil & Gas upstream industry:

Optimization of valve settings in smart wells to maximize NPV.
Machine Learning based rock type classification
Big data analysis on wells downtime.
Data-driven production monitoring.
Identifications of patterns using multiple variables during exploration phase.
Reservoir modelling and history matching using the power of pattern recognition.
Drilling rig automation.
Additional opportunities where gather large volumes of information in real-time or by using multiple analogs.
Deep learning to improve the efficiency and safety of hydraulic fracturing.
Provide more intelligence at the wellhead.
Integrated asset modeling optimization using machine-learning based proxy models.
I have tailored a quick exercise on how to use artificial intelligence using oil price and share price of a few companies. The notebook will focus on loading data and doing some illustrative data visualisations along the way. I will then use linear regression, cluster analysis and Random Forest to create a model predicting the share price in the short term.

I have been using data analysis and machine learning in different tasks in my current and previous jobs as a Reservoir Engineer. Excel is the most common tool for any type of analysis in the industry. However, it is certainly limited to basic data analysis, manipulation and to construct predictive models. We understand this conservative industry is sometimes a bit reluctant or delayed implementing modern analysis and predictive workflows to drive decisions. Also, due to the lack of investment in ""lower for longer"" oil prices and not many young engineers joining us, we are behind in the race implementing these techniques.

Firstly, lets start with what the list of tools required. The modern world of data science offers multiple ways of analyzing and predicting patterns. Python is the programming language used for this exercise. I personally use it under the umbrella of the Anaconda distribution. I will also be hoping to learn a lot from this exercise, so feedback is very welcome.

The main libraries that we will use are:

Numpy: Library for multidimensional arrays with high level mathematical functions to operate them
Pandas: Works on top of Numpy, offer a great way of manipulate and analyse data.
Matblotlib: Plotting and visualization.
Seaborn: Works on top of matplotlib to provide a high level interface for attractive plotting and visualization.
Scikit Learn: Libraries for Machine Learning. In this exercise we will use the following: Linear Regression, Random Forest Regression and K-means
For the purposes of this interactive quick guide, I will use these sets of data:

Oil price dataset from the U.S Energy Information administration.
Share price dataset from Yahoo Finance in a daily frequency from the following companies:
Shell (RDSB.L)

BP (BP.L)

Cairn Energy (CNE.L)

Premier Oil (PMO.L)

Statoil (STL.OL)

TOTAL (FP.PA)

ENGIE (ENGI.PA)

Schlumberger (SLB.PA)

REPSOL (REP.MC)

There are three parts that my workflow will cover:

Loading data and introduction to feature engineering

Data Analysis

Machine Learning and Prediction!

I will skip sections of the code below to keep the article in a reasonable size. If you want to see the entire code with explanations, I have created the following notebook.

Lets start coding!
",3,3,1,0,oil-and-gas,"[data-science, machinelearning, oil-and-gas]",0.0
32,viandika,volve_horizon_trajectory,,https://github.com/viandika/volve_horizon_trajectory,https://api.github.com/repos/volve_horizon_trajectory/viandika,Plotting well trajectory and horizon with volve dataset,"# Volve Horizon and Trajectory

This repository serves as a guide to create a plot seismic horizon surfaces and trajectory using python. In this example we use the open source Volve Dataset. 

A script file witsml.py is also added to show how to get trajectory data from WITSML files.",3,3,1,0,oil-and-gas,[oil-and-gas],0.0
33,pafox38,Automate-the-Geo-Stuff,,https://github.com/pafox38/Automate-the-Geo-Stuff,https://api.github.com/repos/Automate-the-Geo-Stuff/pafox38,Useful code for geoscientists in the energy sector.  More useful code will be added over time.  Special thanks to all those coders out there that helped to pave the way.,"# Automate the Geo Stuff
## Useful code for geoscientists / geologists in the energy sector
 * A workflow that illustrates how you can utilize public data to quickly determine what may be driving production performance for horizontal wells drilled in a particular reservoir.  In this example, we use a dataset containing over 4,000 horizontal wells targeting the Eagle Ford Shale.  Although the resulting regressor models aren't perfect, one can identify some key learnings that will help focus BD efforts or detailed reservoir models.
 * las curve mnemonic aliasing.
 * las API Change.  This program will parse through a folder full of las files and standardize API numbers and output new files with the new API number as the file name.  This makes bulk loading into G&G software much easier.  Additional if/elif and Exceptions can be added if other formats are discovered.
 * Violin plot to illustrate a cross-sectional view of wellbore targeting.  Simple gun barrel plots illustrate an idealized view of post-drill wellbore targeting.  The use of violin plots provides the user to communicate WHERE the wellbore traversed in adjacent formations/units.  Utilizes data exported from geosteering software. 
 * Using machine learning (Random Forest Regressor) to create synthetic log curves.  In this example, DTC.
 * Directory file search based on file extension.  Useful when looking for pesky las files.

## Copyright and Licensing
These programs and files are free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or any later version.

© 2021 pafox38
",3,3,1,0,oil-and-gas,"[data-science, data-visualization, geology, jupyter-notebook, machine-learning, oil-and-gas, python, scikit-learn, well-logs]",0.0
34,FreddyEcu-Ch,Oil-and-Gas-Resources,,https://github.com/FreddyEcu-Ch/Oil-and-Gas-Resources,https://api.github.com/repos/Oil-and-Gas-Resources/FreddyEcu-Ch,"In this repository, you will find resources related to the oil, gas and energy industry.","# Oil-and-Gas-Resources
In this repository, you can find resources related to the oil, gas, and energy industry.
",3,3,1,0,oil-and-gas,"[energy, oil-and-gas, petroleum-engineering, python]",0.0
35,antoniodagnino,Oil-Gas-Drilling-Activity-Prediction,,https://github.com/antoniodagnino/Oil-Gas-Drilling-Activity-Prediction,https://api.github.com/repos/Oil-Gas-Drilling-Activity-Prediction/antoniodagnino,"Drilling Activity Prediction: Oil and Gas operations are dramatically affected by supply, demand and several other factors that compromise the operational planning of resources. To overcome this challenge, predictive analytics could be applied to forecast rotary rig count inside United States using time-series data.","# Oil-Gas-Drilling-Activity-Prediction
Drilling Activity Prediction: Oil and Gas operations are dramatically affected by supply, demand and several other factors that compromise the operational planning of resources. To overcome this challenge, predictive analytics could be applied to forecast rotary rig count inside United States using time-series data.

The Project is divided into the following Jupyter Notebook Files:

1. Obtain Data from EIA
- Obtain data from EIA (Energy Information Administration API)
- Build DataFrame with data obtained from EIA


2. Exploratory Data Analysis
- Data Overview
- Reindex dataframe
- Oil Variables Feature Engineering
- Gas Variables Feature Engineering
- Select Features for forecasting model
- Final Pre-processing Activities
Note: Variables we want to Predict: **Oil Rig Count** and **Gas Rig Count** separately


3. Data Modeling

3.1 Data Modeling - VAR model
- Load datasets and resample to monthly frequency
- Use oil dataset to make predictions in oil rig count variable:
    - Train/test split
    - VAR Model Hyperparameter selection
    - Fit the model
    - Invert Transformation
    - Generate forecast values and plot results
    - Evaluation Metrics
    - Fit the model again but with 100% of the data
    - Forecast real future values
- Use Gas dataset to predict Gas rig count



3.2 Data Modeling - VARMA model
- Load datasets and resample to monthly frequency
- Use oil dataset to make predictions on Oil Rig Count:
    - Get second order differencing data frame (stationary features dataframe) 
    - Train/test split
    - VARMA Model Hyperparameter selection
    - Fit the model
    - Invert Transformation
    - Generate forecast values and plot results
    - Evaluation Metrics
    - Fit the model again but with 100% of the data
    - Forecast real future values
- Use gas dataset to make predictions on Gas Rig Count



3.3 Data Modeling - ARIMA model
- Import dataset and resample to monthly data
- Use oil dataset to predict oil rig count:
    - Find what hyperparameters to be used in the model (p,d,q)
    - Train/test split
    - Create ARIMA model and fit with data
    - Generate Forecast and compare against Test set
    - Evaluation Measures
    - Refit the model with 100% of data and make REAL Forecasting
 - Create ARIMA model with Gas dataframe to predict Gas Rig Count



3.4 Data Modeling - Holt-Winters
- Import dataset and resample to monthly data
- Use oil dataset to predict oil rig count:
    - Train/test split
    - Create Exponential Smoothing model and fit with data
    - Generate Forecast and compare against Test set
    - Evaluation Measures
    - Refit the model with 100% of data and make REAL Forecasting   
 - Create Exponential Smoothing model with Gas dataframe to predict Gas Rig Count


3.5 Data Modeling - Recurrent Neural Network (RNN)
- Load datasets and resample to monthly frequency
- Use oil dataset to make predictions in oil rig count variable:
- Tune Data: To adjust re-train models and check performances
- ETS Decomposition: Identify Trend, Seasonality and Error
- Train/test split
- Scale and Transform data
- TimeSeries Generator
- Create RNN for OIL DF
- EarlyStopping
- Evaluation Batch
- Inverse Transformed data
- Plot Test Set Vs Predictions
- Evaluation Metrics (Pending)
- Create Model with Gas dataset to make predictions in Gas Rig Counts
",3,3,1,0,oil-and-gas,"[forecasting, oil-and-gas, pandas, scikitlearn, statsmodels, time-series, time-series-forecasting]",0.0
36,gwallison,CAS_reference_list,,https://github.com/gwallison/CAS_reference_list,https://api.github.com/repos/CAS_reference_list/gwallison,Now incorporated into openFF-build.  Short code to translate set of text files from SciFinder to tables of CAS# and their names. Includes a synonym list.,,2,2,2,0,oil-and-gas,"[chemicals, fracfocus, oil-and-gas]",0.0
37,yohanesnuwara,pynodal,,https://github.com/yohanesnuwara/pynodal,https://api.github.com/repos/pynodal/yohanesnuwara,Nodal analysis using Python ,"# pynodal
Nodal analysis using Python 
",1,1,2,0,oil-and-gas,"[nodal-analysis, oil-and-gas]",0.0
38,ArslanKAS,Oil---Gas,,https://github.com/ArslanKAS/Oil---Gas,https://api.github.com/repos/Oil---Gas/ArslanKAS,Data Analytics on the Insights of Upstream and Downstream of Oil Production for Forecasting,"# Oil & Gas Analysis

![gas](https://github.com/ArslanKAS/Oil---Gas/blob/master/Oil_stream.png)
",1,1,2,0,oil-and-gas,"[analysis, gas, oil, oil-and-gas, price]",0.0
39,AOlang98,Predictive-Modelling,,https://github.com/AOlang98/Predictive-Modelling,https://api.github.com/repos/Predictive-Modelling/AOlang98,The main objective if this project is: To develop a Predictive Maintenance Model For Fuel Distribution Infrastructure in Kenya.,"# Predictive Modelling for fuel distribution infrastructure in Kenya using Random Forest classifier.

# Executive Summary
The main objective of this project is: To develop a Predictive Maintenance Model For Fuel Distribution Infrastructure in Kenya.

Having worked in the Oil and Gas Industry(3months attachment at KPC), I can confirm that Kenya sometimes faces losses during transportation and distribution of oil and gas products especially during storage, as the products arrive at the port of Mombasa, and during Midstream Operations, which include transportation via pipeline and oil tankers.

These losses may occur mainly due to poor maintenance of infrastructure or sometimes human error(accidents, etc)

For this reason, this model attempts to reduce the amount of losses that is faced by the Kenya Oil and gas sector and this will lead to massive savings in cost, paving way for the industry to explore more investment, to improve the Oil and Gas sector in the country.

Key findings:

- In this project, a column titled ‘Environmental Factors’ was included  which used a combination of KPIs such as Environment Impact score, Maintenance Cost Savings and Safety Improvement metrics to place get further insight into solving the problem statement.
- We can also realize that storage tanks have more leak detection systems compared to pipelines and tankers, which is a concern as the pipeline and tankers usually cover the most distance, and carry the most oil over time.
  
# Methodology
  
## Data Collection
Data for this predictive maintenance project was collected through a combination of web scraping techniques and self-recording of relevant information. The dataset consists of historical records related to the fuel distribution infrastructure, including pipelines, storage tanks, and transport vehicles. Key data sources included maintenance records, equipment performance metrics, and environmental conditions.

Some part of the data was collected during an attachment period at the Kenya Pipeline Company from August to October 2022.
[Kaggle was also one of my data sources] (https://www.kaggle.com/datasets)

## Data Cleaning and Preprocessing
Prior to model development, the collected data underwent a comprehensive preprocessing phase to ensure data quality and consistency. The preprocessing steps included the following:

- Handling Missing Values: Missing values in the dataset were addressed by filling numeric nulls with the mean of their respective columns and categorical nulls with the mode. This ensured that the dataset was complete and ready for analysis.

- Feature Engineering: Relevant features were selected and engineered from the raw data to create meaningful variables that could be used as inputs to the predictive model. This included transformations, aggregations, and the creation of derived features.

- Data Scaling: Numeric features were standardized to have a mean of 0 and a standard deviation of 1 to ensure that they had a consistent scale and distribution.

- Data Splitting: The dataset was divided into three subsets: a training set (70%), a validation set (15%), and a testing set (15%) for model development, hyperparameter tuning, and evaluation, respectively.

## Data Preprocessing - Feature Engineering
Three columns were generated in this project before one hot encoding was performed, as shown below:
- 'Maintenance_Interval' was generated as `df['Maintenance_Interval_Days']=df['Next_Maintenance_Date']-df['Last_Maintenance_Date']`. Shorter schedule may suggest more frequent maintenance and proactive upkeep while longer intervals may suggest deferred maintenance which would lead to increased risk of failure.
- 'Age_at_Inspection' was generated as `df['Age_at_Inspection'] = (df['Last_Maintenance_Date'] - pd.to_datetime('now')).dt.days / 365.25`. This will help provide insights into how the aging process impacts its conditions. Infrastructure that was older at inspection time may have experienced more wear and tear potentially requiring more maintenance and monitoring.
- 'Environmentaal_Factor_Range' was generated as `df['Environmental_Factor_Range'] = pd.cut(df['Environmental_Factor'], bins=bin_edges, labels=bin_labels)`. Categorized into low,medium and high to capture influence of environmental conditions on infrastructure health.

## Model Selection - Random Forest Classifier
For this predictive maintenance project, a Random Forest model was selected as the primary predictive model. Random Forest is an ensemble learning method that combines multiple decision trees to improve predictive accuracy and handle complex relationships within the data. The choice of Random Forest was based on its suitability for handling both categorical and numeric features, robustness against overfitting, and ability to provide feature importance scores.

The Random Forest model was trained on the training dataset and evaluated on the validation dataset. Hyperparameter tuning was performed to optimize the model's performance, including tuning parameters such as the number of trees, maximum tree depth, and minimum samples per leaf.

# Evaluation Metrics
The performance of the Random Forest predictive model was assessed using the following evaluation metrics:

- Confusion Matrix: The confusion matrix was used to calculate metrics such as accuracy, precision, recall, and F1-score. These metrics provide insights into the model's ability to correctly classify maintenance needs and equipment failures.
- Receiver Operating Characteristic (ROC) Curve: The ROC curve was used to visualize the model's ability to distinguish between true positive and false positive rates across different probability thresholds. The Area Under the ROC Curve (AUC-ROC) was used as a single numeric value to measure the model's overall predictive performance.

# Project Implementation and Integration.
Data-Driven Decision-Making: Your model is a valuable tool for data-driven decision-making within the company. It provides actionable insights based on historical data and real-time conditions.

- Scalability: The model can be scaled to cover larger areas or multiple facilities within the fuel distribution network. This scalability is beneficial for companies with extensive infrastructure.
- Environmental Impact: Timely maintenance reduces the risk of leaks and spills, minimizing the environmental impact of fuel distribution operations.
- Finally, to make this model applicable in real-world scenarios, it's important to work closely with stakeholders, such as maintenance teams, operations managers, and regulatory authorities which are concerned with the infrastructure, transportation and storage of oil and gas products in Kenya. These include Kenya Pipeline Company(KPC), Energy and Petroleum Regulatory Authority(EPRA) and the Ministry Of Energy And Petroleum at Large.

# Benefits and ROI
- Reduced Maintenance Costs:
Predictive maintenance allows for timely and targeted repairs, reducing overall maintenance costs by minimizing the need for emergency or unplanned maintenance.

- Decreased Downtime:
Proactive maintenance reduces equipment downtime, increasing operational efficiency and minimizing revenue loss associated with downtime.

- Improved Safety:
Timely maintenance and early detection of issues enhance safety by preventing accidents, leaks, and environmental hazards, reducing potential legal and reputational risks.

- Extended Equipment Lifespan:
Predictive maintenance helps extend the lifespan of equipment, reducing the need for frequent replacements and capital expenditures.

- Optimal Resource Allocation:
Efficient maintenance scheduling and resource allocation result in cost savings and reduced wastage of resources.

- Regulatory Compliance:
Meeting regulatory and environmental compliance requirements helps avoid penalties and legal consequences.

- Enhanced Customer Satisfaction:
Reduced service disruptions lead to improved customer satisfaction and loyalty.

- Environmental Impact:
Minimizing leaks and spills reduces the environmental impact, enhancing the company's environmental stewardship.

# Conclusions
- Improved Maintenance Efficiency and Cost Savings:
The implementation of predictive maintenance has demonstrated a significant improvement in maintenance efficiency. By proactively identifying maintenance needs and addressing them in a timely manner, the project has achieved substantial cost savings in terms of reduced maintenance expenses, minimized equipment downtime, and optimized resource allocation.

- Enhanced Safety and Environmental Impact:
The project has had a positive impact on safety within the fuel distribution infrastructure. By identifying and addressing potential safety risks early, it has contributed to a safer working environment for personnel and reduced the likelihood of accidents, leaks, and environmental incidents. The model's ability to minimize environmental impact by preventing spills and emissions underscores its importance in ensuring regulatory compliance.

- Optimized Resource Allocation and Inventory Management:
Predictive maintenance has enabled the organization to allocate resources more efficiently. The optimized maintenance scheduling and inventory management have resulted in a reduction in excess inventory costs while ensuring that the necessary spare parts are available when needed. This has improved overall resource utilization.

- Data-Driven Decision-Making and Continuous Improvement:
The project underscores the importance of data-driven decision-making in maintenance practices. By leveraging historical and real-time data, the organization can make informed decisions about maintenance priorities and resource allocation. The model's feedback loop allows for continuous improvement, ensuring that maintenance strategies remain effective and adaptable to changing conditions



















",1,1,1,0,oil-and-gas,"[infrastructure, oil-and-gas, petroleum-engineering]",0.0
40,gwallison,WV_production_formatter,,https://github.com/gwallison/WV_production_formatter,https://api.github.com/repos/WV_production_formatter/gwallison,Reformat fracking production numbers from West Virginia to a format more useful to GIS,,1,1,2,0,oil-and-gas,"[formatter, fracking, oil-and-gas]",0.0
41,rensalzado,DiluentESPOptimization,,https://github.com/rensalzado/DiluentESPOptimization,https://api.github.com/repos/DiluentESPOptimization/rensalzado,"NTNU Master Thesis, Energy and Process Engineering Department, Specialization: Industrial Process Technology","# DiluentESPOptimization
TEP4905 – NTNU, Energy and Process Engineering Department, Master Thesis

Specialization: Industrial Process Technology
 
 The objective of the master thesis was to develop a numerical model from a single-phase, vertical oil well
 in stationary state to study the effects of dilution to improve oil production. The model has the option to 
 include a electrical submersible centrifugal pump (ESP). The model was developed using an object-oriented
 programming approach to increase portability and usability.
 
 This reposity includes:
 - Classes codes
 - Technical reports providing insights from different areas of the model 
",1,1,1,0,oil-and-gas,"[matlab, matlab-oop, modelling, ntnu, oil-and-gas, oop, oop-in-matlab]",0.0
42,chews0n,glowing-waffle,,https://github.com/chews0n/glowing-waffle,https://api.github.com/repos/glowing-waffle/chews0n,SPE Calgary Data Science Mentorship program 2021,"# Glowing Waffle

## Introduction
This project is an undertaking for the 2021 SPE Calgary Data Science Mentorship Program. The following people were involved in this project:

- Chris Hewson - Resfrac
- Harrison Wood - Pason
- Pierce Anderson - ARC Resources
- Brendan Danyluik - CalFrac Well Services

The goal of this project was to create model that accurately predicts production values from Montney Basin wells, given a certain fracture design.

## Install
This project requires [Python 3](https://www.python.org/) and the following Python Libraries:

- [NumPy](https://numpy.org/)
- [Pandas](https://pandas.pydata.org/)
- [matplotlib](https://matplotlib.org/)
- [scikit-learn](https://scikit-learn.org/stable/)
- [CatBoost](https://catboost.ai/docs/installation/python-installation-method-pip-install.html#python-installation-method-pip-install)
- [datetime](https://pypi.org/project/DateTime/)
- [requests](https://pypi.org/project/requests/)
- [regex](https://pypi.org/project/regex/)
- [zipfile36](https://pypi.org/project/zipfile36/)

All of these can be installed from the requirements.txt file in the repository using the following command:

    pip install -r requirements.txt

To install the package, run the setup.py file, python 3.5 and above is required

    python setup.py install

## Usage
glowing-waffle can be used as a python package or as a standalone executable. The executable requires certain inputs in order to run that have default values.

Once installed, glowing-waffle can be used as a stand alone executable, the following command will display the usage of the standalone:
```shell script
glowingwaffle -h
```
Additionally, it should be noted that the input file required to predict IP90/IP180 for a particular well should follow the same format as the [template](templates/input_values_template.csv) within this repository. Please note that a lot of text based inputs require numbers to map to the enums, this template should be made more user friendly in the future.


## Model
Using a random forest model implemented in the CatBoost library, we will gather the necessary features and targets from public petroleum data sources for the Montney (using SQL/REST calls). 

Uncertainty can be applied by using separate sets to train the model(s). By acquiring different models using slightly different data sets, we will be able to more quantifiably capture uncertainty and the bounds of the outputs obtained from the model(s).

Once the model is trained, we will test the accuracy and predictability of the model using both a subset of the data that we have set aside and in comparison to a reservoir and fracture simulator. This will give us a good idea of the dependability of the model that has been created.

## Data
Publicly available production and well data was used for the Montney Basin in order to accurately train the model. This choice was made in order for the model to have wide applicability throughout the oil and gas industry.

### Features
This is a working list of features and will be modified as the project evolves:

#### Frac Design

1. Proppant per metre (or per stage)
2. Total Fluid
3. Fluid per metre (or per stage)
4. Total Proppant
5. Proppant concentration (max, or average?)
6. Average injection rate
7. Average treating pressure
8. Average frac gradient
9. Energizer (N2, CO2)
10. Frac type
11. Tonnage per meter
12. Frac fluid - type and composition (if available)

NB: Averages can skew the data significantly, make sure that you're using the right average

#### Well Design

1. Length of well
2. Stage spacing
    1. Cluster spacing and shots per cluster
3. Latitude, longitude and depth/TVD
4. Parent-Child effects - ie. distance from parent well

#### Geology

1. Initial flow back
2. Rate of Flow Back
3. Leak off rate
4. Rock properties (perm, poro, stresses)
5. Reservoir Pressure
6. Saturations 

### Targets

This is a working list of features and will be modified as the project evolves:

1. Initial Production in the first 90 days (IP90)
2. Initial Production in the first 180 days (IP180)

## License
Glowing Waffle is licensed under the Apache License, Version 2.0. See LICENSE for the full license text.",1,1,3,3,oil-and-gas,"[data-science, machine-learning, oil-and-gas]",0.0
43,AyadiGithub,Brent-Crude-Oil-Price-Anomaly-Detection-in-Keras,,https://github.com/AyadiGithub/Brent-Crude-Oil-Price-Anomaly-Detection-in-Keras,https://api.github.com/repos/Brent-Crude-Oil-Price-Anomaly-Detection-in-Keras/AyadiGithub,Brent crude oil Price Anomaly Detection in Time Series Data with Keras,"# Brent-Crude-Oil-Price-Anomaly-Detection-in-Keras

## Brent crude oil price Anomaly Detection in Time Series Data with Keras

## Project Objective:
- Design and Build an LSTM Autoencoder in Keras
- Detect anomalies (sudden price changes) in Brent Crude Oil Price
- Data range: 1988 - 2020

The Dataset is a time series with daily Price changes for Brent Crude Oil from 1988 to 2020. 
Brent Crude is the International Benchmark for Crude oil. 

Source of Dataset: Yahoo Finance

## The dataset is a CSV format with: 
1. Daily timestamp
2. Daily Open and Close Price
3. Daily High and Low Price
4. Volume
5. 24h Change %

## The Project will follow the below steps:
1. Project Overview and Import Libraries
2. Load and Inspect Brent Crude Oil Price History
3. Data Preprocessing
4. Temporalize Data and Create Training and Test Splits
5. Build a CuDNNLSTM Autoencoder for a CuDNN capable GPU and/or LSTM for CPU
6. Train the Autoencoder (GPU or CPU)
7. Plot Metrics and Evaluate the Model
8. Detect Anomalies in the Brent Crude Oil Price History
9. Visualize the Anomalies based on threshold

## The required libraries:
- Numpy
- Tensorflow 2.0+
- Keras
- Pandas
- Seaborn
- Matplotlib
- Plotly
- Scikit Learn

## Other requirements
- CUDA Capable GPU (For CuDNN LSTM)
- CuDNN (For CuDNN LSTM)
- CUDA 10.1
- Tensorflow-gpu 
- Keras-gpu
",1,1,1,0,oil-and-gas,"[anomaly-detection, cudnnlstm, data-science, deep-learning, keras, lstm-neural-networks, machine-learning, oil-and-gas, tensorflow]",0.0
44,ACRRL,RGB-Spectrometer,ACRRL,https://github.com/ACRRL/RGB-Spectrometer,https://api.github.com/repos/RGB-Spectrometer/ACRRL,"In this code, RGB spectrum is generated using Arduino",,1,1,1,0,oil-and-gas,"[acrrl, ai, mechatronic-systems, oil-and-gas, paper, power, rgb, shiraz, spectrometer, system, university, university-project]",0.0
45,FreddyEcu-Ch,Machine-Learning,,https://github.com/FreddyEcu-Ch/Machine-Learning,https://api.github.com/repos/Machine-Learning/FreddyEcu-Ch,Web App to apply EOR methods Screening,"# Machine-Learning WebApp
",1,1,1,0,oil-and-gas,"[eor, machine-learning, oil-and-gas, reservoir-engineering]",0.0
46,gwallison,ohio_production_to_FT_format,,https://github.com/gwallison/ohio_production_to_FT_format,https://api.github.com/repos/ohio_production_to_FT_format/gwallison,Jupyter notebook to reformat Ohio Oil&Gas production numbers to a version for easier GIS use.,,1,1,2,0,oil-and-gas,"[format-converter, fracking, oil-and-gas]",0.0
47,OmarZOS,deep-learning-at-scale,,https://github.com/OmarZOS/deep-learning-at-scale,https://api.github.com/repos/deep-learning-at-scale/OmarZOS,This repository contains the necessary scripts for oil production flow prediction models that make use of spark's MLlib,,1,1,1,1,oil-and-gas,"[cnn-keras, hadoop-hdfs, lstm-neural-networks, oil-and-gas, oil-wells, spark]",0.0
48,Lamy237,Volve-field,,https://github.com/Lamy237/Volve-field,https://api.github.com/repos/Volve-field/Lamy237,Applying Big Data and associated technologies with Volve dataset,"# Volve-field

### Description
This repository is an attempt to apply Big Data and associated technologies with the **Volve** data set.

---

### ⚙ Open Source Products

- [FIPRAN](https://fipran.streamlit.app/): A web application for real-time monitoring of oil and gas production

## 🎓 Acknowledgements
- Equinor ASA
- Equinor Volve team
- Volve license partners: ExxonMobil E&P Norway and Bayern gas Norge
",1,1,1,0,oil-and-gas,"[data-analysis, jupyter-notebook, oil-and-gas, python, volve]",0.0
49,sulaihasubi,tokenization-spaCy,,https://github.com/sulaihasubi/tokenization-spaCy,https://api.github.com/repos/tokenization-spaCy/sulaihasubi,🌶 A tokenizer for oil and gas documents @sulaihasubi,"Tokenization with spaCy
========================
[![Built with spaCy](https://img.shields.io/badge/made%20with%20❤%20and-spaCy-09a3d5.svg)](https://spacy.io)
<a href=""https://github.com/sulaihasubi/tokenization-spaCy/commits/master"">
        <img alt=""Last commit"" src=""https://img.shields.io/github/last-commit/sulaihasubi/tokenization-spaCy"">
</a>


[spaCy](https://github.com/explosion/spaCy) is a library for advanced Natural Language Processing in Python and Cython. It's built on the very latest research, and was designed from day one to be used in real products. (Source: [spaCy](https://github.com/explosion/spaCy))

Introduction
------------
Tokenizer is a tool that splits a given text into tokens using regular expressions. The process of tokenizing a text into segments of words and punctuation in Spacy is done in several processes. It reads text from left to right. To begin, the tokenizer separated the text on whitespace in the same way that the split() method does. The tokenizer then checks to see whether the substring fits the tokenizer exception rules.

## [RiseHill Data Analysis's](https://rdagroups.com/idi-platform/) SaaS Product: Integrated Data Intelligence (IDI)
Application of NLP (cleaned texts) for instant browsing of documents from millions of files less than a second that retrieve and full traceability to original documents.


![Alt Text](https://github.com/sulaihasubi/tokenization-spaCy/blob/main/result/idi.png)

## ⏳ Install spaCy

For detailed installation instructions and other operating system, can be refer
[here](https://spacy.io/usage).

- **Operating system**: macOS / OS X · Linux · Windows (Cygwin, MinGW, Visual
  Studio)
- **Python version**: Python 3.6+ (only 64 bit)
- **Package managers**: [pip] · [conda] (via `conda-forge`)

[pip]: https://pypi.org/project/spacy/
[conda]: https://anaconda.org/conda-forge/spacy

Installation for macOS (my operating system):

```
# Download best-matching version of specific model for your spaCy installation
python -m spacy download en_core_web_sm

pip install -U pip setuptools wheel
pip install -U spacy
python -m spacy download en_core_web_sm
```
## 📦 Loading & Using Models
To load a model, use [`spacy.load()`](https://spacy.io/api/top-level#spacy.load)
with the model name or a path to the model data directory.

```python
import glob
import spacy

nlp = spacy.load(name='en_core_web_sm')
```

Define your local path where you put the text files.
```python
path = '/Users/risehill/Documents/09-NLPProcessing/TextExtracted/steeples1998.txt'
```
## 👾 Let's get through the Codes!
```python
for file in glob.glob(path):
    with open(file, encoding='utf-8', errors='ignore') as file_in:
        text = file_in.read()
```
In this line of code, the text in paragraph converted into one string everytime it is found any new line
```python
# Replace the newline as one string
 Newtext = text.replace('\n', ' ')
# Check number of text contents in original Text files
 print(len(Newtext))
```

Check the tokenisation details:
```python
text_cleans.append(CleanText)
        print('-----------------------------------------------------------------------------------------------------------')
        print('CHECK TOKENISATION DETAILS')
        print('-----------------------------------------------------------------------------------------------------------')
        # Checking the details of the tokens
        for line in lines:
            line = nlp(line)
            for token in TextCleaned:
                my_doc_cleaned = [token for token in line if not token.is_stop and not token.is_punct and not token.is_space]
                analyzeToken ='{:<5}{:<15}{:<15}{:<15}{:<15}{:<15}'.format(token.i, token.idx, token.text_with_ws, token.is_space, token.is_punct, token.is_stop,)
                analyzeToken2 = '{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}'.format(token.i, token.text, token.is_alpha, token.shape_,  token.is_ascii, token.is_digit)
                analyzeToken3 = '{:<5}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}'.format(token.i, token.text, token.like_num,
                                                                                   token.like_url, token.like_email,
                                                                                   token.is_ascii, token.is_digit)
                analyzeToken4 = '{:<5}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}'.format(token.i, token.text, token.is_left_punct,
                                                                                   token.is_right_punct, token.is_bracket,
                                                                                   token.is_quote, token.is_currency)
                analyzeToken5 = '{:<15}{:<15}{:<15}'.format(token.i, token.text, token.text.istitle())
                print(analyzeToken)
```

The program output will be like this:

    8403 44431          fold           0              0              0              
    8407 44446          misleading     0              0              0              
    8412 44467          cases          0              0              0              
    8415 44478          fold           0              0              0              
    8416 44483          level          0              0              0              
    8418 44493          drop           0              0              0              
    8423 44515          data           0              0              0              
    8428 44534          shallowest     0              0              0              
    8429 44545          parts          0              0              0              
    8432 44556          section        0              0              0              
    8435 44571          equivalent     0              0              0              
    8442 44603          common         0              0              0              
    8444 44610          offset         0              0              0              
    8445 44617          section        0              0              0              
    8449 44635          S              0              0              0  
 
Removed words that contain Nouns, Verb, Adj, Adverbs, or Proper Names:
```python
# Remove all the words that are  not nouns, verbs, adj, adverbs, or proper names
        excluded_tags = {""NOUN"", ""VERB"", ""ADJ"", ""ADV"", ""ADP"", ""PROPN""}
        TextCleaned1 = [token for token in textAnalysis if excluded_tags]
        print(len(TextCleaned1))
        print(TextCleaned1)
```
Removed Stopwords and punctuation from the text:
```python
# Removed Stopwords and punctuation from the text : 5247
TextCleaned = [token for token in textAnalysis if not token.is_stop and not token.is_punct
                       and not token.is_space and not token.is_digit
                       and not token.like_num and not token.like_url and not token.like_email and token.is_alpha]
DetectTitle = [token for token in textAnalysis if not token.text.istitle()]
print(len(DetectTitle))
print(DetectTitle)
```
",1,1,1,0,oil-and-gas,"[natural-language-processing, nlp, oil-and-gas, python, python3, spacy, tokenization, tokenizer]",0.0
50,HydraIndustriesOfficial,EnviroGas,,https://github.com/HydraIndustriesOfficial/EnviroGas,https://api.github.com/repos/EnviroGas/HydraIndustriesOfficial,The EnviroGas app and database are being designed to track and monitor CO2 and methane emissions in the oil and gas sector.,"# EnviroGas

EnviroGas is an interactive app and database that is being developed to visualize CO2 and methane emissions in the oil and gas sector. The project aims to promote decarbonization initiatives, raise awareness, drive transparency, and inspire positive change within the industry.

## Project Description

EnviroGas will offer comprehensive data visualization of emissions from various sources, including flare booms, oil and gas operations, transportation, and refining. It will feature interactive charts, graphs, and maps for better insights into emissions patterns, hotspots, and trends across different sectors, companies, and regions. Users will be able to use comparative analysis tools to benchmark emissions performance and identify areas for improvement.

The project will also highlight best practices and success stories to inspire companies within the oil and gas industry to take proactive steps toward decarbonization. The impact assessment feature will illustrate equivalent emissions in terms of global warming potential, air quality deterioration, and climate change impact. Moreover, users will have access to educational resources, including articles, videos, and infographics, to help them understand the complexities and challenges of transitioning to a low-carbon future. The app will also provide industry news, reports, and policy changes related to decarbonization efforts within the oil and gas sector.

The project is currently a work in progress and welcomes contributions from the community to refine and enhance its features.

## Installation Guide

To install the app and database, follow these steps:

1. Clone the repository from Github.
2. Install the required dependencies.
3. Run the app using the command line interface.

## Usage Guide

To use the app and database, follow these steps:

1. Open the app in your preferred web browser.
2. Navigate through the different sections of the app.
3. Use the interactive charts, graphs, and maps to gain insights into emissions patterns, hotspots, and trends.
4. Use the comparative analysis tools to benchmark emissions performance and identify areas for improvement.
5. Use the impact assessment feature to understand the equivalent emissions in terms of global warming potential, air quality deterioration, and climate change impact.

## Contribution Guide

To contribute to the project, follow these steps:

1. Fork the repository from Github.
2. Make changes to the code or documentation.
3. Submit a pull request with a detailed description of the changes made.
4. Follow the project's guidelines for bug reporting, feature requests, and pull requests.

## Project Checklist

### Completed Tasks

- [x] Project ideation and concept development.
- [x] Finalized app name as ""EnviroGas.""
- [x] Created initial README file.
- [x] Established project repository on GitHub.
- [x] Defined project description and goals.
- [x] Added licensing information.

### To-Do Tasks
- [ ] Designed data visualization features for CO2 and methane emissions.
- [ ] Implemented interactive charts, graphs, and maps for emissions visualization.
- [ ] Developed comparative analysis tools for benchmarking emissions performance.
- [ ] Integrated impact assessment feature to showcase environmental consequences.
- [ ] Added educational resources section for decarbonization content.
- [ ] Included industry news and updates section.
- [ ] Provided installation guide for local setup.
- [ ] Created usage guide for app navigation and features.
- [ ] Developed contribution guide for community involvement.
- [ ] Included contact information for project team.
- [ ] Refine user interface for enhanced user experience.
- [ ] Implement user authentication and user-specific features.
- [ ] Add additional data sources for comprehensive emissions tracking.
- [ ] Enhance data visualization with more interactive elements.
- [ ] Incorporate machine learning algorithms for emissions prediction.
- [ ] Perform comprehensive testing and debugging.
- [ ] Optimize app performance and ensure scalability.
- [ ] Conduct user feedback sessions and make necessary improvements.
- [ ] Explore partnerships with industry stakeholders for data collaboration.
- [ ] Continuously update educational resources and industry news.
- [ ] Improve documentation and code commenting for better understanding.
- [ ] Prepare for public release and deployment.
- [ ] Plan marketing and promotional strategies to reach target audience.
- [ ] Collaborate with environmental organizations for potential partnerships.


",1,1,1,10,oil-and-gas,"[carbon-emissions, co2, decarbonization, methane, oil, oil-and-gas]",0.0
51,m4theus4ndr4de,regression-oil-production-prediction,,https://github.com/m4theus4ndr4de/regression-oil-production-prediction,https://api.github.com/repos/regression-oil-production-prediction/m4theus4ndr4de,Production prediction is one of the core problems in a company. The provided dataset is a set of nearby wells located in the United States and their 12 months cumulative production. The company data scientist needs to build a model from scratch to predict production.,"
<img src=""image/extraction.jpg"" alt=""logo"" style=""zoom:100%;"" />

<h1>Oil Production Prediction</h1>

<p align=""justify"">This is a fictional project for studying purposes. The business context and the insights are not real.

<h2>1. Description of the Business Problem</h2>

<p align=""justify"">Production prediction is one of the core problems in a company. The provided dataset is a set of nearby wells located in the United States and their 12 months cumulative production. The company needs a production prediction model to serve as one of the tools to support the company decisions. So, the company data scientist needs to build a model from scratch to predict production and show the manager that the model can perform well on unseen data.</p>

<h3>The tools that were created:</h3>

<p align=""justify""><b>Machine Learning Regression Model: </b>Using the dataset provided by the company. A machine learning regression model was created to be used for future predictions.</p> The notebook used to create the model is available <a href=""https://github.com/m4theus4ndr4de/regression-oil-production-prediction/blob/main/notebook/production_prediction.ipynb"" target=""_blank"">here</a>.</p>

<p align=""justify""><b>Streamlit App for Production Prediction: </b>The model is available on the Streamlit Cloud and can be used through the Streamlit App created. The App is available <a href=""https://m4theus4ndr4de-regression-oil-production--prediction-app-lhyr2y.streamlit.app/"" target=""_blank"">here</a>.</p>

<h2>2. Dataset Attributes</h2>

<table style=""width:100%"">
<tr><th>Attribute</th><th>Description</th></tr>
<tr><td>treatment company</td><td>The treatment company who provides treatment service.</td></tr>
<tr><td>azimuth</td><td>Well drilling direction.</td></tr>
<tr><td>md (ft)</td><td></td></tr>
<tr><td>tvd (ft)</td><td>True vertical depth.</td></tr>
<tr><td>date on production</td><td>First production date.</td></tr>
<tr><td>operator</td><td>The well operator who performs drilling service.</td></tr>
<tr><td>footage lateral length</td><td>Horizontal well section.</td></tr>
<tr><td>well spacing</td><td>Distance to the closest nearby well.</td></tr>
<tr><td>porpoise deviation</td><td>How much max (in ft.) a well deviated from its horizontal.</td></tr>
<tr><td>porpoise count</td><td>How many times the deviations (porpoises) occurred.</td></tr>
<tr><td>shale footage</td><td>How much shale (in ft) encountered in a horizontal well.</td></tr>
<tr><td>acoustic impedance</td><td>The impedance of a reservoir rock (ft/s * g/cc).</td></tr>
<tr><td>log permeability</td><td>The property of rocks that is an indication of the ability for fluids (gas or liquid) to flow through rocks.</td></tr>
<tr><td>porosity</td><td>The percentage of void space in a rock.</td></tr>
<tr><td>poisson ratio</td><td>Measures the ratio of lateral strain to axial strain at linearly elastic region.</td></tr>
<tr><td>water saturation</td><td>The ratio of water volume to pore volume.</td></tr>
<tr><td>toc</td><td>Total Organic Carbon, indicates the organic richness (hydrocarbon generative potential) of a reservoir rock.</td></tr>
<tr><td>vcl</td><td>The amount of clay minerals in a reservoir rock.</td></tr>
<tr><td>p-velocity</td><td>The velocity of P-waves (compressional waves) through a reservoir rock (ft/s).</td></tr>
<tr><td>s-velocity</td><td>The velocity of S-waves (shear waves) through a reservoir rock (ft/s).</td></tr>
<tr><td>youngs modulus</td><td>The ratio of the applied stress to the fractional extension (or shortening) of the reservoir rock parallel to the tension (or compression) (giga pascals).</td></tr>
<tr><td>isip</td><td>When the pumps are quickly stopped, and the fluids stop moving, these friction pressures disappear and the resulting pressure is called the instantaneous shut-in pressure, ISIP.</td></tr>
<tr><td>breakdown pressure</td><td>The pressure at which a hydraulic fracture is created/initiated/induced.</td></tr>
<tr><td>pump rate</td><td>The volume of liquid that travels through the pump in a given time.</td></tr>
<tr><td>total number of stages</td><td>Total stages used to fracture the horizontal section of the well.</td></tr>
<tr><td>proppant volume</td><td>The amount of proppant in pounds used in the completion of a well (lbs).</td></tr>
<tr><td>proppant fluid ratio</td><td>The ratio of proppant volume/fluid volume (lbs/gallon).</td></tr>
<tr><td>production</td><td>The 12 months cumulative gas production (mmcf).</td></tr>
</table>

<h2>3. Solution Strategy</h2>

<ol>
<li>Understand the Business problem.</li>
<li>Clean the dataset removing outliers, NA values and unnecessary features.</li>
<li>Explore the data to create hypothesis, think about a few insights and validate them.</li>
<li>Prepare the data to be used by the modeling algorithms encoding variables, splitting train and test dataset and other necessary operations.</li>
<li>Create the models using machine learning algorithms.</li>
<li>Evaluate the created models to find the one that best fits to the problem.</li>
<li>Tune the model to achieve a better performance.</li>
<li>Deploy the model in production so that it is available to other people.</li>
<li>Find possible improvements to be explored in the future.</li>
</ol>

<h2>4. The Insights</h2>

<p><b>I1:</b> Wells with a greater number of stages produce more,</p>
<p><b>True:</b> This relationship doesn't apply for all values of total number of stages, but it tends to be true.</p>
<p><b>I2:</b> Wells that started producing longer ago produce less.</p>
<p><b>True:</b> Productions from newer wells are better.</p>
<p><b>I3:</b> Wells that are farther from the others produce more.</p>
<p><b>False:</b> The production doesn't increase according to the distance from other wells.</p>
<p><b>I4:</b> Wells in which more proppant were used produce more.</p>
<p><b>True:</b> More proppant indicates a greater production.</p>
<p><b>I5:</b> Wells in which the rocks have higher values of porosity produce more.</p>
<p><b>False:</b> More porosity does not mean more production.</p>

<h2>5. Machine Learning Modeling</h2>

<p align=""justify"">The final result of this project is a regression model. Therefore, some machine learning models were created. So, 7 models were created, Linear Regression, Lasso, SVM, Random Forest, XGBoost, LightGBM and CatBoost.

Boruta (feature selection algorithm) was used to select features for the model and 11 features were selected to the final model. The models were evaluated considering three metrics, Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE) and Root Mean Squared Error (RMSE). The initial models performances are in the table below.</p>

<p align=""justify""></p>

<table style=""width:100%"">
<tr><th>Model Name</th><th>MAE</th><th>MAPE</th><th>RMSE</th></tr>
<tr><td>CatBoost</td><td>502.93</td><td>0.2817</td><td>781.34</td></tr>
<tr><td>LightGBM</td><td>522.03</td><td>0.2936</td><td>806.55</td></tr>
<tr><td>XGBoost</td><td>535.10</td><td>0.3094</td><td>813.48</td></tr>
<tr><td>Random Forest</td><td>564.38</td><td>0.3281</td><td>852.23</td></tr>
<tr><td>SVM</td><td>648.01</td><td>0.4468</td><td>931.77</td></tr>
<tr><td>Linear Regression</td><td>679.33</td><td></td><td>1012.51</td></tr>
<tr><td>Lasso</td><td>1018.08</td><td>0.4259</td><td>1396.98</td></tr>
</table>

<h2>6. Final Model</h2>

<p align=""justify"">To decide which would be the final model, a cross-validation was carried out to evaluate the performance of the algorithms in a more robust way. These metrics are represented in the table below.</p>

<table style=""width:100%"">
<tr><th>Model Name</th><th>MAE</th><th>MAPE</th><th>RMSE</th></tr>
<tr><td>Linear Regression</td><td>687.8 +/- 49.40</td><td>0.49 +/- 0.04</td><td>974.12 +/- 90.88</td></tr>
<tr><td>Lasso</td><td>1023.65 +/- 61.45</td><td>0.89 +/- 0.06</td><td>1348.19 +/- 96.97</td></tr>
<tr><td>SVM</td><td>651.62 +/- 28.27</td><td>0.51 +/- 0.06</td><td>897.34 +/- 60.87</td></tr>
<tr><td>Random Forest</td><td>521.82 +/- 26.99</td><td>0.36 +/- 0.02</td><td>768.7 +/- 74.63</td></tr>
<tr><td>XGBoost</td><td>526.78 +/- 14.36</td><td>0.35 +/- 0.02</td><td>773.11 +/- 52.73</td></tr>
<tr><td>LightGBM</td><td>525.71 +/- 31.97</td><td>0.34 +/- 0.02</td><td>767.4 +/- 58.25</td></tr>
<tr><td>CatBoost</td><td>490.18 +/- 16.5</td><td>0.32 +/- 0.02</td><td>724.79 +/- 54.17</td></tr>
</table>

<p align=""justify"">As the table presents, the Catboost model was the best one and was chosen to be deployed. After choosing which would be the final model, a random search hyperparameter optimization algorithm was used to improve the performance of the model. The final model evaluation metrics are in the table below.</p>

<table style=""width:100%"">
<tr><th>Model Name</th><th>MAE</th><th>MAPE</th><th>RMSE</th></tr>
<tr><td>CatBoost Tuned</td><td>485.66 +/- 23.01</td><td>0.32 +/- 0.02</td><td>714.4 +/- 64.6</td></tr>
</table>

<h2>7. Conclusion</h2>

<p align=""justify"">Although the dataset has many features, it is small and has a significant amount of missing values. The model presented a larger error than expected, this problem could be circumvented with a larger amount of data. Using the app, other people can easily make predictions just setting the values and pressing the prediction button.</p>

<h2>8. Future Work</h2>

<ul>
<li>Find a better way to replace missing values.</li>
<li>Find the best way of dealing with the outliers.</li>
<li>Search for models that could perform better with this small dataset.</li>
<li>Try some dimensionality reduction algorithm to improve the model prediction capabilities.</li>
<li>Improve the <a href=""https://m4theus4ndr4de-regression-oil-production--prediction-app-lhyr2y.streamlit.app/"" target=""_blank"">Streamlit app</a> adding more functions.</li>
</ul>
",1,1,1,0,oil-and-gas,"[catboost, catboostregressor, data-science, machine-learning, oil-and-gas, petroleum, regression]",0.0
52,gwallison,FF-cleanup-test,,https://github.com/gwallison/FF-cleanup-test,https://api.github.com/repos/FF-cleanup-test/gwallison,NOTE: This project has been superceded by FF-POC.  Creating a clean and searchable data set from the FracFocus collection of chemical disclosures.  ,,1,1,2,0,oil-and-gas,"[chemicals, environmental-monitoring, fracfocus, fracking, oil-and-gas]",0.0
53,gwallison,OK_injection_wells_formatting,,https://github.com/gwallison/OK_injection_wells_formatting,https://api.github.com/repos/OK_injection_wells_formatting/gwallison,Code to translate Oklahoma's state data on injection wells to a different format,,1,1,2,0,oil-and-gas,"[chemicals, formatting, fracking, oil-and-gas, pollution]",0.0
54,damianlujep,Hydraulic_Pump_Design_React,,https://github.com/damianlujep/Hydraulic_Pump_Design_React,https://api.github.com/repos/Hydraulic_Pump_Design_React/damianlujep,User interface which collects all the necessary parameters to design a hydraulic pump for the Oil & Gas industry. ,"# Hydraulic Pump Design with React

Project developed with React and Material-UI from scratch. 
User interface which collects all the necessary parameters to design a hydraulic pump for the Oil & Gas industry. 

This project is in continuous development process.

### HPD API:
The application consumes an API created with **Java 17**, the Spring Framework (**Spring Boot 2.5.3**) and Hibernate,
which hosts a remote **MariaDB** database.

It exposes Spring Data Repositories over REST via **Spring Data REST**.

#### Security:
The API is protected by **Spring Security** and uses **JSON Web Tokens** in the HTTP responses for a secure connection 
with the user interface.

### Deployment:
This project has been deployed with Docker in an Ubuntu 20 server.

### Technologies and tools:
`The app was developed using:` Webstorm, GitHub, React, Redux, React Router, React Swipeable Views, Material-UI,
Recharts.

`The API was developed using:` IntelliJ, GitHub, Maven, Spring Boot, Spring Data REST, Spring Security, Jasyp, 
JSON Web Tokens, MariaDB, Spring Data JPA, Hibernate, JUnit5, Mockito Lombok, Jackson, Swagger UI.

### How to try the app:
[CLICK HERE](https://hpd-app.phi-rms.com/) **to visit the live application.**

**Use the credentials to login:**
`user: tester` , 
`password: Apptester.2022`

[CLICK HERE](https://drive.google.com/file/d/1BBrfyxMSa7QgC_43133NdnROsQpTmOMn/view?usp=sharing) **to download a project example file**

Select **Upload From File**, and upload the previously downloaded file:

![](public/create-project-options.png)

## Available Scripts

In the project directory, you can run:

### `npm start`

Runs the app in the development mode.\
Open [http://localhost:3000](http://localhost:3000) to view it in the browser.

The page will reload if you make edits.\
You will also see any lint errors in the console.

### `npm run build`

Builds the app for production to the `build` folder.\
It correctly bundles React in production mode and optimizes the build for the best performance.

The build is minified and the filenames include the hashes.\

",1,1,1,0,oil-and-gas,"[jwt-tokens, materila-ui, oil-and-gas, react, redux]",0.0
55,swoyam2609,Oil-and-Gas-Price-Analysis-and-Prediction,,https://github.com/swoyam2609/Oil-and-Gas-Price-Analysis-and-Prediction,https://api.github.com/repos/Oil-and-Gas-Price-Analysis-and-Prediction/swoyam2609," Prediction  This repository contains the code and datasets for analyzing and predicting fuel prices, specifically Brent Oil, Crude Oil, Natural Gas, and Heating Oil. The project focuses on creating models to predict the closing prices of these fuels based on their opening, highest, and lowest prices.",,1,1,1,0,oil-and-gas,"[data, data-analysis, data-visualization, linear-regression, machine-learning, oil-and-gas, price-prediction]",0.0
56,WanIlmie,wanilmie.github.io,,https://github.com/WanIlmie/wanilmie.github.io,https://api.github.com/repos/wanilmie.github.io/WanIlmie,My portfolio,"###### wanilmie.github.io
# **About Me**
Researching data analytic application in oil & gas. Interests in data standards for exploration & production.

Types of data: Well-site Information Transfer Standard Markup Language (WITSML) for drilling, completion and well intervention. Production XML (PRODML) for production optimization.

Exploring data visualization, data analysis, machine-learning and collaboration method for Volve Field Data Set. The most complete open-source Exploration & Production data available.

My experience and skills are mainly in oil & gas upstream. My portfolio is a mix of process engineering, well intervention, project proposal and data analytical skills.

Lets connect on LinkedIn https://www.linkedin.com/in/wanilmie/ or explore my github page https://wanilmie.github.io/

# **Data Visualization**

Extract from big data storage to provide snippet visualization. Additional customization for a light-weight web application. Engineers can use it to quickly visualize the data in graphs and 3D models. Helps to communicate complex scenario to team and stakeholders.

File path: [myapp/views.py](myapp/views.py)

Function: return_graph()

> Volve Field Data Set: https://www.equinor.com/energy/volve-data-sharing

> Equinor Open Data Licence (new Sept. 2020) (PDF): [data_license.pdf](data_license.pdf)

> Disclaimer:
> The users of the information and data are expressly informed that the data provided by Equinor and the former Volve Licence partners is provided on an “as is” basis and may contain errors and omissions. Equinor and former Volve licence partners provide no warranties, expressed or implied, either relating to the content or to the relevance of the data. Equinor disclaims any liability for errors and defects associated with the data to the maximum extent permitted by law. Equinor and the former Volve Licence partners shall not be liable for any direct or indirect losses as a result of use of the data or concerning possible infringement of any intellectual property and/or patent rights of a third party.

> Data Set Credits: Equinor and the Volve license partners
",0,0,1,0,oil-and-gas,"[collaboration, data-analysis, data-visualization, exploratory-data-analysis, machine-learning, oil-and-gas, production-data-analysis]",0.0
57,rafferti95,My-projects,,https://github.com/rafferti95/My-projects,https://api.github.com/repos/My-projects/rafferti95,useful programs for my job,"# Мои ""подручные"" программы

Программы которые я пишу для автоматизации и оптимизации рабочих процессов в сфере 3D-геологического моделирования. И одна моя самая первая собственная тестовая программка-копилка 😊

## Программы 📁:
* [Программа-копилка](https://github.com/rafferti95/My-projects/tree/test-programs/0_Mondey%20box) 💰
* [Список файлов](https://github.com/rafferti95/My-projects/tree/test-programs/1_Files%20names) 📄
* [Координаты забоев скважин](https://github.com/rafferti95/My-projects/tree/test-programs/2_Wells%20bottoms) 📍
* [Препобразование файлов инклинометрии](https://github.com/rafferti95/My-projects/tree/test-programs/3_Inclinometria) 📝

",0,0,1,0,oil-and-gas,"[3d-modelling, oil-and-gas, python]",0.0
58,Ayberk-Uyanik,GeoCos-v2.0,,https://github.com/Ayberk-Uyanik/GeoCos-v2.0,https://api.github.com/repos/GeoCos-v2.0/Ayberk-Uyanik,Web version of GeoCos!,"The first version of GeoCos has been developed as a desktop application allowing users to calculate geolocial chance of success values for exploration wells interactively.

By this new version, GeoCos goes ONLINE! In addition to calculation of CoS with 3 table-based methods published in the last 20 years, results for each petroleum system element can be compared by bar and radar charts that can be downloaded for to display risk assessment summary. A short description for each table-based method and links to the research papers are also present.",0,0,1,0,oil-and-gas,"[css, css3, drilling, exploration, exploration-geophysics, html-css-javascript, html5, hydrocarbon, hydrocarbons, javascript, nodejs, oil-and-gas, oilandgas, plotlyjs, vanilla-javascript]",0.0
59,energypriceapi,energypriceapi-nodejs,,https://github.com/energypriceapi/energypriceapi-nodejs,https://api.github.com/repos/energypriceapi-nodejs/energypriceapi,Official Node.js API wrapper for energypriceapi.com,"# EnergypriceAPI

energypriceapi is the official Node.js wrapper for EnergypriceAPI.com. This allows you to quickly integrate our foreign exchange rate API and currency conversion API into your application. Check https://energypriceapi.com documentation for more information.



## Installation

#### NPM

```
$ npm i energypriceapi
```
---
## Usage

```js
const api = require('energypriceapi');

api.setAPIKey('SET_YOUR_API_KEY_HERE');
await api.fetchLive('USD', ['BRENT','GASOLINE','NATURALGAS','WTI']);
```
---
## Documentation

#### setAPIKey(apiKey)

- `apiKey` <[string]> API Key

In order to use this library, you must first call this function with an API key.

```js
api.setAPIKey('SET_YOUR_API_KEY_HERE');
```
---
#### fetchSymbols()
```js
await api.fetchSymbols();
```

[Link](https://energypriceapi.com/documentation#api_symbol)

---
#### fetchLive(base, currencies)

- `base` <[string]> Optional. Pass in a base currency, defaults to USD.
- `currencies` <[Array]<[string]>> Optional. Pass in an array of currencies to return values for.

```js
await api.fetchLive('USD', ['BRENT','GASOLINE','NATURALGAS','WTI']);
```

[Link](https://energypriceapi.com/documentation#api_realtime)

---
#### fetchHistorical(date, base, currencies)

- `date` <[string]> Required. Pass in a string with format `YYYY-MM-DD`
- `base` <[string]> Optional. Pass in a base currency, defaults to USD.
- `currencies` <[Array]<[string]>> Optional. Pass in an array of currencies to return values for.

```js
await api.fetchHistorical('2021-04-05', 'USD', ['BRENT','GASOLINE','NATURALGAS','WTI']);
```

[Link](https://energypriceapi.com/documentation#api_historical)

---
#### convert(from, to, amount, date)

- `from` <[string]> Optional. Pass in a base currency, defaults to USD.
- `to` <[string]> Required. Specify currency you would like to convert to.
- `amount` <[number]> Required. The amount to convert.
- `date` <[string]> Optional. Specify date to use historical midpoint value for conversion with format `YYYY-MM-DD`. Otherwise, it will use live exchange rate date if value not passed in.

```js
await api.convert('USD', 'EUR', 100, '2021-04-05');
```

[Link](https://energypriceapi.com/documentation#api_convert)

---
#### timeframe(start_date, end_date, base, currencies)

- `start_date` <[string]> Required. Specify the start date of your timeframe using the format `YYYY-MM-DD`.
- `end_date` <[string]> Required. Specify the end date of your timeframe using the format `YYYY-MM-DD`.
- `base` <[string]> Optional. Pass in a base currency, defaults to USD.
- `currencies` <[Array]<[string]>> Optional. Pass in an array of currencies to return values for.

```js
await api.timeframe('2021-04-05', '2021-04-06', 'USD', ['BRENT','GASOLINE','NATURALGAS','WTI']);
```

[Link](https://energypriceapi.com/documentation#api_timeframe)

---
#### change(start_date, end_date, base, currencies)

- `start_date` <[string]> Required. Specify the start date of your timeframe using the format `YYYY-MM-DD`.
- `end_date` <[string]> Required. Specify the end date of your timeframe using the format `YYYY-MM-DD`.
- `base` <[string]> Optional. Pass in a base currency, defaults to USD.
- `currencies` <[Array]<[string]>> Optional. Pass in an array of currencies to return values for.

```js
await api.change('2021-04-05', '2021-04-06', 'USD', ['BRENT','GASOLINE','NATURALGAS','WTI']);
```

[Link](https://energypriceapi.com/documentation#api_change)

---
**[Official documentation](https://energypriceapi.com/documentation)**


---
## FAQ

- How do I get an API Key?

    Free API Keys are available [here](https://energypriceapi.com).

- I want more information

    Checkout our FAQs [here](https://energypriceapi.com/faq).


## Support

For support, get in touch using [this form](https://energypriceapi.com/contact).


[array]: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array 'Array'
[number]: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Data_structures#Number_type 'Number'
[string]: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Data_structures#String_type 'String'",0,0,1,0,oil-and-gas,"[currency, currencyconverterapi, energy-data, gas, oil-and-gas, wti-oil]",0.0
60,DeflateAwning,alberta-og-data-tools,,https://github.com/DeflateAwning/alberta-og-data-tools,https://api.github.com/repos/alberta-og-data-tools/DeflateAwning,"A collection of Python tools for dealing with Alberta oil and gas data, including UWIs","# alberta-og-data-tools
A collection of Python tools for dealing with Alberta oil and gas data, including UWIs.

While I briefly worked in the oil and gas industry in Alberta, I noticed a lack of efficiency in dealing with large lists of Unique Well Identifiers (or UWIs for short), especially when parsing lists of files, or parsing text from human-written notes. 

This library implements a `WellLoc()` class, which represents a UWI surface location or bottom hole location (with optional prefix and completion event). It also implements parsing of these UWIs from large strings of text, especially in cases where inconsistent formatting was used.

## Usage Examples
See `test_uwi_tools.py` for a list of capabilities and conversions.

Star this repo or open an issue if you want more examples!
",0,0,1,0,oil-and-gas,"[alberta, oil-and-gas, parser, regex, unique-well-identifier, uwi]",0.0
61,bfotland,force-hack-2019,,https://github.com/bfotland/force-hack-2019,https://api.github.com/repos/force-hack-2019/bfotland,Code for formation description to transcription/classification,"# FORCE Hackathon 2019
Code for translating formation descriptions to transcriptions/classifications (grain size) using machine learning.

## Retrieving dataset 

Download the following Excel spreadsheet to a folder named __data__:
[RealPore Por Perm Lithology data 1240 Wells Norway public.xlsx]<https://drive.google.com/open?id=17kOO2637vXV1jADdo6i4PpYt7X6BYY4N>

If named exactly **RealPore Por Perm Lithology data 1240 Wells Norway public.xlsx**, it will be picked up automatically when running the machine_translation.py script. Otherwise, specify the path to the dataset with the **--dataset** option.

Original link seems to be dead. 

The spreadsheet is also available at <https://zenodo.org/record/4419060>

A version 2.0 of the data is available at <https://zenodo.org/record/4723018>


## Python dependencies

Tested with Python 3.9.

See requirements.txt file for further dependencies

Optionally install tensorflow-gpu instead of tensorflow package if a GPU is available,
to speed up training.

## Training formation description to transcriptions or formation descriptions to grain size

Run the machine_translation.py script to start the machine learning


Example usages:
```bash
python machine_translation.py transcription
# or
python machine_translation.py ""grain size""
# or
python machine_translation.py --help # to see other options.
```
",0,0,2,1,oil-and-gas,"[machine-learning, machine-translation, oil-and-gas, python]",0.0
62,giovannivelludo,Antea-IFC-Export,,https://github.com/giovannivelludo/Antea-IFC-Export,https://api.github.com/repos/Antea-IFC-Export/giovannivelludo,"Library to convert .eywa files (proprietary format of Antea S.r.l.) to .ifc files, written during my internship at Antea S.r.l.","# Antea IFC Export
Library to convert .eywa files to .ifc files.  
Class `tech.antea.ifc.Main` contains an example of usage.

### Usage
.eywa files can be deserialized with:
```java
import buildingsmart.ifc.IfcProject;
import com.fasterxml.jackson.databind.ObjectMapper;
import it.imc.persistence.po.eytukan.EywaRoot;
//...
    ObjectMapper objectMapper = new ObjectMapper();
    EywaRoot eywaRoot = objectMapper.readValue(eywaFile, EywaRoot.class);
```

The conversion between `EywaRoot` and `IfcProject` is done this way:
```java
    EywaToIfcConverter builder = new EywaToIfcConverter();
    EywaReader director = new EywaReader(builder);
    director.convert(eywaRoot);
    IfcProject result = builder.getResult();
```

Finally, .ifc files can be serialized with:
```java
    EywaToIfcConverter.writeToFile(result, outputFile);
```

### Tests and deterministic output
The content of .ifc files generated from the same input .eywa won't always be
the same, because of timestamps and random UUIDs. These are removed before
comparing output files and expected output files in tests, so there are no
issues there.  
Another issue could arise from elements of `Sets` being serialized in different
orders (because when iterating through a Set the order is generally
unpredictable), so at the moment `LinkedHashSets` are used, and they should keep
being used in future modifications for all `Sets` to avoid test failures.  
Another way to solve this issue would be writing a parser to create an
`IfcProject` from each expected output file, and then compare that to the
`IfcProject` generated from the actual output file (after removing timestamps
and UUIDs).

### Useful links
[IFC 2x3 documentation](https://standards.buildingsmart.org/IFC/RELEASE/IFC2x3/TC1/HTML/)  
[CoordinationView 2.0](https://standards.buildingsmart.org/MVD/RELEASE/IFC2x3/TC1/CV2_0/IFC2x3_CV2_0.zip)  
[Implementation agreements](https://standards.buildingsmart.org/documents/Implementation/IFC_Implementation_Agreements/)  
[EXPRESS specification (it's a more recent version than the one used in IFC, but they're similar)](https://www.steptools.com/stds/step/IS_final_p21e3.html)  
[Implementation guidance](https://technical.buildingsmart.org/resources/ifcimplementationguidance/)  ",0,0,1,0,oil-and-gas,"[3d-models, ifc, ifc2x3, oil-and-gas]",0.0
63,sergey-frolov-pets,sergey-frolov-blog,,https://github.com/sergey-frolov-pets/sergey-frolov-blog,https://api.github.com/repos/sergey-frolov-blog/sergey-frolov-pets,my personal blog on Jekyll,"<h1>Flexton is an ultra-minimalist and responsive theme for Jekyll</h1>

<p>Flexton created especially for those who appreciate minimalism and functionality.</p>
<p>Amazing flexibility with pleasant colors and custom design makes Flexton a simple theme, suitable for almost any blog.</p>

<h2>Demo</h2>

Check the theme in action [Demo](https://flexton.netlify.com/)

![Page preview](https://github.com/artemsheludko/flexton/blob/master/images/preview.png?raw=true)

<h2>Features</h2>

<ul>
	<li>100% responsive and clean theme</li>
 	<li>Optimized for mobile devices</li>
	<li>Valid HTML5 code</li>
	<li>Included site search</li>
	<li>Contact Page</li>
	<li>Post sharing</li>
	<li>Supports Mail Chimp Subscriber</li>
	<li>Supports Disqus Comments</li>
	<li>Social Media Profiles</li>
	<li>Contact Form - FormsPree</li>
	<li>Evil Icons</li>
	<li>Google Fonts</li>
</ul>

<h2>Credits</h2>

<p>I have used the following scripts, fonts or other files as listed.</p>

<ul>
  <li><a href=""https://fonts.google.com/"">Google Fonts</a> (Volkhov, Open Sans).</li>
  <li><a href=""http://evil-icons.io/"">Evil Icons</a></li>
  <li><a href=""http://fitvidsjs.com/"">FitVids.js</a></li>
  <li><a href=""https://jquery.com/"">jQuery.com</a></li>
  <li><a href=""https://github.com/christian-fei/Simple-Jekyll-Search"">Simple-Jekyll-Search</a></li>
  <li>Preview Images form <a href=""https://unsplash.com/"">unsplash.com</a>, <a href=""https://www.pexels.com/"">pexels.com</a></li>
</ul>

<h2>Deployment</h2>

To run the theme locally, navigate to the theme directory and run `bundle install` to install the dependencies, then run `jekyll serve` or `bundle exec serve` to start the Jekyll server.

I would recommend checking the [Deployment Methods](https://jekyllrb.com/docs/deployment-methods/) page on Jekyll website.

<h2>Buy me a coffee</h2>

<p>If you want to show your appreciation, buy me one <a href=""https://www.buymeacoffee.com/artemsheludko"" target=""_blank""><img src=""https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png"" alt=""Buy Me A Coffee"" style=""height: auto !important;width: auto !important;"" ></a> ! Every five cups of coffee and a new theme for Jekyll is ready 😋</p>
<p>Either way, your support is a way to thank me ❤️</p>
<p align=""center""><b>Thank you for your support!</b></p>
",0,0,0,0,oil-and-gas,"[ai, automation, bi, oil-and-gas]",0.0
64,aggg97,CapIV,,https://github.com/aggg97/CapIV,https://api.github.com/repos/CapIV/aggg97,"This repository contains a Streamlit app built to visualize and analyze energy production data from unconventional wells using a public dataset. The app allows users to interactively explore the dataset, filter data by operator, well type, and well identifier, and view visualizations of production rates over time.",,0,0,1,0,oil-and-gas,"[argentina, oil-and-gas, petroleum, shale, streamlit-dashboard, vacamuerta]",0.0
65,EduardoPach,Fast_Offshore_Wells_Model_Julia,,https://github.com/EduardoPach/Fast_Offshore_Wells_Model_Julia,https://api.github.com/repos/Fast_Offshore_Wells_Model_Julia/EduardoPach,This Repository will be destinated to the study of Offshore Well Modeling. The model that will be implemented is called Fast Offshore Wells Model (FOWM)  from the paper: Fast Offshore Wells Model (FOWM): A practical dynamic model formultiphase oil production systems in deepwater and ultra-deepwaterscenarios,"# Fast Offshore Wells Model (FOWM)

In this repo the FOWM, from [Diehl et al., 2017](https://www.sciencedirect.com/science/article/abs/pii/S0098135417300443), will be replicated using Julia Programming Language.

## System Representation
Here is a representation of the system available in the work of [Diehl et al., 2017](https://www.sciencedirect.com/science/article/abs/pii/S0098135417300443)

![](images/representation.jpg)

## Model

$$\dfrac{dm_{ga}}{dt} = W_{gc} - W_{iV}$$ (1)
$$\dfrac{dm_{gt}}{dt} = W_r\alpha_{gw} + W_{iV}-W_{whg}$$ (2)
$$\dfrac{dm_{lt}}{dt} = W_r(1-\alpha_{gw}) - W_{whl}$$ (3)
$$\dfrac{dm_{gb}}{dt} = (1-E)W_{whg}-W_g$$ (4)
$$\dfrac{dm_{gr}}{dt} = EW_{whg}+W_g-W_{gout}$$ (5)
$$\dfrac{dm_{lr}}{dt} = W_{whl}-W_{lout}$$ (6)
<br>
<br>


Being:

|   Variables   |                        Description                        |
| :-----------: | :-------------------------------------------------------: |
|   $m_{ga}$    |                 Gas mass in the annualar                  |
|   $m_{gt}$    |                  Gas mass in the tubbing                  |
|   $m_{lt}$    |                  Liq mass in the tubing                   |
|   $m_{gb}$    |                  Gas mass in the bubble                   |
|   $m_{gr}$    |                 Gas mass in the flowline                  |
|   $m_{lr}$    |                Liquid mass in the flowline                |
|   $W_{iV}$    |           Gas mass flow from annular to tubing            |
|    $W_{r}$    |          Reservoir to the bottom hole mass flow           |
|   $W_{whg}$   |                Gas mass flow at Xmas Tree                 |
|   $W_{whl}$   |                Liq mass flow at Xmas Tree                 |
|   $W_{gc}$    |                Gas lift mass flow annular                 |
|    $W_{g}$    |            Gas mass flow at the virtual valve             |
|  $W_{gout}$   |        Gas mass flow through topside valve (Choke)        |
|  $W_{lout}$   |        Gas mass flow through topside valve (Choke)        |
|      $E$      |         Mass fraction of gas bypassing the bubble         |
| $\alpha_{gw}$ | Gas mass fraction at resorvoir's pressure and temperature |


<br>
<br>

Where:

$$W_{iV} = K_a\sqrt{\rho_{ai}(P_{ai}-P_{tb})}$$ (7)
$$W_{r} = K_r \left [1 - 0.2\dfrac{P_{bh}}{P_{r}} - \left(0.8\dfrac{P_{bh}}{P_r}\right)^2 \right]$$ (8)
$$W_{whg} = K_w\sqrt{\rho_{L}(P_{tt}-P_{rb})}\alpha_{gt}$$ (9)
$$W_{whl} = K_w\sqrt{\rho_{L}(P_{tt}-P_{rb})}(1-\alpha_{gt})$$ (10)
$$W_{g} = C_g(P_{eb} - P_{rb}) $$ (11)
$$W_{gout} = \alpha_{gr} C_{out} z  \sqrt{\rho_L(P_{rt}-P_{s})}$$ (12)
$$W_{lout} = \alpha_{lr} C_{out} z  \sqrt{\rho_L(P_{rt}-P_{s})}$$ (13)

<br>
<br>

Being:

|   Variables   |                      Description                       |
| :-----------: | :----------------------------------------------------: |
|    $K_{a}$    |      Flow coefficient between annular and tubing       |
|    $K_{r}$    |              Resorvoir's flow coefficient              |
|    $K_{w}$    |             Flow coefficient at Xmas Tree              |
|  $\rho_{ai}$  |               Gas density in the annular               |
|  $\rho_{L}$   |           Liquid density (assumed constant)            |
| $\alpha_{gt}$ |              Gas mass fraction in tubing               |
| $\alpha_{gr}$ |        Gas mass fraction in the subsea pipeline        |
| $\alpha_{lr}$ |      Liquid mass fraction in the subsea pipeline       |
|    $C_{g}$    |              Virtual valve flow constant               |
|   $C_{out}$   |                  Choke valve constant                  |
|      $z$      |              Choke valve opening fraction              |
|    $P_{r}$    |                  Reservoir's Pressure                  |
|    $P_{s}$    |               Pressure after Choke valve               |
|   $P_{rt}$    |            Pressure at the top of the riser            |
|   $P_{rb}$    |       Pressure at the flowline before the bubble       |
|   $P_{tt}$    |             Pressure at the top of tubing              |
|   $P_{eb}$    |                    Bubble Pressure                     |
|   $P_{ai}$    | Pressure in the annular gas injection point to tubing  |
|   $P_{tb}$    | Pressure in the gas injection point on the tubing side |
|   $P_{bh}$    |              Pressure in the bottom hole               |

<br>
<br>
Where:

$$\rho_{ai} = \dfrac{MP_{ai}}{RT}$$ (14)
$$\alpha_{gt} = \dfrac{m_{gt}}{m_{gt}+m_{lt}}$$ (15)
$$\alpha_{gr} = \dfrac{m_{gr}}{m_{gr}+m_{lr}}$$ (16)
$$\alpha_{lr} = 1-\alpha_{gr}$$ (17)
$$P_{ai} = \left(\dfrac{RT}{V_aM} + \dfrac{gL_a}{V_a} \right)m_{ga} $$ (18)
$$P_{tb} = P_{tt} + \rho_{mt}gH_{vgl}$$ (19)
$$P_{bh} = P_{pdg} + \rho_{mres}g(H_t-H_{pdg})$$ (20)
$$P_{pdg} = P_{tb} + \rho_{mres}g(H_{pdg}-H_{vgl})$$ (21)
$$P_{tt} = \dfrac{\rho_{gt}RT}{M}$$ (22)
$$P_{rb} = P_{rt}+\dfrac{(m_{lr}+m_{L,still})gsin(\theta)}{A_{ss}}$$ (23)
$$P_{eb} = \dfrac{m_{gb}RT}{MV_{eb}}$$ (24)
$$P_{rt} = \dfrac{m_{gr}RT}{M\left(\omega_{u}V_{ss}-\dfrac{m_{lr}+m_{L,still}}{\rho_L}\right)}$$ (25)

<br>
<br>

Being:

|   Variables   |                Description                 |
| :-----------: | :----------------------------------------: |
|      $R$      |           Universal gas constant           |
|      $T$      |            Average temperature             |
|      $M$      |            Gas molecular weight            |
|  $\rho_{mt}$  |              Mixture density               |
| $\rho_{mres}$ |            Reservoir's density             |
|  $\rho_{gt}$  |                Gas density                 |
|      $g$      |            Gravity acceleration            |
|    $V_{a}$    |               Annular volume               |
|   $V_{eb}$    |               Bubble Volume                |
|   $V_{ss}$    |    Pipe volume downstream virtual valve    |
|    $H_{t}$    |  Vertical length Xmas Tree - Bottom Hole   |
|   $H_{pdg}$   |   Vertical length Xmas Tree - PDG point    |
|   $H_{vgl}$   |    Vertical length Xmas Tree - Gas Lift    |
|   $A_{ss}$    |         Riser cross sectional area         |
|    $L_{a}$    |               Annular length               |
| $m_{L,still}$ | Minimum mass of liq in the subsea pipeline |
| $\omega_{u}$  |   Bubble Location (assistant parameter)    |
|   $\theta$    |         Average riser inclination          |


<br>
<br>

Where:

$$\rho_{mt} = \dfrac{m_{gt}+m_{lt}}{V_t}$$ (26)
$$\rho_{gt} = \dfrac{m_{gt}}{V_{gt}}$$ (27)
$$V_{gt} = V_t - \dfrac{m_{lt}}{\rho_L}$$ (28)
$$A_{ss} = \dfrac{\pi D_{ss}^2}{4}$$ (29)
$$V_{ss} = \dfrac{\pi D_{ss}^2L_r}{4}+\dfrac{\pi D_{ss}^2L_{fl}}{4}$$ (30)
$$V_a = \dfrac{\pi D_{a}^2L_a}{4}$$ (31)
$$V_t = \dfrac{\pi D_{t}^2L_t}{4}$$ (32)

<br>
<br>

Being:

| Variables |       Description        |
| :-------: | :----------------------: |
|   $V_t$   |     Volume of tubing     |
| $V_{gt}$  | Gas volume in the tubing |
|   $L_r$   |       Riser length       |
| $L_{fl}$  |     Flowline length      |
|   $L_t$   |      Tubing length       |
|   $D_a$   |     Annular diameter     |
|   $D_t$   |     Tubing diameter      |
| $D_{ss}$  | Subsea pipeline diameter |


## Simulation

### Parameters

|   Variables   |         Value         |   Unit    |
| :-----------: | :-------------------: | :-------: |
|   $\rho_L$    |         $900$         | $kg/m^3$  |
|     $P_r$     |         $225$         |   $bar$   |
|     $P_s$     |         $10$          |   $bar$   |
| $\alpha_{gw}$ |       $0.0188$        |    $-$    |
| $\rho_{mres}$ |         $892$         | $kg/m^3$  |
|      $M$      |         $18$          | $kg/kmol$ |
|      $T$      |         $298$         |    $K$    |
|    $L_{r}$    |        $1569$         |    $m$    |
|   $L_{fl}$    |        $2928$         |    $m$    |
|    $L_{t}$    |        $1639$         |    $m$    |
|    $L_{a}$    |        $1118$         |    $m$    |
|    $H_{t}$    |        $1279$         |    $m$    |
|   $H_{pdg}$   |        $1117$         |    $m$    |
|   $H_{vgl}$   |         $916$         |    $m$    |
|   $D_{ss}$    |        $0.15$         |    $m$    |
|    $D_{t}$    |        $0.15$         |    $m$    |
|    $D_{a}$    |        $0.14$         |    $m$    |
| $m_{L,still}$ | $6.222\times 10^{+1}$ |    kg     |
|    $C_{g}$    | $1.137\times 10^{-3}$ |    $-$    |
|   $C_{out}$   | $2.039\times 10^{-3}$ |    $-$    |
|   $V_{eb}$    | $6.098\times 10^{+1}$ |   $m^3$   |
|      $E$      | $1.545\times 10^{-1}$ |    $-$    |
|    $K_{w}$    | $6.876\times 10^{-4}$ |    $-$    |
|    $K_{a}$    | $2.293\times 10^{-5}$ |    $-$    |
|    $K_{r}$    | $1.269\times 10^{+2}$ |    $-$    |
| $\omega_{u}$  | $2.780\times 10^{0}$  |    $-$    |",0,0,1,0,oil-and-gas,"[chemical-engineering, numerical-integration, oil-and-gas, oil-wells]",0.0
66,caseyhnguyen,CA-Oil-Well-Data-Prediction-Program,,https://github.com/caseyhnguyen/CA-Oil-Well-Data-Prediction-Program,https://api.github.com/repos/CA-Oil-Well-Data-Prediction-Program/caseyhnguyen,A data prediction algorithm implemented in Python to infer depth and casing size data of California oil wells missing data using surrounding wells in the same oil field. Python libraries such as pandas for manipulating dataframes and geopy were utilized.,"# CA-Oil-Well-Data-Prediction-Program
A data prediction algorithm implemented in Python to infer depth and casing size data of California oil wells missing data using surrounding wells in the same oil field. Python libraries such as pandas for manipulating dataframes and geopy were utilized.
",0,0,2,0,oil-and-gas,"[geopy, oil-and-gas, oil-wells, pandas, python]",0.0
67,gwallison,openFF-build,,https://github.com/gwallison/openFF-build,https://api.github.com/repos/openFF-build/gwallison,Code for Open-FF constructor scripts; A project to transform the industry's FracFocus disclosure data into a usable resource.,"# README for open-FF repository and project

This CodeOcean capsule is a system of code to transform data from 
the online chemical disclosure site 
for hydraulic fracturing, FracFocus.org, into a usable database.  Currently,
these data include over 6,000,000 chemical records in over 175,000 fracking events.

The code performs cleaning, flagging, and 
curating techniques to yield organized data sets and sample analyses 
from a difficult collection of chemical records.   
For a majority of these records, the **mass** of the chemicals used 
in fracking operations is calculated. 

The output of this project includes full data sets and filtered data sets. All 
sets include many of the original raw FracFocus fields and many generated
fields that correct and add context to the raw data.  Filtered data sets remove
the FracFocus records that have significant problems, which gives the user a 
product that is usable without much work.  On the other hand, full sets do not 
filter out any of the original raw FracFocus records allowing 
the user to construct their own appropriate set (by using the flags, etc.).  

Portions of the raw bulk data that are filtered out include: 
- fracking events with no chemical records (mostly 2011-May 2013)
- fracking events with multiple entries (and no indication which entries 
    are correct).
- chemical records that are identified as redundant within the event.

Additionally, the mass for reported chemicals is calculated **only** for those disclosures
that meet more stringent criteria, such as consistent percentages, base water volume,
and a match with MassIngredient field (when available).

Finally,  we clean up some of the labeling fields by consolidating multiple 
versions of a single category into an easily searchable name. For instance, 
we collapse the 80+ versions of the supplier name 'Halliburton' to a single
value 'halliburton'.

This code is designed to facilitate adding new disclosures to data sets 
periodically (after curation) to keep the output data sets relatively 
up to date. 

By removing or cleaning the difficult data from this unique data source, 
we have produced a data set that can facilitate more in-depth 
analyses of chemical use in the fracking industry.

### The online browser
Find a browsable version of the processed data at the [Open-FF Catalog](https://frackingchemicaldisclosure.wordpress.com/data-navigator/).
This includes a chemical index with extensive profiles of every material, 
a data dictionary, and other resources.

Find the github repositories of code at:
- [openFF-build](https://github.com/gwallison/openFF-build)
- [openFF-curation](https://github.com/gwallison/openFF-curation)
- [openFF-raw](https://github.com/gwallison/openFF-raw)
- [openFF-catalog](https://github.com/gwallison/openFF-catalog)

## CodeOcean Versions of Open-FF 

**Beta - in cloud version**
- tweak automated  carrier  identifiers to keep disclosures on the edges that were
    previously avoided but were still viable for calculating mass.

**VERSION 16**
- Current download: February 2023

- added Reportable Quantities for all bgCAS that have them.  The field
    added is named `rq_lbs`.  This value is NaN if the bgCAS is not on the
    list. (Added Aug 14, 2022)

- Cleaned up the bgCAS that were not striped of trailing spaces.

- Using new comptox batch cleanup routines. Old EPA names are no longer
    available, but IUPAC name now included from comptox search.

- `bgLatitude` and `bgLongitude` are now replaced with state-derived values
    when errors are detected in the FracFocus versions of those varibales.
    The field `bgLocationSource` indictates where the bg values come from.
    Also added `stLatitude` and `stLongitude`, the state version of those
    location data.

- added `bgFederalLand` and `bgNativeAmericanLand` that is derived from
    the PADUS-3 data set


**VERSION 15**

- Current download: July 2022

- Revamp location cleanup: require curation of State-County|Name-Number 
    combinations so that ALL bgStateName/CountyName values are consistent with
    formal list.  Reproject bgLatitude/Longitude so it is all in WGS84 and 
    easy to reproject into any other Projection. Compare all reported lat/lon
    to state and county boundaries to identify pairs that are inconsistent with
    reported names. Use geopandas. Removed a few flags but added three
    new: `loc_name_mismatch`, `loc_within_county`, `loc_within_state`; keeping
    `latlon_too_coarse`. (May 2, 2022)
    
- Add another criteria to find inconsistent `MassIngredient`: remove records
    with `PercentHFJob` = 0.

**VERSION 14**:

- Current download: April 2, 2022

- Add a ""fresh"" scraped version of the early years of FracFocus. These data
    were assembled by downloading (in Apr 2021) 43,000+ PDFs of the ""empty"" disclosures of
    the bulk download data (which have some meta data but no chemical records),
    scraping them with a PDF scraper program (keeping only data from the well-formed
    PDFs) and then formatting the data to be compatible with the bulk download data.
    This data set is kept separate from the bulk download data, but can be
    compiled using the data_source term: ""FFV1_scrape"".  
    
- Separated the ""SkyTruth"" archive from the bulk download data.  This is in recognition
    to our discovery that that archive no longer reflects what the industry has
    published as up-to-date.  The changes we have detected between the archive
    and more recent versions of the data impact a small fraction of records, but
    because scraping PDF files is a large task, especially when so many disclosures
    are poorly formatted, we cannot with certainty account for all records in
    this archive.  Nevertheless, the archive may be of interest to researchers
    looking into transparency and how published data changes over time.  This set
    can be compiled using the data_source term: ""SkyTruth""
    
- Add new external lists: EPA Comptox lists: WATERQUALCRIT, NWATRQHHC, IRIS, 
    PFAS_master and VOLITILOME;
    TSCA's list of UVCB (unknown, variable composition or biological); 
    National Primary Drinking Water Regulation list (curated by Angelica Fiuza in
    Feb. 2022); EPA's list of 5 ""diesel"" ingredients that are regulated

**VERSION 13 (March 2022)**

- Remove the ""SkyTruth archive"" from the standard_filtered data sets. In previous
    versions, this archive has been used to fill in the chemical records for
    disclosures between 2011-May 2013, because the FracFocus bulk download
    does not report chemical records for that period, even though the chemicals
    are reported in the PDF files available at the web site.  We are removing the
    archive because it has come to our attention that those data are occasionally
    not a good representation of currently available PDF files on the FracFocus
    website.  This is apparently due largely to changes to the PDF files **after**
    they were published and SkyTruth downloaded them.  While such changes are
    not announced or recorded in any way to the public, they are premitted by 
    FracFocus.  It appears that the only way that the public can tell that such
    changes have been made is to manually compare new versions to older versions,
    which is a very large task for big-picture analysis.

- Note that although the SkyTruth archive will no longer be in the ""standard filtered""
    data set, those data are still available in the ""full"" data set which includes
    all other available records such as duplicates.  Use the SkyTruth data with care 
    and remember that some records do not reflect what the industy claims as 
    the proper published records.

**Version 12** :

- Data downloaded from FracFocus on Feb. 5, 2022

- Added testing module to confirm consistency of final data sets.

- Changed `calcMass` values of zero to NaN to indicate they are a non-disclosed 
    quantity (Dec. 20,2021)
    
- in DataDictionary, fixed massCompFlag description to reflect that it is **True** when
    massComp is **out of** tolerance.
    
- mark disclosures without chemical records as out-of-tolerance for total_percent.

- add hash checks on repository files (Dec 29, 2021)

- add new filters (4,5,6) to find problem disclosures for mass calculation (Jan 15-28, 2022)

- add new carrier detection sets: s6,s7,s8 and s9. (Jan 28, 2022)

- incorporate filter of SkyTruth disclosures that we have detected as changed since
    originally scraped. The removed disclosures are still available in the Full set,
    but not the filtered set. (Jan 26, 2022)
    
- add the raw field `IngredientComment` back into the Full data set for proprietary
    claim analysis. (Jan 30, 2022) 


**Version 11**:

- Data downloaded from Dec. 4, 2021. 

- cleanMI field added that is MassIngredient with values that are inconsistent at
    the disclosure level removed.

- Added carrier detection sets: set2, set3, set4 and set5. This adds about 25,000
    disclosures that are eligible for mass calculations. 
    
- Flagged SkyTruth-based disclosures that have been deleted from FracFocus's pdf
    database (n=140).  They are no longer included in the standard filtered data set,
    but can still be accessed in the Full data set.  See the list at /data/ST_api_without_pdf.csv

- Changed the file structure of the project to be more in line with python
    package structure. Started new github repository for this branch (openFF-build).
    
- Removed `clusterID` and other fields that were experimental or not used.

    
**Version 10 - MAJOR REVISION**:

- Data downloaded from Oct 10, 2021. Updated versions of the data may be periodically 
    available without code changes.  See the [Open-FF blog](https://frackingchemicaldisclosure.wordpress.com/).

- Chemical identification is now curated using both CASNumber and IngredientName instead of by 
    automated analysis of CASNumber only. This includes separate evaluation of the
    two fields using comparisons to authoratative references (Chemical Abstracts's ""SciFinder"" and 
    the EPA's ""CompTox"". This
    change allows for many simple typos to be corrected and for many obviously
    wrong CASNumbers to be changed or flagged as 'ambiguous'.  Further, this curation
    produces a more thorough characterization of proprietary claims and the
    'category' of the change is available to the end user for further analysis.
    The translation file (""casing_curated.csv"") between original `CASNumber`/`IngredientName` pairs and
    resulting `bgCAS` is in the /sources directory (""/data"" folder for CodeOcean).
    
- The carrier records of a disclosure are now determined by combination of automation
    and manual curation (instead of just automation as in previous versions). The
    code introduced here allows many more disclosures to be included in mass calculations.
    This version does not yet apply the manual curation, but the version adds the
    functionality.
    
- The code and reference files created to translate SciFinder CAS naming is
    now included in the core file ""process_CAS_ref_files.py.""
    
- The code used to do pre-processing can be found in the /builder_tasks folder, 
    though it is not performed in CodeOcean. It is included for completeness
    and transparency.  The output of these scripts are mostly held in the 
    /data (/sources) folder.
    
- The calculation of mass for every chemical now incorporates the density of the
    carrier fluid indicated in the `IngredientComments` field, when available. This is
    currently available for about 24,000 disclosures.
    
- The calculation of mass now is checked against the undocumented but informative
    FracFocus field, `MassIngredient`.  While this field is only available for a subset of disclosures and
    can be internally inconsistent, when checked against `calcMass`, we can find (and filter)
    records or whole disclosures that may be more error prone.
    
- `clusterID` changed to fixed length string for more consistent searches.

- Individual boolean flags are available for fine-grained filtering in the full data set.

- Classes of code to create data sets in a consistent manner across different
    bulk data inputs are available in the Analysis_set class and subclasses.  The
    output from the CodeOcean run includes two zipped data set from Analysis_set.
    
- Text indications of missing values in `CASNumber`, `IngredientName`, and `Supplier`
    have been consolidated into the single token: ""MISSING."" See the tranlation table,
    \data\missing_values.csv.
    
- We include a more comprehensive and searchable Data Dictionary with tables
    showing components of canned data sets.

- New external reference data sets have been added: The Clean Water Act list as 
    curated by the EPA, the EPA's ""Drinking Water Standard and Health Advisories Table"",
    EPA's master PFAS list and 
    EPA's list of volatile chemicals. These fields, `is_on_CWA`, `is_on_DWSHA`,
    `is_on_PFAS_list` and `is_on_volatile_list`,
    can be used to quickly identify those groups of chemicals.

- Large-scale refactoring of code has removed unused or overly complicated 
    sections.

**Version 9**: 
- Data download from FracFocus on March 5, 2021.
- Correct FF_stats.py calculation for percent non-zero in the integer
    and float section. 
- Generate geographic clusters as proxy of wellpad identity;
    clusters are found in the string field `clusterID`. (Note that a specific clusterID will NOT be
    consistent across data set versions in the way that UploadKey is; don't depend
    on it!).  
- The fields `FederalWell` and `IndianWell` have been changed to string type -
    previously, they were boolean (T/F) but that type does not allow for empty
    cells which occurs in the SkyTruth data, leading to misinformation. 
- Added `PercentHighAdditive` to full data output to allow for better investigations
    of TradeName usage. 
- Rename the old field `infServiceCo` to `primarySupplier` to
    better reflect its generation.
- Added chemical lists of the Clean Water Act, the Safe Drinking Water Act,
    and from California's Proposition 65 lists to help identify chemicals of
    concern.

**Version 8.1**: Correct slight documentation omission.

**Version 8**: Added WellName field to filtered data output.  Added chemical ingredient
   codes from the WellExplorer project (www.WellExplorer.org) -- fields with
   the prefix `we_` are from that project. See https://doi.org/10.1093/database/baaa053
   Data download from FracFocus on October 23, 2020.

**Version 7**: Data downloaded from FracFocus on July 31, 2020.  `TradeName` added
   to exported data.

**Version 6**: Save data tables in pickled form into results section so that it may be
   exported to other projects.

**Version 5**: Data downloaded from FracFocus on May 14, 2020.  No other changes.

**Version 4**: Data downloaded from FracFocus on March 20, 2020.  Added the generated
   field, `infServiceCo`. This field is an attempt to identify the primary
   service company of a fracking event.  Including in the output files the 
   raw field `Projection` which is needed to accurately map using lat/lon
   data.

**Version 3**: Data downloaded from FracFocus on Jan. 22, 2020. Modified the 
   FF_stats module to generate separate reports for the ""bulk download"" and
   the ""SkyTruth"" data sources.  Both are reported in ""ff_raw_stats.txt"" in the
   results section.)

**Version 2**: Data downloaded from FracFocus on Jan. 22, 2020. Incorporated 
   basic statistics on the raw FracFocus data (see ""ff_raw_stats.txt"" in the
   results section.)

**Version 1**: Data downloaded from FracFocus on Jan. 22, 2020. Similar 
   to the Proof-of-Concept version with the following new features:
   SkyTruth archive
   has been incorporated.  Links to references include: Elsner & Hoelzer 2016, 
   TEDX chemical list and TSCA list.


",0,0,2,0,oil-and-gas,"[chemicals, data-science, disclosures, fracking, oil-and-gas]",0.0
68,gubkinbot,pipecounter,,https://github.com/gubkinbot/pipecounter,https://api.github.com/repos/pipecounter/gubkinbot,Simple tubing counting tool,# pipecounter,0,0,1,0,oil-and-gas,"[cv, oil-and-gas, telegram-bot]",0.0
69,OmarZOS,petroleum-data-aquisition-and-monitoring-dashboard,,https://github.com/OmarZOS/petroleum-data-aquisition-and-monitoring-dashboard,https://api.github.com/repos/petroleum-data-aquisition-and-monitoring-dashboard/OmarZOS,"As a part of production management architecture, This repository holds the web component of the full information system..","# petroleum-data-aquisition-and-monitoring-dashboard
As a part of production management architecture, This repository holds the web component of the full information system..
",0,0,1,6,oil-and-gas,"[docker, express-js, mysql, node-js, oil-and-gas]",0.0
70,energypriceapi,energypriceapi-python,,https://github.com/energypriceapi/energypriceapi-python,https://api.github.com/repos/energypriceapi-python/energypriceapi,Official Python API wrapper for energypriceAPI.com,"# EnergypriceAPI

energypriceapi is the official Python API wrapper for EnergypriceAPI.com. This allows you to quickly integrate our foreign exchange rate API and currency conversion API into your application. Check https://energypriceapi.com documentation for more information.

## Installation

Install the latest release with:


    pip install energypriceapi

## Usage

```python
from energypriceapi.client import Client

api_key = 'SET_YOUR_API_KEY_HERE'
client = Client(api_key)
```
---
## Documentation

#### fetchSymbols()
```python
client.fetchSymbols()
```

[Link](https://energypriceapi.com/documentation#api_symbol)

---
#### fetchLive(base, currencies)

- `base` <[string]> Optional. Pass in a base currency, defaults to USD.
- `currencies` <[List]<[string]>> Optional. Pass in an list of currencies to return values for.

```python
client.fetchLive(base='USD', currencies=['BRENT','GASOLINE','NATURALGAS','WTI'])
```

[Link](https://energypriceapi.com/documentation#api_realtime)

---
#### fetchHistorical(date, base, currencies)

- `date` <[string]> Required. Pass in a string with format `YYYY-MM-DD`
- `base` <[string]> Optional. Pass in a base currency, defaults to USD.
- `currencies` <[List]<[string]>> Optional. Pass in an list of currencies to return values for.

```python
client.fetchHistorical(date='2021-04-05', base='USD', currencies=['BRENT','GASOLINE','NATURALGAS','WTI'])
```

[Link](https://energypriceapi.com/documentation#api_historical)

---
#### convert(from_currency, to_currency, amount, date)

- `from_currency` <[string]> Optional. Pass in a base currency, defaults to USD.
- `to_currency` <[string]> Required. Specify currency you would like to convert to.
- `amount` <[number]> Required. The amount to convert.
- `date` <[string]> Optional. Specify date to use historical midpoint value for conversion with format `YYYY-MM-DD`. Otherwise, it will use live exchange rate date if value not passed in.

```python
client.convert(from_currency='USD', to_currency='EUR', amount=100, date='2021-04-05')
```

[Link](https://energypriceapi.com/documentation#api_convert)

---
#### timeframe(start_date, end_date, base, currencies)

- `start_date` <[string]> Required. Specify the start date of your timeframe using the format `YYYY-MM-DD`.
- `end_date` <[string]> Required. Specify the end date of your timeframe using the format `YYYY-MM-DD`.
- `base` <[string]> Optional. Pass in a base currency, defaults to USD.
- `currencies` <[List]<[string]>> Optional. Pass in an list of currencies to return values for.

```python
client.timeframe(start_date='2021-04-05', end_date='2021-04-06', base='USD', currencies=['BRENT','GASOLINE','NATURALGAS','WTI'])
```

[Link](https://energypriceapi.com/documentation#api_timeframe)

---
#### change(start_date, end_date, base, currencies)

- `start_date` <[string]> Required. Specify the start date of your timeframe using the format `YYYY-MM-DD`.
- `end_date` <[string]> Required. Specify the end date of your timeframe using the format `YYYY-MM-DD`.
- `base` <[string]> Optional. Pass in a base currency, defaults to USD.
- `currencies` <[List]<[string]>> Optional. Pass in an list of currencies to return values for.

```python
client.change(start_date='2021-04-05', end_date='2021-04-06', base='USD', currencies=['BRENT','GASOLINE','NATURALGAS','WTI'])
```

[Link](https://energypriceapi.com/documentation#api_change)

---
**[Official documentation](https://energypriceapi.com/documentation)**


---
## FAQ

- How do I get an API Key?

    Free API Keys are available [here](https://energypriceapi.com).

- I want more information

    Checkout our FAQs [here](https://energypriceapi.com/faq).


## Support

For support, get in touch using [this form](https://energypriceapi.com/contact).


[List]: https://www.w3schools.com/python/python_datatypes.asp 'List'
[number]: https://www.w3schools.com/python/python_datatypes.asp 'Number'
[string]: https://www.w3schools.com/python/python_datatypes.asp 'String'",0,0,1,0,oil-and-gas,"[currency, currency-exchange-rates, gasoline-prices, oil, oil-and-gas, wti, wti-oil]",0.0
71,prince13i-zz,AL_selenium_bot,,https://github.com/prince13i-zz/AL_selenium_bot,https://api.github.com/repos/AL_selenium_bot/prince13i-zz,A powerful python selenium based bot to download files using multi threading and OOPS concept.,"# AL_SeleniumBot
A powerful python selenium based bot to download files using multi threading and OOPS concept.

> ### Language: Python, T-SQL

***State:*** Alabama - AL - 01

***Industry:*** Oil & Gas Industry

***Website Type:*** Regulatory Authority

***Description:***

WebScraping Spider built using powerful modules as Selenium, Pandas, Threading, sqlalchemy etc.

The purpose of this spider is to download the following datafiles and feed into SQL server for further analytics.
     
     1. Well production data
     
     2. Well injection data

***Website used:***
* https://www.gsa.state.al.us
",0,0,1,0,oil-and-gas,"[alabama, automation, multithreading, oil-and-gas, oops, python, selenium, webscraping]",0.0
72,energypriceapi,energypriceapi-swift,,https://github.com/energypriceapi/energypriceapi-swift,https://api.github.com/repos/energypriceapi-swift/energypriceapi,Official Swift API wrapper for energypriceAPI.com,"# EnergypriceAPI

energypriceapi is the official Swift wrapper for EnergypriceAPI.com. This allows you to quickly integrate our API and foreign exchange rate API into your application. Check https://energypriceapi.com documentation for more information.

## Installation

We offer multiple ways to install our library.

### Cocoapods

In your `Podfile`, specify:

`pod EnergypriceAPI`

### Carthage

In your `Cartfile`, specify:

`github ""EnergypriceAPI""`

### Swift Package Manager

In Xcode, select File ▸ Swift Packages ▸ Add Package Dependency… and use the url: `https://github.com/energypriceapi/energypriceapi-swift.git`

## Usage

```swift
import EnergypriceAPI

apiKey = 'SET_YOUR_API_KEY_HERE'
let client = EnergypriceAPIService(apiKey: apiKey)
```
---
## Documentation

#### fetchSymbols()
```swift
client.fetchSymbols(completion: { result in
  // do something
}
```

[Link](https://energypriceapi.com/documentation#api_symbol)

---
#### fetchLive(base, currencies)

- `base` <[string]> Optional. Pass in a base currency, defaults to USD.
- `currencies` <[List]<[string]>> Optional. Pass in an list of currencies to return values for.

```swift
client.fetchLive(base: ""USD"", currencies: [""BRENT"", ""GASOLINE"", ""NATURALGAS"", ""WTI""], completion: { result in
  // do something
}
```

[Link](https://energypriceapi.com/documentation#api_realtime)

---
#### fetchHistorical(date, base, currencies)

- `date` <[string]> Required. Pass in a string with format `YYYY-MM-DD`
- `base` <[string]> Optional. Pass in a base currency, defaults to USD.
- `currencies` <[List]<[string]>> Optional. Pass in an list of currencies to return values for.

```swift
clientfetchHistorical(date: ""2021-04-05"", base: ""USD"", currencies: [""AUD"", ""XAG"", ""GBP"", ""JPY""], completion: { result in
  // do something
}
```

[Link](https://energypriceapi.com/documentation#api_historical)

---
#### convert(from_currency, to_currency, amount, date)

- `from_currency` <[string]> Optional. Pass in a base currency, defaults to USD.
- `to_currency` <[string]> Required. Specify currency you would like to convert to.
- `amount` <[number]> Required. The amount to convert.
- `date` <[string]> Optional. Specify date to use historical midpoint value for conversion with format `YYYY-MM-DD`. Otherwise, it will use live exchange rate date if value not passed in.

```swift
client.convert(fromCurrency: ""USD"", toCurrency: ""EUR"", amount: 100.0, date: ""2021-04-05"", completion: { result in
  // do something
}
```

[Link](https://energypriceapi.com/documentation#api_convert)

---
#### timeframe(start_date, end_date, base, currencies)

- `start_date` <[string]> Required. Specify the start date of your timeframe using the format `YYYY-MM-DD`.
- `end_date` <[string]> Required. Specify the end date of your timeframe using the format `YYYY-MM-DD`.
- `base` <[string]> Optional. Pass in a base currency, defaults to USD.
- `currencies` <[List]<[string]>> Optional. Pass in an list of currencies to return values for.

```swift
client.timeframe(startDate: ""2021-04-05"", endDate: ""2021-04-06"", base: ""USD"", currencies: [""AUD"", ""XAG"", ""GBP"", ""JPY""], completion: { result in
  // do something
}
```

[Link](https://energypriceapi.com/documentation#api_timeframe)

---
#### change(start_date, end_date, base, currencies)

- `start_date` <[string]> Required. Specify the start date of your timeframe using the format `YYYY-MM-DD`.
- `end_date` <[string]> Required. Specify the end date of your timeframe using the format `YYYY-MM-DD`.
- `base` <[string]> Optional. Pass in a base currency, defaults to USD.
- `currencies` <[List]<[string]>> Optional. Pass in an list of currencies to return values for.

```swift
client.change(startDate: ""2021-04-05"", endDate: ""2021-04-06"", base: ""USD"", currencies: [""AUD"", ""XAG"", ""GBP"", ""JPY""], completion: { result in
  // do something
}
```

[Link](https://energypriceapi.com/documentation#api_change)

---
**[Official documentation](https://energypriceapi.com/documentation)**


---
## FAQ

- How do I get an API Key?

    Free API Keys are available [here](https://energypriceapi.com).

- I want more information

    Checkout our FAQs [here](https://energypriceapi.com/faq).


## Support

For support, get in touch using [this form](https://energypriceapi.com/contact).


[List]: https://www.w3schools.com/python/python_datatypes.asp 'List'
[number]: https://www.w3schools.com/python/python_datatypes.asp 'Number'
[string]: https://www.w3schools.com/python/python_datatypes.asp 'String'",0,0,1,0,oil-and-gas,"[energy-api, energy-data, exchange-rates, foreign-exchange-rates, gasoline-prices, oil-analysis, oil-and-gas]",0.0
73,StartrexII,CalibrationFloatLevelGauge,,https://github.com/StartrexII/CalibrationFloatLevelGauge,https://api.github.com/repos/CalibrationFloatLevelGauge/StartrexII,studying the float level gauge,"# <center> Изучение поплавкового уровнемера </center>
## Оглавление
1. [Описание проекта](#Описание-проекта)
2. [Описание данных](#Описание-данных)
3. [Зависимости](#Используемые-зависимости)
4. [Установка проекта](#Установка-проекта)
5. [Использование проекта](#Использование)
6. [Авторы](#Авторы)
7. [Выводы](#выводы)

## Описание проекта

**Принцип действия поплавкового уровнемера с ультразвуковым каналом связи** 

<center> <img src=art/levelGaugeDesign.png> </center>

Прибор состоит из головки прибора (1) с блоком электроники, металлической штанги (2) с фторопластовым покрытием и поплавка (3), двигающегося вдоль штанги.

Измерение уровня основано на измерении времени распространения ультразвукового импульса от поплавка чувствительного элемента в нижней части головки прибора до головки прибора.

Внутри штанги (2) находится стальная проволока с катушкой возбуждения, импульс тока в которой создает магнитное поле вдоль штанги. В подвижном поплавке, находящемся на границе раздела двух сред, находится постоянный магнит. В месте расположения поплавка (3), засчёт эффекта магнитострикции, возникает импульс крутильной деформации, который распространяется по проволоке вдоль штанги и фиксируется чувствительным пьезоэлементом в головке прибора.

В головке прибора (1) вычисляется время о момента формирования импульса тока, создающего магнитное поле, до момента приёма импульса упругой деформации.


**Данный проект** направлен на построение градуировочной характеристики поплавкового уровнемера ДУУ2 на основе данных прямых измерений уровня жидкости в резервуаре.

**О структуре проекта:**
* [data](./data) - папка с исходными табличными данными
* [art](./art) - папка с изображениями, необходимыми для проекта 
* [plotly](./plotly) - папка с графиками, для возможности их просмотра в браузере
* [floatLevelGaugeChar.ipynb](./floatLevelGaugeChar.ipynb) - jupyter-ноутбук, содержащий основной код проекта, в котором расчитываются основные статистические характеристики измерений и строится градуировочная характеристика
* [requirements.txt](./requirements.txt) - файл с версиями используемых модулей, для воспроизводимости кода

## Описание данных
В этом проекте используются данные прямых измерений уровня жидкости в резервуаре, установленного при помощи линейки по сравнению с показаниями уровнемера.

Для каждого значения уровня представлены 6 измерений уровнемером (3 повторных для прохода уровнемера вверх(от наибольшего уровня к наименьшому) и 3 для прохода вверх(от наименьшего уровня к наибольшему)).

__Важный момент - уровень измеренный линейкой(составляет столбцы исходной таблицы) представлены в сантиметрах, в значения, полученные с уровнемера - в миллиметрах.__

Файл с данными можно найти [здесь](./data/directMeasurement.csv).

## Используемые зависимости
* Python (3.11.1):
    * [pandas (1.5.3)](https://pandas.pydata.org)
    * [plotly (5.13.0)](https://plotly.com/python/)

## Установка проекта

```
git clone https://github.com/StartrexII/CalibrationFloatLevelGauge.git
```

## Использование
* Расчет статистических показателей представлен в jupyter-ноутбуке floatLevelGaugeChar.ipynb.

* Градуировочная характеристика представлена [html-файлом floatLevelGaugeChar.html (открывается в браузере)](plotly/floatLevelGaugeChar.html 'Ссылка на файл с графиком').

## Авторы

* [Егор Орлов](https://vk.com/liquidlogic)

## Выводы

В результате расчета и построения характеристики понятно, что уровнемеры ультразвуковые - очень точные устройства, погрешность прямых измерений которых составляет порядка нескольких миллиметров (в нашем случае стандартное отклонение составило не более 3мм), что совершенно не критично при измерении высоты жидкости в резервуаре, которая может составлять более 10 метров.",0,0,1,0,oil-and-gas,"[calibration, oil-and-gas, pandas-dataframe, python, science]",0.0
74,sahanextractives,sahan,sahanextractives,https://github.com/sahanextractives/sahan,https://api.github.com/repos/sahan/sahanextractives,,# sahan,0,0,1,0,oil-and-gas,"[mining, oil-and-gas]",0.0
75,rensalzado,DiluentESPOptimizationDynamic,,https://github.com/rensalzado/DiluentESPOptimizationDynamic,https://api.github.com/repos/DiluentESPOptimizationDynamic/rensalzado,"NTNU TEP4240 System Simulation, final project","# DiluentESPOptimizationDynamic

TEP4240 – NTNU, Energy and Process Engineering Department, Subject: System Simulation

  Final course project: Diluent Injection in Production Systems with ESP-Lifted Wells

  The objective of the project was to simulate a system using Bond-graph representations
  The model provides a way to observe the dynamic behavior of the system. 

  This reposity includes:
   - Classes codes
   - Simulation main file
   - Final project report
",0,0,1,0,oil-and-gas,"[bond-graphs, dynamic-simulations, matlab, matlab-oop, modelling, ntnu, oil-and-gas, oop-in-matlab]",0.0
76,Phreak87,OilMeter,,https://github.com/Phreak87/OilMeter,https://api.github.com/repos/OilMeter/Phreak87,Measure oil consumption on a ESP8266 with a TCS34725 / APDS9960 Color Sensor on the burner status LED or HC-SR04 distance sensor,"# OilMeter

Measure fluid consumption and levels on a ESP8266 via different Sensors:
- TCS34725 or APDS9960 color sensor (Consumption via status-LED and Time)
- HC-SR04/JSN-SR04T distance-sensor (just leveling).
- VL53L0X Laser Distance            (just leveling, not use for clear Water or add a swimmer)
- Trigger Sensor                    (Trigger Sensors like IR, Button, Motion detector, sound or proximity switch (time based and trigger consumption)

## Oilmeter comes with the following features included:
- First start as AccessPoint (OilMeter, PWD: OilMeter), after saving Wifi-Settings it will connect to default Wifi.
- Nice handmade and Mobile-friendly Webinterface for configuration and status with live streaming updates via web-events.
- Reads the sensor status (Preheat, Blower start, burn, error and off) based on predefined color-ranges and calculates oil-consumption
  by the definition of L/h of your heater or calculates the level based on laser/sound measuring on water tanks (konical, circular and rectangular).
- MQTT-Connection with Homeassistant: Publishes status to a MQTT-Broker and automatically creates entities in Homeassistant.
- Updates on the fly with direct Firmware Uploads (powered by ElegantOTA).
- existing Sensor adapter plate for burner can be 3D-Printed.

## Flashing
Please install Visual Studio code and inside visual studio code the PlatformIO Workbench.
Load the project folder and adapt your Com-Port in Platform.ini. Click Build and Upload.
for updates you can use the OTA-Feature and the .bin files included in this repository.
More installation Methods will follow (please help how to setup Chrome-Online Firmware flasher).

## Notes

If you have problems with ESP-restarts please consider that the burner needs a high voltage to start burning.
(in the webinterface you can see the reboot-cause. 'exception' can be a hint for such a high-voltage reset).
This high voltage can be transmitted via the wires and causes the ESP to restart. To avoid restarts you should
use isolated wires for the power-switch and sensor cabling and a separate isolation (e.g. Tabac-Box) to shim the esp
itself. make sure the isolation is connected to some ground of your house. Also make sure you use a stable power-supply.

To make good measures for your oil tank you need to know the actual tank volume. You can do this by estimate based on the height of the oil in the tank
or you start from scratch if the tank is filled up to its maximum. After the first refill you can adapt your values based on whats 
really used vs. whats refilled via the adaption factor in the webinterface.

For the Level Mesurements via Laser or Sound distance measuring calculate the surface like:
(if konical then do seperate calculations for top and bottom, if not use same surface for both).
Circular:     diameter * 3,14
Quadratic:    diameter 
Rectangular:  (h * w) / 2

## Pictures

![Installation](Pictures/Installation.jpg)
![Rainwater](Pictures/RainWater.jpeg)
![3D Model](Pictures/Adapter_Plate.png)
![Webinterface](Pictures/Webinterface.png)
![Homeassistant](Pictures/HassIO.png)

# Connection diagram (3.3 or 5V based on your Sensor)

| ESP   | GYP | TCS | HC/JSN | VL52 | TRIG |
| ----- | --- | --- | ------ | ---- |----- |
| 3V3/5V| VCC | VIN | VCC    | VIN  | VIN  |
| GND   | GND | GND | GND    | GND  | GND  |
| D6    | INT | LED | ------ | ---- | ---- | 
| D2    | SDA | SDA | ------ | SDA  | ---- |
| D1    | SCL | SCL | ------ | SCL  | INT  |
| D7    |     |     | TRIG   | ---- | ---- |
| D8    |     |     | ECHO   | ---- | ---- |

#Todo

- Multicolor LED for sensor status and system messages
- auto adaptation of the correct factor value on fill-up (needs to be between 2 times full)
- animated svg-picture of the tank volume in percent
- compress webpages and js with compressors and gzip before uploading.
- webpages based on local bootstrap.
- Include SKU237545 Pressure Sensor
- Include HK1100C Pressure Sensor
- Include HX711 weight Sensor
- Include Liter per Percent heigth measurements.
",0,0,1,0,oil-and-gas,"[apds-9960, esp8266, hc-sr04, home-assistant, home-automation, home-automaton, mqtt, oil, oil-and-gas, tcs34725]",0.0
77,catalyst-cooperative,pudl,catalyst-cooperative,https://github.com/catalyst-cooperative/pudl,https://api.github.com/repos/pudl/catalyst-cooperative,"The Public Utility Data Liberation Project provides analysis-ready energy system data to climate advocates, researchers, policymakers, and journalists.",,393,393,16,362,coal,"[cems, climate, coal, ddj, eia, eia860, eia923, electricity, emissions, energy, epa, etl, ferc, ghg, natural-gas, open-data, pudl, python, sqlite, utility]",0.0
78,3dfxdev,EDGE,3dfxdev,https://github.com/3dfxdev/EDGE,https://api.github.com/repos/EDGE/3dfxdev,EDGE Source Code,"Welcome to EDGE!
---------------------

![](https://a.fsdn.com/con/app/proj/edge2/screenshots/edgelogo6.png)

[![Build status](https://ci.appveyor.com/api/projects/status/ghevag13p9ue60no?svg=true)](https://ci.appveyor.com/project/raa-eruanna/hyper3dge) [![Build Status](https://travis-ci.org/3dfxdev/EDGE.svg?branch=master)](https://travis-ci.org/3dfxdev/EDGE) [![EDGE](https://github.com/3dfxdev/EDGE/actions/workflows/edge.yml/badge.svg)](https://github.com/3dfxdev/EDGE/actions/workflows/edge.yml)

[![Generic badge](https://img.shields.io/badge/Language-C++-blue.svg)](https://shields.io/)[![Codacy Badge](https://api.codacy.com/project/badge/Grade/cf7282627d254b89aa466d3291e8b8fe)](https://www.codacy.com/app/3dfxdev/EDGE?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=3dfxdev/EDGE&amp;utm_campaign=Badge_Grade)[![GitHub issues](https://img.shields.io/github/issues/3dfxdev/EDGE)](https://github.com/3dfxdev/EDGE/issues)

#### EDGE is an advanced OpenGL source port spawned from the DOOM engine, with focus on easy development and expansion for modders and end-users.

# (C) 1999-2022 The EDGE Team & [Contributors](https://github.com/3dfxdev/hyper3DGE/blob/master/AUTHORS.md)
##### Uses GPL-licensed technology from id Tech 1-4 (C) 1997-2011 id Software, LLC
##### DOSDoom originally developed by Chi Hoang and the DOSDoom Team, (C) 1997-1999
##### Visit our sister port which derives from an earlier branch of EDGE, called [Edge-Classic](https://github.com/dashodanger/EDGE-classic). (C) The Edge-Classic Team, 2021-2022
#### Licensed under the GPLv2 (or greater)
http://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html
# See our EDGE Wiki (http://3dfxdev.net/edgewiki/index.php/Main_Page)
--

## Development Builds
---
Occasionally we will package and upload builds for Windows and Linux. You can get these builds via [DRDTeam](https://devbuilds.drdteam.org/3dge/).
* IMPORTANT: these builds, despite the Git revision that EDGE reports via the console, don't always reflect the commit hash; often they will have their code or content modified in a way that is non-standard to official releases. We are working on an Autobuild system via Github - currently this is only usable for Linux.


## Build System for EDGE
---
To build EDGE, please see the [build guides](https://github.com/3dfxdev/hyper3DGE/tree/master/build_guide) and choose
based on your platform and compiler. Some of this information may be out of date; feel free to open up a Github Issue for assistance.

## Libraries
---
For the list of libraries required by EDGE, please see the
following document: docs/tech/libraries.txt.
##### You can also obtain the pre-compiled libraries by looking in the build_guide, depending on what compiler you are planning to use.

## The EDGE Team
---
### Team and Contributors

EDGE has and will continue to exist thanks to all the people who contribute.

<a href=""https://github.com/3dfxdev/EDGE/graphs/contributors"">
  <img src=""https://contrib.rocks/image?repo=3dfxdev/EDGE"" />
</a>

## Development History
---
Read the historical timeline of EDGE at [The Doom Wiki](https://doomwiki.org/wiki/EDGE).

---

# Support
* Visit the [EDGE forums](http://tdgmods.net/smf) and get involved with the
community and the various projects for the engine.
* The [EDGEWiki](http://3dfxdev.net/edgewiki) is also a great resource for
editing documentation and other information related to the engine.

(C) 1999 - 2022 [Isotope SoftWorks](https://www.facebook.com/IsotopeSoftWorks/) and Contributors (The EDGE Team). All Rights Reserved.
",69,69,17,7,coal,"[3dge, 3dge-engine, coal, ddf, doom, dosdoom, edge, edge-engine, glbsp, hyper-3dge, hyperedge, rts, source-port]",0.0
79,capstone-coal,pycoal,capstone-coal,https://github.com/capstone-coal/pycoal,https://api.github.com/repos/pycoal/capstone-coal,Python toolkit for characterizing Coal and Open-pit surface mining impacts on American Lands,,24,24,9,9,coal,"[classification, classification-algorithm, coal, hyperspectral-image-classification, open-surface-mining, python, surface-mining-activities]",0.0
80,aapple,sm-coal-app,,https://github.com/aapple/sm-coal-app,https://api.github.com/repos/sm-coal-app/aapple,这是一个使用Ionic2开发的集数据展示，交易，交流于一体的APP,"# 初衷
跳槽以后来到了新的公司，新的项目，每天朝九晚五，终于摆脱了以前朝九晚九的加班生活，谁说程序员就必须加班了，感觉自己作息也变得正常了起来。

空余出的大段时间拿来干什么呢？闲不住的小少年就想着自己做个啥，嗯，做个APP吧，毕竟PC已经是上个时代的事情了，人工智能AI、大数据啥的咱也不懂，又作为一只喜欢抢热点的全站攻城狮，就自己动手撸前后台代码搞一个APP吧，没事装在手机里玩儿也可以装装逼嘛是吧。

说干就干，做个啥APP呢？又把我难住了，出师未捷身先死呀，人类的大部分行动都倒在了第一步，不行，必须想出来。嗯，突然想起来前一段时间有朋友找我想做一个煤炭物流APP，说你不是会写代码吗，给我搞一个APP呗，我流着泪在半夜12点回过去消息说，哥，我才下班，你看能行不。现在有时间了呀，煤炭+物流，符合地方特色，就这样定了。

然后就是蒙头撸代码，中间暂时省略10w字，反正一个月以后做完了。不瞎比比了，先上图再说。



![首页](http://upload-images.jianshu.io/upload_images/3529154-a6b6a94e05b98e29.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


![煤价](http://upload-images.jianshu.io/upload_images/3529154-711d2730c0a66607.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


![物流](http://upload-images.jianshu.io/upload_images/3529154-b00de989da915e7a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


![我的](http://upload-images.jianshu.io/upload_images/3529154-247ef2fff5cde007.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

二级页面就不做展示了，总之自我感觉还是肥肠的丰满啦（自信脸）。

# 技术准备
虽然任何开发的第一步都是可行性分析和设计，但是鉴于讲起来太庞杂，也不符合这次分享的主题，就略过了。直接从开发阶段讲起。

万里长征第一步当然是技术选型啦，为了配合这次高大上的APP，使用的技术当然也要高大上，不然怎么显示我的全站水平呢（颜面），毕竟能站着编程的人肯定不一般。

本次前端APP的核心框架是Ionic2（其实已经是Ionic3.5了，统一起见还是称它为Ionic2吧），后端的核心框架是Spring Boot。都是基于我现在的技术栈，在不超出自己控制能力的前提下选择了当前最优秀的框架，用之可以大大提高生产力。哈哈，不吹了，总之目前我的感觉就是选的漂亮，谁用谁知道。

### 工具准备
- 前端框架：Ionic2
- 后端框架：Spring Boot
- 开发工具：Idea
- 前端调试工具：Chrome
- 代码仓库：Github

为啥要特意说一下开发工具呢，low不low啊，又不是小学生。因为Idea相对于eclipse来说实在是好用了无数倍啊（捂脸哭），如果还没体验过的墙裂介意无论是前端还是后端都试一下，简直就是开发的救星。

同时，即便是一个人开发也推荐用Github，因为实在是太方便了，随时随地，修改提交你的代码。如果是多人协作，那就更应该用Github了，因为谁也不想把代码用QQ传来传去对吧？

# 开发阶段分解
因为本专题重点想讲的是基于Ionic2的APP开发，所以就以前端为核心去分解，后端如有必要，后面再补充。
###### 1. [Ionic2实战-项目初始化](http://www.jianshu.com/p/307afd700fef)；
###### 2. [Ionic2实战-框架和模块目录结构说明和设计](http://www.jianshu.com/p/cd7fae4ef267)；
###### 3. [Ionic2实战-Android版打包](http://www.jianshu.com/p/37633bda766c)；
###### 4. [Ionic2实战-iOS版打包](http://www.jianshu.com/p/3486154719d1)；
###### 5. [Ionic2实战-网页版编译部署](http://www.jianshu.com/p/11772fed6f90)；
###### 6. [Ionic2实战-功能模块开发基本说明](http://www.jianshu.com/p/3703e1f47b1a)；
###### 7. [Ionic2实战-Icon图片和Splash开屏图片自定义](http://www.jianshu.com/p/f7b5d9832b7e)；
###### 8. [Ionic2实战-Tab菜单栏自定义](http://www.jianshu.com/p/b316e46e412c)；
###### 9. [Ionic2实战-Cordova插件安装](http://www.jianshu.com/p/ce4080c97345)；
###### 10. [Ionic2实战-第三方类库依赖添加](http://www.jianshu.com/p/c41b4050a5ef)；
###### 11. [Ionic2实战-框架样式自定义](http://www.jianshu.com/p/3f0d25340919)；
###### 12. [Ionic2实战-Http请求模块设计](http://www.jianshu.com/p/eb8736d7b603)；
###### 13. [Ionic2实战-跨域问题处理](http://www.jianshu.com/p/7ce5a1d90ed7)；
###### 14. [Ionic2实战-路由导航功能说明](http://www.jianshu.com/p/f0cd6ff9ad81)；
###### 15. [Ionic2实战-如何使用阿里字体图标库](http://www.jianshu.com/p/bae9f3ed37cc)；
###### 16. [Ionic2实战-APP应用内升级模块开发](https://www.jianshu.com/p/3d85a6398102)；
###### 17. [Ionic2实战-图片点击缩放功能开发](https://www.jianshu.com/p/454a0a16ce54)；
###### 18. [Ionic2实战-时间转换库moment的使用](https://www.jianshu.com/p/90a54e546639)；
###### 19. [Ionic2实战-第三方页面嵌入功能开发](https://www.jianshu.com/p/9afab9180376)；
###### 20. [Ionic2实战-微信分享功能开发](https://www.jianshu.com/p/36485438e5af)；
###### 21. [Ionic2实战-图片上传功能开发](https://www.jianshu.com/p/598de407620b)；
###### 22. [Ionic2实战-拍照功能开发](https://www.jianshu.com/p/212fa3b7548e)；
###### 23. [Ionic2实战-注册登录模块开发](https://www.jianshu.com/p/5cb92e049b9f)；
###### 24. [Ionic2实战-首页模块开发](https://www.jianshu.com/p/1bc4cee83835)；
###### 25. [Ionic2实战-煤价模块开发](https://www.jianshu.com/p/98b944cd2e7f)；
###### 26. [Ionic2实战-物流模块开发](https://www.jianshu.com/p/bc841327820b)；
###### 27. [Ionic2实战-个人中心模块开发](https://www.jianshu.com/p/a92d077b3f8c)；
###### 28. [Ionic2实战-APP如何在Android应用商店上架发布](https://www.jianshu.com/p/f56787610c69)；
###### 29. [Ionic2实战-APP如何在App Store上架发布](https://www.jianshu.com/p/76619c7852d4)；
###### 30. [Ionic2实战-APP如何生成推广链接进行多渠道推广](https://www.jianshu.com/p/16e154364036)；


# 编译运行方法

> 1、npm install

> 2、ionic serve

# 最后说点
以上过程包含了Ionic2APP从开始的项目创建到基础功能搭建，到业务功能开发，再到打包和发布，还有如何推广，包含了一个商业APP应用的整个生命周期，一些新手坑我会重点写出来提醒大家，因为自己作为新手也踩的很蛋疼。

以上每一点都会写一篇文章来详细说明，过程中如果想到其他重要的功能就补充进去。另外上面都是APP的技术框架模块的说明，可能不够完全，想到什么会再继续补充。

如果你有什么关于Ionic2或者Spring Boot想交流的也可以留言或者私信再或者微信联系我，欢迎欢迎。

# 前端项目开源地址
项目的完整代码在我的GitHub上，如感兴趣可以下载查看：
https://github.com/aapple/sm-coal-app

# 后端项目开源地址
应广大同学的强烈要求，支撑该APP后端的Spring-Boot项目也开放出来了，可以自由下载学习：
https://github.com/aapple/coalapp

",21,21,3,1,coal,"[app, coal, html, ionic, ionic2, ionic3, ionicframework, spring-boot]",0.0
81,ewoken,world-data-app,,https://github.com/ewoken/world-data-app,https://api.github.com/repos/world-data-app/ewoken,,"# world-data-app
[![Build Status](https://travis-ci.com/ewoken/world-data-app.svg?branch=master)](https://travis-ci.com/ewoken/world-data-app)

Demo at [https://ewoken.github.io/world-data-app/](https://ewoken.github.io/world-data-app/)

Snapshot:
![Screenshot of demo](https://ewoken.github.io/world-data-app//homeSnapshot.jpg)

## Contributing

Feel free to propose PR with new features or updated data.

### Get started

```bash
git clone https://github.com/ewoken/world-data-app.git
cd world-data-app
npm i
npm start
```
",9,9,4,41,coal,"[coal, data, energy, gas, oil, vizualisation, world]",0.0
82,zero-one-group,global-coal-countdown,zero-one-group,https://github.com/zero-one-group/global-coal-countdown,https://api.github.com/repos/global-coal-countdown/zero-one-group,This is the accompanying repository for the Bloomberg Global Coal Countdown website.,"![](https://bloombergcoalcountdown.com/_next/static/images/big-black-logo-46b32763924ec161ae53ca88b2c25517.webp)

This is the accompanying repository for the **[Bloomberg Global Coal Countdown](https://bloombergcoalcountdown.com/) (BGCC)** website.

<ul>
  <li><a href=""#data-sources"">Data Sources</a></li>
  <li><a href=""#dashboard-data"">Dashboard Data</a></li>
  <li><a href=""#schema-and-validation"">Schema and Validation</a></li>
  <li><a href=""#license"">License</a></li>
</ul>

## Data Sources

The data presented on the BGCC website comes from the following sources:

1. [Global Energy Monitor's (GEM)](https://globalenergymonitor.org/) [Global Coal Plant Tracker (GCPT)](https://globalenergymonitor.org/projects/global-coal-plant-tracker/) and the accompanying GCPT status changes since 2015. GEM's GCPT dataset provides unit-level data and it is the basis for all calculations related to coal-plant capacity, status, technology and age.
2. Country-level coal phaseout strategies, country-level expected retirements and global emission pathways from [University of Maryland's (UMD)](https://www.umd.edu/) [Center for Global Sustainability (CGS)](https://cgs.umd.edu/). Datasets from UMD are outputs from published research, and are therefore mostly served as is with very few modifications for aesthetic purposes such as smooth interpolation.
3. [Ember](https://ember-climate.org/)'s [Global Electricity Review 2022](https://ember-climate.org/project/global-electricity-review-2022/) data. Ember's dataset provides panel data over three dimensions, namely area (such as country and regions), year and variable (such as energy sources). It is used as the basis for all calculations related to energy mixes and trends.
4. In the Headlines content is scraped from [CoalWire](https://endcoal.org/category/coalwire/), published by [GEM](https://globalenergymonitor.org/). A CoalWire issue consists of multiple articles, news and analysis pieces, which are individually presented on the BGCC site.
5. Research and analysis pieces as curated by [Bloomberg Philanthropies](https://www.bloomberg.org/).
6. [Gapminder data, HYDE, and UN Population Division (2019) estimates](https://www.gapminder.org/data/documentation/gd003/) as presented by [Our World in Data's (OWD)](https://ourworldindata.org/) [world population data since 1800](https://ourworldindata.org/grapher/population).
8. [Natural Earth Data](https://www.naturalearthdata.com/)'s [country vectors](https://www.naturalearthdata.com/downloads/110m-cultural-vectors/110m-admin-0-countries/) as presented by [Graydon Hoare](https://gist.github.com/graydon/11198540) with our own manual modifications for aesthetic reasons in order to make the interactive map appear more presentable.
9. Country commitments to phasing out coal as well as no new coal are based on research from [UMD](https://www.umd.edu/) [CGS](https://cgs.umd.edu/) and the [Powering Past Coal Alliance (PPCA)](https://www.poweringpastcoal.org/).
10. Expected retirements by 2030 of each country are collected and maintained by [PPCA](https://www.poweringpastcoal.org/).

The countries’ political affiliations such as EU27+1, G20, G7, OECD and PPCA are taken as of July 2021. We have decided to include the United Kingdom into the EU27+1 list in order to maintain comparability with other EU figures that have been reported in the past.

## Dashboard Data

The resulting datasets are available for download as part of a release attached to the repository - see [Releases](https://github.com/zero-one-group/global-coal-countdown/releases).

The following JSONs correspond to a page on the BGCC website:

* `coal_capacity_landscape.json`
* `coal_power_generation.json`
* `country_main.json`
* `homepage.json`
* `newsfeed.json`

The following JSONs are utility lookup datasets:
* `country_coal_status.json`
* `country_iso_lookup.json`
* `iso_country_lookup.json`
* `website_texts.json`

The `mapbox.json` data is uploaded one [feature](https://docs.mapbox.com/api/maps/datasets/#the-feature-object) at a time to [Mapbox](https://www.mapbox.com/) using the [Mapbox API](https://docs.mapbox.com/api/overview/). The `country_bounding_boxes.json` is used to center the map view on the selected country.

## Schema and Validation

Each JSON file is accompanied by a [Pydantic model](https://pydantic-docs.helpmanual.io/), which is used as a schema documentation and runtime validation. The Pydantic models can be found in `schema/models.py`. The source code references two other files, namely `schema/validation.py` and `schema/enum_key_value_pairs.py`, which contain custom validators and valid country names and the corresponding ISO 3166-1 alpha-2 country codes.

In the `docker` directory, we make available a `Dockerfile` and a `requirements.txt` to build an ephemeral Docker container, which we used to generate the data at the time writing.

## License

As the datasets are built upon the GEM's GCPT dataset, the BGCC datasets are also licensed under the [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/) license. Therefore, it is allowed to copy and redistribute, as well as to build upon the datasets for any purpose including for commercial purposes. If you do build upon the datasets, you must distribute it under the same license.
",7,7,4,0,coal,"[climate-change, climate-data, coal, electricity, energy, energy-data]",0.0
83,MahadMuhammad,Learn-Assembly-The-Hard-Way,,https://github.com/MahadMuhammad/Learn-Assembly-The-Hard-Way,https://api.github.com/repos/Learn-Assembly-The-Hard-Way/MahadMuhammad,A single & unified source of all source codes & helping material in learning the Intel 8088 assembly language using DOSBOX as an emulator,"<p align=""middle"">
<a href=""https://github.com/MahadMuhammad""><img alt=""views"" title=""Github views"" src=""https://komarev.com/ghpvc/?username=MahadMuhammad&style=flat-circle"" width=""125""/></a>

![](https://img.shields.io/badge/Welcome%20to-blue.svg)
![Welcome Badge](https://img.shields.io/badge/Mahad's%20Intel%208080-blue.svg)
![Welcome](https://img.shields.io/badge/Assembly%20Programming-brightgreen.svg)
![Welcome](https://img.shields.io/badge/with%20DOSBOX%20(as%20an%20emulator)-brightgreen.svg)
![Welcome](https://img.shields.io/badge/and%20NASM%20(as%20an%20Assembler)-brightgreen.svg)
![Welcome](gh-intro-mhd.png)

# 🤔 Why Assembly?
- Why would anyone learn that language, which is hard to learn, write, debug & maintain.

- Why would anyone learn that language, which is different for different hardware & architecture.
- Why would anyone learn that language, which is not used in any modern programming language, operating system, compiler.

Great! You are reading this means that you are:
1.	Student being forced to learn assembly.

1.	Student who wants to learn assembly for [fun](https://youtu.be/9bZkp7q19f0?t=9).
1.	You are just exploring this stuff. 
1.	You have interest in learning assembly. 
1.	You want to become [hacker](https://en.wikipedia.org/wiki/Hacker_culture), [cracker](https://en.wikipedia.org/wiki/Cracker_(computing)), [computer scientist](https://en.wikipedia.org/wiki/Computer_scientist), [computer engineer](https://en.wikipedia.org/wiki/Computer_engineer), [computer architect](https://en.wikipedia.org/wiki/Computer_architect), [computer technician](https://en.wikipedia.org/wiki/Computer_technician).
### Drawbacks of Assembly:
1.	Assembly is hard to learn, read, understand, write, maintain, debug and explain to others.

1.	It is different for different hardware & [architecture](https://en.wikiversity.org/wiki/Assembly_language#Architecture). 
1.	For speed & efficiency, we use better algorithms & improved [compiler technology](https://learn.microsoft.com/en-us/archive/msdn-magazine/2015/february/compilers-what-every-programmer-should-know-about-compiler-optimizations). 
### Benefits of Assembly:
But on the other side, assembly language gives you more speed, space, capability and knowledge of your hardware. That the reason why [GOAT](https://en.wikipedia.org/wiki/Greatest_of_All_Time) programmer [Chris Sawyer](https://en.wikipedia.org/wiki/Chris_Sawyer) able to run such resource intensive game [RollerCoaster Tycoon](http://www.chrissawyergames.com/feature5.htm) on an [old processors](https://en.wikipedia.org/wiki/Pentium). So, the benefits are:

1.	You will understand how computers work.

1.	You wlll understand how [operating system](https://en.wikipedia.org/wiki/Operating_system), [CPU](https://en.wikipedia.org/wiki/Central_processing_unit), [memory](https://en.wikipedia.org/wiki/Memory_(computing)), [cache](https://en.wikipedia.org/wiki/Cache_(computing)), [register](https://en.wikipedia.org/wiki/Processor_register), [instruction](https://en.wikipedia.org/wiki/Instruction_set) works.
1.	You will understand the [computer architecture](https://en.wikipedia.org/wiki/Computer_architecture).
1.	You want to write efficient code.
1.	You want complete control of your computer.
1.	You love binary (you are not [human](http://www.captcha.net/images/recaptcha-example.gif)).


---
# 🌏 Contributors:
This repository would not be possible without the help of the following people:
- [Mohammad Nauman](https://github.com/recluze)

- [Haris Muneer](https://github.com/harismuneer)

---
# 🤑Resources:

####  Here are some resources that you can use to learn assembly language:

- [assembly-lang-course](https://github.com/recluze/assembly-lang-course)

- [BelalHashmi-Assembly-Exercise-Solutions](https://github.com/harismuneer/BelalHashmi-Assembly-Exercise-Solutions)
- [Bit-Manipulation-and-Subroutines-8086_Assembly](https://github.com/harismuneer/Bit-Manipulation-and-Subroutines-8086_Assembly)
- [Graphical-Display-Memory-and-Software_Hardware-Interrupts-x86_Assembly](https://github.com/harismuneer/Graphical-Display-Memory-and-Software_Hardware-Interrupts-x86_Assembly)
- [Stop-Watch_x86-Assembly](https://github.com/harismuneer/Stop-Watch_x86-Assembly)



#### If you want to write assembly language for [x86](https://en.wikipedia.org/wiki/X86) architecture, you can use the following assemblers:

 The offical/notes documentation of [x86](https://en.wikipedia.org/wiki/X86) assembly language is available [here](https://www.felixcloutier.com/x86/).
- [TASM](https://en.wikipedia.org/wiki/Turbo_Assembler) (Turbo Assembler)

- [MASM](https://en.wikipedia.org/wiki/Microsoft_Macro_Assembler) (Microsoft Macro Assembler)
- [FASM](https://flatassembler.net/) (Flat Assembler)
- [GAS](https://en.wikipedia.org/wiki/GNU_Assembler) (GNU Assembler)
- [NASM](https://www.nasm.us/) (Netwide Assembler)
- [YASM](http://yasm.tortall.net/) (Yet Another [x86](https://en.wikipedia.org/wiki/X86) Assembler)

#### If you want to write assembly language for [MIPS](https://en.wikipedia.org/wiki/MIPS_architecture) architecture, you can use the following assemblers:
 - The offical/notes documentation of [MIPS](https://en.wikipedia.org/wiki/MIPS_architecture) assembly language is available [here](https://www.mips.com/develop/tools/).

- [MARS](http://courses.missouristate.edu/KenVollmar/MARS/) (MIPS Assembler and Runtime Simulator)
- [QtSpim](http://spimsimulator.sourceforge.net/) (QtSpim is a graphical user interface for the [MIPS](https://en.wikipedia.org/wiki/MIPS_architecture) [R2000](https://en.wikipedia.org/wiki/R2000) simulator [SPIM](http://spimsimulator.sourceforge.net/).)
- [SPIM](http://spimsimulator.sourceforge.net/) (SPIM is a [MIPS](https://en.wikipedia.org/wiki/MIPS_architecture)32 simulator written in C by [John E. Stone](https://en.wikipedia.org/wiki/John_E._Stone).)
- [QtMips](http://qtmips.sourceforge.net/) (QtMips is a graphical user interface for the [MIPS](https://en.wikipedia.org/wiki/MIPS_architecture) [R2000](https://en.wikipedia.org/wiki/R2000) simulator [SPIM](http://spimsimulator.sourceforge.net/).)

#### If you want to write assembly language for [ARM](https://en.wikipedia.org/wiki/ARM_architecture) architecture, you can use the following assemblers:
 - The offical documentation of [ARM](https://en.wikipedia.org/wiki/ARM_architecture) assembly language is available [here](https://developer.arm.com/documentation/).

- [ARMKeil](https://www.keil.com/) (Keil is a [software development environment](https://en.wikipedia.org/wiki/Software_development_environment) for [embedded](https://en.wikipedia.org/wiki/Embedded_system) software development.)
- [ARM](https://en.wikipedia.org/wiki/ARM_architecture) [GCC](https://gcc.gnu.org/) (The [GNU](https://www.gnu.org/) [Compiler](https://gcc.gnu.org/) [Collection](https://gcc.gnu.org/) includes front ends for [C](https://en.wikipedia.org/wiki/C_(programming_language)), [C++](https://en.wikipedia.org/wiki/C%2B%2B), [Objective-C](https://en.wikipedia.org/wiki/Objective-C), [Fortran](https://en.wikipedia.org/wiki/Fortran), [Ada](https://en.wikipedia.org/wiki/Ada_(programming_language)), [Go](https://en.wikipedia.org/wiki/Go_(programming_language)), and [D](https://en.wikipedia.org/wiki/D_(programming_language)), as well as libraries for these languages (libstdc++, [libgcj](https://gcc.gnu.org/onlinedocs/libgcj/), [libobjc](https://gcc.gnu.org/onlinedocs/libobjc/), [libgfortran](https://gcc.gnu.org/onlinedocs/libgfortran/), [libgo](https://gcc.gnu.org/onlinedocs/libgo/), [libada](https://gcc.gnu.org/onlinedocs/libada/), [libffi](https://gcc.gnu.org/onlinedocs/libffi/), and [libjava](https://gcc.gnu.org/onlinedocs/libjava/))).
- [ARM DS-5](https://developer.arm.com/tools-and-software/embedded/arm-development-studio) (ARM Development Studio is a [software development environment](https://en.wikipedia.org/wiki/Software_development_environment) for [embedded](https://en.wikipedia.org/wiki/Embedded_system) software development.)




#### If you want to write assembly language for [RISC-V](https://en.wikipedia.org/wiki/RISC-V) architecture, you can use the following assemblers:
    
- The offical documentation of [RISC-V](https://en.wikipedia.org/wiki/RISC-V) assembly language is available [here](https://riscv.org/wp-content/uploads/2017/05/riscv-spec-v2.2.pdf).

#### If you want to write assembly language for [8088](https://en.wikipedia.org/wiki/Intel_8088) architecture

- Then you can use [8088](https://en.wikipedia.org/wiki/Intel_8088) assembly language. The offical documentation of [8088](https://en.wikipedia.org/wiki/Intel_8088) assembly language is available [here](https://www.cs.virginia.edu/~evans/cs216/guides/x86.html).

#### If you want to write assembly language for [Apple ARM architecture](https://www.apple.com/newsroom/2020/11/apple-unleashes-m1/)

- The official documentation of ARM architecture is available [here](https://developer.arm.com/documentation).

- Then you can use Apple ARM assembly language. The offical documentation of Apple ARM assembly language is available [here](https://developer.apple.com/documentation/xcode/writing-arm64-code-for-apple-platforms).
- There is cool GitHub repository for Apple ARM assembly language. The GitHub repository is available [here](https://github.com/below/HelloSilicon).
---

# 📺 8088 Assembly Language Video Tutorials:

- Belal Hashmi video lectures available [here](https://www.youtube.com/playlist?list=PLKyB9RYzaFRh9pYhDsTGXnDqRx0YZQFbn).

- Video lectures of [Mohammad Nauman](https://github.com/recluze) availabe [here](https://youtube.com/playlist?list=PLnd7R4Mcw3rJCvAduQxyySvejtBIaPs0O).
- Video lectures by Malik Shahzaib available [here](https://youtube.com/playlist?list=PLAZj-jE2acZLdYT7HLFgNph190z2cjmAG).


---
<!-- <iframe src=""https://drive.google.com/file/d/1MOidFawmpqAbZCAJlRwPkpCNTRed6Hgf/view"" width=""640"" height=""480""></iframe> 


---

[![Mr Belal Hashmi Lectures](https://i.imgur.com/aRVAaXZ_d.webp?maxwidth=1520&fidelity=grand)](https://www.youtube.com/playlist?list=PLKyB9RYzaFRh9pYhDsTGXnDqRx0YZQFbn ""Everything Is AWESOME"") -->

[![Mr Belal Hashmi](https://lh3.googleusercontent.com/drive-viewer/AFDK6gMB8jOi8GwQQ6LqH7FdElY6PsVRirtZfxO4NJ6HCbZu9sARR5E61obU9dwWnLppZOMLV--yh7-WXFjESKhn0YoEExdetA=w1920-h932)](https://www.youtube.com/playlist?list=PLKyB9RYzaFRh9pYhDsTGXnDqRx0YZQFbn ""Click me to view: PlayList of video lectures by Belal hasmi"")

## 📚 Book for 8088 Assembly Language:

1. **Handbook** of Belal Hashmi available [here](https://drive.google.com/file/d/1NYdo4t-VM1D8cc_QH5NnkwxtMPFVBoRu/view?usp=sharing).
1. [**Assembly Language for x86 Processors**](http://www.asmirvine.com/) by Kip R. Irvine.
1. **Assembly language step by step** by Jeff Duntemann.
1. **Computer organization and Architecture** by William Stallings.
---
# 🛠️ Setup and Installation of [8088](https://en.wikipedia.org/wiki/Intel_8088) Assembler and Simulator:

> The procedure is same for major operating systems like `Windows`, `Linux`, `MacOS`, etc.
- Download the DOSBOX from the following link:

    - [DOSBOX](https://www.dosbox.com/download.php?main=1)

    - On debian based linux distributions like `ubuntu`, you can install the `DOSBOX` using the following command:
        ```bash
        sudo apt install dosbox
        ```

        - Or, you can install the fork of `DOSBOX` project `DOSBox-X`, (which was mainatined by [Snapcrafters community](https://snapcraft.io/publisher/snapcrafters)) using the following command:
            ```bash
            sudo apt install dosbox-x
            ```

    - If you are on `Windows`, then you can install the `DOSBOX` using the following command from `PowerShell` or `Command Prompt` as `Administrator` using [Chocolatey](https://chocolatey.org/) package manager:
        ```bash
        choco install dosbox
        ```
        - Or you can use [winget-cli](https://github.com/microsoft/winget-cli) package manager:
            ```bash
            winget install -e --id DOSBox.DOSBox
            ```

    - If you are on `MacOS`, then you can install the `DOSBOX` using the following command from `Terminal` using [Homebrew](https://brew.sh/) ( free and open-source software package management system for macOS and Linux):
        ```bash
        brew install dosbox
        ```
#### Then you can use the following procedure to use the [8088](https://en.wikipedia.org/wiki/Intel_8088) Assembler and Simulator:

1. [8088](https://en.wikipedia.org/wiki/Intel_8088) Assembler and Simulator is available [here](https://drive.google.com/file/d/16rATc7y1x0Xw1V6MEip7waSMP316dxmy/view?usp=sharing).

1. Otherwise, If you have `git` installed, then you can clone the repository using the following command:
    ```bash
    git clone https://github.com/MahadMuhammad/Learn-Assembly-The-Hard-Way.git
    ```
1. Or, you can download the zip file from the GitHub repository main page.
1. Extract the zip file.
1. Open the extracted folder.
1. Open the `folder`.
1. Open the `8088 Assembler and Simulator` folder.
1. Now, you can use the [8088](https://en.wikipedia.org/wiki/Intel_8088) Assembler and Simulator `DOSBOX`.
---
# 💭 Logic Building and Problem Solving with Assembly Language:
### Common Mistakes:
- Many people start writing code without thinking about the logic.

- They just start writing code.
- My **advice** is that you should first think about the logic on **paper**, make an **algorithm** and then start writing code. 
### Logic Building:
- If you are completely **stuck** on a problem and didn't think about the logic, then first write the code in the high level language like `Python` or `C/C++` and then convert it into assembly language.

- If you are unable to convert that `high level langauge` code into `assembly language` code, then use the [compiler explorer](https://godbolt.org/z/6oaTnh8nz) to convert it into assembly language code.
### Resolving Errors:
- If you get an unkown error, then first try to understand that error.
- Use assembly language debugger to debug your code.

- Debug your code `line by line`.
- If this doesn't work, then try taking help from your friends, seniors, teachers, etc. 
- Also if you are good at this then try helping your friends, juniors in solving their problems.
---
# 💖 Sponsorship:
- Currently, we are not accepting any sponsership. But, if you want to sponsor this project, then you can contact us at [Mahad](https://github.com/MahadMuhammad/).

- Just pray for us and my contributors. And, we will be thankful to you.
---
# 📜 License:
- This project is licensed under the [MIT License](https://github.com/MahadMuhammad/Learn-Assembly-The-Hard-Way/blob/main/LICENSE)

---

",4,4,1,1,coal,"[8086-emulator, 8088, arm, assembly, assembly-language, belal-hashmi-assembly-programs, belal-hashmi-exercise-solutions, c, coal, dosbox, intel, nasm-assembly]",0.0
84,lrharris215,coal,,https://github.com/lrharris215/coal,https://api.github.com/repos/coal/lrharris215,"Fullstack Steam clone built with Ruby on Rails, PostgreSQL, and React.js","# README

# Coal

## About

[Coal](https://coal-powered.herokuapp.com/#/) is a clone of the website [Steam](https://store.steampowered.com/)! Users can log in and purchase games from their shopping cart for their Coal Library. Featured games are displayed front and center on the Feature Carousel on the home page, while the full list of available games displayed below. Each game also has a show page with more information, as well as the ability to add the game to your cart. Finally, after purchasing the game from your shopping cart, you can view your collection of games in your Coal Library, or go back to the game's show page and leave a review!

## Technologies

Coal was built using a combination of Ruby on Rails, Javascript, React, Redux, HTML, and SCSS. Postgres was used as the database.

## Main Features

### UserAuth

Users can log in, log out, or sign up to Coal. Some features, like the shopping cart, are only available to users who are logged in. There is a guest account available for demo purposes.

### Carousels

The featured and show page carousels are the components that took up the most lines of code and the most time spent styling. The carousel component keeps track of the active picture in its state. Every five seconds, the 'activePicIdx' updates to the next one on the list, causing the next game or image to appear on the screen. I also used debouncing to prevent the image from transitioning away while the user is hovering over it, or while the webpage isn't active.

```javascript
    debounceInterval() {
        if (this.intervalId) {
            clearInterval(this.intervalId);
        }
        this.intervalId = setInterval(() => {
            this.setState({
                activeGameIdx:
                    this.state.activeGameIdx === this.props.games.length - 1 ? 0 : this.state.activeGameIdx + 1,
            });
        }, 5 * 1000);
    }
```

### Shopping Cart

The shopping cart allows users to 'save' the games they want to purchase before they are ready to purchase them. This way, the user only has to make one payment to aquire multiple games. The shopping cart uses local storage to keep track of the games the user intends to buy. This allows the cart to persist even if the user logs out before completing their purchase.

```javascript
class ShoppingCart extends React.Component {
    constructor(props) {
        super(props);

        this.state = {
            games: JSON.parse(localStorage.getItem(this.props.currentUserId)),
        };
    ...}

...}

```

## Credits

-   All icons are from Fontawesome.com
-   The gift card pic: https://toppng.com/show_download/84324/steam-gift-card-usd-50-steam-digital/large
-   All other pictures and screenshots are from the [Steam](https://store.steampowered.com/) website. 
",3,3,1,0,coal,"[coal, games, shopping-cart]",0.0
85,RazaRizvii,Cash-and-Carry-Project-in-Assembly-Language,,https://github.com/RazaRizvii/Cash-and-Carry-Project-in-Assembly-Language,https://api.github.com/repos/Cash-and-Carry-Project-in-Assembly-Language/RazaRizvii,"This is a online Cash and Carry Project, implemented in Assembly language. ",,3,3,1,0,coal,"[assembly, assembly-8086, assembly-arm, assembly-language, assembly-language-programming, assembly-x86, assemblyscript, cash-and-carry, coal, management, management-system, online-store]",0.0
86,saumyatas,Coal_Mines_Visualization,,https://github.com/saumyatas/Coal_Mines_Visualization,https://api.github.com/repos/Coal_Mines_Visualization/saumyatas,Geospatial Data Analysis for detecting and analyzing coal mines.,"# Coal Mines Visualization on GEE
A visualization tool created on Google Earth engine (GEE) to help analyze the coal mining area and predict the period of mining operation. 
GEE is an open-source platform for geospatial data analysis and its visualization. We all know how tedious it is to download each satellite data tile and then curate it according to our need for further analysis. And if the area is large, say you are working on the entire country or on time series analysis, then downloading the satellite data (Landsat or Sentinel) for each and every region in a particular time range will take hours or days and require high density data storage. Even their analysis might need high end system with fast computing software. All this could be done in just few minutes and within 10-15 lines of code on GEE. Geospatial data computation on GEE not only saves time and storage but also provides flexibility. They have open-source [data catalog](https://developers.google.com/earth-engine/datasets/) including `Landsat` datasets, `Sentinel` datasets, `MODIS` Datasets, `NAIP` data, precipitation data, sea surface temperature data, CHIRPS climate data, and elevation data. You can check out this paper to know more about GEE platform. - [Google Earth Engine: Planetary-scale geospatial analysis for everyone](https://www.sciencedirect.com/science/article/pii/S0034425717302900)

For our current visualization we will import the Landsat 8 OLI/TIRS sensors tier 2 data, which is atmospherically corrected surface reflectance dataset. After applying cloud mask and cloud shadow mask, and filtering it from the year 2014 to 2021, image is clipped for the study area by extracting the pixels from the entire area and for each year. But in order to access GEE one will need to sign up on [Google Earth Engine](https://earthengine.google.com/) using your google account.


## Installing
```bash
git clone https://github.com/saumyatas/Coal_Mines_Visualization.git
```

## Visulization Plots
With the help of the surrounding areas of the mining area a NDVI reference graph is created to distinguish forest and water bodies from the mining area.

### NDVI reference chart
![NDVI reference chart](Plots/NDVI_ref.png)

With the help of change in NDVI we can predict that the period of operation for this mine started in early 2016 or late 2015.

### NDVI of the mining area
![NDVI_chart](Plots/NDVI_chart.png)

## Visulization GIFs

The expansion of mining area and change in operation can be visualized using Landsat series satellite data.

### Normalized Difference Built-up Index GIFs
![NDBI gifs](GIFs/NDBI.gif)

### RGB Visualization GIFs
![RGB gifs](GIFs/RGB.gif)

## Author
[Saumyata Srivastava](https://www.linkedin.com/in/ss-97b05a103/)

[![GitHub Saumyata Srivastava](https://img.shields.io/github/followers/saumyatas?label=follow&style=for-the-badge&logo=github&logoColor=white&labelColor=333333)](https://github.com/saumyatas)
[![Email](https://img.shields.io/badge/Mail-004788?style=for-the-badge&logo=gmail&logoColor=white)](mailto:saumyata.srivastava@gmail.com)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/ss-97b05a103/)
[![kaggle](https://img.shields.io/badge/kaggle-31C3FF?style=for-the-badge&logo=kaggle&logoColor=white)](https://www.kaggle.com/saumyatas1202)
[![Medium](https://img.shields.io/badge/Medium-12100E?style=for-the-badge&logo=medium&logoColor=white)](https://medium.com/@srivastava.saumyata)

## Credit
- The code in python is built on geemap, a python package for interactive mapping with GEE, created by [Qiusheng Wu](https://github.com/giswqs)

## License
This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details
",2,2,1,0,coal,"[coal, google-earth-engine, ndbi, ndvi, python, raster, satellite-data]",0.0
87,gu-stav,coaltransitions,,https://github.com/gu-stav/coaltransitions,https://api.github.com/repos/coaltransitions/gu-stav,Research and Dialogue on the Future of Coal,"Frontend for [coaltransitions.org](https://coaltransitions.org) build with
Gatsby. The data is fetched from a [wordpress backend](https://github.com/zoff-kollektiv/coaltransitions-cms).

## Structure

- `gatsby-config.js`: Contains the site title, the header and footer menu aswell
  as the information which is used to fetch tweets from twitter
- `netlify.toml`: Contains the legacy redirects from the old site
- `src/pages`: All pages which don't have any parameters (frontpage, overviews ...)
- `src/templates`: Pages which have input parameters (finding, publication ...)
- `src/lib`: Helper functions, for page creation aswell as publication filtering
- `src/components`: All components which are used in pages and templates to render
  the pages. Components are reusable chunks of code.
- `src/tokens.js`: Variables for colors, mediaqueries and fonts

### Tweets

Tweets are embedded from the coaltransitions twitter account. `./functions/twitter-timeline.js`
functions as a proxy, for authentication and data fetching. The function
is deployed as an AWS Lambda function by netlify.

#### Environment variables

```
WP_ENDPOINT: Wordpress Graphql endpoint
WP_AUTH_USERNAME: Wordpress Basic auth username
WP_AUTH_PASSWORd: Wordpress Basic auth password

// to show recent tweets on the homepage you'll also need

TWITTER_CONSUMER_KEY: Twitter consumer key
TWITTER_CONSUMER_SECRET: Twitter consumer secret
TWITTER_ACCESS_TOKEN: Twitter access token
TWITTER_ACCESS_TOKEN_SECRET: Twitter access token secret
```


### Build status

[![Netlify Status](https://api.netlify.com/api/v1/badges/f051ac1e-f9b0-424a-9477-dd9bdef6e833/deploy-status)](https://app.netlify.com/sites/coaltransitions/deploys)


## Development

First you need to install the project dependencies by running

`npm run install`

Now create a copy of the `.env.example` file and fill in the environment
variables. You should be able to start the development now by running:

`npm run develop`

This should give you access to [localhost:8000](http://localhost:8000) where
you can access the development build of the site, with production data.


### Functions

To fetch the data from the twitter account you also have to boot up the lamda
development environment (check the required environment variables mentioned
above):

`npm run serve-functions`

Afterwards you should be able to access [localhost:9000](http://localhost:9000).

Gatsby automatically proxies the required requests to the functions server. No
additional setup is required.

The twitter search parameters can be found in `gatsby-config.js`.


## Production build

By running `npm run build` gatsby creates a static version of the site in
`./public`.
",1,1,0,43,coal,"[climate-change, coal, energy, gatsby, gatsbyjs, wordpress]",0.0
88,zia-foisal,Bangladseh-Power-Plant-Database,,https://github.com/zia-foisal/Bangladseh-Power-Plant-Database,https://api.github.com/repos/Bangladseh-Power-Plant-Database/zia-foisal,"The Global Power Plant Database is a comprehensive, open source database of power plants around the world. It centralizes power plant data to make it easier to navigate, compare and draw insights for Bangladesh . The database covers approximately 35,000 power plants from 167 countries and includes thermal plants (e.g. coal, gas, oil, nuclear, biomass, waste, geothermal) and renewables (e.g. hydro, wind, solar). Each power plant is geolocated and entries contain information on plant capacity, generation, ownership, and fuel type. It will be continuously updated as data becomes available.","# Bangladseh-Power-Plant-Database
Power Plants is an establishment for power generation. In Bangladesh, the consumption of per capita generation is very low only 220 kWh. Presently about 47% of the total population have access to electricity. Bangladesh Power Development Board' (BPDB) is the sole government authority for generation of electricity. Major power distribution agencies include the BPDB itself and the rural electrification board (REB). The Dhaka Electric Supply Authority (desa) and Dhaka Electric Supply Company (DESCO) for Dhaka or the Khulna Electric Supply Company (KESCO) for Khulna. The power division of the Ministry of Energy and Mineral Resources is the umbrella organisation that controls power generation, transmission and distribution. An Independent Power Project (IPP) of the ministry is under implementation for improvement in generation and distribution of electricity by government and private agencies.<br><br><br>
[Power Plant Database Webmap](https://zia-foisal.github.io/Bangladseh-Power-Plant-Database/)
",1,1,1,0,coal,"[bangladesh, coal, energy, gas, hydro, oil, solar-energy]",0.0
89,awakechaudhry,slop-fired-boiler-efficiency-and-fuel-calculation,,https://github.com/awakechaudhry/slop-fired-boiler-efficiency-and-fuel-calculation,https://api.github.com/repos/slop-fired-boiler-efficiency-and-fuel-calculation/awakechaudhry,Boiler Direct & Indirect Efficiency calculator with Fuel imbalance and Fuel accounting,"# slop-fired-boiler-efficiency-and-fuel-calculation
**The project is done in asociation with Dhampur sugar mills ltd.**<br>
The aim of this project is to evaluate the efficiency of the boiler and finding fuel imbalance as well as fuel accounting which shows the fuel loss during the process.<br>
Boiler efficiency can be tested by the two ways-<br> 
**1)The Direct Method**: Where the energy gain of the working fluid (water and steam) is compared with the energy content of the boiler fuel. 
**2)The Indirect Method**: Where the efficiency is the difference between the losses and the energy input.<br>
The data for this project is collected by using algorathims from **DCS**.<br>
In this project both type of **efficiecy,fuel imbalance,fuel accounting** is calculated for a 35tph slop fired boiler by analysing the data.<br>
But efficiency,fuel imbalance and fuel accounting of any capacity slop fired boiler (which uses the fuel as mentions in excel sheet) can be calculated by using this sheet and much more things can be done. 
",1,1,1,0,coal,"[bagasse, boiler, coal, distillery, mechanical-engineering, powerplant, ricehusk, slop, slop-fired-boiler, steamboiler]",0.0
90,drganghe,Rapid-and-Just-Coal-Transition-in-China,,https://github.com/drganghe/Rapid-and-Just-Coal-Transition-in-China,https://api.github.com/repos/Rapid-and-Just-Coal-Transition-in-China/drganghe,"He, Gang, Jiang Lin, Ying Zhang, Wenhua Zhang, Guilherme Larangeira, Chao Zhang, Wei Peng, Manzhi Liu, and Fuqiang Yang. 2020. “Enabling a Rapid and Just Transition Away from Coal in China.” One Earth 3 (2): 187–94. https://doi.org/10.1016/j.oneear.2020.07.012","# Rapid-and-Just-Coal-Transition-in-China
This repository include the data and file for paper He, Gang, Jiang Lin, Ying Zhang, Wenhua Zhang, Guilherme Larangeira, Chao Zhang, Wei Peng, Manzhi Liu, and Fuqiang Yang. 2020. “Enabling a Rapid and Just Transition Away from Coal in China.” One Earth 3 (2): 187–94. https://doi.org/10.1016/j.oneear.2020.07.012.
",1,1,0,0,coal,"[china, coal, energy-transition, just-transition]",0.0
91,kamarad-coal,minecraft-forge,kamarad-coal,https://github.com/kamarad-coal/minecraft-forge,https://api.github.com/repos/minecraft-forge/kamarad-coal,Minecraft Forge container to deploy servers on Kubernetes infrastructure in order to provide easy server spin-ups.,"Minecraft Forge Container
==========================

![Testing](https://github.com/kamarad-coal/minecraft-forge/workflows/Testing/badge.svg?branch=master)

Kamarad Coal is using the Minecraft Forge container to deploy servers on Kubernetes infrastructure in order to provide easy server spin-ups.

## 🤝 Supporting

**Renoki Games. is a [Renoki Co.](https://github.com/renoki-co) subsidiary, made with ❤. Consider reaching out and supporting [Renoki Co.](https://github.com/renoki-co).**

## Supported Minecraft versions

Kamarad Coal tests & builds only the latest patch versions for each minor versions. However, you might find images for non-latest-patches, because at some point that version was the latest patch.

The following Minecraft Forge versions are deployed on an day-to-day basis:

- `1.16.5-36.1.0`
- `1.16.4-35.1.4`
- `1.15.2-31.2.0`
- `1.14.4-28.2.0`
- `1.13.2-25.0.219`
- `1.12.2-14.23.5.2854`
- `1.11.2-13.20.1.2386`
- `1.10.2-12.18.3.2185`
- `1.9.4-12.17.0.1976`
- `1.8.9-11.15.1.1722`

## Versioning

Forge builds support multiple Minecraft versions for each Github tag. The format for container tags is the following:

```
quay.io/kamaradcoal/minecraft-forge:[forge_version]-[repo_tag]
```

For example, this is going to be the latest tag for Minecraft Forge `1.16.5-36.1.0`:

```
quay.io/kamaradcoal/minecraft-forge:1.16.5-36.1.0-latest
```

### Version Specific Tags

For version-specific tags, you might use the following image and tag, `1.0.0` being the repo tag:

```
quay.io/kamaradcoal/minecraft-forge:1.16.5-36.1.0-1.0.0
```

For Minecraft version-specific tags, you might use the following image and tag, where `1.0.0` is the repo tag and `1.16.5` is the Minecraft version:

```
quay.io/kamaradcoal/minecraft-forge:1.16.5-1.0.0
```

### Minecraft Majors and Minor versions

You can also specify major repo versions, where `1.0` means `1.0.x`:

```
quay.io/kamaradcoal/minecraft-forge:1.16.5-1.0
```

You can also specify major.minor repo versions, where `1` means `1.x` (all 1.x versions):

```
quay.io/kamaradcoal/minecraft-forge:1.16.5-1
```

### Latest Tags

For latest tags, use `latest` instead any other version, either by Minecraft version or by Forge version:

```
quay.io/kamaradcoal/minecraft-forge:1.16.5-latest
```

```
quay.io/kamaradcoal/minecraft-forge:1.16.5-36.1.0-latest
```

## Kubernetes Integration

Coming soon.
",1,1,1,7,coal,"[coal, container, containerization, docker, image, k8s, kamarad, kamarad-coal, kubernetes, minecraft, minecraft-forge]",0.0
92,awais922609,Assembly-Programs,,https://github.com/awais922609/Assembly-Programs,https://api.github.com/repos/Assembly-Programs/awais922609,This repository contains a large number of assembly programmes that I created throughout my Bachelor's semester. There are also readme files connected to each lab that explain which lab contains which questions. Enjoy learning and give the repository a like if it helped you with coding.,"# Assembly Programs #
Welcome to my repository of Assembly Programs! These programs were developed during my Bachelor's degree in Computer Science and are now available for public use. They include a variety of programs for different purposes, from simple arithmetic operations to more complex algorithms.

# Usage
To use any of these programs, simply download the source code and assemble it using an appropriate assembler of masm as they are masm compatible not nasm.
Uncomment each present code 1 present in files 1 at a time to see its desired output.
#### Download AssmSoft folder and extract it.
#### Place your code in that folder. Open Dosbox. Mount that folder using mount c /location/to/AssmSoft_Folder.
#### Then load Dosbox memory by typing (c:) on dosbox.

# How to Assemble and Run
#### masm filename.asm
#### link filename.obj
#### filename.exe
If you want to debug your code then instead of filename.exe use:
### afd filename.exe
AFD is Advance Free Debugger which is used for understanding how Code is being saved in Memory and then being utilised.

# Learn 
I would like to express my sincere gratitude to Dr. Muhammad Nouman from Fast Peshawar, who inspired me to create this repository. If you're interested in learning more about Assembly Language and how these codes work, I highly recommend checking out his channel at **https://shorturl.at/iHZ01**. It's a great resource for anyone looking to expand their knowledge and skills in this area.

## Best Book to Study for Graphical Games using Assembly Language
#### **http://www.phatcode.net/res/226/files/pcasm-book.pdf**

# Contributions
Contributions to this repository are always welcome! Feel free to submit pull requests with your own programs or improvements to existing ones. Together, we can build a comprehensive library of assembly programs for everyone to use.
",1,1,1,0,coal,"[advance-free-debugger, afd, asm, assembly, assembly-debugger, assembly-language, assembly-x86, coal, masm32, masm64, programming]",0.0
93,kamarad-coal,minecraft-bungee,kamarad-coal,https://github.com/kamarad-coal/minecraft-bungee,https://api.github.com/repos/minecraft-bungee/kamarad-coal,Minecraft Bungee container to deploy servers on Kubernetes infrastructure in order to provide easy server spin-ups.,"Minecraft Bungee Container
==========================

![Testing](https://github.com/kamarad-coal/minecraft-bungee/workflows/Testing/badge.svg?branch=master)

Kamarad Coal is using the Minecraft Bungee container to deploy servers on Kubernetes infrastructure in order to provide easy server spin-ups.

## 🤝 Supporting

**Renoki Games. is a [Renoki Co.](https://github.com/renoki-co) subsidiary, made with ❤. Consider reaching out and supporting [Renoki Co.](https://github.com/renoki-co).**

## Versioning

Bungee builds support for each Github tag. The format for container tags is the following:

```
quay.io/kamaradcoal/minecraft-bungee:[repo_tag]
```

For example, this is going to be the `1.0`tag

```
quay.io/kamaradcoal/minecraft-bungee:1.0-latest
```

### Version Specific Tags

For version-specific tags, you might use the following image and tag, `1.0.0` being the repo tag:

```
quay.io/kamaradcoal/minecraft-bungee:1.0.0
```

### Majors and Minor versions

You can also specify major repo versions, where `1.0` means `1.0.x`:

```
quay.io/kamaradcoal/minecraft-bungee:1.0
```

You can also specify major.minor repo versions, where `1` means `1.x` (all 1.x versions):

```
quay.io/kamaradcoal/minecraft-bungee:1
```

### Latest Tags

For latest tags, use `latest` instead:

```
quay.io/kamaradcoal/minecraft-bungee:latest
```

## Kubernetes Integration

Coming soon.
",1,1,0,7,coal,"[bungee, coal, container, docker, image, k8s, kamarad, kamarad-coal, kubernetes, minecraft]",0.0
94,anserwaseem,print-diamonds,,https://github.com/anserwaseem/print-diamonds,https://api.github.com/repos/print-diamonds/anserwaseem,A program that draws a solid diamond on the screen.,"# print-diamonds

## PrintDiamond 
A function that draws a solid diamond on display memory after taking following parameters from main function:
- location (rowno, colno) i.e. the top position of the diamond
- width of the diamond
- character with which the diamond has to be filled
- color/attribute of the diamond

## swap
After printing the diamond for 5 seconds (using delay), swap the first 12 rows of the display memory with the last 12 rows using movs instruction. Do not clear the introduction screen in the start.
",0,0,1,0,coal,"[asm-8086, asmx86, assembly-language, assembly-programming, belal-hashmi-answers, bios-interrupt, bit-manipulation, coal, dos-emulator, dosbox, software-interrupts, subroutines]",0.0
95,manahilfatima31,digital_library,,https://github.com/manahilfatima31/digital_library,https://api.github.com/repos/digital_library/manahilfatima31,"The program will show the user menu of different categories available. The user can select a category by inputting a number. User can see the books available, issue the book or return the book. The number of books issued will increase when the user will return the book.","# digital_library
The program will show the user menu of different categories available. The user can select a category by inputting a number. User can see the books available, issue the book or return the book. The number of books issued will increase when the user will return the book.

# Introduction
We have built a type of assisting software for staff and students for searching, issuing and returning books.

# Future Work /Recommendations
The future work is to add more sections and increase number of copies of each book in our library. We also intend to add policy ensuring mechanisms that will guarantee that the information produced is visible only to those who have the appropriate rights to access it.

# Conclusion
Thus, this program will help the user to issue or return a book at any time, it is actually a network of multimedia system, which provides fingertip access.
",0,0,1,0,coal,"[assembly, assembly-language, assembly-x86, coal, digital-library, library, visual-studio, visual-studio-2019]",0.0
96,zyn10,Assembly_Language,,https://github.com/zyn10/Assembly_Language,https://api.github.com/repos/Assembly_Language/zyn10,Assembly language Course Fall 2020,# ddddd21,0,0,1,0,coal,"[assembly-language-programming, assembly-x86, coal]",0.0
97,unravelwwx,The-effect-of-the-Promoting-the-Big-and-Quashing-the-Small-Policy-,,https://github.com/unravelwwx/The-effect-of-the-Promoting-the-Big-and-Quashing-the-Small-Policy-,https://api.github.com/repos/The-effect-of-the-Promoting-the-Big-and-Quashing-the-Small-Policy-/unravelwwx,"codes of coal transport model in Shandong Province, China","#  Coal-fired-power-plant-Shandong
This repository is for the life-cycle study of coal-fired power plant in Shandong, China
The newly-built and retired power plants are derived by examining the difference between power plants in 2010 and in 2014.
",0,0,1,0,coal,"[coal, plant, transport]",0.0
98,codyben,phone-mine-map-atlas,,https://github.com/codyben/phone-mine-map-atlas,https://api.github.com/repos/phone-mine-map-atlas/codyben,"A ""better"" web client for PASDA georeferenced colliery maps. ",,0,0,2,0,coal,"[coal, geoscience]",0.0
99,mustan-ali,Medical-Billing-System,,https://github.com/mustan-ali/Medical-Billing-System,https://api.github.com/repos/Medical-Billing-System/mustan-ali,Computer Organization and Assembly Language (COAL) Project,"# Medical-Billing-System

CSC2201 - Computer Organization and Assembly Language  
CSCL2201 - Computer Organization and Assembly Language

A medical billing system project is a system designed to help manage the daily operations of a medicine store. It can track inventory levels and process orders and sales information.
The main goal of a medical billing system project is to improve the efficiency and effectiveness of the store's operations by automating tasks and providing a central location for storing and accessing important
data. By using this system, store owners and managers can save time and reduce the risk of errors.
",0,0,1,0,coal,"[assembly-language, coal, emu8086]",0.0
100,VybzTech,Miners,,https://github.com/VybzTech/Miners,https://api.github.com/repos/Miners/VybzTech,Platinum Ville Mines limited is a mining company comprising of astute professionals with ownership/control of licensed mining sites in various locations in Nigeria.,"# miners

Platinum Ville Mines limited is a mining company comprising of astute professionals with ownership/control of licensed mining sites in various locations in Nigeria.
",0,0,1,0,coal,"[brine, coal, hard, limited, lithium, minecraft, mines, mining, platinum, rock, sites, websites]",0.0
101,syedakashafatima,CasinoNumberGeneratorCOAL,,https://github.com/syedakashafatima/CasinoNumberGeneratorCOAL,https://api.github.com/repos/CasinoNumberGeneratorCOAL/syedakashafatima,This is a small beginners project using Computer Organization and Assembly Language (COAL) using Irvine32.inc Library. Hope it helps.,"# CasinoNumberGeneratorCOAL
This is a small beginners project using Computer Organization and Assembly Language (COAL) using Irvine32.inc Library. 

A casino Numbering Game is a game in which you can make a bet on the amount of your choice on a number. 

It first asks for your name and the total amount you have.

![](images/Picture1.png)

On pressing ENTER, it displays some rules for you to understand how the game works. 
Rules are as follow:
1. Choose a number between 1 to 10
2. Winner gets 5 times of the money bet
3. Wrong bet and you lose the amount you bet

It displays the total amount and asks you to enter the amount you want to make bet on. 
If the amount is within range of the total amount then it asks you to choose the number you want to 
put money on. 
If the number is equal to the randomly generated number, you win 5 times the money you bet on. 

![](images/Picture3.png)

Else you end up losing the money

![](images/Picture2.png)

It asks you if you want to continue the game. If you say ‘0’ which means ‘yes’ then it goes again to ask you the 
amount you want to bet again. It goes on until either the user chooses ‘1’ which means ‘no’ he does not wish to continue the 
game or either he has zero money in his account. Either time it displays ""Thanks for playing the game. Your Balance is:” and 
displays the remaining amount.
",0,0,1,0,coal,"[assembly, assembly-language, coal, irvine32, lang, language, programming]",0.0
102,FS-Artisan,CoalsCrafting-Core,FS-Artisan,https://github.com/FS-Artisan/CoalsCrafting-Core,https://api.github.com/repos/CoalsCrafting-Core/FS-Artisan,,"小组：FS工匠铺  
程序：化龙焚天  
美术：树芙蓉  
相关链接： 
  MCBBS：https://www.mcbbs.net/thread-1148699-1-1.html  
  MC百科：https://www.mcmod.cn/class/3356.html  
",0,0,0,0,coal,"[coal, minecraft-mod]",0.0
103,amnmalik,PhDThesis,,https://github.com/amnmalik/PhDThesis,https://api.github.com/repos/PhDThesis/amnmalik,Contains code and figures to my PhD thesis,"## PhDThesis
Contains code and figures to my PhD thesis titled ""Barriers to power sector decarbonisation in India"".
Thesis submitted to Technical University of Berlin, November 2021

## Download
To simply download the thesis (as submitted) in `.pdf`, click [here](thesisUpdated20220401-red.pdf). Note that the uploaded pdf is compressed to save space. To see it in high quality, you must download and run the latex code. To see and run the code online, see the thesis on [Overleaf](https://www.overleaf.com/read/nyqmtrgpkcdq). To see the core chapters of the thesis, based on published research articles, click [here](chapters/pdfs). To download directly from the online repository of doctoral disserations of TU Berlin , click [here](https://depositonce.tu-berlin.de/items/25cc825f-904f-4403-a092-7d6190f92149)

## Usage
The thesis is written in latex and uses Xelatex to make the PDF.

## Abstract
The role of developing countries like India in climate action has undergone a shift in the last five to ten years. Several factors have led to this development. Firstly, with the signing of the Paris Agreement and its emphasis on bottom-pledges, all countries have become co-enactors to mitigation. Secondly, continued scientific research on co-benefits and climate damages has reduced the gap between mitigation and development priorities. Lastly, capital costs of renewable energy (RE) have plummeted making them cheaper than new coal plants in most countries, thereby providing a solid economic incentive to increase the share of RE. Despite these developments, decarbonisation of the power sector in low-income countries faces significant socio-economic and political barriers. This dissertation identifies some of those barriers, eventually suggesting policy solutions to overcome them. While one publication of this cumulative dissertation % I changed it here
has a global scope, the other two papers focus on India, a country with low cumulative historic emissions, but is currently the third-largest emitter of greenhouse gases (GHG). Per-capita energy consumption is still low, but it has one of the fastest growing electricity markets in the world. Thus, the policy decisions in the power sector in India can substantially affect the global goal to decarbonisation.

The first publication identifies the risk of carbon lock-ins in the power sector if India were to continue a trajectory based on current policies. We find that continued investment into fossils could eventually lead to stranded assets in the future because of the faster pace of decarbonisation required in scenarios achieving the Paris Agreement goals. Since most of the stranding arises from plants yet to be built, it can be avoided through additional capacity installations of RE, i.e., increasing current ambition in RE-deployment and limiting new coal power plants to those under construction. Most of the additional capacity would come from solar and wind, given their large resource potentials and favourable economic viability in India. The expansion potential of other sources like gas, nuclear, and hydro remains low, owing to constraints on supply, cost, and construction duration.

The second article uses different mitigation scenarios and analyses, on a global level but based on country-specific data, the labor market implications of a decarbonisation policies. Although ambitious policies supporting RE and discouraging coal power, e.g., through a coal moratorium, discussed above are favourable for (future) deep decarbonisation, they could lead to disruptive changes adversely affecting the employment situation, specifically the drastic losses in the fossil sector. We show that in the near-term, stringent mitigation results in a net increase in jobs compared to a weaker climate action scenario (based on currently pledged country objectives), mainly through gains in solar and wind jobs in construction, installation, and manufacturing, despite significantly higher losses in coal fuel supply. However, global energy jobs eventually peak, because the falling labour intensity (i.e. jobs per megawatt, due to increasing productivity) outpace increases in RE installations. In the future, total jobs are still higher in stringent mitigation  than in a scenario with less mitigation with most people employed in the operation and maintenance of RE infrastructure, unlike fuel extraction today.  Although stricter mitigation could lead to higher jobs globally, the role of employment in decarbonisation in specific regions could play out very differently. In countries with significant people employed in fossil-fuel industries, a just transition for those workers could become important. 

The third publication highlights that the regional mismatch of energy infrastructure in India could become a significant barrier to effective decarbonisation. Most of the coal mines and coal power plants in India are concentrated in the poorer eastern states of Chhattisgarh, Odisha, and Jharkhand, where it is an important source of both employment and public economy. On the other hand, the best RE potentials in India are concentrated in the relatively wealthier western and southern states and are home to current and planned RE installations.  Continued fossil investments in coal-bearing regions could widen this gap and in pathways to deep decarbonisation, strongly accelerate the loss of coal jobs. Without complementary opportunities, this would negatively impact the livelihood of people living in these areas. We show that dedicated policies to increase solar installations in coal regions could ensure early geographic diversification of solar energy. It could help build broad support for the energy transition, required for climate targets, and could give India important benefits in terms of avoided climate impacts and local health. At the same time, solar alone cannot provide a just transition and there is an urgent need for engagement with all stakeholders exploring challenges and other opportunities into the transition. 

In summary, despite the proliferation of climate considerations into decision-making at all political levels, there are still significant barriers to decarbonisation. Some of the most pressing challenges for fast-growing economies like India involve avoiding lock-ins in the power sector, which could have far-reaching consequences on the pace and cost of future decarbonisation. Higher-income nations could support the transition by providing cheaper RE-related finance and knowledge of increasing power system flexibility. At the same time, changes in the quantity and structure of jobs in the energy sector could also affect the pace of decarbonisation. Here, one key factor is the just transition of predominantly coal-bearing regions. The regional divide of fossil and RE assets and resources in India means that a regionally balanced transition from a fossil to a RE-based economy would not happen on its own; it needs dedicated policies supporting future solar installations in coal-bearing states. However, given the large size of the current coal workforce, additional solar capacity alone (in these regions) cannot replace all the lost jobs. It therefore requires to look for alternatives beyond the energy sector. 
",0,0,1,0,coal,"[climate-change, coal, energy, energy-transition, india, phd, phd-thesis]",0.0
104,kinverarity1,lasio,,https://github.com/kinverarity1/lasio,https://api.github.com/repos/lasio/kinverarity1,Python library for reading and writing well data using Log ASCII Standard (LAS) files,"# lasio

<p align=""center"">
<a href=""https://lasio.readthedocs.io/en/stable/""><strong>Documentation</strong> (stable)</a> •
<a href=""https://lasio.readthedocs.io/en/latest/""><strong>Documentation</strong> (main branch)</a>
</p>

[![Run tests](https://github.com/kinverarity1/lasio/actions/workflows/ci-tests.yml/badge.svg)](https://github.com/kinverarity1/lasio/actions/workflows/ci-tests.yml)
[![PyPI version](https://img.shields.io/pypi/v/lasio.svg)](https://pypi.python.org/pypi/lasio/)
[![Code Style](https://img.shields.io/badge/code%20style-black-000000.svg)](https://black.readthedocs.io/en/stable/the_black_code_style/index.html)
[![License](http://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/kinverarity1/lasio/blob/master/LICENSE)

Read and write Log ASCII Standard files with Python.

This is a Python 3.7+ package to read and write Log ASCII Standard
(LAS) files, used for borehole data such as geophysical, geological, or
petrophysical logs. It's compatible with versions 1.2 and 2.0 of the LAS file
specification, published by the [Canadian Well Logging
Society](https://www.cwls.org/products/#products-las). Support for LAS 3 is 
[being worked on](https://github.com/kinverarity1/lasio/issues/5).

lasio is primarily for reading and writing data and metadata to and from 
LAS files. It is designed to read as many LAS files as possible, including
those containing common errors and non-compliant formatting. It can be used
directly, but you may want to consider using some other packages, depending
on your priorities:

- [welly](https://github.com/agile-geoscience/welly) is a Python package that 
  uses lasio for I/O but provides a **lot** more functionality aimed at working
  with curves, wells, and projects. I would recommend starting there in most 
  cases, to avoid re-inventing the wheel!
- [lascheck](https://github.com/MandarJKulkarni/lascheck) is focused on
  checking whether your LAS file meets the specifications.
- [lasr](https://github.com/donald-keighley/lasr) is an R package which 
  is designed to read large amounts of data quickly from LAS files; this is 
  a great thing to check out if speed is a priority for you, as lasio is not 
  particularly fast.
- LiDAR surveys are also called ""LAS files"", but they are quite different and
  lasio will not help you -- check out [laspy](https://github.com/laspy/laspy)
  instead.

lasio [stopped](https://github.com/kinverarity1/lasio/issues/364) 
supporting Python 2.7 in August 2020. The final version of lasio with Python 2.7 support 
is version 0.26.

## Code of conduct

See our [code of conduct](https://lasio.readthedocs.io/en/latest/contributing.html#code-of-conduct).

## Quick start

For the minimum working requirements, you'll need numpy installed. Install
lasio with:

```bash
$ pip install lasio
```

To make sure you have everything, use this to ensure pandas, chardet, and
openpyxl are also installed:

```bash
$ pip install lasio[all]
```

Example session:

```python
>>> import lasio
```

You can read the file using a filename, file-like object, or URL:

```python
>>> las = lasio.read(""sample_rev.las"")
```

Data is accessible both directly as numpy arrays

```python
>>> las.keys()
['DEPT', 'DT', 'RHOB', 'NPHI', 'SFLU', 'SFLA', 'ILM', 'ILD']
>>> las['SFLU']
array([ 123.45,  123.45,  123.45, ...,  123.45,  123.45,  123.45])
>>> las['DEPT']
array([ 1670.   ,  1669.875,  1669.75 , ...,  1669.75 ,  1670.   ,
        1669.875])
```

and as ``CurveItem`` objects with associated metadata:

```python
>>> las.curves
[CurveItem(mnemonic=DEPT, unit=M, value=, descr=1  DEPTH, original_mnemonic=DEPT, data.shape=(29897,)),
CurveItem(mnemonic=DT, unit=US/M, value=, descr=2  SONIC TRANSIT TIME, original_mnemonic=DT, data.shape=(29897,)),
CurveItem(mnemonic=RHOB, unit=K/M3, value=, descr=3  BULK DENSITY, original_mnemonic=RHOB, data.shape=(29897,)),
CurveItem(mnemonic=NPHI, unit=V/V, value=, descr=4   NEUTRON POROSITY, original_mnemonic=NPHI, data.shape=(29897,)),
CurveItem(mnemonic=SFLU, unit=OHMM, value=, descr=5  RXO RESISTIVITY, original_mnemonic=SFLU, data.shape=(29897,)),
CurveItem(mnemonic=SFLA, unit=OHMM, value=, descr=6  SHALLOW RESISTIVITY, original_mnemonic=SFLA, data.shape=(29897,)),
CurveItem(mnemonic=ILM, unit=OHMM, value=, descr=7  MEDIUM RESISTIVITY, original_mnemonic=ILM, data.shape=(29897,)),
CurveItem(mnemonic=ILD, unit=OHMM, value=, descr=8  DEEP RESISTIVITY, original_mnemonic=ILD, data.shape=(29897,))]
```

Header information is parsed into simple HeaderItem objects, and stored in a
dictionary for each section of the header:

```python
>>> las.version
[HeaderItem(mnemonic=VERS, unit=, value=1.2, descr=CWLS LOG ASCII STANDARD -VERSION 1.2, original_mnemonic=VERS),
HeaderItem(mnemonic=WRAP, unit=, value=NO, descr=ONE LINE PER DEPTH STEP, original_mnemonic=WRAP)]
>>> las.well
[HeaderItem(mnemonic=STRT, unit=M, value=1670.0, descr=, original_mnemonic=STRT),
HeaderItem(mnemonic=STOP, unit=M, value=1660.0, descr=, original_mnemonic=STOP),
HeaderItem(mnemonic=STEP, unit=M, value=-0.125, descr=, original_mnemonic=STEP),
HeaderItem(mnemonic=NULL, unit=, value=-999.25, descr=, original_mnemonic=NULL),
HeaderItem(mnemonic=COMP, unit=, value=ANY OIL COMPANY LTD., descr=COMPANY, original_mnemonic=COMP),
HeaderItem(mnemonic=WELL, unit=, value=ANY ET AL OIL WELL #12, descr=WELL, original_mnemonic=WELL),
HeaderItem(mnemonic=FLD, unit=, value=EDAM, descr=FIELD, original_mnemonic=FLD),
HeaderItem(mnemonic=LOC, unit=, value=A9-16-49, descr=LOCATION, original_mnemonic=LOC),
HeaderItem(mnemonic=PROV, unit=, value=SASKATCHEWAN, descr=PROVINCE, original_mnemonic=PROV),
HeaderItem(mnemonic=SRVC, unit=, value=ANY LOGGING COMPANY LTD., descr=SERVICE COMPANY, original_mnemonic=SRVC),
HeaderItem(mnemonic=DATE, unit=, value=25-DEC-1988, descr=LOG DATE, original_mnemonic=DATE),
HeaderItem(mnemonic=UWI, unit=, value=100091604920, descr=UNIQUE WELL ID, original_mnemonic=UWI)]
>>> las.params
[HeaderItem(mnemonic=BHT, unit=DEGC, value=35.5, descr=BOTTOM HOLE TEMPERATURE, original_mnemonic=BHT),
HeaderItem(mnemonic=BS, unit=MM, value=200.0, descr=BIT SIZE, original_mnemonic=BS),
HeaderItem(mnemonic=FD, unit=K/M3, value=1000.0, descr=FLUID DENSITY, original_mnemonic=FD),
HeaderItem(mnemonic=MATR, unit=, value=0.0, descr=NEUTRON MATRIX(0=LIME,1=SAND,2=DOLO), original_mnemonic=MATR),
HeaderItem(mnemonic=MDEN, unit=, value=2710.0, descr=LOGGING MATRIX DENSITY, original_mnemonic=MDEN),
HeaderItem(mnemonic=RMF, unit=OHMM, value=0.216, descr=MUD FILTRATE RESISTIVITY, original_mnemonic=RMF),
HeaderItem(mnemonic=DFD, unit=K/M3, value=1525.0, descr=DRILL FLUID DENSITY, original_mnemonic=DFD)]
```

The data is stored as a 2D numpy array:

```python
>>> las.data
array([[ 1670.   ,   123.45 ,  2550.   , ...,   123.45 ,   110.2  ,   105.6  ],
       [ 1669.875,   123.45 ,  2550.   , ...,   123.45 ,   110.2  ,   105.6  ],
       [ 1669.75 ,   123.45 ,  2550.   , ...,   123.45 ,   110.2  ,   105.6  ],
       ...,
       [ 1669.75 ,   123.45 ,  2550.   , ...,   123.45 ,   110.2  ,   105.6  ],
       [ 1670.   ,   123.45 ,  2550.   , ...,   123.45 ,   110.2  ,   105.6  ],
       [ 1669.875,   123.45 ,  2550.   , ...,   123.45 ,   110.2  ,   105.6  ]])
```

You can also retrieve and load data as a ``pandas`` DataFrame, build LAS files
from scratch, write them back to disc, and export to Excel, amongst other
things.

See the [package documentation](https://lasio.readthedocs.io/en/latest/) for
more details.

## Contributing

Contributions are invited and welcome.

See [Contributing](https://lasio.readthedocs.io/en/latest/contributing.html) for how to get started.

## License

[MIT](https://github.com/kinverarity1/lasio/blob/master/LICENSE)
",320,320,32,46,petroleum,"[data-format, data-management, data-mining, geology, geophysics, geotechnical-engineering, groundwater, io, las-files, lasio, log-ascii-standard, mineral-exploration, petroleum, python, swung-t21]",0.0
105,agilescientific,welly,agilescientific,https://github.com/agilescientific/welly,https://api.github.com/repos/welly/agilescientific,"Welly helps with well loading, wireline logs, log quality, data science","![Welly banner](https://www.dropbox.com/s/a8jg7zomi4wgolb/welly_banner.png?raw=1)

[![Run tests](https://github.com/agilescientific/welly/actions/workflows/run-tests.yml/badge.svg)](https://github.com/agilescientific/welly/actions/workflows/run-tests.yml)
[![Build docs](https://github.com/agilescientific/welly/actions/workflows/build-docs.yml/badge.svg)](https://github.com/agilescientific/welly/actions/workflows/build-docs.yml)
[![PyPI version](https://img.shields.io/pypi/v/welly.svg)](https://pypi.python.org/pypi/welly/)
[![PyPI versions](https://img.shields.io/pypi/pyversions/welly.svg)](https://pypi.org/project/welly//)
[![PyPI license](https://img.shields.io/pypi/l/welly.svg)](https://pypi.org/project/welly/)

**`welly` facilitates the loading, processing, and analysis of subsurface wells and well data, such as striplogs, formation tops, well log curves, and synthetic seismograms.**


## Installation

    pip install welly

For developers, there are `pip` options for installing `test`, `docs` or `dev` (docs plus test) dependencies.


## Quick start

```python
from welly import Well, Project

w = Well.from_las('my_wells/my_well.las')  # Load a single well.
p = Project.from_las('my_wells/*.las')     # Load lots of wells.

gr = w.data['GR']  # One log...
gr.plot()          # ...with some superpowers!
```

Next, check out the tutorial notebooks.


## Documentation

[The `welly` documentation](https://code.agilescientific.com/welly) is a work in progress.


## Questions or suggestions?

[![slack](https://img.shields.io/badge/chat-on_slack-808493.svg?longCache=true&style=flat&logo=slack)](https://swung.slack.com/)

**If you'd like to chat about `welly` with us or other users, look for the **#welly-and-lasio** channel in the [Software Underground's Slack](https://softwareunderground.org/slack).**

To report bugs or suggest new features/improvements to the code, please [open an issue](https://github.com/agilescientific/welly/issues).


## Contributing

Please see [`CONTRIBUTING.md`](CONTRIBUTING.md).


## Philosophy

The [`lasio`](https://github.com/kinverarity1/lasio) project provides a very nice way to read and write [CWLS](http://www.cwls.org/) Log ASCII Standard files. The result is an object that contains all the LAS data — it's more or less analogous to the LAS file.

Sometimes we want a higher-level object, for example to contain methods that have nothing to do with LAS files. We may want to handle other well data, such as deviation surveys, tops (aka picks), engineering data, striplogs, synthetics, and so on. This is where `welly` comes in.

`welly` uses `lasio` for data I/O, but hides much of it from the user. We recommend you look at both projects before deciding if you need the 'well-level' functionality that `welly` provides.
",282,282,37,114,petroleum,"[data-management, data-mining, geology, geophysics, petroleum, petrophysics, python, swung-stack]",0.0
106,akashlevy,Deep-Learn-Oil,,https://github.com/akashlevy/Deep-Learn-Oil,https://api.github.com/repos/Deep-Learn-Oil/akashlevy,Deep learning tools for predicting oil well data,"# Applying Deep Learning to Petroleum Well Data
## QRI Project at TRiCAM

<img src=""logos/iacs.png"" alt=""IACS Logo"" height=""100""/><img src=""logos/qri.png"" alt=""QRI Logo"" height=""100""/>

This repository contains the source files required to reproduce the results in ""Applying Deep Learning to Petroleum Well Data."" This README will explain how to use these files.

## Dependencies

- [Python 2.7](https://www.python.org/)
- [Numpy/Matplotlib](http://www.scipy.org/)
- [Theano](http://deeplearning.net/software/theano/)
- [Keras](http://keras.io/)

## Usage

### Preprocessing

In order to preprocess the data, you will need to go into the folder `datasets/` and run the script `dataset_gen.py`. This script reads in the CSV files from `data/` and converts it into chunks. It does this based on several parameters. `IN_MONTHS`, `OUT_MONTHS` and `STEP_MONTHS`, specify how many months of input, how many months of output and how often to sample for chunks. It also requires two preprocessing parameters, `REMOVE_ZEROS` and `NORMALIZE_DATA`. `REMOVE_ZEROS`, when set to true, will eliminate all zeros from the datasets and push the points together. `NORMALIZE_DATA` will normalize each chunk with respect to the input portion. The random seed `SEED` determines how the data is shuffled. As the data from each well is made into chunks, the chunks are assigned to the training, validation, and testing datasets. The wells are assigned in a train:valid:test = 6:1:1 ratio. Each dataset is represented as a tuple in Python; the first element of the tuple is a NumPy array containing the chunk inputs (the ""x""), and the second element of the tuple is a NumPy array containing the chunk outputs (the ""y""). The three datasets are then pickled and stored in a gzipped file called `qri.pkl.gz`. After the dataset is careated, the chunks are plotted using matplotlib.

### Testing a Single Model
In the `keras/` folder, there are several scripts with names of different neural network architectures. Each contains the code required to construct a single neural network. Each file consists of a similar structure.

#### Structure

After importing the necessary libraries, a model name is specified through `MDL_NAME`. Next, NumPy's random number generator is seeded with a number to ensure reproducibility of the neural network's results. Then the QRI data is loaded from the gzipped pickle file `qri.pkl.gz` and split into either 2D or 3D datasets. After this comes the architecture specification. The stochastic gradient descent algorithm parameters are then specified; `lr` refers to the learning rate, `momentum` specifies the extent to which past gradient values should be incorporated into the optimization, `decay` specifies the rate at which the learning rate decreases, and `nesterov` specifies whether or not Nesterov's formula should be used to compute the gradient. After the optimization technique is specified, the model is compiled with Theano using a particular loss function.

Next, the early stopping parameters are specified. The validation loss is monitored and `patience` specifies how long the neural network should wait to observe a new best validation loss. The best model is saved to the subfolder `models/<MDL_NAME>.mdl`. These features are incorporated using a callback mechanism during training.

The model is then trained. The lines
```python
t0 = time.time()
```
and
```python 
time_elapsed = time.time() - t0
```
are used to determine how long training took. There are three parameters to the training function `model.fit`; the first is `verbose` that specifies how often data should be printed to the console. The second is `nb_epoch` that specifies the maximum number of training steps. The last is `batch_size` that specifies the number of chunks that should be trained on at once.

After the model is done training, the best model is loaded from the MDL file. Then the model is evaluated on the testing set and the training time and testing set error are displayed. The results and the training/validation error are saved to `results/<MDL_NAME>.out` and `models/<MDL_NAME>.hist` respectively. Then the training and validation error are plotted as well as the test predictions.

#### Model Specifics

Every model begins with 
```python
model = Sequential()
```
which denotes that the neural network consists of a series of stacked layers. There are many different kinds of layers:
- **Dense**: a regular fully-connected layer; specify number of inputs, number of outputs, and activation function
- **Convolution1D**: a convolutional layer; specify *stack size* (how many filters you used in the previous layer, 1 if first layer), number of kernels per filter, and activation function
- **SimpleRNN, GRU, LSTM, MUT123**: different kinds of recurrent layers; specify number of inputs, number of outputs, and activation function
- **SimpleDeepRNN**: a multi-layer recurrent network; specify number of inputs, number of outputs, number of layers, and activation function
- **Dropout**: used to make a network more sparse; specify the fraction of inputs to randomly set to 0
- **Flatten**: convert a multi-dimensional input into a 1D input.

Using these Keras layers, we can construct custom neural networks to perform time series prediction on oil wells.

#### Custom Neural Network Tools (found in `qri.py`)

- `load_data`: loads the data from `qri.pkl.gz`
- `plot_test_predictions`: plots each chunk from the test set along with the prediction made for that set
- `plot_train_valid_loss`: plots how the training and validation error decreased in training
- `print_output_graph`: prints the computational graph for producing predictions to filename in a specified image format; useful for debugging and seeing how the network actually works
- `plot_weights`: plots the weight matrix for each layer in the neural network; useful for understanding what the neural network is learning
- `mae_clip`: provides a Theano expression for the mean absolute error with clipping to provide resistance to outliers; the `CLIP_VALUE` can be changed to adjust the number of standard deviations at which to begin clipping
- `save_results`: pickles the results and saves them to a file
- `save_history`: saves the training and validation loss history to a file

### Hyperparameter Optimization using Grid Search

We used variants of the scripts provided in `cluster` to run our models on Harvard's Odyssey computing cluster. They can be modified to work on different kinds of clusters.

### Bayesian Hyperparameter Optimization
For more information, see [Spearmint](https://github.com/JasperSnoek/spearmint).

## Cited By
- Stamp, Alexander. ""The relationship between weather forecasts and observations for predicting electricity output from wind turbines."" (2017).
- Abdullayeva, Fargana, and Yadigar Imamverdiyev. ""Development of oil production forecasting method based on Deep Learning."" Statistics, Optimization & Information Computing 7, no. 4 (2019): 826-839.
- Da Silva, Luciana Maria, Guilherme Daniel Avansi, and Denis José Schiozer. ""Development of proxy models for petroleum reservoir simulation: a systematic literature review and state-of-the-art.""

Thanks to all our citers!

## Contact
Please contact <akashlevy@gmail.com>, <janette_garcia08@hotmail.com>, <albert.tung0902@my.riohondo.edu> or <michelleyang@berkeley.edu> with any questions about this repository. Thank you!
",81,81,18,0,petroleum,"[data-processing, dataset, deep-learning, harvard, keras, neural-network, petroleum, python, research]",0.0
107,pro-well-plan,well_profile,pro-well-plan,https://github.com/pro-well-plan/well_profile,https://api.github.com/repos/well_profile/pro-well-plan,The python tool for well trajectories,"[![Cover](https://github.com/pro-well-plan/opensource_apps/raw/master/resources/pwp-bgd.gif)](https://prowellplan.com)

[![Open Source Love svg2](https://badges.frapsoft.com/os/v2/open-source.svg?v=103)](https://github.com/pro-well-plan/well_profile/blob/master/LICENSE.md)
[![PyPI version](https://badge.fury.io/py/well-profile.svg)](https://badge.fury.io/py/well-profile)
[![License: LGPL v3](https://img.shields.io/badge/License-LGPL_v3-blue.svg)](https://www.gnu.org/licenses/lgpl-3.0)
[![Webapp](https://img.shields.io/badge/WebApp-On-green.svg)](https://share.streamlit.io/jcamiloangarita/opensource_apps/app.py)
[![Documentation Status](https://readthedocs.org/projects/well_profile/badge/?version=latest)](http://well_profile.readthedocs.io/?badge=latest)
[![Tests](https://github.com/pro-well-plan/well_profile/workflows/Tests/badge.svg)](https://github.com/pro-well-plan/well_profile/actions)
[![Downloads](https://pepy.tech/badge/well-profile)](https://pepy.tech/project/well-profile)


## Introduction
well_profile is a tool to generate or load well profiles in 3D. Features are added as they
are needed; suggestions and contributions of all kinds are very welcome.

## Documentation

See here for the [complete well_profile package documentation](https://well_profile.readthedocs.io/en/latest/).

## Getting Started

These instructions will get you a copy of the project up and running on your local machine for development and testing purposes.

### Get well_profile

* Users: Wheels for Python from [PyPI](https://pypi.python.org/pypi/well-profile/) 
    * `pip install well_profile`
* Developers: Source code from [Github](https://github.com/pro-well-plan/well_profile)
    * `git clone https://github.com/pro-well-plan/well_profile`
 
### Quick examples

Color by specified parameter: `e.g. 'dls'|'dl'|'tvd'|'md'|'inc'|'azi'`
```
import well_profile as wp
well = wp.load('trajectory1.xlsx')     # LOAD WELL
well.plot(style={'color': 'dls', 'size': 5}).show()
```
[![](https://user-images.githubusercontent.com/52009346/108047411-0e028580-7046-11eb-9de9-84c1cda2c903.png)](https://well-profile.readthedocs.io/en/latest/)

Also with dark mode:
```
well.plot(style={'darkMode': True, 'color': 'dls', 'size': 5}).show()
```
[![](https://user-images.githubusercontent.com/52009346/108048173-fed00780-7046-11eb-89f8-2a3b437b3047.png)](https://well-profile.readthedocs.io/en/latest/)

Plotting 3 wellbores:
* `Well 1 -> excel file: trajectory1.xlsx`
* `Well 2 -> generated well`
* `Well 3 -> excel file: trajectory2.xlsx`
```
import well_profile as wp
well_1 = wp.load('trajectory1.xlsx')      # LOAD WELL 1
well_2 = wp.get(6000, profile='J', kop=2000, eob=3000, build_angle=85, set_start={'east':2000})       # GET WELL 2 --> North: 0 m, East: 2000 m
well_3 = wp.load('trajectory2.xlsx', set_start={'north':-3000})        # LOAD WELL 3 --> North: -3000 m, East: 0 m
well_1.plot(add_well=[well_2, well_3],
            names=['first well name',
                   'second well name',
                   'third well name']).show()        # Generate 3D plot for well 1 including wells 2 and 3
```
<a href=""https://youtu.be/X7Bs9_7NdRM"">
   <img alt=""Qries"" src=""https://well-profile.readthedocs.io/en/latest/_images/multiple_diff_loc.png""
   width=700"" height=""400"">
</a>        

## Contributing

Please read [CONTRIBUTING](CONTRIBUTING.md) for details on our code of conduct, and the process for submitting pull requests to us.

## History ##
This tool was initially written and is maintained by [Pro Well Plan
AS](http://www.prowellplan.com/) as a free, simple, easy-to-use way to perform
torque and drag calculations along the well that can be tailored to our needs, and as contribution to the
free software community.

## License

This project is licensed under the GNU Lesser General Public License v3.0 - see the [LICENSE](LICENSE.md) file for details


*for further information contact juan@prowellplan.com*

[![](https://user-images.githubusercontent.com/52009346/69100304-2eb3e800-0a5d-11ea-9a3a-8e502af2120b.png)](https://prowellplan.com)
",70,70,10,2,petroleum,"[directional-drilling, drilling, drilling-engineering, petroleum, well-design, well-engineering, wellbore]",0.0
108,pro-well-plan,pwptemp,pro-well-plan,https://github.com/pro-well-plan/pwptemp,https://api.github.com/repos/pwptemp/pro-well-plan,pwptemp,"[![Cover](https://github.com/pro-well-plan/opensource_apps/raw/master/resources/pwp-bgd.gif)](https://prowellplan.com)

[![Open Source Love svg2](https://badges.frapsoft.com/os/v2/open-source.svg?v=103)](https://github.com/pro-well-plan/pwptemp/blob/master/LICENSE.md)
[![PyPI version](https://badge.fury.io/py/pwptemp.svg)](https://badge.fury.io/py/pwptemp)
[![License: LGPL v3](https://img.shields.io/badge/License-LGPL_v3-blue.svg)](https://www.gnu.org/licenses/lgpl-3.0)
[![Webapp](https://img.shields.io/badge/WebApp-On-green.svg)](https://share.streamlit.io/jcamiloangarita/opensource_apps/app.py)
[![Documentation Status](https://readthedocs.org/projects/pwptemp/badge/?version=latest)](http://pwptemp.readthedocs.io/?badge=latest)
[![Build Status](https://www.travis-ci.com/pro-well-plan/pwptemp.svg?branch=master)](https://www.travis-ci.com/pro-well-plan/pwptemp)
[![Downloads](https://pepy.tech/badge/pwptemp)](https://pepy.tech/project/pwptemp)

## Contributors

* **Juan Camilo Gonzalez Angarita** - [jcamiloangarita](https://github.com/jcamiloangarita)
* **Muhammad Suleman** - [msfazal](https://github.com/msfazal)
* **Eirik Lyngvi** - [elyngvi](https://github.com/elyngvi)
* **Magnus Tvedt** - [magnusbj](https://github.com/magnusbj)

See the full list of [contributors](https://github.com/pro-well-plan/pwptemp/graphs/contributors) involved in this project.

## Introduction
Pwptemp is a LGPL licensed library for easy calculation of the
temperature distribution along the well. Features are added as they
are needed; suggestions and contributions of all kinds are very welcome.

To catch up on the latest development and features, see the [changelog](CHANGELOG.md).

## Documentation

See here for the [complete pwptemp package documentation](https://pwptemp.readthedocs.io/en/latest/).

## Getting Started

These instructions will get you a copy of the project up and running on your local machine for development and testing purposes.

### Get pwptemp

* Users: Wheels for Python from [PyPI](https://pypi.python.org/pypi/pwptemp/) 
    * `pip install pwptemp`
* Developers: Source code from [Github](https://github.com/pro-well-plan/pwptemp)
    * `git clone https://github.com/pro-well-plan/pwptemp`

## Quick Use

Take a look at the [tutorial](https://github.com/pro-well-plan/pwptemp/blob/master/Tutorial.md)
to check how to use the different functions available. 
    
## Contributing

Please read [CONTRIBUTING](CONTRIBUTING.md) for details on our code of conduct, and the process for submitting pull requests to us.

## History ##
Pwptemp was initially written and is maintained by [Pro Well Plan
AS](http://www.prowellplan.com/) as a free, simple, easy-to-use way of getting
temperature data that can be tailored to our needs, and as contribution to the
free software community.

## License

This project is licensed under the GNU Lesser General Public License v3.0 - see the [LICENSE](LICENSE.md) file for details


*for further information contact juan@prowellplan.com*

[![](https://user-images.githubusercontent.com/52009346/69100304-2eb3e800-0a5d-11ea-9a3a-8e502af2120b.png)](https://prowellplan.com)
",56,56,11,8,petroleum,"[drilling, injection, petroleum, production, simulation, temperature]",0.0
109,GeostatisticsLessons,GeostatisticsLessonsNotebooks,,https://github.com/GeostatisticsLessons/GeostatisticsLessonsNotebooks,https://api.github.com/repos/GeostatisticsLessonsNotebooks/GeostatisticsLessons,These are python notebooks accompanying Lessons available at GeostatisticsLessons.com,"# Geostatistics Lessons Notebooks

[Geostatistics Lessons](http://geostatisticslessons.com/) is an open disclosure of some guidance in geostatistical modeling reviewed by an editorial board. These python notebooks and data are prepared by Lesson authors to supplement their Lesson. As new Lessons are authored and notebooks created, this repository will be updated. 

## Lessons

Lessons with notebooks and data available include:

* An Application of Bayes Theorem to Geostatistical Mapping ([notebook](notebooks/bayesmapping/bayesmapping.ipynb) and [lesson](http://geostatisticslessons.com/lessons/bayesmapping)), Jared Deutsch and Clayton Deutsch, 2018
* Multidimensional Scaling ([notebook](notebooks/mds/mds.ipynb) and [lesson](http://geostatisticslessons.com/lessons/mds)), Steven Mancell and Clayton Deutsch, 2019
* Collocated Cokriging ([notebook](notebooks/collocatedcokriging/collocatedcokriging.ipynb) and [lesson](http://geostatisticslessons.com/lessons/collocatedcokriging)), Matthew Samson and Clayton Deutsch, 2020

## Dependencies

Notebooks are implemented in Python 3 using the scientific python stack (including NumPy, Pandas, Matplotlib). Refer to the individual notebooks for any particular dependencies. 

## License

Notebooks are licensed under the [MIT](LICENSE) license separately from Lessons. Refer to [Geostatistics Lessons](http://geostatisticslessons.com/) for licensing information on the Lessons.
",48,48,7,0,petroleum,"[applied-statistics, geostatistics, geostatistics-lessons, mining, notebook, petroleum, python-notebook, statistics]",0.0
110,pro-well-plan,petrodc,pro-well-plan,https://github.com/pro-well-plan/petrodc,https://api.github.com/repos/petrodc/pro-well-plan,Petroleum Data Collector,"[![Cover](https://github.com/pro-well-plan/opensource_apps/raw/master/resources/pwp-bgd.gif)](https://prowellplan.com)


[![Open Source Love svg2](https://badges.frapsoft.com/os/v2/open-source.svg?v=103)](https://github.com/pro-well-plan/pwptemp/blob/master/LICENSE.md)
[![PyPI version](https://badge.fury.io/py/petrodc.svg)](https://badge.fury.io/py/petrodc)
[![License: LGPL v3](https://img.shields.io/badge/License-LGPL_v3-blue.svg)](https://www.gnu.org/licenses/lgpl-3.0)
[![Webapp](https://img.shields.io/badge/WebApp-On-green.svg)](https://share.streamlit.io/jcamiloangarita/opensource_apps/app.py)
[![Documentation Status](https://readthedocs.org/projects/petrodc/badge/?version=latest)](http://petrodc.readthedocs.io/?badge=latest)
[![Build Status](https://www.travis-ci.com/pro-well-plan/petrodc.svg?branch=master)](https://www.travis-ci.com/pro-well-plan/petrodc)
[![Downloads](https://pepy.tech/badge/petrodc)](https://pepy.tech/project/petrodc)

## Introduction
PetroDC is a LGPL licensed tool to get datasets from public sources. 
New sources are added as they are tested; suggestions and contributions of 
all kinds are very welcome.

## Documentation

Check here for the [complete petrodc package documentation](https://petrodc.readthedocs.io/en/latest/).

## Getting Started

These instructions will get you a copy of the project up and running on your local machine for development and testing purposes.

### Get petrodc

* Users: Wheels for Python from [PyPI](https://pypi.python.org/pypi/petrodc/) 
    * `pip install petrodc`
* Developers: Source code from [Github](https://github.com/pro-well-plan/petrodc)
    * `git clone https://github.com/pro-well-plan/petrodc`

## Quick Use

> import petrodc.npd as dc
>
> dataframe = dc.wellbore(3)

* 3 is the dataset with lithostratigraphy, check [here](https://github.com/pro-well-plan/petrodc/blob/master/petrodc/npd/wellbore.py)
to see the respective number for the different datasets.

## License

This project is licensed under the GNU Lesser General Public License v3.0 - see the [LICENSE](LICENSE.md) file for details


*for further information contact juan@prowellplan.com*

[![](https://user-images.githubusercontent.com/52009346/69100304-2eb3e800-0a5d-11ea-9a3a-8e502af2120b.png)](https://prowellplan.com)
",38,38,8,0,petroleum,"[data, engineering, geophysics, petroleum, petrophysics, well-logs]",0.0
111,derrickturk,aRpsDCA,,https://github.com/derrickturk/aRpsDCA,https://api.github.com/repos/aRpsDCA/derrickturk,R package for Arps decline curve analysis.,"# aRpsDCA
### an R package for Arps decline-curve analysis

aRpsDCA provides R implementations of functions for carrying out Arps decline-curve analysis on oil and gas production data.  

aRpsDCA currently implements the following decline-curve types:  
* Exponential  
* Hyperbolic (and harmonic)  
* Hyperbolic with terminal exponential (aka ""modified hyperbolic"", ""hyperbolic-to-exponential"")  
* Any of the above with initial rate curtailment  
* Any of the above with initial linear buildup periods  

aRpsDCA provides functions for  
* computing rate, cumulative production, and instantaneous decline over time  
* computing EUR and time to economic limit  
* performing best fits of various decline curve types to actual production data  
* rate, decline, and time unit conversions  

aRpsDCA is released under the LGPL v2.1 and is free for commercial and non-commercial use.  

The current ""released"" version of aRpsDCA is 1.1.1 and is available from [CRAN](https://cran.r-project.org/package=aRpsDCA).  

The current pre-release version of aRpsDCA can also be installed from github using the devtools library:  
```R
install.packages('devtools')
devtools::install_github('derrickturk/aRpsDCA')
```

Release notes:  
v1.0.0 (2014-04-03): initial release  
v1.0.1 (2015-06-21): S3 methods for formatting now correctly print curve family; handling of Np for D = 0 is corrected  
v1.0.2 (2016-01-06): evaluation of hyperbolic-to-exponential declines with Di = Df now handled correctly  
v1.1.0 (2016-04-04): Arps declines with linear initial buildup periods, and fitting to interval-volume data; additional bug fixes for daily data and better initial guesses for decline parameters  
v1.1.1 (2017-07-23): EUR for declines with buildup now handled correctly; zero results from arps.q and arps.Np when decline with buildup was passed with only post-buildup time values are now corrected  

(c) 2017 [dwt](http://www.github.com/derrickturk) | [terminus data science, LLC](http://www.terminusdatascience.com)
",32,32,10,3,petroleum,"[decline-curve-analysis, petroleum, petroleum-engineering]",0.0
112,f0nzie,rNodal,,https://github.com/f0nzie/rNodal,https://api.github.com/repos/rNodal/f0nzie,Nodal Analysis for Petroleum Production Engineering,"
<!-- README.md is generated from README.Rmd. Please edit that file -->

# rNodal

The goal of rNodal is to provide nodal analysis for oil and gas wells.

## Installation

You can install rNodal from github with:

``` r
# install.packages(""devtools"")
devtools::install_github(""f0nzie/rNodal"")
```

## Example

This is a basic example which shows you how to solve a common problem:

``` r
## basic example code
```

## Using `zFactor`

`zFactor` is a R package. Calling from CRAN.

We will use zFactor for gas compressibility calculations.

``` r
# use the new library zFactor
library(zFactor)

z.HallYarborough(pres.pr = 4.5, temp.pr = 1.4)
#> [1] 0.7373812
z.DranchukAbuKassem(pres.pr = 4.5, temp.pr = 1.4)
#> [1] 0.7396345
z.BeggsBrill(pres.pr = 4.5, temp.pr = 1.4)
#> [1] 0.7343367
z.Ann10(pres.pr = 4.5, temp.pr = 1.4)
#> [1] 0.736032
z.Papp(pres.pr = 4.5, temp.pr = 1.4)
#> [1] 0.7299354
```

## How rNodal works

Start by looking at the examples in the vignettes. We will use in this
example `VLP Brown - Example C13`.

This is example C.13 in the Kermit Brown book.

### Input well data

We enter the well data with the function `setWellInput`:

``` 
input.example.C13 <- setWellInput(field.name = ""HAGBR.MOD"",
                                    well.name = ""Brown_C13"", 
                                    depth.wh = 0, depth.bh = 2670, 
                                    diam.in = 1.995, 
                                    GLR = 500, liq.rt = 1000, wcut = 0.6, 
                                    thp = 500, tht = 120, bht = 150, 
                                    API = 22, gas.sg = 0.65, 
                                    wat.sg = 1.07, if.tens = 30)
                                    
                                    
```

The field name and well name are used for archival purposes.

### Enter the parameters of the VLP model

The parameters of the model consist of:

`vlp.model`: the correlation or mechanistical model

`segments`: the number of segments to split the well

`tol`: the tolerance of the delta-P iterations

``` 
well.model <- setVLPmodel(vlp.model = ""hagbr.mod"", 
                           segments = 11, 
                                tol = 0.000001)
                                
                                
                                
```

### Run the model

To run the model is necessary to provide:

`well.input`: all the well parameters as entered in the first step

`well.model`: the VLP model as entered in the second step

    runVLP(well.input = input.example.C13, well.model))

### Results

The results are given in the form of a dataframe where the rows
represent the number of segment plus one and the columns are the
calculations or variables.

<img src=""man/figures/README-results_df.jpg"" width=""800px"" />
",30,30,4,1,petroleum,"[data-science, nodal-analysis, petroleum, rstats]",0.0
113,derrickturk,ocd_production,,https://github.com/derrickturk/ocd_production,https://api.github.com/repos/ocd_production/derrickturk,Unpack and process a wcproduction.zip file from the New Mexico OCD's FTP site,"# OCD Production Parser

A Rust program for parsing the `wcproduction.zip` monthly production file from the [New Mexico Oil Conservation Division](http://www.emnrd.state.nm.us/OCD/), as described in [""XMHell: Handling 38GB of UTF-16 XML with Rust""](https://usethe.computer/posts/14-xmhell.html).

Freely available under the Apache License, version 2.0.

### (c) 2020 [dwt](https://usethe.computer) | [terminus data science, LLC](https://terminusdata.science)
",25,25,2,0,petroleum,[petroleum],0.0
114,abhishekdbihani,synthetic_well-log_polynomial_regression,,https://github.com/abhishekdbihani/synthetic_well-log_polynomial_regression,https://api.github.com/repos/synthetic_well-log_polynomial_regression/abhishekdbihani,"This project attempts to construct a missing well log from other available well logs, more specifically an NMR well log from the measured Gamma Ray (GR), Caliper, Resistivity logs and the interpreted porosity from a well. ","## Constructing a Synthetic NMR Well-log using Machine Learning


### By Abhishek Bihani

### Final Project for PGE 383 – Subsurface Machine Learning taught by Dr. Michael Pyrcz (Fall - 2019)

### Hildebrand Department of Petroleum and Geosystems Engineering

### The University of Texas at Austin

****


**Executive Summary:** 

The nuclear magnetic resonance (NMR) log is a useful tool to understand lithological information such as the variation of pore size distribution with depth, but it may not be measured in all wells. The [project](https://github.com/abhishekdbihani/synthetic_well-log_polynomial_regression/blob/master/KC151%20-%20PGE383%20v1.ipynb) attempts to predict a missing well log from other available well logs using machine learning tools, more specifically an NMR well log from the measured Gamma Ray (GR) log, Caliper log, Resistivity log, and the interpreted porosity from one well at the Keathley Canyon in the Gulf of Mexico. The constructed model is then used to predict the NMR log at Walker Ridge in Gulf of Mexico, which is another nearby site of methane hydrate accumulation. 

In Keathley Canyon Block 151 (KC-151), the analyzed well was drilled and logged during Leg I of the U.S. Department of Energy/Chevron Gas Hydrate Joint Industry Project (JIP) (Ruppel et al., 2008). At Walker Ridge 313 (WR-313), the analyzed well was drilled and logged during JIP Leg II (Collett et al., 2012). The raw well logs for KC-151 are available [here](http://mlp.ldeo.columbia.edu/data/ghp/JIP1/KC151-2/index.html?) and for WR-313 are available [here](http://mlp.ldeo.columbia.edu/data/ghp/JIP2/WR313-H/). The processed well logs used in this project for KC-151 are available [here](https://github.com/abhishekdbihani/synthetic_well-log_polynomial_regression/blob/master/KC151_logs.csv) and for WR-313 are available [here](https://github.com/abhishekdbihani/synthetic_well-log_polynomial_regression/blob/master/WR313H_logs.csv).

**Approach:**

1) For an easier characterization of the NMR data, the NMR log, i.e. relaxation time distribution was converted into Mean of T2 (MLT2) and Standard Deviation of T2 (SDT2) which are considered as the two response features to be predicted. The other well logs: GR, Caliper, Resistivity, and the interpreted porosity are the predictor features used to train the model.

2) An initial analysis is conducted on the well logs to check the univariate and bivariate distributions of the data, and the well-logs are plotted with depth. 

3) Then a linear regression is conducted for both MLT2 and SDT2 using the other predictor variables to observe the behavior with a basic model. It is seen that the linear regression could not capture the response behavior well due to noise, i.e. short-distance variations as well as non-linearities in the data relationships. 

4) This is followed by feature standardization before applying more complex models to reduce effect of outliers and predictor features having different units. Feature ranking was conducted to compare the order in which predictor variables affect the response variables.

5) Then, the logs are processed to reduce noise, and after a train-test split, polynomial regression modeling is conducted to predict the NMR log at Keathley Canyon until a good fit is obtained.

6) Finally, the trained model is used to predict the NMR log at Walker Ridge where it was not recorded.

**Pre-requisites:**

1. Python3

1. Anaconda

**Instructions:**

Run the following commands using the anaconda command line utility (after navigating to the project folder), to install the required packages, activate the environment and the notebook. 

Commands:
```bash

conda create --name swlpr
conda activate swlpr
pip install -r requirements.txt --ignore-installed --user
jupyter notebook ""KC151 - PGE383 v1.ipynb""

```

*Note: The code and procedures used for this project have been adapted from the workflows followed by Dr. Pyrcz in the class (Pyrcz, 2019 a, b, c, d) and my Master's thesis supervised by Dr. Daigle (Bihani, 2016).*

<img src=""https://github.com/abhishekdbihani/synthetic_well-log_polynomial_regression/blob/master/KC151-logs.png"" align=""middle"" width=""800"" height=""600"" alt=""Well-logs at KC-151"" >

                                    Figure- Well logs from Keathley Canyon 151


**Assumptions:**

1) The conditions at both KC-151 and WR-313 locations are assumed to be similar enough so the same model can be applied.

2) The model is assumed to be sufficiently trained to make predictions but can be improved if more training data is available.

3) The porosity has been calculated from the bulk density log since porosity is a function of the grain density of the formation (2.65 gm/cm3 in sands, 2.70 gm/cm3 in clays; Daigle et al., 2015) and of the pore-filled fluid (assumed to be water, with a density of 1.03 gm/cm3; Daigle et al., 2015).

4) During polynomial regression, it was assumed that all the relationships between predictors and response features could be captured by basis expansion until the 3rd power.

**Citation:**
 
 If you find this repository useful, please cite-
 
Bihani A., Pore Size Distribution and Methane Equilibrium Conditions at Walker Ridge Block 313, Northern Gulf of Mexico, M.S. thesis, University of Texas, Austin, Texas, 2016. doi:10.15781/T2542J80Z
 
**Related publications:**

Bihani A., Pore Size Distribution and Methane Equilibrium Conditions at Walker Ridge Block 313, Northern Gulf of Mexico, M.S. thesis, University of Texas, Austin, Texas, 2016. doi:10.15781/T2542J80Z

Bihani A., Daigle H., Cook A., Glosser D., Shushtarian A. (2015). OS23B-1999: Pore Size Distribution and Methane Equilibrium Conditions at Walker Ridge Block 313, Northern Gulf of Mexico. AGU Fall Meeting, 14-18 December, San Francisco, USA. 

**References:**

Collett, T. S., Lee, M. W., Zyrianova, M. V., Mrozewski, S. a., Guerin, G., Cook, A. E., and Goldberg, D. S. (2012). Gulf of Mexico Gas Hydrate Joint Industry Project Leg II logging- while-drilling data acquisition and analysis. Marine and Petroleum Geology, 34(1),41-61, doi:10.1016/j.marpetgeo.2011.08.003

Daigle, H., Cook, A., and Malinverno, A. (2015). Permeability and porosity of hydrate- bearing sediments in the northern Gulf of Mexico. Marine and Petroleum 	Geology, 68, 	551–564, doi:10.1016/j.marpetgeo.2015.10.004

Pyrcz M., (2019a) Feature Selection for Subsurface Data Analytics in Python. Retrieved December 5, 2019, from https://github.com/GeostatsGuy/PythonNumericalDemos/blob/master/SubsurfaceDataAnalytics_Feature_Ranking.ipynb

Pyrcz M., (2019b) Principal Component Analysis for Subsurface Data Analytics in Python. Retrieved December 5, 2019, from
https://github.com/GeostatsGuy/PythonNumericalDemos/blob/master/SubsurfaceDataAnalytics_PCA.ipynb

Pyrcz M., (2019c) Time Series Analysis for Subsurface Modeling in Python. Retrieved December 5, 2019, from
https://github.com/GeostatsGuy/PythonNumericalDemos/blob/master/SubsurfaceDataAnalytics_TimeSeries.ipynb

Pyrcz M., (2019d) Polygonal Regression for Subsurface Data Analytics in Python. Retrieved December 5, 2019, from
https://github.com/GeostatsGuy/PythonNumericalDemos/blob/master/SubsurfaceDataAnalytics_PolygonalRegression.ipynb

Ruppel, C., Boswell, R., and Jones, E. (2008). Scientific results from Gulf of Mexico Gas Hydrates Joint Industry Project Leg 1 drilling: Introduction and overview. Marine and Petroleum Geology, 25(9), 819–829. doi:10.1016/j.marpetgeo.2008.02.007

*****








",24,24,3,0,petroleum,"[feature-engineering, feature-selection, keathley-canyon, nmr, nmr-log, petroleum, petrophysics, polynomial-regression, porosity, resistivity-log, walker-ridge, well-logs]",0.0
115,donald-keighley,lasr,,https://github.com/donald-keighley/lasr,https://api.github.com/repos/lasr/donald-keighley,An R package for reading Log Ascii Standard (LAS) files for well log data.,"lasr
================
Donny Keighley
6/8/2022

[lasr](https://github.com/donald-keighley/lasr) is a package designed
for reading and writing [Log Ascii Standard
(LAS)](https://www.cwls.org/products/) files in R. Currently it is in
the beta testing stages. As such, it is subject to significant ongoing
changes and is not complete. For instance it can’t write LAS files yet…

## Goals

lasr is primarily designed to import LAS files at high speed and in
large batches. To accomplish this, most of it is written in C++ and
connected to R with [Rcpp](http://www.rcpp.org/). It stores the data in
lists of [data.table’s](https://rdatatable.gitlab.io/data.table/) for
fast manipulation.

Currently, the focus is on supporting reading [LAS
3.0](https://www.cwls.org/wp-content/uploads/2014/09/LAS_3_File_Structure.pdf)
as fully as possible. There will be some effort to handle non-standard
LAS files but nothing too cute. lasr is being written to load files
using as little information from the header as possible which should
alleviate many common issues. Beyond that, the aim is to output helpful
error messages so that non-standard files can be fixed. Afterall, one of
the great things about LAS files is that they are human readable and can
often be fixed using a simple text editor.

[**lasr**](https://github.com/donald-keighley/lasr) is not designed to
accomplish traditional petrophysical workflows and there are no plans to
do so. There’s plenty of industry software for that. lasr is intended as
a building block to facilitate people doing new and creative things with
large volumes of log data. It’s also intended to help build up the
library of geoscience packages for R.

For a more traditional approach, or if you’d prefer to use Python, you
should check out the excellent
[**lasio**](https://lasio.readthedocs.io/en/latest/index.html) package.

## Installation

You can install [**lasr**](https://github.com/donald-keighley/lasr) from
github using the
[`install_github`](https://www.rdocumentation.org/packages/devtools/versions/1.13.6/topics/install_github)
function from the [devtools](https://devtools.r-lib.org/) package. You
will need to install
[RTools](https://cran.r-project.org/bin/windows/Rtools/) first since the
package needs compilation.

``` r
if (!require('devtools')) install.packages('devtools')
library(devtools)
install_github('https://github.com/donald-keighley/lasr')
```

Currently, the only function is `read.las` which will import a vector of
LAS file paths into a multi-part list. Each section of the file is
stored as a separate element. In order to accomodate LAS 3.0 files which
may have multiple log data sections, the log parameter, log definition,
and log data are combined into numbered log elements. If your vector of
paths contains more than one file, the output list will have an element
for each file.

Here is an example reading a single LAS file that is included with the
package:

``` r
library(lasr)
las = read.las(system.file(""extdata"", ""las_3_cwls.las"", package = ""lasr""))

#Display the WELL section
head(las$well, 10)
```

    ##     mnemonic unit                    value           comment     format
    ##  1:     STRT    M                1660.1250 First Index Value           
    ##  2:     STOP    M                1660.8750  Last Index Value           
    ##  3:     STEP    M                   0.1250              STEP           
    ##  4:     NULL                       -999.25        NULL VALUE           
    ##  5:     COMP          ANY OIL COMPANY INC.           COMPANY           
    ##  6:     WELL         ANY ET AL 01-02-03-04              WELL           
    ##  7:      FLD                       WILDCAT             FIELD           
    ##  8:      LOC                    1-2-3-4W5M          LOCATION           
    ##  9:     SRVC      ANY LOGGING COMPANY INC.   SERVICE COMPANY           
    ## 10:     DATE                    13/12/1986      Service DATE DD/MM/YYYY
    ##     association
    ##  1:            
    ##  2:            
    ##  3:            
    ##  4:            
    ##  5:            
    ##  6:            
    ##  7:            
    ##  8:            
    ##  9:            
    ## 10:

``` r
#Display the log curves
head(las$log$log.1$data, 10)
```

    ##        DEPT     DT DPHI NPHI      YME                     CDES NMR[1] NMR[2]
    ## 1: 1660.125 123.45 0.11 0.37 1.45E+12          DOLOMITE W/VUGS     10     12
    ## 2: 1660.250 123.45 0.12 0.36 1.47E+12                LIMESTONE     12     15
    ## 3: 1660.375 123.45 0.13 0.35 2.85E+12            LOST INTERVAL     18     25
    ## 4: 1660.500 123.45 0.14 0.34 2.85E+12            LOST INTERVAL     18     25
    ## 5: 1660.625 123.45 0.15 0.33 2.85E+12            LOST INTERVAL     18     25
    ## 6: 1660.750 123.45 0.16 0.32 2.85E+12 SANDSTONE, SHALE STREAKS     18     25
    ## 7: 1660.875 123.45 0.17 0.31 2.85E+12            LOST INTERVAL     18     25
    ##    NMR[3] NMR[4] NMR[5]
    ## 1:     14     18     13
    ## 2:     21     35     25
    ## 3:     10      8     17
    ## 4:     10      8     17
    ## 5:     10      8     17
    ## 6:     10      8     17
    ## 7:     10      8     17

Most LAS files are version 2 and only have one log data section. If you
know this is the case you can set `flatten = TRUE` and only the first
log section will be returned. This makes referencing the log data
quicker.

``` r
las = read.las(system.file(""extdata"", ""las_3_cwls.las"", package = ""lasr""), flatten=TRUE)
head(las$log$data, 10)
```

    ##        DEPT     DT DPHI NPHI      YME                     CDES NMR[1] NMR[2]
    ## 1: 1660.125 123.45 0.11 0.37 1.45E+12          DOLOMITE W/VUGS     10     12
    ## 2: 1660.250 123.45 0.12 0.36 1.47E+12                LIMESTONE     12     15
    ## 3: 1660.375 123.45 0.13 0.35 2.85E+12            LOST INTERVAL     18     25
    ## 4: 1660.500 123.45 0.14 0.34 2.85E+12            LOST INTERVAL     18     25
    ## 5: 1660.625 123.45 0.15 0.33 2.85E+12            LOST INTERVAL     18     25
    ## 6: 1660.750 123.45 0.16 0.32 2.85E+12 SANDSTONE, SHALE STREAKS     18     25
    ## 7: 1660.875 123.45 0.17 0.31 2.85E+12            LOST INTERVAL     18     25
    ##    NMR[3] NMR[4] NMR[5]
    ## 1:     14     18     13
    ## 2:     21     35     25
    ## 3:     10      8     17
    ## 4:     10      8     17
    ## 5:     10      8     17
    ## 6:     10      8     17
    ## 7:     10      8     17

## Speed Test

Since the purpose of this package is to load LAS files as quickly as
possible, a speed test is included here with a comparison to python’s
[**lasio**](https://lasio.readthedocs.io/en/latest/index.html). First,
download a test dataset from the KGS website. In this case we’re using
the [2016 logs](http://www.kgs.ku.edu/PRS/Scans/Log_Summary/2016.zip)
data. Download and unzip them into a folder called *“C:/temp/logs”*, or
modify the code for wherever you put it.

Next, import the first 500 files. We’ll use 4 threads for this
comparison, although if you have more cores you can increase the number
of threads to speed it up further. Only use this option if you are
importing more than a handful of files, otherwise the parallel overhead
will slow it down.

``` r
files = list.files('C:/temp/logs', pattern = '.las?', full.names=TRUE)
start.time = Sys.time()
las = read.las(files[1:500],nthreads=4)
end.time = Sys.time()
time.taken = end.time - start.time
time.taken
```

    ## Time difference of 25.96407 secs

Now in Python in parallel using 4 cores:

``` python
import lasio, glob, datetime, multiprocessing
from joblib import Parallel, delayed

num_cores = 4
files = glob.glob('C:/temp/logs/*.las')
start_time = datetime.datetime.now()
if __name__ == ""__main__"":
    las = Parallel(n_jobs=num_cores)(delayed(lasio.read)(file) for file in files[0:499])
end_time = datetime.datetime.now()
print('Duration: {}'.format(end_time - start_time))
```

    ## Duration: 0:04:45.961226

Clearly, lasr is faster, however, please don’t take this as a shot at
lasio. The primary goal of this package is speed, and as such countless
hours have been put into speed testing, de-bottlenecking, and enduring
the pain of writing in C++. As with anything, there are tradeoffs, and
lasr errs toward speed where lasio tends more toward user convenience.
They are simply different products.

Good luck, and if you have any suggestions reach out!
",16,16,1,0,petroleum,"[gas, geology, geophysics, las, logs, oil, petroleum, petrophysics, well]",0.0
116,mwentzWW,petrolpy,,https://github.com/mwentzWW/petrolpy,https://api.github.com/repos/petrolpy/mwentzWW,"Open source petroleum engineering projects, useful scripts, functions, and jupyter notebooks","# petrolpy

This repository is meant to serve as an open source option for petroleum engineers and geoscientists with Python packages/modules. Check the wiki page for current projects. Please share any project ideas you may have. **All projects are a work in progress**.

[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/mwentzWW/petrolpy/master)

---

You can run the notebooks by clicking the ""launch binder"" button above. The required packages are in the environment.yaml file.

---

## Highlights

* Log Normal Distribution Probability Density Function from data (EUR Example)
![alt text](petrolpy/Examples/Example_plots/pdf_example_output.png ""PDF Plot"")
* Log Normal Distribution Probit Plot (EUR Example)
![alt text](petrolpy/Examples/Example_plots/probit_example_output.png ""Probit Plot"")
",15,15,6,0,petroleum,"[petroleum, petroleum-economics, petroleum-engineering, reservoir, reservoir-modeling]",0.0
117,derrickturk,public-oil-gas-data,,https://github.com/derrickturk/public-oil-gas-data,https://api.github.com/repos/public-oil-gas-data/derrickturk,Freely-available public oil & gas data,"# Public Oil & Gas Data
===================

This list collects sources of freely-available public data pertaining to the oil and gas industry, including data on production, completions, geology, and more.

Pull requests welcome!

[The List](https://github.com/derrickturk/public-oil-gas-data/blob/master/public-oil-gas-data.md)

Maintained by [dwt](https://github.com/derrickturk) | [terminus data science, LLC](http://www.terminusdatascience.com)
",14,14,2,0,petroleum,"[petroleum, petroleum-dataset]",0.0
118,derrickturk,pydca,,https://github.com/derrickturk/pydca,https://api.github.com/repos/pydca/derrickturk,"A ""simple"" decline-curve analysis example in Python","# pydca

A simple example of implementing Arps decline curve analysis in Python, as described [on my blog](https://usethe.computer/posts/15-minimalist-dca-in-python.html). Example data is courtesy of the [New Mexico Oil Conservation Division](http://www.emnrd.state.nm.us/OCD/).

Freely available under the Apache License, version 2.0.

### (c) 2020 [dwt](https://usethe.computer) | [terminus data science, LLC](https://terminusdata.science)
",14,14,3,0,petroleum,"[decline-curve-analysis, petroleum, petroleum-engineering]",0.0
123,derrickturk,oil-gas-mock-data,,https://github.com/derrickturk/oil-gas-mock-data,https://api.github.com/repos/oil-gas-mock-data/derrickturk,Mock data generators for oil & gas tools,"# Oil & Gas Mock Data Generators
##### by [dwt](http://www.github.com/derrickturk) | [terminus data science, LLC](http://www.terminusdatascience.com)

These generators produce ""plausible"" data for demonstrating oil and gas engineering and data science tools.
They are freely available for commercial and non-commercial use under the GNU Public License.

(c) 2016 [dwt](http://www.github.com/derrickturk) | [terminus data science, LLC](http://www.terminusdatascience.com)
",5,5,2,0,petroleum,"[petroleum, petroleum-dataset]",0.0
124,oilmap,oilmap,oilmap,https://github.com/oilmap/oilmap,https://api.github.com/repos/oilmap/oilmap,Visualizing oil data on country maps using OilMap,"# OilMap-Web 🗺🛢 🌟
Visualizing oil data on country maps using OilMap
---

### OilMap is an independent initiative to monitor the Oil & Gas industry of world promoting transparency and accountability in decision making and investment.
---
[![FOSSA Status](https://app.fossa.com/api/projects/git%2Bgithub.com%2Froqueleal%2Foilmap.svg?type=shield)](https://app.fossa.com/projects/git%2Bgithub.com%2Froqueleal%2Foilmap?ref=badge_shield)

[live demo](https://oilmap.xyz)

![screenshot](screenshot.gif)
---
## License
This project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details

[![FOSSA Status](https://app.fossa.com/api/projects/git%2Bgithub.com%2Froqueleal%2Foilmap.svg?type=large)](https://app.fossa.com/projects/git%2Bgithub.com%2Froqueleal%2Foilmap?ref=badge_large)
---
## Authors
* **Roque Leal** - *Initial work* - [Roque](https://www.roqueleal.me/)
---
## Community
- [Tiwtter](https://twitter.com/oilmapxyz)
- [Medium](https://medium.com/@roqueleal/world-oil-map-e46b774ea82b)
---

## Certifications
* **ODI** - *Open Data Certificate * - [ODI](https://certificates.theodi.org/en/datasets/220195/certificate)

---
## Acknowledgments
* **MapBox** - *Maps* - [MapBox](https://www.mapbox.com/)
* **Inspiration** - *Amnesty Oil Spills* - [MapBox Labs](https://labs.mapbox.com/amnesty/)
",3,3,3,0,petroleum,"[data-science, data-visualization, geographic-data, lease, mapbox-gl-js, mapping, maps, oil, oil-analysis, oil-exploration, oil-field-units, oil-market, petroleum, petroleum-economics, vizualize-data, world]",0.0
125,lanl-ansi,PetroleumModels.jl,lanl-ansi,https://github.com/lanl-ansi/PetroleumModels.jl,https://api.github.com/repos/PetroleumModels.jl/lanl-ansi,A Julia/JuMP Package for Petroleum Network Optimization,"# PetroleumModels.jl

<img src=""https://lanl-ansi.github.io/PetroleumModels.jl/dev/assets/logo.svg"" align=""left"" width=""200"" alt=""PetroleumModels logo"">

Release: [![](https://img.shields.io/badge/docs-stable-blue.svg)](https://lanl-ansi.github.io/PetroleumModels.jl/stable)

Dev:
[![Build Status](https://travis-ci.org/lanl-ansi/PetroleumModels.jl.svg?branch=master)](https://travis-ci.org/lanl-ansi/PetroleumModels.jl)
[![codecov](https://codecov.io/gh/lanl-ansi/PetroleumModels.jl/branch/master/graph/badge.svg)](https://codecov.io/gh/lanl-ansi/PetroleumModels.jl)
[![](https://img.shields.io/badge/docs-latest-blue.svg)](https://lanl-ansi.github.io/PetroleumModels.jl/dev)

PetroleumModels.jl is a Julia/JuMP package for Steady-State Petroleum (petroleum products) Network Optimization.
It is designed to optimize the operations of existing single liquid commodity pipeline systems subject to physical flow and pump engineering constraints. The code is engineered to decouple problem specifications from the network formulations. This enables the definition of a variety of liquid network formulations and their comparison on common problem specifications.

**Core Problem Specifications**

* Optimal Petro Flow (opf)

**Core Network Formulations**

* LP

## Basic Usage


Once PetroleumModels is installed, a optimizer is installed, and a network data file has been acquired, a Petro Flow can be executed with,
```
using PetroleumModels
using <solver_package>

run_opf(""foo.m"", FooPetroleumModel, FooSolver())
```

Similarly, an expansion optimizer can be executed with,
```
run_ne(""foo.m"", FooPetroleumModel, FooSolver())
```

where FooPetroleumModel is the implementation of the mathematical program of the Petroleum equations you plan to use (i.e. ) and FooSolver is the JuMP optimizer you want to use to solve the optimization problem (i.e. IpoptSolver).


## Acknowledgments

This code has been developed as part of the Advanced Network Science Initiative at Los Alamos National Laboratory.
The primary developer is Elena Khlebnikova, with significant contributions from Kaarthik Sundar and Russell Bent.



## License

This code is provided under a BSD license as part of the Multi-Infrastructure Control and Optimization Toolkit (MICOT) project, LA-CC-13-108.
",3,3,5,14,petroleum,"[optimization, petroleum]",0.0
126,Ali-Saud,ML-in-PE,,https://github.com/Ali-Saud/ML-in-PE,https://api.github.com/repos/ML-in-PE/Ali-Saud,"Application of Machine Learning to predict future performances, and to do sensitivity analysis in reservoir simulations, which helps to solve petroleum engineering computational problems and taking decisions in a timely manner.","# Applications of Machine Learning in Petroleum Engineering
 
#### Application of Machine Learning in petroleum engineering for predicting future performance, to help minimize the time and computational cost, in order to achieve History-Matched numerical models.

#### Data Driven analysis helps find the optimum variables instead of matching model that is highly uncertain and takes time to give results.

#### accuracy in ML models is very high and more accurate than simulation models data, beacause of Upscaling and other procedures that lead to un-representative numerical models.

##### The project is in progress for 2022 year, If you are interested and want to contribute, please contact me on email: a.saoud2000@gmail.com
",3,3,2,0,petroleum,"[learning, machine, petroleum, prediction, regression]",0.0
127,BDI-UFRGS,O3POntology,BDI-UFRGS,https://github.com/BDI-UFRGS/O3POntology,https://api.github.com/repos/O3POntology/BDI-UFRGS,A domain ontology for offshore petroleum production plants that uses BFO as a top-level ontology.,"# Offshore Petroleum Production Plant Ontology (O3PO)
A domain ontology for offshore petroleum production plants that uses BFO as a top-level ontology.

Directly imported ontologies:
  - Basic Formal Ontology (BFO)
  - GeoCore
  - IOF-Core
  - Information Artifact Ontology (IAO)
",2,2,2,1,petroleum,"[applied-ontology, bfo, ontology, petroleum]",0.0
128,derrickturk,typecurve.js,,https://github.com/derrickturk/typecurve.js,https://api.github.com/repos/typecurve.js/derrickturk,typecurve.js: some tools for decline-curve analysis and aggregation on the client side,"# typecurve.js
## Being a javascript library suitable for interactive aggregation and decline-curve analysis of production data,
## Or, the ramblings of a madman

(c) 2014 dwt | terminus data science, LLC
",2,2,2,0,petroleum,"[decline-curve-analysis, petroleum, petroleum-engineering]",0.0
129,daimessdn,las_converter,,https://github.com/daimessdn/las_converter,https://api.github.com/repos/las_converter/daimessdn,well log LAS data converter into various file,"# las_converter
## Python projects for well log analysis

[What is LAS file](#what-is-las-file) | [Project feature(s)](#project-features) | [Project dependencies](#project-dependencies) | [Project setup](#project-setup) | [Getting started](#getting-started)

---

### Some notes before using las_converter
This project development will be moved to [oilshit repository](https://github.com/oilshit/las_converter). You can check the latest version there or just use this old repository instead.

### What is LAS file

[back to top](#las_converter)

**LAS** file contains physical properties data of vertical subsurface
used in well log analysis. Well log data saved in LAS file contains
some information, including its file **version**, **well description**,
**physical rock curve** along with **data table** and **other information** related to the well data.

[back to top](#las_converter)

---

### Project feature(s)

[back to top](#las_converter)

- Load LAS data from various sources:
    - URL link (`https://example.com/.../.../path/to/lasfile.LAS`)
    - Local file (`path/to/lasfile.LAS` instead without `https`)
- Getting well log description.
- Save well log data into JSON file (as `well.json` in `results` folder).
- It also can save data into CSV file with two different outputs in `results` folder
    - `well.csv` contains well data table
    - `description.csv` contains well data legends and description

[back to top](#las_converter)

---

### Project dependencies

[back to top](#las_converter)

This project uses **Python 3** with dependencies provided in **requirements.txt**. 

[back to top](#las_converter)

---

### Project setup

[back to top](#las_converter)

Python environment setup is recommended for using this project repository. Type `./check-pyenv.sh` (using Linux/Unix terminal console or WSL console) for validating Python environments. By default, Python `virtualenv` has not been set yet so that it will be return results as below.

```sh
'env' directory is not exist.
 you can install Python virtualenv (and also activate it) by
 virtualenv env; source env/bin/activate 

 install Python dependencies then by
 pip install -r requirements.txt
```

In terminal, just type the yellow text given to proceed.

[back to top](#las_converter)

---

### Getting started

[back to top](#las_converter)

For the first time use, firstly import the external function by
```py
# import las_converter
import las_converter

# get help
help(las_converter)
```

or

```py
# import las_converter
from las_converter import WellLog

# get help
help(WellLog)
```

There is a file named `las_testing.py` used for testing purposes. There is also Jupyter Notebook file called `using_las_converter_in_well_log_analysis.ipynb` that also can be used in Jupyter console.

For testing the saved files in `results` folder, there is also Jupyter Notebook file `using_csv_made_from_las_converter_for_well_log_analysis.ipynb`.

[back to top](#las_converter)
",1,1,2,0,petroleum,"[petroleum, python, well-log]",0.0
130,softlandia,glasio,,https://github.com/softlandia/glasio,https://api.github.com/repos/glasio/softlandia,golang library for read/write/control las files ,"# golang las library #

(c) softlandia@gmail.com

>download: go get -u github.com/softlandia/glasio  
>install: go install

The library makes it easy to read or write data in LAS format.
The main reason for the development was the need to read and standardize a large number of LAS files obtained from various sources

Features:

1. The encoding is determined automatically
2. On reading perform validation of the key parameters and integrity of the structure LAS file
3. Messages are generated for all inconsistencies:
    - zero value of important parameters
    - depth step change in data section
    - lack of curves section
    - conversion errors to a numerical value
    - mismatch of the step parameter declared in the header to the actual
    - duplication of curve names
4. Excluding critical errors, the library allows you to read the file and get data
5. Saving a file ensures the integrity of the structure and its readability for most other programs
6. It is possible to specify a dictionary of standard mnemonics; when reading a file, messages about curves that do not match the specified ones will be generated
7. It is possible to specify a dictionary of automatic substitution of mnemonics, respectively, curves with the given names will be renamed

Wrapped (__WRAP__) las files are not supported

## dependences ##

- github.com/softlandia/cpd
- github.com/softlandia/xlib

## examples ##

simple

- make empty LAS file
- reads sample file ""expand_points_01.las"", write md file with messages
- saves the recovered LAS file ""expand_points_01+.las""

repaire

- reads all LAS files in current folder
- saves the recovered files to the same folder

lasin

- reads LAS file
- print warning

## tests ##

coverage 91%  
The ""data"" folder contains files for testing, no remove/change/add

## technical info ##

### how type Las store data ###

access to main parameters:  
las.VERS()  
las.WRAP()  
las.STEP()  
las.STRT()  
las.STOP()  
las.NULL()  
las.WELL()

number of points and curves:  
las.NumPoints() - number of points
len(las.Logs) - number of curves

access to curves and data:  
las.Logs[0].D[0] - first depth  
las.Logs[1].V[100] - value of first curve on 101 depth step  
las.Logs[2].Name - name of second curve  
las.Logs[2].Unit - unit of second curve  
las.Logs[2].Mnemonic - mnemonic of second curve, the value is determined if the dictionary was applied  

if las file contains duplication of any parameter, then use the first in curve
section used all curves name. The subsequent duplicated parameter is renamed.

## warnings generated when reading a LAS file ##

### warning format ###

extended:  
> x, y, ""message text""  
> x - section number  
> y - line number of input file  

",1,1,2,3,petroleum,"[data-format, data-mining, geology, golang, golang-library, golang-package, las, las-files, log-ascii-standart, petroleum, petroleum-engineering]",0.0
131,mtsousa,FER-2022-2_PetroleumReservoirSimulation,,https://github.com/mtsousa/FER-2022-2_PetroleumReservoirSimulation,https://api.github.com/repos/FER-2022-2_PetroleumReservoirSimulation/mtsousa,Simulação de reservatório de petróleo utilizando MRST,"# FER-2022-2_PetroleumReservoirSimulation

Simulação de reservatório de petróleo utilizando MRST no contexto da disciplina Fundamentos de Engenharia de Reservatório da Universidade de Brasília.

## Objetivo

O trabalho tem por objetivo criar e simular um reservatório contendo dois fluidos (água e óleo) e quatro poços (dois produtores e dois injetores) ao longo de oito anos para um determinado conjuto de propriedades (rocha, fluidos e poços).

## Características do reservatório

### Malha
- Tamanho da malha: 40x60x7;
- Dimensão: 200x300x35 metros;
- As duas camadas mais profundas saturadas com água;
- As cinco camadas superiores saturadas com óleo;
- Pressão inicial de 1 bar;

### Rocha
- Permeabilidade: x = y = 300 mD, z = 10 mD;
- Porosidade = 0,25;

### Fluidos
- Viscosidade dinâmica da água: 0,00045 Pa.s;
- Massa específica da água: 1010 kg/m^3;
- Viscosidade dinâmica da óleo: 0.001 Pa.s;
- Massa específica da óleo: 800 kg/m^3;
- Permeabilidade relativa para os dois fluidos: 1,3;

### Poços
- Produtor 1: vertical (células 1 a 12001), controlado por BHP (120 bar);
- Produtor 2: horizontal (células 40 a 840), controlado por BHP (120 bar);
- Injetor 1: horizontal (células 15961 a 16761), controlado por BHP (210 bar);
- Injetor 2: vertical (células 12000 a 16800), controlado por BHP (210 bar);

## Resultados

|  Saturação de óleo ao longo do tempo  |
|:-------------------------------------:|
|           ![](imgs/sat.gif)           |
",1,1,1,0,petroleum,"[matlab, mrst, petroleum, reservoir-engineering, reservoir-simulation]",0.0
132,derrickturk,libdca,,https://github.com/derrickturk/libdca,https://api.github.com/repos/libdca/derrickturk,Vaguely-specified project to create native library for various decline-curve analysis tasks.,"# libDCA
## a C++ (header-only) library for oil & gas decline curve analysis

To do:
* Expand test coverage
* Implement DLL/extern ""C"" interface
* Improve examples
* Improve (aka ""write"") documentation
* First public release!

license TBA open-source

(c) 2014 dwt | terminus data science, LLC
",1,1,2,2,petroleum,"[decline-curve-analysis, petroleum, petroleum-engineering]",0.0
133,oilshit,las_converter,oilshit,https://github.com/oilshit/las_converter,https://api.github.com/repos/las_converter/oilshit,LAS well log file converter to various files (JSON and CSV supported) to be readable and manageable with data science approach,"# las_converter
## Python projects for well log analysis

[What is LAS file](#what-is-las-file) | [Project feature(s)](#project-features) | [Project dependencies](#project-dependencies) | [Project setup](#project-setup) | [Getting started](#getting-started)

---

### What is LAS file

[back to top](#las_converter)

**LAS** file contains physical properties data of vertical subsurface
used in well log analysis. Well log data saved in LAS file contains
some information, including its file **version**, **well description**,
**physical rock curve** along with **data table** and **other information** related to the well data.

[back to top](#las_converter)

---

### Project feature(s)

[back to top](#las_converter)

- Load LAS data from various sources:
    - URL link (`https://example.com/.../.../path/to/lasfile.LAS`)
    - Local file (`path/to/lasfile.LAS` instead without `https`)
- Getting well log description.
- Save well log data into JSON file (as `well.json` in `results` folder).
- It also can save data into CSV file with two different outputs in `results` folder
    - `well.csv` contains well data table
    - `description.csv` contains well data legends and description

[back to top](#las_converter)

---

### Project dependencies

[back to top](#las_converter)

This project uses **Python 3** with dependencies provided in **[requirements.txt](https://github.com/oilshit/las_converter/blob/master/requirements.txt)**. 

[back to top](#las_converter)

---

### Project setup

[back to top](#las_converter)

Firstly, you need to clone this repository using this command below on Terminal (Linux or Mac) or <a href=""https://en.wikipedia.org/wiki/Windows_Subsystem_for_Linux"" target=""_blank""><abbr title=""Windows Subsystem for Linux"">WSL</abbr></a> (Windows).
```sh
git clone https://github.com/oilshit/las_converter
cd las_converter
```

Python environment setup is recommended for using this project repository. Type `./check-pyenv.sh` (using Linux/Unix terminal console or WSL console) for validating Python environments. By default, Python `virtualenv` has not been set yet so that it will be return results as below.

```sh
'env' directory is not exist.
 you can install Python virtualenv (and also activate it) by
 virtualenv env; source env/bin/activate 

 install Python dependencies then by
 pip install -r requirements.txt
```

or you can [create the environment variable manually](https://docs.python.org/3/library/venv.html) by typing command below on Linux or MacOS (and also WSL console).

```
python -m venv venv
source venv/bin/activate
```

and also for Windows.
```
python -m venv venv
venv\Scripts\activate
```

In terminal, just type the yellow text given to proceed.

[back to top](#las_converter)

---

### Getting started

[back to top](#las_converter)

For the first time use, firstly import the external function by
```py
# import las_converter
import las_converter

# get help
help(las_converter)
```

or

```py
# import las_converter
from las_converter import WellLog

# get help
help(WellLog)
```

There is a file named `las_testing.py` used for testing purposes. There is also Jupyter Notebook file called `using_las_converter_in_well_log_analysis.ipynb` that also can be used in [Google Colab console](https://colab.research.google.com/drive/1_XJR7pNvJtV66NiKFD4v60GIAjYK3f-C?usp=sharing).

For testing the saved files in `results` folder, there is also [Google Colab Notebook file](https://colab.research.google.com/drive/1a-HC1smv6r34TH6bhvemqkEOwjpyiYiV?usp=sharing) `using_csv_made_from_las_converter_for_well_log_analysis.ipynb`.

[back to top](#las_converter)
",1,1,2,0,petroleum,"[csv, json, petroleum, petrophysics, python, well-log, well-logs]",0.0
134,Marta-Barea,machine-learning_VisNIRS-Waxes-Type-Classification,,https://github.com/Marta-Barea/machine-learning_VisNIRS-Waxes-Type-Classification,https://api.github.com/repos/machine-learning_VisNIRS-Waxes-Type-Classification/Marta-Barea,Spectroscopic data processing approaches for petroleum wax discrimination,"# Rapid Classification of Petroleum Waxes: A Vis-NIR Spectroscopy and Machine Learning Approach

## Description

This repository contains the source code for all data processing and the application of machine learning algorithms used in the article ""Rapid Classification of Petroleum Waxes: A Vis-NIR Spectroscopy and Machine Learning Approach"".

## Contents

- `spectra/`: Folder containing the spectra data.
- `supervised algorithms/`: Source code for all the supervised machine learning models and experiments.
- `unsupervised algorithms/`: Source code related to unsupervised learning techniques and clustering.
- `App/`: A Shiny application to demonstrate and visualize the findings.

## Requirements

All data analysis was performed with **R (version 4.1.2)**. The software and packages used include:

- **prospectr (version 0.2.3)**: Used for calculating the first derivative for each sample spectrum with the `savitzkyGolay` function.
- **stats (version 4.1.2)**: Utilized for HCA with the `hclust` function, PCA with the `prcomp` function, and one-way ANOVA with the `aov` function.
- **cluster (version 2.1.2)**: Linkage method selection for the HCA established using the `agnes` function.
- **factoextra (version 1.0.7)**: Used for visualizing HCA results with the `fviz_dend` function and for extracting and visualizing the PCA result with the `fviz_eig` function.
- **ggplot2 (version 3.3.5)**: Employed for plotting the scores and loadings of the PCA with the `ggplot` function and generating the spectralprint radar chart.
- **caret (version 6.0-90)**: Utilized for developing the SVM and RF models.
- **graphics (version 4.1.2)**: The `filled.contour` function was used to generate the contour plot for the SVM model.
- **ggiraphExtra (version 0.3.0)**: Assisted in generating the spectralprint radar chart.
- **shiny (version 1.7.1)**: Utilized for developing the web application.

## Usage

1. Clone the repository.
2. Ensure that the necessary versions of R and packages are installed.
3. Navigate to the respective folders and run the scripts to reproduce the results or launch the Shiny app.

",1,1,1,0,petroleum,"[cheminformatics, chemistry, chemoinformatics, machine-learning, petroleum, r, vis-nir-spectroscopy]",0.0
135,lelliott-geo,Springboard_Capstone_Project1,,https://github.com/lelliott-geo/Springboard_Capstone_Project1,https://api.github.com/repos/Springboard_Capstone_Project1/lelliott-geo,"This repository houses all files related to my first Springboard Capstone.  The work is done in partnership with my Springboard mentor and friend Ajith Patnaik.  It is a synthesis of Denver-Julesburg (DJ) Basin Oil and Gas Production from 1999 through 2019.  The focus was to use the plethora of publicly available data and open source software only to identify trends and insights in that production over time.  The machine learning aspect of the project was to determine how accurately one may be able to predict abandonment of a horizontal well. Currently the data wrangling, story telling, and EDA sections are complete.  We are currently working to complete the modelling / machine learning aspects so you will so those parts are in still in progress.","# Springboard Capstone Project 1: Synthesis of Denver-Julesburg Basin Production (1999-2019): A Data Science Approach

# Introduction

The Denver-Julesburg (DJ) Basin is a mature oil and gas producing province centered in southeastern Wyoming and northeastern Colorado.   The earliest production occurred in 1901 with the drilling of the McKenzie well which led to the discovery of the Boulder Oil Field in Boulder County Colorado (Wikipedia).  The Wattenberg Field, discovered in 1970, is one of the top 10 largest oil and gas fields in the United States, according to the Energy Information Agency (Sonnenberg, 2019).  It has been revitalized in the last ten years by the success of horizontal production in the Codell and Niobrara Formations.  However, weak commodity prices and investment uncertainties over the past few years coupled with increasing costs of collecting and/or subscribing to geological, geochemical, and engineering datasets and software have put operators under increasingly difficult financial pressures.  
 
A great deal of perspective can be gained by examining the plethora of free publicly available data and by using open source software.  Such questions as the average well lifecycle span, the most productive target formations by location, the optimal lateral length (in the case of horizontal wells) and spacing, and expected ultimate recovery prediction can be ascertained.  In this paper, the authors use a 'top-down' approach based on historic well production behavior to investigate whether it is possible for a given well A and for a given point X in its life cycle, to predict the probability of its permanent abandonment within the next year. Only publicly available data sources are used, primarily data from the Colorado Oil and Gas Conservation Commission (COGCC).  This approach is in contrast to more traditional techniques using subscription and proprietary data.

Python APIs, geospatial and statistical analysis, and machine learning techniques are used to synthesize, classify, and predict horizontal and vertical well performance in the basin.  The approach employs the building of segmentation use cases, in which geo-located data points are generated that describe and classify the well's behavior.  The segments are created based on economic, geologic, and engineering criteria. These segments can then be used as either ranked categories and/or further used to synthesize, classify, and predict horizontal well performance in the basin. 

For those prospectors working the area with proprietary data available to them, insight gathered from this type of analysis can be used as a complement to traditional methods, providing an independent, higher level look at the basin to optimize development strategy and recovery enhancement.
",1,1,2,0,petroleum,"[data-analysis, data-science, data-visualization, freeware, machine-learning, open-source, petroleum]",0.0
136,oilshit,oilprice-api,oilshit,https://github.com/oilshit/oilprice-api,https://api.github.com/repos/oilprice-api/oilshit,Node.js based API for getting oil and gas prices based on oilprice.com,"# oilprice-api

[Live Preview](#live-preview) | [Project Overview](#project-overview) | [Setup Requirements](#setup-requirements) | [Project Setup](#project-setup) | [Project Implementation](#project-implementation)

## Live Preview

> This project is live on [https://oilprice-api.herokuapp.com](https://oilprice-api.herokuapp.com). Documentation is available on [https://oilprice-api.herokuapp.com/docs](https://oilprice-api.herokuapp.com/docs).

## Project Overview

This project displays basic API to get oil and gas prices based on oilprice.com.

## Setup Requirements

- Node.js (you can download it [here](https://nodejs.org))

## Project Setup

Using **Terminal** (Linux and MacOS) or **WSL console** (Windows), you can clone this repository by

```bash
git clone https://github.com/oilshit/oilprice-api.git
cd oilprice-api
```

Assuming Node.js installation has been done, you can install project dependencies by

```bash
npm install
```

or

```bash
yarn
```

After installing dependencies, you can start the server by

```bash
npm run dev
```

or

```bash
yarn run dev
```

This will starts localhost server in `http://localhost:3000` (port 3000). You can access the API for the first testing by typing `http://localhost:3000` in your browser. The documentation of API can be accessed in `http://localhost:3000/docs`.

![documentation in localhost](assets/doc1.png)

## Project Implementation

This project has been implemented on following stuffs:

- Extracting oil and gas price data into CSV file ([oilprice-extract](https://github.com/oilshit/oilprice-extract))
- more coming soon...
",1,1,1,0,petroleum,"[javascript, nodejs, oil-price, oilandgas, petroleum]",0.0
137,daimessdn,py_lopatin,,https://github.com/daimessdn/py_lopatin,https://api.github.com/repos/py_lopatin/daimessdn,Predicting oil window based on burial history with Lopatin method using Python,,1,1,2,0,petroleum,"[burial-history, geology, lopatin, miscpurposes, petroleum, python]",0.0
138,oilshit,oilprice_extract,oilshit,https://github.com/oilshit/oilprice_extract,https://api.github.com/repos/oilprice_extract/oilshit,extract oil and gas prices data based on oilprice-api into CSV file,"# oilprice-extract

[Overview](#overview) | [Requirements](#requirements) | [Project Setup](#project-setup)
| [How to use](#how-to-use)

## Overview

This project uses [oilprice-api](https://github.com/oilshit/oilprice-api) initiated by [oilshit](https://github.com/oilshit) to extract oil and gas prices data into tables.

## Requirements

- Python 3

## Project Setup

You have to start the **oilprice-api** server as documented in [oilprice-api repository](https://github.com/oilshit/oilprice-api) before using this project.

> Or you can use the live projects on https://oilprice-api.herokuapp.com/ instead.

## How to use

In your console (**Terminal** on Linux and MacOS, **WSL console** on Windows), just type:

```bash
python3 main.py <blend> <period>
```

with `blend` and `period` arguments can be search in https://oilprice-api.herokuapp.com/blend-list.

### Example

In case of getting **monthly** prices in **WTI** (West Texas Intermediate), you can simply type

```bash
python3 main.py wti montly
```

**Result**: the CSV file with filename **prices-wti-monthly-`<current-timestamp>`.csv** will be appeared once the script was run. In case of getting other prices data, you can view the blend list via `https://oilprice-api.herokuapp.com/blend-list` in browser.
",1,1,1,0,petroleum,"[csv, oil-price, oilandgas, petroleum, python]",0.0
139,MaximeGuinard,Oil-MX,,https://github.com/MaximeGuinard/Oil-MX,https://api.github.com/repos/Oil-MX/MaximeGuinard,⛽ An addon that allows you to have pretrol as a resource,,1,1,1,0,petroleum,"[addon, addons, gmod, gmod-lua, petrol, petroleum, petroleum-engineering, petrology, resources]",0.0
140,daimessdn,petro-codes,,https://github.com/daimessdn/petro-codes,https://api.github.com/repos/petro-codes/daimessdn,my codes of petroleum cases,,1,1,2,0,petroleum,[petroleum],0.0
142,oilshit,gloss-oleum,oilshit,https://github.com/oilshit/gloss-oleum,https://api.github.com/repos/gloss-oleum/oilshit,petroleum engineering glossary finder,"# gloss-oleum
## glossary collections for petroleum

GLOSS-oleum collects lists of oil and gas definitions in many references.
",0,0,2,2,petroleum,"[glossary, glossary-finder, petroleum]",0.0
143,oilshit,lifecycle,oilshit,https://github.com/oilshit/lifecycle,https://api.github.com/repos/lifecycle/oilshit,"one app for petroleum ""everything"" purposes (coming soon)","# lifecycle

Coming soon supporting your ""every"" oil and gas lifecycle stuffs.

![coming soon](banner.gif)
",0,0,1,0,petroleum,[petroleum],0.0
144,Tbarros1996,_petroleo_br,,https://github.com/Tbarros1996/_petroleo_br,https://api.github.com/repos/_petroleo_br/Tbarros1996,Análise de Produção de Petróleo e Combustivel no Brasil,"<img align=""right"" width=""30%"" src=""https://upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Oil_platform_P-51_%28Brazil%29-2.jpg/1280px-Oil_platform_P-51_%28Brazil%29-2.jpg"">

![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)
![R](https://img.shields.io/badge/r-%23276DC3.svg?style=for-the-badge&logo=r&logoColor=white)
![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=for-the-badge&logo=numpy&logoColor=white)
![Matplotlib](https://img.shields.io/badge/Matplotlib-%23ffffff.svg?style=for-the-badge&logo=Matplotlib&logoColor=black)
![Microsoft Excel](https://img.shields.io/badge/Microsoft_Excel-217346?style=for-the-badge&logo=microsoft-excel&logoColor=white)
![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=for-the-badge&logo=pandas&logoColor=white)
[![Kaggle](https://img.shields.io/badge/Kaggle-035a7d?style=for-the-badge&logo=kaggle&logoColor=white)](https://www.kaggle.com/tbarros1996)

## Computação Científica Para Engenharia - Análise de Dados da Produção de Petróleo no Brasil no Período de 1954 a 2022

Projeto de análise dos dados disponibilizados referente a produção de petróleo e gás natural na costa brasileira entre 1954 a 2022 com os dados disponibilizados publicamente pela plataforma Dados Abertos do Governo Federal pela Agência Nacional do Petróleo, Gás Natural e Biocombustíveis - ANP. Esse projeto tenta mapear e fazer projeções referentes a série histórica utilizando as ferramentas computacionais utilizadas frequentemente por mim: Python, R e Excel. A base de dados pode mapeada e adquirida nos links abaixo.


## Links Recomendados

-[Estatística Aplicada e Probabilidade para Engenheiros do Douglas C. Montgomery](https://www.amazon.com.br/Estat%C3%ADstica-Aplicada-Probabilidade-para-Engenheiros-ebook/dp/B08VDXT358/ref=sr_1_1?__mk_pt_BR=%C3%85M%C3%85%C5%BD%C3%95%C3%91&crid=1E5G60O0FZFZX&keywords=Estat%C3%ADstica+Aplicada+%C3%A0+Engenharia&qid=1674394476&s=books&sprefix=estat%C3%ADstica+aplicada+%C3%A0+engenharia+%2Cstripbooks%2C191&sr=1-1)

-[Datasets: Agência Nacional do Petróleo, Gás Natural e Biocombustíveis - ANP](https://dados.gov.br/dados/conjuntos-dados/dados-historicos-com-producao-de-petroleo-e-gas-natural-terra-e-mar)




",0,0,1,0,petroleum,"[data-analysis, mechanical-engineering, petroleum, python]",0.0
145,daimessdn,petroleum_notes,,https://github.com/daimessdn/petroleum_notes,https://api.github.com/repos/petroleum_notes/daimessdn,my petroleum note from semester 6,,0,0,2,0,petroleum,"[college, petroleum]",0.0
146,underr,eduardo,,https://github.com/underr/eduardo,https://api.github.com/repos/eduardo/underr,Interface em Python para o Robô Ed - rip :-(,"# eduardo

#### Interface em Python para o [Robô Ed](http://www.ed.conpet.gov.br/br/converse.php)

## Uso

```
from eduardo import Ed

ed = Ed()

print('Pressione <ctrl> + C para sair')

while True:
    msg = input('Você: ')
    texto = ed.name + ': ' + ed.say(msg)
    print(texto)
```

Vide o script `dialogo.py` para um exemplo mais elaborado.

## Parâmetros

**name**: nome do robô (Eduardo por padrão)

**port**: necessária para a interação entre robôs distintos (8085 por padrão)

**server**: ""servidor"" de onde partirão os requests (0.0.0.0 por padrão)

**url**: url do servidor (URL do Robô Ed por padrão)
",0,0,2,0,petroleum,"[ambiental, artificial-intelligence, education, petroleum]",0.0
147,Marta-Barea,machine-learning_HSGCMS_ParaffinWax_Odor,,https://github.com/Marta-Barea/machine-learning_HSGCMS_ParaffinWax_Odor,https://api.github.com/repos/machine-learning_HSGCMS_ParaffinWax_Odor/Marta-Barea,Data processing approaches for paraffin wax odor discrimination and quantification,"# machine-learning_HSGCMS_ParaffinWax_Odor
 
",0,0,1,0,petroleum,"[chemistry, chemoinformatics, chemometrics, gc-ms, gc-ms-data, machine-learning, machine-learning-algorithms, petroleum, petroleum-science, r]",0.0
148,WilsonKinyua,dalbitpetroleum.com,,https://github.com/WilsonKinyua/dalbitpetroleum.com,https://api.github.com/repos/dalbitpetroleum.com/WilsonKinyua,"Dalbit is a member of JCG Holdings Ltd. Established in 2002 in Kenya, where they supply and distribute petroleum products and services across East, Central and Southern Africa.","## Old site UI

<p align=""center""><a href=""https://laravel.com"" target=""_blank""><img src=""https://raw.githubusercontent.com/laravel/art/master/logo-lockup/5%20SVG/2%20CMYK/1%20Full%20Color/laravel-logolockup-cmyk-red.svg"" width=""400""></a></p>

<p align=""center"">
<a href=""https://travis-ci.org/laravel/framework""><img src=""https://travis-ci.org/laravel/framework.svg"" alt=""Build Status""></a>
<a href=""https://packagist.org/packages/laravel/framework""><img src=""https://img.shields.io/packagist/dt/laravel/framework"" alt=""Total Downloads""></a>
<a href=""https://packagist.org/packages/laravel/framework""><img src=""https://img.shields.io/packagist/v/laravel/framework"" alt=""Latest Stable Version""></a>
<a href=""https://packagist.org/packages/laravel/framework""><img src=""https://img.shields.io/packagist/l/laravel/framework"" alt=""License""></a>
</p>
",0,0,1,0,petroleum,"[backup, cms, cms-backend, cooperative, laravel, old-project, petroleum]",0.0
149,eu-cristofer,vba-pump-performance,,https://github.com/eu-cristofer/vba-pump-performance,https://api.github.com/repos/vba-pump-performance/eu-cristofer,VBA Pump Performance is a set of tools to help engineers and technicians to asses the condition of an API 610 pump during its Performance and Mechanical Running Test trials.,"# VBA Pump Performance

VBA Pump Performance is a set of tools to help engineers and technicians to asses the condition of API 610 pumps during its Performance and Mechanical Running Test trials.

# Resources
* Performance computation
* Operational curves generation
* Speed correction
* Relative density correction
* Viscous effects correction
* Report issue automation

## Installation in Microsoft Excel

1. Go to the Developer tab.
2. Click the Add-ins Button.
3. Inside the Add-ins Dialog Box, click the Browse… ...
4. The Explorer Window should default to the Microsoft add-in folder location.
5. Navigate and select the file `VBA-Pump-Performance.xlam`, then click OK.

### Sample File

You can try the functionalities with the file `SampleData.xlsx`.

## Technical specifications

The solution was written in VBA and built in a Microsoft Office Add-in.

The performance acceptance criteria adopted in this project is the API 610 11th edition, Centrifugal pumps for petroleum, petrochemical and natural gas industries.

The computation of test parameters follows the ANSI/HI 14.6 Rotodynamic Pumps - Hydraulic Performance Acceptance Tests.

The computation of viscosity correction is accordingly the ANSI/HI 9.6 Rotodynamic Pumps -  Guideline for Effects of Liquid Viscosity on Performance.

## Special notes

The author of the code does not make any warranty or representation, either express or implied, with respect to the accuracy, completeness, or usefulness of the results contained herein, or assume any liability or responsibility for any use, or the results of such use, of any information or process disclosed in this software.
",0,0,1,0,petroleum,"[performance, petroleum, pumps]",0.0
151,hjfitz,spe-site,,https://github.com/hjfitz/spe-site,https://api.github.com/repos/spe-site/hjfitz,Website for the UoP's Petroleum engineering society,"# SPE Site
Built in 2017 for the UoP's Petroleum Engineers

Content driven via [Contentful](https://contentful.com)

## Running
Ensure that your `.env` is configured correctly and `yarn install`. Run locally with `heroku local`.
",0,0,1,2,petroleum,"[engineering, express, node, petroleum, portsmouth, society, university, website]",0.0
152,daimessdn,gloss-oleum,,https://github.com/daimessdn/gloss-oleum,https://api.github.com/repos/gloss-oleum/daimessdn,glossary finder of petroleum engineering,"# gloss-oleum
## glossary collections for petroleum

GLOSS-oleum collects lists of oil and gas definitions in many references.
",0,0,2,3,petroleum,"[glossary-finder, petroleum]",0.0
153,amirhossein92,petroleum.dictionary,,https://github.com/amirhossein92/petroleum.dictionary,https://api.github.com/repos/petroleum.dictionary/amirhossein92,Petroleum technical dictionary,"# ترجمه فارسی لغات مربوط به رشته مهندسی نفت

کاربرد:
نوشتن مقالات، پایان نامه یا پروژه های فارسی
",0,0,2,0,petroleum,"[dictionary, persian, petroleum, petroleum-engineering]",0.0
154,Marta-Barea,machine-learning_VisNIRS-Waxes-Type-mixture-Regression,,https://github.com/Marta-Barea/machine-learning_VisNIRS-Waxes-Type-mixture-Regression,https://api.github.com/repos/machine-learning_VisNIRS-Waxes-Type-mixture-Regression/Marta-Barea,Spectroscopic data processing approaches for petroleum wax blends quantification,# machine-learning_VisNIRS-Waxes-Type-mixture-Regression,0,0,1,0,petroleum,"[chemistry, chemoinformatics, chemometrics, machine-learning, petroleum, r]",0.0
155,TankerTrackers,convert,TankerTrackers,https://github.com/TankerTrackers/convert,https://api.github.com/repos/convert/TankerTrackers,Convert freely between various types of petroleum-related fluid measurements,"# Convert

This package is mainly concerned with converting between the following three units of measurement:

- [API Gravity](https://en.wikipedia.org/wiki/API_gravity)
- [Barrels per Tonne](https://en.wikipedia.org/wiki/Barrel_(unit))
- [Specific Gravity](https://en.wikipedia.org/wiki/Relative_density), AKA ""Relative Density""

It was developed to be compatible with PHP 8.0 and greater and is intentionally as light-weight as possible, not requiring any external dependencies or 
relying on any non-standard PHP libraries.

> Please note that future versions may require PHP 8.1 and greater due to the built-in support for Enums that it offers. A new Major release of this library 
> will be released if PHP version dependencies change. At the moment, no significant changes are planned that would require this package to require PHP 8.2 
> or greater. 

# Installation

```sh
composer require tankertrackers/convert
```

## Usage

If you are doing calculations on an oil grade with a specific gravity (relative density) of 0.983, you can create an `Gravity` object by calling on the 
appropriate static method on the `TankerTrackers\Convert` class like so: `Convert::gravity(0.983)`. If you wanted to see the corresponding API value for 
this Gravity value, you can instead call `Convert::gravity(0.983)->toApi()`.

Converting all three measurements between each other is done in the same way, and all three methods - `->toApi()`, `->toBpt()`, and `->toGravity()` - are 
available on all three measurements.

```php
>>> TankerTrackers\Convert::api(34.12)->toBpt();
=> TankerTrackers\FluidMeasures\BarrelsPerTonne {#2684
     +value: 7.3613796475321,
   }

>>> TankerTrackers\Convert::bpt(6.59)->toGravity()
=> TankerTrackers\FluidMeasures\Gravity {#2686
     +value: 0.95437149864956,
   }
   
>>> TankerTrackers\Convert::gravity(1.19)->toApi()
=> TankerTrackers\FluidMeasures\ApiGrade {#2689
     +value: 5.897726515101,
   }

// You can also juggle types back and forth as much as you want.
>>> TankerTrackers\Convert::api(30)->toGravity()->toBpt()->toApi()->toBpt()->toGravity()->toApi()
=> TankerTrackers\FluidMeasures\ApiGrade {#2682
     +value: 30.0,
   }
```

> Note that the `$value` attribute is always returned as a `float`; even if the value is `1.0` it is not cast to an `int`.  

You can also access the values directly by calling `->apiValue()`, `->bptValue()` or `->gravityValue()`. This gives the same value as converting to that 
measurement type and accessing the `$value` attribute:

```php
>>> TankerTrackers\Convert::api(31)->toGravity()->value
=> 0.87076923076923

>>> TankerTrackers\Convert::api(31)->gravityValue()
=> 0.87076923076923
```

## Future Development

### Api Grade Categories

In the future, this package may see some additional bells and whistles when it comes to analyzing the values of the various measurements. For example, the 
`ApiGrade` class might offer a `->getCategory()` method so that something like this is possible:

```php
>>> $api = Convert::bpt(7.12)->toApi()
>>> $api->getCategory()
=> ""Medium""
```

### Pre-defined Listings

I might also consider adding a number of well known oil grades via `Enum` classes so that one can reference values directly, something like:

```php
>>> \TankerTrackers\Common\Grade::ATHABASCA_BITUMEN->getApi()
=> 31.0
>>> \TankerTrackers\Common\Grade::ATHABASCA_BITUMEN->getGravity()
=> 0.87076923076923
```

### Improvements to Value Checking

At the moment, the system trusts that the values you are entering are valid for that category, so it has no reason to suspect anything is weird if you ask 
it to create a `BarrelsPerTonne` object with the value of `-718`. Some sanity checks could be implemented that catch scenarios like this and throw 
Exceptions when we know something is out of the ordinary.

# Copyright / License

This library is released under an MIT License. See the `LICENSE` file for further details.
",0,0,2,0,petroleum,"[conversion, petroleum]",0.0
156,oilshit,IPyR,oilshit,https://github.com/oilshit/IPyR,https://api.github.com/repos/IPyR/oilshit,Calculation of Inflow Performance Relationship (IPR) using Python,"# IPyR

## Overview
This project descibes how to obtain **max flow rate** (defined as `q_max: NUMERIC (float | int)`) using several equations.

## Language Features
Since this repository was created, **Python 3.8.10** is used in this project.

## Program Features
- Calculate the `q_max` based on relations of variables
    - Reservoir pressure (`p_res: NUMERIC (float | int)`)
    - Wellbore pressure (`p: NUMERIC (float | int)`)<sup>[1]</sup>
    - Flow rate at current wellbore pressure (`q: NUMERIC (float | int)`)<sup>[1]</sup>
- Illustrate extensions of production data for graphic and charting purposes
    
> <sup>[1]</sup> Notice that `p` and `q` are combined in single dictionary defined as `Dict[NUMERIC (float | int), NUMERIC (float | int)]`. For instance, for single data of `q` and `p`, it will be defined as `data = { ""p"": NUMERIC, ""q"": NUMERIC }`.

## Testing Files
### Two-phase production
#### Vogel
- `vogel.py`: Testing for calculation of `q_max` in several wellbore pressures using **Vogel Equation**
- `graph-vogel-1.py`: Demonstrating graph correlation of wellbore pressures between flow rates based on **Vogel Equation**
- `graph-vogel-2.py`: Demonstrating graph of wellbore pressure (in this case, using `p_wf` = 1335 psia) comparing with real production data **Vogel Equation**

#### Fetkovich
- `fetkovitch.py`: Testing for calculation of `q_max` in several wellbore pressures using **Fetkocivh Equation**
- `graph-fetkovich-1.py`: Demonstrating graph correlation of wellbore pressures between flow rates based on  **Fetkovich Equation**

#### Production graph comparison
- `graph-2-phase-comparison.py`: Compare of production graph between **Vogel** and **Fetkovich** methods.

### Three-phase production
#### Wiggin
- `wiggin.py`: Demonstrating graph correlation of wellbore pressures between flow rates based on **Wiggin Equation**
- `graph-wiggin-1.py`: Testing for calculation of `q_max` of oil and water (in this case, using `p_wf` = 1335 psia) using **Wiggin Equation**",0,0,1,0,petroleum,"[petroleum, production-engineering, python]",0.0
158,JustinGOSSES,predictatops,,https://github.com/JustinGOSSES/predictatops,https://api.github.com/repos/predictatops/JustinGOSSES,Stratigraphic pick prediction via supervised machine-learning,"# predictatops

_Code for stratigraphic pick prediction via supervised machine-learning_


<a title=""Triceratops logo based on MathKnight based on photo by Nicholas R. Longrich and Daniel J. Field [CC BY-SA 4.0 (https://creativecommons.org/licenses/by-sa/4.0)], via Wikimedia Commons"" href=""https://commons.wikimedia.org/wiki/File:Yale-Peabody-Triceratops-004Trp.png""><img width=""512"" alt=""Yale-Peabody-Triceratops-004Trp"" src=""docs/Yale-Peabody-Triceratops-004Trp.png""></a>


[![DOI](https://zenodo.org/badge/151658252.svg)](https://zenodo.org/badge/latestdoi/151658252)

<a href=""https://github.com/JustinGOSSES/predictatops/blob/master/LICENSE"">![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)</a>

## THIS REPOSITORY HAS BEEN ARCHIVED TO SIGNIFY THERE WILL NOT BE ADDITIONAL WORK. HOWEVER, IT IS HAS ALWAYS BEEN A PROOF OF CONCEPT OF AN APPROACH RATHER THAN A TOOL SO YOUR USE OF IT SHOULD NOT REALLY CHANGE. YOU CAN STILL STAR OR FORK IT.

<b>Status</b>: Runs and ready for others to try. This code project is most useful as a working proof-of-concept. It is not optimized to be used in a plug-n-play or as a dependency. Updated to v0.0.4-alpha October 26th, 2019. Updates to dependencies are done but not frequently.
<i>NOTE: Running in a standard google colab notebook may fail during model training due to memory requirements exceeding the default initial amount of RAM.</i>

#### Current best RMSE on Top McMurray surface is 6.6 meters.

Related Content
-------
The <b><a href=""https://justingosses.github.io/predictatops/html/index.html"">docs</a></b> provide information additional to this README.

This code is the subject of an <b><a href=""https://github.com/JustinGOSSES/predictatops/blob/master/AAPG_Abstract_2019ACE.md"">abstract</a></b> submitted to the AAPG ACE convention in 2019. 

The <b><a href=""https://github.com/JustinGOSSES/predictatops/blob/master/docs/ACE2019_Gosses_theme8_StratigraphicTopML_201905018_submitted.pdf"">slides</a></b> I presented at AAPG ACE 2019 are available in PDF form. They give an introduction to the theory and thought process behind Predictatops.

Development was in this repo: <a href=""https://github.com/JustinGOSSES/MannvilleGroup_Strat_Hackathon"">MannvilleGroup_Strat_Hackathon</a> but is now moving here as the code gets cleaned and modulized. This project is under active development. A few portions of the code still only exist on MannvilleGroup_Strat_Hackathon repo at this time. This is a nights and weekend side project, but will continue to be developed by the main developer.

A more non-coder friendly description of the work can be found in <a href=""http://justingosses.com/predictatops/"">this</a> blog post. 

Philosophy
-------

In human-generated stratigraphic correlations there is often talk of lithostratigraphy vs. chronostratigraphy. We propose there is a weak analogy between lithostratigraphy and chronostratigraphy and the different methods of computer assisted stratigraphy. Some of the past efforts, which work very well under certain circumstances, are similar to lithostratigraphy in terms of what they accomplish. They match curve patterns between neighboring wells and rely on the assumption that changes in lithology ~ curve shapes are equivelant to stratigraphy.

Other papers attempt to use code to correlate well logs assuming there was a mathematical or pattern basis for stratigraphic surfaces that can be teased out of individual logs. Although there are recent papers that seem to do better with this type of approach, no code was released, the earlier ones seem to have problems that at least in part were related to their assumption that stratigraphic changes had similar expression across large spatial areas.

In contrast to lithostratigraphy, chronostratigraphy assumes lithology equates to facies belts that can fluctuate gradually in space over time, and are not correlated with time. Two wells with similar lithology patterns can be in different time packages. Traditional chronostratigraphy relies on models of how facies belts should change in space when not otherwise constrained by biostratigraphy, chemostratigraphy, or radiometric dating. 

Instead of relying on stratigraphic models, this project proposes known picks can define spatial distribution of, and variance of, well log curve patterns that are then used to predict picks in new wells. This project attempts to focus on creating programatic features and operations that mimic the low level observations of a human geologist and progressively build into higher order clustering of patterns occuring across many wells that would have been done by a human geologist.

Datasets
-------
The default demo dataset used is a collection of over 2000 wells made public by the Alberta Geological Survey's Alberta Energy Regulator. To quote their webpage, ""In 1986, Alberta Geological Survey began a project to map the McMurray Formation and the overlying Wabiskaw Member of the Clearwater Formation in the Athabasca Oil Sands Area. The data that accompany this report are one of the most significant products of the project and will hopefully facilitate future development of the oil sands."" It includes well log curves as LAS files and tops in txt files and xls files. There is a word doc and a text file that describes the files and associated metadata. 

_Wynne, D.A., Attalla, M., Berezniuk, T., Brulotte, M., Cotterill, D.K., Strobl, R. and Wightman, D. (1995): Athabasca Oil Sands data McMurray/Wabiskaw oil sands deposit - electronic data; Alberta Research Council, ARC/AGS Special Report 6._

Please go to the links below for more information and the dataset:

Report for Athabasca Oil Sands Data McMurray/Wabiskaw Oil Sands Deposit http://ags.aer.ca/document/OFR/OFR_1994_14.PDF

Electronic data for Athabasca Oil Sands Data McMurray/Wabiskaw Oil Sands Deposit http://ags.aer.ca/publications/SPE_006.html Data is also in the repo folder: SPE_006_originalData of the original repo for this project <a href=""https://github.com/JustinGOSSES/MannvilleGroup_Strat_Hackathon/tree/master/SPE_006_originalData"">here.</a>

In the metadata file <a href=""https://github.com/JustinGOSSES/MannvilleGroup_Strat_Hackathon/blob/master/SPE_006_originalData/Metadata/SPE_006.txt"">SPE_006.txt</a> the dataset is described as `Access Constraints: Public` and `Use Constraints: Credit to originator/source required. Commercial reproduction not allowed.`

_The Latitude and longitude of the wells is not in the original dataset._ <a href=""https://github.com/dalide"">@dalide<a> used the Alberta Geological Society's UWI conversion tool to find lat/longs for each of the well UWIs. A CSV with the coordinates of each well's location can be found <a href=""https://github.com/JustinGOSSES/MannvilleGroup_Strat_Hackathon/blob/master/well_lat_lng.csv"">here.</a> These were then used to find each well's nearest neighbors.

Please note that there are a few misformed .LAS files in the full dataset, so the code in this repository skips those.

If for some reason the well data is not found at the links above, you should be able to find it <a href=""https://github.com/JustinGOSSES/MannvilleGroup_Strat_Hackathon/tree/master/SPE_006_originalData"">here.</a>


Architecture and Abstraction
-------
PLEASE REFER TO THE SECTION <a href=""https://justingosses.github.io/predictatops/html/readme.html#architecture-and-abstraction"">Architecture and Abstraction</a> in the DOCs. Information is provided on code architecture, tasks, and folder organization.


GettingStarted
-------
See the <a href=""https://justingosses.github.io/predictatops/html/usage.html"">Usage</a> and the <a href=""https://justingosses.github.io/predictatops/html/installation.html"">Installation</a> sections of the docs.


Credits
-------
There's a theme here. Check the <a href=""https://justingosses.github.io/predictatops/html/authors.html"">docs</a>.

______________________________
## Status

The root mean squared error for the Top McMurray surface is down to ~7 meters (with a handful of wells identified as too difficult to predict, -8% depending on settings). 

#### Distribution of Absolute Error in Test Portion of Dataset for Top McMurray Surface in Meters. 
Y-axis is number of picks in each bin, and X-axis is distance predicted pick is off from human-generated pick.
<img src=""docs/images/Histogram_Error_predictatops_6.6_vA.png""
     alt=""image of current_errors_TopMcMr_20190517""
     style=""float: left; margin-right: 25px;"" />

Current algorithm used is XGBoost.
",55,55,9,29,well-logs,"[athabasca, athabasca-preprocessed, dataunderground, geology, geoscience, hackathon-project, machine-learning, stratigraphy, well-logs]",0.0
159,JustinGOSSES,wellioviz,,https://github.com/JustinGOSSES/wellioviz,https://api.github.com/repos/wellioviz/JustinGOSSES,d3.js v5 visualization of well logs ,"# wellio_Viz.js

WELLIOVIZ is a JavaScript library that provides functionality to visualize well logs, particularly those already converted to JSON, using the d3.js visualization library.

[![NPM](https://nodei.co/npm/wellioviz.png?compact=true)](https://npmjs.org/package/wellioviz)

<a href=""https://observablehq.com/@justingosses/well-log-in-d3-js-v5-notebook-2""><img src=""docs/images/well_log_screenshot_new.png""></a>


## Demos & Examples
#### Full Websites
1. HTML demo as github pages page in this repository
    - Status: Up to Date (almost always). 
    - Link: Basic webpage demo here: https://justingosses.github.io/wellioviz/demo.html Note that the user interface on this page is part of the demo but not the only way to use wellioviz as wellioviz is just the visualization capability.
    - Who for: People who want to see it work with a well log provided by the website, so they don't have to provide one, or people who went to load their own well log file into the browser from their local computer. All data stays in the browser window, nothing leaves your browser.

#### Observable Demos
1. Hello Wellioviz Mini Demo in ObservableHQ.com
    - Link: https://observablehq.com/@justingosses/hello-wellioviz
    - Status: Up to date.
    - Who for: People who wnat to see what it does quickly. Litterally the hello world version. Loads well that exists at an URL.
2. Demo that consumes well logs directly from Government Open Data sites, so you don't have to download files locally
    - Link: https://observablehq.com/@justingosses/a-notebook-using-wellio-js-wellioviz-js-for-quick-looks-of-la
    - Status: Up to date
    - Who for: People who want to check out well logs that exist on a government site without having to download the file and work with it locally. Requires each well log to be in LAS 2.0 file format and downloadable from a specific URL.
3. Demo in Observable of Most Recent Published Code with 3 Examples:
    - Link: <a href=""https://observablehq.com/@justingosses/first-wellio-example-with-all-wellioviz-functions-from-npm"">https://observablehq.com/@justingosses/first-wellio-example-with-all-wellioviz-functions-from-npm</a>
    - Status: Code is pulled from NPM, so this reflects the latest code that has been published to NPM. May lag slightly compared to the code on this github repository in master branch. 
    - Who for: Good demo to check out if you want to see wellioviz work with both well log data in wellio.js style JSON and sparse style JSON. Uses whatever most recent version of wellioviz and wellio published to NPM.
4. Observable Notebook Where Code is Developed. Has functions written-out inline. 
    - Status: Stale. Development has shifted to local development with UI using demo.html found in docs folder. 
    - Link: <a href=""https://observablehq.com/@justingosses/well-log-in-d3-js-v5-notebook-2"">here</a>
    - Who for: If you want to see some of the actual functions in a setting where all changes are live, with the understanding that this code is old and some functions might have changed,this is a good one to check out as it is decidedly less ""magical"". Brings in well log that exists at an URL.


#### Alterative Observable-notebook-based demos out of date
- Status: Way out of date! but usefull to check out for illustrative purposes maybe.
- Link: Other older Observable notebooks that use older versions of wellioviz code but give further idea what is possible are: 
- https://observablehq.com/@justingosses/well-log-in-d3-js-v5
- https://observablehq.com/@justingosses/overly-simplified-stratigraphic-modeling
- https://observablehq.com/@justingosses/well-log-curve-cross-sections

#### Data in the demos
Data for the demos is mostly wells from an open source dataset of the <a href=""https://ags.aer.ca/publication/spe-006"">Mannville Group from Alberta Canada</a>. A preprocessed version of this dataset can be found on <a href=""https://dataunderground.org/dataset/athabasca-preprocessed"">dataunderground</a> along with many other well dataset.


## DOCS
Docs are here: https://justingosses.github.io/wellioviz/  

## Architecture 
<a href=""images/wellioviz_architecture.png""><img src=""docs/images/wellioviz_architecture.png""></a>

For more information on how wellioviz is organized, check out the <a href=""docs/ARCHITECTURE.MD"">ARCHITECTURE.MD</a> document. 


## Why?

Most geologists who make charts of well logs via code seem to do so in Python, often working in Jupyter notebooks. This is fine for a lot of things, but there are some use-cases where having a JavaScript visualization library makes for a better option.

1. <b>GUIs for well correlation.</b> Currently, there really isn't a free open-source application for well log correlation, at least to the best of my knowledge. This means if you don't have Enterprise scale money, (academics, non-profits, hobbyest, students, people in between jobs, etc.) you're often limited to correlating a handfull of wells at most using paper and pencil. Wellio (something to convert LAS files to JSON) and Wellioviz (something to visualize well logs as JSON into SVGs on a webpage) are critical pieces for a free non-code web-based well log visualization to exist. Although a full GUI (graphic user interface) is slightly out of scope for wellioviz, wellioviz could be extended to do this. 

2. <b>Websites that provide well logs to audiences on the web.</b> Although PNG images of well logs could be created and stored on server before being sent on demand to the front-end, this is less than ideal as all the images have to be created, stored, and loaded before the user needs them. This takes a lot of storage space. Additionally, it prevents scrolling, zooming, and overlays. Visualizing the logs on the fly in JavaScript is a better option for websites that want to given users an idea what a well log contains.

3. <b>Interactive well plotting with export into SVG or full HTML Pages from inside Jupyter Notebook</b>  Building the visualization in JavaScript also opens up the possibility of working in a Jupyter notebook and exporting wells as SVGs or full HTML webpages. Additionally, there aren't any limits on interactivity that you might hit in a python visualization package that is wrapping JavaScript behind the scenes. Kepler.gl in Jupyter is an example of this functionality where beautiful maps get created inside Jupyter notebook and get published as self-contained front-end only full HTML/CS/JS pages.

<i>Point of Caution: To some of extent, all of these are not completely in scope of the project. Wellioviz is a visualization library. 1,2,&3 also require graphic user interfaces and other things that are better to be built as projects that utilize wellioviz.</i>


## Contributing
Check out the Contributing <a href=""https://justingosses.github.io/wellioviz/#contributing"">Guidelines</a>. Issues, documentation, pull requests, examples, test cases, and questions needed!

I also have a description in Contributing.md of how I tend to work on the project and what things to expect will always be in sync in case there are questions on that.

#### Organization of Issues on Kanban board here: https://github.com/JustinGOSSES/wellioviz/projects/1

## Code of Conduct
<a href=""CODE_OF_CONDUCT.md"">Code of Conduct</a>


#### Further Thinking...

<a href=""docs/BRAINSTORMS.md"">BRAINSTORM.md</a>
<a href=""docs/audiences.md"">AUDIENCES.md</a>

",43,43,8,45,well-logs,"[athabasca, athabasca-preprocessed, d3, dataunderground, geology, javascript, swung-t20, tool, visualization, well-logs, welllogs]",0.0
161,andymcdgeo,las_explorer,,https://github.com/andymcdgeo/las_explorer,https://api.github.com/repos/las_explorer/andymcdgeo,LAS Explorer is a Streamlit web app that allows you to understand the contents of a LAS file. Also includes the ability to identify missing data intervals.,,35,35,4,3,well-logs,"[las, las-files, petrophysics, well-logs]",0.0
162,vkazei,deeplogs,,https://github.com/vkazei/deeplogs,https://api.github.com/repos/deeplogs/vkazei,Velocity model building by deep learning. Multi-CMP gathers are mapped into velocity logs.,"# deeplogs
Velocity model building by deep learning. Multi-CMP gathers are mapped into velocity logs.

This repository reproduces the results of the papers: 

Kazei, V., Ovcharenko, O., Plotnitskii, P., Zhang, X., Peter, D. & Alkhalifah, T.
**""Mapping full seismic waveforms to vertical velocity profiles by deep learning""**,
Geophysics, in moderate revision (2020)
[https://repository.kaust.edu.sa/handle/10754/656082]

Kazei, V., Ovcharenko, O., Plotnitskii, P., Zhang, X., Peter, D. & Alkhalifah, T.
**""Deep learning tomography by mapping full seismic waveforms to vertical velocity profiles""**,
EAGE Annual meeting, 2020
Run:

    data/velocity_logs_from_seismic.ipynb

Common-midpoint gathers are used to build a velocity log at the central midpoint location. 
This allows us to utilize relevant traces for inversion and exploit the regualrity of sampling in typical active seismic acquisition.

With deep learning and regularly sampled data inversion can be set up as a search for mapping from data cubes to 1D vertical velocity profiles. Which is a lot easier to learn compared to mapping to the whole velocity models (2D or 3D).
<p float=""left"">
  <img src=""latex/Fig/relevantCMP.png"" width=""400"" />
  <img src=""latex/Fig/in_out_shape.png"" width=""400"" /> 
</p>

![cmp_to_log](latex/Fig/architecture.png)

We generate a set of pseudo-random models for training by cropping and skewing:
![cmp_to_log](latex/Fig/random_models.png)

Velocity model is then retrieved as an assembly of depth profiles. Deep learning models are naturally stochastic, so we train as set of five to provide initial uncertainty estimates:
![cmp_to_log](latex/Fig/inverted_models_EAGE2020.png)

New training data is generated on-the-fly to save space and boost generalization power.
Full-waveform inversion can help refine the results.
![cmp_to_log](latex/Fig/dynamic_overthrust.png)







",31,31,3,0,well-logs,"[deep-learning, deep-neural-networks, full-waveform-inversion, seismic-imaging, seismic-inversion, well-logs]",0.0
163,JustinGOSSES,wellio.js,,https://github.com/JustinGOSSES/wellio.js,https://api.github.com/repos/wellio.js/JustinGOSSES,JavaScript for converting well-log standard .las file format to json format,"# wellio.js
#### JavaScript for converting well-log standard .las file format to json format and then back again.

[![DOI](https://zenodo.org/badge/116549236.svg)](https://zenodo.org/badge/latestdoi/116549236)

[![NPM](https://nodei.co/npm/wellio.png?compact=true)](https://npmjs.org/package/wellio)

## Purpose
<b> Wellio.js is a JavaScript library for converting a LAS 2.0 well log file into a wellio style JSON. It was created to enable well logs to be easily visualized on the web.s</b>s

##### You might be wondering...Why Bother? Geologists Use Python.
Although Python is great for data analysis, and it is the language most learned by geologists, it isn't great for building user interfaces that live on the web. If you want to enable that functionality, you need to get well logs into a format JavaScript visualization libraries can consume and that's JSON.

Further explanation on why create wellio is given in the <a href=""https://justingosses.github.io/wellio.js/docs/"">docs</a>.

## Demos

#### 1. Github pages demo <a href=""https://justingosses.github.io/wellio.js/"">page</a>: 
Open the demo page running on github pages. Click one of the big blue buttons up top to  open a file loader. You can either use a LAS files already part of the webpage or you can load a local LAS file from your computer. 

If you want to test the 'load local file' feature and don't have any local LAS files, you can quickly get one by going to <a href=""https://raw.githubusercontent.com/JustinGOSSES/wellio.js/master/assets/00-01-01-073-05W5-0.LAS"">this</a> link and saving the results to a "".las"" file using your browser. That is a raw las file for well UWI 00-01-01-073-05W5-0.

#### 2. ObservableHQ demo <a href=""https://beta.observablehq.com/@justingosses/upload-well-logs-convert-las-to-json-with-wellio-then-visual/2"">page</a>:
ObservableHQ is new way to explore and play with JavaScript code. Think Jupyter notebook but in a more reactive and interactive form. It runs JavaScript code instead of Python/Julia/R. There's <a href=""https://observablehq.com/@justingosses/upload-well-logs-convert-las-to-json-with-wellio-then-visual/2"">this</a> demo that uses vega to visualize the well log but is limited to horizontal visualization. There's also <a href=""https://observablehq.com/@justingosses/a-notebook-using-wellio-js-wellioviz-js-for-quick-looks-of-la""> this </a> demo that uses wellioviz to visualize the well log as people expect it to be visualized in a vertical orientation with shading, etc. 

#### 3. Jupyter Notebook Node.js <a href=""https://github.com/JustinGOSSES/wellio.js/blob/master/notebooks/Wellio%20Demo%20in%20Jupyter%20Notebook%20Node.js.ipynb"">demo</a>
Wellio can also be worked with in a jupyter notebook running a node.js kernal.

### Data in the demos
Data for the demos is mostly wells from an open source dataset of the <a href=""https://ags.aer.ca/publication/spe-006"">Mannville Group from Alberta Canada</a>. A preprocessed version of this dataset can be found on <a href=""https://dataunderground.org/dataset/athabasca-preprocessed"">dataunderground</a> along with many other well dataset.

## Documentation

#### Please find the documentation here: https://justingosses.github.io/wellio.js/docs/ 
Contents include:
- PURPOSE
- USAGE
- HOW TO INSTALL
- HOW TO USE ONCE INSTALLED
- WELLIO-STYLE JSON VS OTHERS
- HOW TO EDIT DOCUMENTATION
- FUNCTIONS

## Contributing
There are a variety of <a href=""https://github.com/JustinGOSSES/wellio.js/issues"">issues</a> that need worked. Several of which are suitable for those who are new to JavaScript. 

Please add any suggestions you'd like or bugs you find to the issues.

Docs are a great way to make pull request contributions even if you aren't immmersed in the code base yet.

## Contributors: 
- https://github.com/JustinGOSSES
- https://github.com/dcslagel


## Road Map
Right now, the main functionality of wellio.js is LAS file -> Wellio-style JSON. 

There is also functionality to:
- save wellio-style JSON as a .json file.
- load & convert <a href=""https://lasio.readthedocs.io/en/latest/"">LASIO</a>-style json into wellio-style JSON.

In the future, we may add functionality to convert <a href=""https://jsonwelllogformat.org/"">JSON well log format</as>, or what becomes the unfortunately named JSON-style JSON, to wellio-style json and back.

## Wellio.js & Wellioviz.js
<i>Wellioviz</i> is the visualization companion to <i>wellio</i>!

Where are <i>Wellio</i> is just concerned with the conversion of LAS 2.0 files into JSON, <i>Wellioviz</i> is concerned with making a visualization of the resulting JSON using d3.js v5. This means you can load, convert, and visualize well logs entirely on the web with front-end JavaScript.

<a href=""https://github.com/JustinGOSSES/wellioviz""><b>Find out more about WELLIOVIZ here</b></a>

## Where To Get Open-Source Well Logs in .LAS format?
You can use the file upload button to load into your browsers memory any LAS files from your local computer. I've also included a few well logs in the /assets/ folder of this repo from the electronic data file below. 

Electronic data (including well logs, tops, etc.) for Athabasca Oil Sands Data McMurray/Wabiskaw Oil Sands Deposit <a href=""http://ags.aer.ca/publications/SPE_006.html"">http://ags.aer.ca/publications/SPE_006.html Data is also in the repo folder: SPE_006_originalData</a>

Report for Athabasca Oil Sands Data McMurray/Wabiskaw Oil Sands Deposit <a href=""http://ags.aer.ca/document/OFR/OFR_1994_14.PDF"">http://ags.aer.ca/document/OFR/OFR_1994_14.PDF</a>

You can also find them on USGS and Kansas open data sites as is done in <a href=""https://observablehq.com/@justingosses/a-notebook-using-wellio-js-wellioviz-js-for-quick-looks-of-la"">this Observable notebook</a> that leverages wellio & wellioviz.


## Example of LAS format and JSON formated well log data

### Original las file example
or go <a href=""https://justingosses.github.io/wellio.js/"">here</a> for live example.
```
~VERSION INFORMATION
 VERS.                 2.0:   CWLS LOG ASCII STANDARD -VERSION 2.0
 WRAP.                  NO:   ONE LINE PER DEPTH STEP
~WELL INFORMATION BLOCK
#MNEM.UNIT           DATA                    DESCRIPTION OF MNEMONIC
#---------    -------------------            -------------------------------
# Generated from Intellog Unique Number	CW_73_75/WELL/2722
WELL.         CHEVRON MGSU 1 MITSUE 01-01    : Well_name    - WELL
LOC .         00/01-01-073-05W5/0            : Location     - LOCATION
UWI .         00/01-01-073-05W5/0            : Uwi          - UNIQUE WELL ID
ENTR.         JAYE                           : Entered      - ENTERED BY
SRVC.         SCHLUMBERGER                   : Scn          - SERVICE COMPANY
DATE.         23 DEC 86                      : Date         - LOG DATE
STRT.M        390                            : top_depth    - START DEPTH
STOP.M        650                            : bot_depth    - STOP DEPTH
STEP.M        0.25                           : increment    - STEP LENGTH
 NULL. -999.2500:NULL Value
~CURVE INFORMATION BLOCK
#MNEM UNIT       ERCB CURVE CODE    CURVE DESCRIPTION
#-----------   ------------------   ----------------------------------
DEPT.M        00 001 00 00         : DEPTH        - DEPTH
DPHI.V/V      00 890 00 00         : PHID         - DENSITY POROSITY (SANDSTONE)
NPHI.V/V      00 330 00 00         : PHIN         - NEUTRON POROSITY (SANDSTONE)
GR  .API      00 310 00 00         : GR           - GAMMA RAY
CALI.MM       00 280 01 00         : CAL          - CALIPER
ILD .OHMM     00 120 00 00         : RESD         - DEEP RESISTIVITY (DIL)
~PARAMETER INFORMATION
#MNEM.UNIT           DATA             DESCRIPTION OF MNEMONIC
#---------         -----------     ------------------------------
GL  .M        583.3                : gl           - GROUND LEVEL ELEVATION
EREF.M        589                  : kb           - ELEVATION OF DEPTH REFERENCE
DATM.M        583.3                : datum        - DATUM ELEVATION
TDD .M        733.4                : tdd          - TOTAL DEPTH DRILLER
RUN .         ONE                  : Run          - RUN NUMBER
ENG .         SIMMONS              : Engineer     - RECORDING ENGINEER
WIT .         SANK                 : Witness      - WITNESSED BY
BASE.         S.L.                 : Branch       - HOME BASE OF LOGGING UNIT
MUD .         GEL CHEM             : Mud_type     - MUD TYPE
MATR.         SANDSTONE            : Logunit      - NEUTRON MATRIX
TMAX.C        41                   : BHT          - MAXIMUM RECORDED TEMPERATURE
BHTD.M        733.8                : BHTDEP       - MAXIMUM RECORDED TEMPERATURE
RMT .C        17                   : MDTP         - TEMPERATURE OF MUD
MUDD.KG/M     1100                 : MWT          - MUD DENSITY
NEUT.         1                    : NEUTRON      - NEUTRON TYPE
RESI.         0                    : RESIST       - RESISTIVITY TYPE
RM  .OHMM     2.62                 : RM           - RESISTIVITY OF MUD
RMC .OHMM     0                    : RMC          - RESISTIVITY OF MUD CAKE
RMF .OHMM     1.02                 : RMF          - RESISTIVITY OF MUD FILTRATE
SUFT.C        0                    : SUFT         - SURFACE TEMPERATURE
~A  DEPTH     PHID     PHIN       GR      CAL     RESD
  390.000    0.199    0.457   82.478  238.379    2.923
  390.250    0.208    0.456   86.413  238.331    2.925
  390.500    0.246    0.452   90.229  238.069    2.917
  390.750    0.266    0.475   90.944  238.752    2.898
  391.000    0.287    0.484   88.866  239.724    2.890
  391.250    0.288    0.474   82.638  241.951    2.844
  391.500    0.241    0.461   83.345  244.478    2.748
  391.750    0.215    0.471   88.403  247.116    2.725
  392.000    0.190    0.448   91.038  250.475    2.748
  392.250    0.219    0.478   89.579  254.764    2.845
  392.500    0.269    0.552   84.092  258.019    2.939
  392.750    0.316    0.458   78.479  260.143    3.088
  393.000    0.299    0.429   72.249  256.370    3.338
```

#### LAS -> JSON
```var lasjson = function las2json(onelas)```
will give you something like this though in the example below all the data is taken out to save space:
``` 
var lasjson = {
			""VERSION INFORMATION"":{
				""VERS"":{""MNEM"":"""",""UNIT"":"""",""DATA"":"""",""DESCRIPTION OF MNEMONIC 1"":"""",""DESCRIPTION OF MNEMONIC 2"":""""},
				""WRAP"":{""MNEM"":"""",""UNIT"":"""",""DATA"":"""",""DESCRIPTION OF MNEMONIC 1"":"""",""DESCRIPTION OF MNEMONIC 2"":""""}
			}
			,
			""WELL INFORMATION BLOCK"":{
					""GENERATED"":"""",
					""MNEM_0"":{""MNEM"":"""",""UNIT"":"""",""DATA"":"""",""DESCRIPTION OF MNEMONIC 1"":"""",""DESCRIPTION OF MNEMONIC 2"":""""},
					""MNEM_1"":{""MNEM"":"""",""UNIT"":"""",""DATA"":"""",""DESCRIPTION OF MNEMONIC 1"":"""",""DESCRIPTION OF MNEMONIC 2"":""""},
					""MNEM_2"":{""MNEM"":"""",""UNIT"":"""",""DATA"":"""",""DESCRIPTION OF MNEMONIC 1"":"""",""DESCRIPTION OF MNEMONIC 2"":""""}
				}
			,
			""CURVE INFORMATION BLOCK"":{
					""MNEM_0"":{""MNEM"":"""",""UNIT"":"""",""ERCB CURVE CODE"":"""",""CURVE DESCRIPTION 1"":"""",""CURVE DESCRIPTION 2"":""""}, 
					""MNEM_0"":{""MNEM"":"""",""UNIT"":"""",""ERCB CURVE CODE"":"""",""CURVE DESCRIPTION 1"":"""",""CURVE DESCRIPTION 2"":""""},
				}		
			,
			""PARAMETER INFORMATION"":{
					""MNEM_0"":{""MNEM"":"""",""UNIT"":"""",""DATA"":"""",""DESCRIPTION OF MNEMONIC 1"":"""",""DESCRIPTION OF MNEMONIC 2"":""""}, 
					""MNEM_1"":{""MNEM"":"""",""UNIT"":"""",""DATA"":"""",""DESCRIPTION OF MNEMONIC 1"":"""",""DESCRIPTION OF MNEMONIC 2"":""""},
				}
			,
			""CURVES"":{
					""Curve_NAME_ONE"" :[1,2,3,4,5,6,7,8,9,10,11],
					""Curve_NAME_ONE"" :[1,2,3,4,5,6,7,8,9,10,11],
				}
		}
```

",30,30,9,7,well-logs,"[athabasca, athabasca-preprocessed, dataunderground, geology, geoscience, javascript, json-parser, las, well-logs]",0.0
164,Oslandia,QGeoloGIS,Oslandia,https://github.com/Oslandia/QGeoloGIS,https://api.github.com/repos/QGeoloGIS/Oslandia,Migrated to: https://gitlab.com/Oslandia/qgis/QGeoloGIS,"# QGeoloGIS plugin

This project allows the visualization of logs of drilling wells or boreholes and time series.

It is based on QT and the QGIS rendering engine to plot series of measurements. This allows:
- the use of the rich symbology engine of QGIS to display underground data
- having decent display performances, since QGIS is optimized to quickly render geometries possibly made of lots of points

Currently three types of data are handled:
- **stratigraphy** data, where a polygon is defined by a depth range and a pattern fill is given by a rock code
- **continuous series** of data that represent data sampled continuously underground (a sample every centimer for instance). This could also be reused to plot time series.
- **scatter plots** of data

![Example in a QGIS application](qgeologis.png)

See the [corresponding video](https://vimeo.com/303279452)

# How to use it as a standalone plugin

Install the plugin in the QGIS plugin directory and enable it. You can install it by typing `make deploy` in the main folder.

It requires a configuration that describes what is the base layer that displays measure points and how to access the different measure layers.

Some sample resources are available in the [sample folder](./sample). Apart from the small [toy dataset](./sample/qgeologistest.sql) itself, you can find an [example QGIS project file](sample/project.qgs) and the associated [configuration file](./sample/qgeologistest.json), which may be loaded from the plugin menu.
",28,28,11,11,well-logs,"[borehole, geology, plot, python, qgis, well-logs]",0.0
166,laslibs,las-js,laslibs,https://github.com/laslibs/las-js,https://api.github.com/repos/las-js/laslibs,Typescript/JavaScript library for parsing standard well log files (Geophysical well logs)),"![](https://github.com/laslibs/las-js/workflows/Node%20CI/badge.svg?)
![Snyk Vulnerabilities for npm package](https://img.shields.io/snyk/vulnerabilities/npm/las-js?style=flat-square)
![npm type definitions](https://img.shields.io/npm/types/las-js?style=flat-square)
![npm bundle size](https://img.shields.io/bundlephobia/minzip/las-js?style=flat-square)
![GitHub issues](https://img.shields.io/github/issues/iykekings/las-js?style=flat-square)
![NPM](https://img.shields.io/npm/l/las-js?style=flat-square)
![npm](https://img.shields.io/npm/v/las-js?style=flat-square)

# las-js is a zero-dependency JavaScript library for parsing .Las file (Geophysical well log files).

### Currently supports only version 2.0 of [LAS Specification](https://www.cwls.org/wp-content/uploads/2017/02/Las2_Update_Feb2017.pdf).  For more information about this format, see the Canadian Well Logging Society [product page](https://www.cwls.org/products/).

## How to use

- Installing

  > NPM

  ```sh
  $npm install las-js
  ```

  > Yarn

  ```sh
  $yarn add las-js
  ```

  > Browser

  ```html
  <script defer src=""https://cdn.jsdelivr.net/npm/las-js/dist/browser.js""></srcipt>
  ```

- Usage

  import/require las-js module

  > Node

  ```js
  // common js
  const { Las } = require('las-js');
  // esm
  import { Las } from 'las-js';
  const myLas = new Las(`./sample/example1.las`);
  ```

  You can also pass LAS file contents directly to constructor

  ```js
  // common js
  const { Las } = require('las-js');
  const fs = require('fs');
  const data = fs.readFileSync(`./sample/example1.las`, { encoding: 'utf8' });
  // add loadFile: false to constructor options
  const myLas = new Las(data, { loadFile: false });
  ```

> Browser

las-js adds a global class Lasjs

```js
const input = document.getElementById('file-input');
input.addEventListener('change', async e => {
  const file = e.target.files[0];
  const myLas = new Lasjs(file);
});
// or
const myLas = new Lasjs('https://raw.githubusercontent.com/iykekings/las-js/master/src/__test__/sample/A10.las'); // url - only on browser
```

- Read data

  > Use Laspy.data to get a 2-dimensional array containing the readings of each log,
  > Or Lasjs.dataStripped to get the same as above but with all rows containing null values stripped off

  ```js
  async function read() {
    try {
      const data = await myLas.data();
      console.log(data);
      /**
         [[2650.0, 177.825, -999.25, -999.25],
          [2650.5, 182.5, -999.25,-999.25],
          [2651.0,180.162, -999.25, -999.25],
          [2651.5, 177.825, -999.25, -999.25],
          [2652.0, 177.825, -999.25, -999.25] ...]
        */

      const dataStripped = await myLas.dataStripped();
      console.log(dataStripped);
      /**
       [[2657.5, 212.002, 0.16665, 1951.74597],
       [2658.0, 201.44, 0.1966, 1788.50696],
       [2658.5, 204.314, 0.21004, 1723.21204],
       [2659.0, 212.075, 0.22888, 1638.328],
       [2659.5, 243.536, 0.22439, 1657.91699]...]
       */
    } catch (error) {
      console.log(error);
    }
  }
  ```

- Get the log headers


    ```javascript
        // ...
        const headers = await myLas.header();
        console.log(headers);
        // ['DEPTH', 'GR', 'NPHI', 'RHOB']
       // ...
    ```

- Get the log headers descriptions


    ```Js
        //...
        const headerAndDescr = await myLas.headerAndDescr();
        console.log(headerAndDescr)
        // {DEPTH: 'DEPTH', GR: 'Gamma Ray', NPHI: 'Neutron Porosity', RHOB: 'Bulk density'}
        // ...
    ```

- Get a particular column, say Gamma Ray log


    ```Js
        // ...
        const gammaRay = await myLas.column('GR');
        console.log(gammaRay);
        // [-999.25, -999.25, -999.25, -999.25, -999.25, 122.03, 123.14, ...]
        // ...
    ```
    ```Js
        // ...
        // get column with null values stripped
        const gammaRay = await myLas.columnStripped('GR');
        console.log(gammaRay);
        // [61.61, 59.99, 54.02, 50.87, 54.68, 64.39, 77.96, ...]
        // ...
    ```
    > Note this returns the column, after all the data has been stripped off their null values, which means that valid data in a particular column would be stripped off if there is another column that has a null value at that particular row

- Get the Well Parameters

  ### Presents a way of accessing the details of individual well parameters.

  ### The details include the following:

        1. descr - Description/ Full name of the well parameter
        2. units - Its unit measurements
        3. value - Value

  ```Js
    // ...
    const well = await myLas.wellParams()
    const start = well.STRT.value // 1670.0
    const stop = well.STOP.value // 1669.75
    const null_value = well.NULL.value //  -999.25
    // Any other well parameter present in the file, can be gotten with the same syntax above
    // ...
  ```

- Get the Curve Parameters

  ### Presents a way of accessing the details of individual log columns.

  ### The details include the following:

        1. descr - Description/ Full name of the log column
        2. units - Unit of the log column measurements
        3. value - API value of the log column

  ```Js
    // ...
    const curve = await myLas.curveParams()
    const NPHI = curve.NPHI.descr // 'Neutron Porosity'
    const RHOB = curve.RHOB.descr // 'Bulk density'
    // This is the same for all log column present in the file
    // ...
  ```

- Get the Parameters of the well

  ### The details include the following:

        1. descr - Description/ Full name of the log column
        2. units - Unit of the log column measurements
        3. value - API value of the log column

  ```Js
    // ...
    const param = await myLas.logParams(); // 'BOTTOM HOLE TEMPERATURE'
    const BHT = param.BHT.descr // 'BOTTOM HOLE TEMPERATURE'
    const BHTValaue = param.BHT.value // 35.5
    const BHTUnits = param.BHT.units // 'DEGC'
    // This is the same for all well parameters present in the file
    // ...
  ```

- Get the number of rows and columns


    ```Js
        // ...
        const numRows = await myLas.rowCount() // 4
        const numColumns = await myLas.columnCount() // 3081
        // ...
    ```

- Get the version and wrap


    ```Js
        // ...
        const version = await myLas.version() // '2.0'
        const wrap = await myLas.wrap() // true
        // ...
    ```

- Get other information

  ```Js
      // ...
      const other = await myLas.other()
      console.log(other)
      // Note: The logging tools became stuck at 625 metres causing the data between 625 metres and 615 metres to be invalid.
      // ...
  ```

- Export to CSV

  ### For node, this writes a csv file to the current working directory, with headers of the well and data section only for node. For browser, this returns a File Blob, that can be downloaded by using _URL.createObjectURL_

  ```Js
      //...
      await myLas.toCsv('result')
      // result.csv has been created Successfully!
      //...
  ```

  > result.csv

  | DEPT | RHOB    | GR      | NPHI  |
  | ---- | ------- | ------- | ----- |
  | 0.5  | -999.25 | -999.25 | -0.08 |
  | 1.0  | -999.25 | -999.25 | -0.08 |
  | 1.5  | -999.25 | -999.25 | -0.04 |
  | ...  | ...     | ...     | ...   |
  | 1.3  | -999.25 | -999.25 | -0.08 |

  Or get the version of csv with null values stripped

  ```Js
      // ...
      await myLas.toCsvStripped('clean')
      // clean.csv has been created Successfully!
      // ...
  ```

  > clean.csv

  | DEPT | RHOB  | GR   | NPHI  |
  | ---- | ----- | ---- | ----- |
  | 80.5 | 2.771 | 18.6 | -6.08 |
  | 81.0 | 2.761 | 17.4 | -6.0  |
  | 81.5 | 2.752 | 16.4 | -5.96 |
  | ...  | ...   | ...  | ...   |
  | 80.5 | 2.762 | 16.2 | -5.06 |

- Browser and Node Supports

  > las-js is written in typescript and compiles to es6.

  - Browser
    Supports IE 10 and 11 - (doesn't yet support url)
    Doesn't support Opera Mini
  - Node
    Tested 0n 8, 10 and 12

- ## Support
  las-js is an MIT-licensed open source project. You can help it grow by becoming a sponsor/supporter.[Become a Patron!](https://www.patreon.com/bePatron?u=19152008)
",23,23,3,3,well-logs,"[csv, javascript-library, las, las-js, parsing, typescript, well-logs]",0.0
167,JustinGOSSES,MannvilleGroup_Strat_Hackathon,,https://github.com/JustinGOSSES/MannvilleGroup_Strat_Hackathon,https://api.github.com/repos/MannvilleGroup_Strat_Hackathon/JustinGOSSES,stratigraphic machine-learning - active work moved to Predictatops,"# Development has moved to <a href=""https://github.com/JustinGOSSES/predictatops"">predictatops</a> repository.

# MannvilleGroup_Strat_Hackathon
- Team: Big Data > Big Lore
- Team members: <a href=""https://github.com/JustinGOSSES"">JustinGOSSES</a>,<a href=""https://github.com/jazzskier"">jazzskier</a>, <a href=""https://github.com/GeophysicsPanda"">GeophysicsPanda</a>, and <a href=""https://github.com/dalide"">dalide</a>.
- When: September 24th, 2017
- Where: <a href=""http://stationhouston.com/"">Station Houston</a>
- What: Agile Hackathon -> <a href=""https://agilescientific.com/events/subsurface2018"">event & sponsors</a>
- Thanks: to <a href=""https://www.meetup.com/AWS-Houston/"">AWS cloud services Houston</a> and <a href=""https://sigopt.com/"">Sigopt</a> for technical help and the other sponsors for feeding us so we didn't have to leave our keyboard

## Project Statement
Predict stratigraphic surfaces based on training on human-picked stratigraphic surfaces. Used 2000+ wells with Picks from the Mannville, including McMurray, in Alberta, Canada.

## Philosophy 
Instead of assuming there is a mathematical or pattern basis for stratigraphic surfaces that can be teased out of logs, focus on creating programatic features and operations that mimic the comparison-based observations that would have been done by a geologist.

## Project Summary
There has been studies that attempt to do similiar things for decades. A lot of them assume a mathematical pattern to stratigraphic surfaces and either don't train specifically on human-picked tops or do so lightly. We wanted to try as close a geologic approach (as opposed to mathematical or geophysical approach) as possible. What we managed to get done by the end of the hackathon is sorta a small scale first pass. 

Eventually, we want to get to the point where we've identified a large number of feature types that both have predictive value and can be tied back to geologist insight. There are a lot of observations happening visually (and therefore not consciously)  when a geologist looks at a well log and correlates it. We want to focus on engineering features that mimic these observations and the multitude of scales at which they occur.

In addition to automating correlation of nearby wells based on picks that already exist, which has value, we think this will help geologist have better discussions, and more quantitative discussions, about the basis of their correlation and why correlations might differ between geologists. You can imagine a regional area with two separate teams with different approaches to picking a top. You could use this to programmatically pick tops in area B like they are picked in area A and also the inverse. The differences in pick style then becomes easier to analyze with less additional work. 

## Datasets for Hackathon project

Report for Athabasca Oil Sands Data McMurray/Wabiskaw Oil Sands Deposit
http://ags.aer.ca/document/OFR/OFR_1994_14.PDF

Electronic data for Athabasca Oil Sands Data McMurray/Wabiskaw Oil Sands Deposit
http://ags.aer.ca/publications/SPE_006.html
Data is also in the repo folder: <a href=""https://github.com/JustinGOSSES/MannvilleGroup_Strat_Hackathon/tree/master/SPE_006_originalData"">SPE_006_originalData</a>

@dalide used the Alberta Geological Society's <a href=""http://www1.aer.ca/GISConversionTools/conversion_tools.html"">UWI conversion tool</a> to find <a href=""https://github.com/JustinGOSSES/MannvilleGroup_Strat_Hackathon/blob/master/well_lat_lng.csv"">lat/longs for each of the well UWIs</a>. These were then used to find each well's nearest neighbors as demonstrated in <a href=""https://github.com/JustinGOSSES/MannvilleGroup_Strat_Hackathon/blob/master/notebooks_2018/mapmaking/Map_Exploration_v2-KDtree.ipynb"">this</a> notebook. 

## Folder Re-Organization
On February 11th, 2018, @JustinGosses reorganized the folder to get a lot of the notebooks out of the top-level and into sub-folders as things were getting too crowded. This might cause the directory urls to some files to be incorrect. This will be the case for any notebook from the Hackathon or 2017. Fixing this problem will just require adding a ../ or ../../ to the front of the directory in most cases.

## Key Jupyter Notebooks finished during Hackathon project

Final Data Prep & Machine Learning for the prediction finished by end of hackathon
https://github.com/JustinGOSSES/MannvilleGroup_Strat_Hackathon/blob/master/data_prep_wells_xgb.ipynb

Version of feature engineering work done during hackathon (but didn't get to include during hackathon)
https://github.com/JustinGOSSES/MannvilleGroup_Strat_Hackathon/blob/master/Feature_Brainstorm_Justin_vD-Copy1.ipynb

## Key Jupyter Notebooks Post Hackathon
Code development has moved to the `modular_redo` sub-folder. Things were made more modular to better enable short bits of work when time available. The notebooks are a bit messy but will clean up in near future.
https://github.com/JustinGOSSES/MannvilleGroup_Strat_Hackathon/tree/master/notebooks_2018/modular_redo]

## Recent updates
The code runs faster and and mean absolute error is down from 90 to 15.03 and now 7+. Key approaches were:
1. Leverage knowledge from nearby wells.
2. Instead of distinguishing between 2 classes, pick and not pick, distinguish between 3 classes: (a) pick, (b) not pick but within 3 meters and (c) not pick and not within 3 meters of pick.
3. More features
4. A Two step approach to machine-learning: 

4-1. First step is class-based prediction. Classes are groups based on distance from actual pick. For example, depths at the pick, depths within 0.5 meter, depths within 5 meters above, etc. 
4-2. Second step is more concerned with picking between the depths predicted as being in the classes nearest to the pick. We've explored both a rule-based scoring and a regression machine-learning process for this. 
4-2-1. The rule-based approach uses the class prediction and simple additive scoring of the class predictions based across different size windows. In a scenario where there are two depths with a predicted class of 100, we decide between them by adding up all the class scores across different size windows above and below each depth. The depth with the highest aggregate score wins  and it declared the ""predicted depth"". We take this route as we assume the right depth will have more depths near it that look like the top pick and as such have higher classes predicted for depths around it while false positives will be more likely to have more lower level classes around it.
4-2-2. We're also trying regression-based machine-learning to predict the distance from each depth in question to the actual pick. The depth with the lowest predicted distance between it and actual pick is chosen as the ""predicted pick"". This approach hasn't given any better results than the simple rule-based aggregate scoring.
 

#### Distribution of Absolute Error in Test Portion of Dataset for Top McMurray Surface in Meters. 
Y-axis is number of picks in each bin, and X-axis is distance predicted pick is off from human-generated pick.
<img src=""current_errors_TopMcMr_20181006.png""
     alt=""image of current_errors_TopMcMr_20181006""
     style=""float: left; margin-right: 25px;"" />

Current algorithm used is XGBoost.

## Future Work [also see issues]
7. Visualize probabilty of pick along well instead of just returning max probability prediction in each well. 
8. Generate average aggregate wells in different local areas for wells at different prediction levels. See if there are trends or if this helps to idenetify geologic meaningful features that correlate to many combined machine-learning model features. 
9. Explore methods to visualize weigtings of features on individual well basis using techniques similar to those learned in image-based deep-learning. 
10. Cluster wells using unsupervised learning and then see if clusters can be created that correlated with supervised prediction results. (initial trials with UMAP give encouraging results)
11. Rework parts of this into more object oriented approach.
12. Use H2O's automl library to try to improve on standard XGBoost approach.

## Eventual Move of this Repository Contents to a Different Repository
The plan is that once things are winnowed down to a final approach, the resulting code will be moved the <a href=""https://github.com/JustinGOSSES/StratPickSupML"">StratPickSupML</a> repository will it will be cleaned into one or more modules and demo notebooks with less clutter of failed but possibly useful if reworked approaches.

## Help Wanted
This repo isn't particularly organized and there hasn't be a lot of time spent (actually no time spent) to make jumping in and helping out easy. That being said, there's no reason you couldn't just jump in an start improving things. The original group is working on this at a low level when we have time. There are a few issues that are enhancements that would be a good place to start.
",17,17,8,9,well-logs,"[curve, geology, pandas, stratigraphy, well-logs, xgboost]",0.0
168,aruss175,DLISIO_Notebooks,,https://github.com/aruss175/DLISIO_Notebooks,https://api.github.com/repos/DLISIO_Notebooks/aruss175,"Open source, public notebooks for working with DLISIO ","# DLISIO_Notebooks 
- Open source, public notebooks for working with DLISIO 
- Examples of the functionality in the dlisio package
- Utilizing open data from the Volve field: https://data.equinor.com/dataset/Volve 
  - License: CC BY NC SA
- **DLISIO still in development** https://pypi.org/project/dlisio/ - this means that I will try and keep the notebook up to date with changes as they occur, but things will break, please also read their Read The Docs https://dlisio.readthedocs.io/en/stable

# Current Status
- dlisio recently added lis parsing functionality.  I have just added a new notebook to demonstrate how to go from a .lis file to a dataframe, and to examine the .lis headers.
- Will continue to transform this into the coveted .lis to .las conversion!

# DLIS to LAS
- There is a notebook and a set of functions for a Dlis to las file converter.  Thanks for the help on this one!

# License and Use
I hope this work can be useful for any geoscientist working in academia, during a hackathon, or at a company dealing with subsurface well data.  I am a firm believer that file format parsing and file type conversions should not come at a cost in this day and age.

Work released under MIT License (MIT)
",9,9,4,1,well-logs,"[dlis, geology, las, parser, well-logs]",0.0
169,luthfigeo,DTW-Stratigraphic-Correlation,,https://github.com/luthfigeo/DTW-Stratigraphic-Correlation,https://api.github.com/repos/DTW-Stratigraphic-Correlation/luthfigeo,Well logs correlation using dynamic time warping,"# DTW-Stratigraphic-Correlation
Well logs correlation using dynamic time warping. Here, I applied to Alaska well.
",9,9,2,0,well-logs,"[correlation, dtw-algorithm, stratigraphy, well-logs]",0.0
170,laslibs,las-go,laslibs,https://github.com/laslibs/las-go,https://api.github.com/repos/las-go/laslibs,Go library for parsing standard well log files (Geophysical well logs),"![](https://github.com/laslibs/las-go/workflows/Test/badge.svg?)


# las-go is a GoLang library/package for parsing .Las file (Geophysical well log files).

### Currently supports only version 2.0 of [LAS Specification](https://www.cwls.org/wp-content/uploads/2017/02/Las2_Update_Feb2017.pdf).  For more information about this format, see the Canadian Well Logging Society [product page](https://www.cwls.org/products/).

## How to use

- Installing

  > GO GET

  ```sh
    go get github.com/laslibs/las-go
  ```
- Test

  ```sh
    go test ./...
  ```

- Usage

  ```go
  import (
    lasgo ""github.com/laslibs/las-go""
  )
  ```

- Read data

  > Use `Las.Data()` to get a 2-dimensional slice containing the readings of each log

  ```go
      func main() {
      las, err := lasgo.Las(""sample/A10.las"")
      if err != nil {
        panic(err)
      }
      fmt.Println(las.Data())
    /**
       [[2650.0 177.825 -999.25 -999.25],
        [2650.5 182.5 -999.25-999.25]
        [2651.0180.162 -999.25 -999.25]
        [2651.5 177.825 -999.25 -999.25]
        [2652.0 177.825 -999.25 -999.25] ...]
      */
     }
  ```

- Get the log headers


    ```go
        // ...
        headers := las.Header();
        fmt.Println(headers);
        // [DEPTH GR NPHI RHOB]
        // ...
    ```

- Get the log headers descriptions as `map[string]string`


    ```go
        //...
        headerAndDesc := las.headerAndDesc()
        fmt.Println(headerAndDesc)
        // [DEPTH: DEPTH GR: Gamma Ray NPHI: Neutron Porosity RHOB: Bulk density]
        // ...
    ```

- Get a particular column, say Gamma Ray log


    ```go
        // ...
        gammaRay := las.Column(""GR"");
        fmt.Println(gammaRay);
        // [-999.25 -999.25 -999.25 -999.25 -999.25 122.03 123.14 ...]
        // ...
    ```

- Get the Well Parameters

  ### Presents a way of accessing the details of individual well parameters.

  ### The details include the following:

        1. description - Description/ Full name of the well parameter
        2. unit - Its unit of measurement
        3. value - Value measured

  ```go
    // ...
    well := las.WellParams()
    start := well[""STRT""].value // 1670.0
    stop := well[""STOP""].value // 1669.75
    null_value := well[""NULL""].value //  -999.25
    // Any other well parameter present in the file, can be gotten with the same syntax above
    // ...
  ```

- Get the Curve Parameters

  ### Presents a way of accessing the details of individual log columns.

  ### The details include the following:

        1. description - Description/ Full name of the log column
        2. unit - Unit of the log column measurements
        3. value - API value of the log column

  ```go
    // ...
    curve := las.CurveParams()
    NPHI := curve[""NPHI""].description // Neutron Porosity
    RHOB := curve[""RHOB""].description // Bulk density
    // This is the same for all log column present in the file
    // ...
  ```

- Get the Parameters of the well

  ### The details include the following:

        1. description - Description/ Full name of the log column
        2. unit - Unit of the log column measurements
        3. value - API value of the log column

  ```go
    // ...
    param := await las.LogParams(); // BOTTOM HOLE TEMPERATURE
    BHT := param[""BHT""].description // BOTTOM HOLE TEMPERATURE
    BHTValaue := param[""BHT""].value // 35.5
    BHTUnits := param[""BHT""].unit // DEGC
    // This is the same for all well parameters present in the file
    // ...
  ```

- Get the number of rows and columns


    ```go
        // ...
        numRows := las.RowCount() // 4
        numColumns := las.ColumnCount() // 3081
        // ...
    ```

- Get the version and wrap


    ```go
        // ...
        version := las.Version() // 2.0
        wrap := las.Wrap() // true
        // ...
    ```

- Get other information

  ```go
      // ...
      other := myLas.Other()
      fmt.Println(other)
      // Note: The logging tools became stuck at 625 metres causing the data between 625 metres and 615 metres to be invalid.
      // ...
  ```

- Export to CSV

  ### This writes a csv file to the current working directory, with headers of the well and data section

  ```go
      //...
      las.ToCSV(""result"")
      //...
  ```

  > result.csv

  | DEPT | RHOB    | GR      | NPHI  |
  | ---- | ------- | ------- | ----- |
  | 0.5  | -999.25 | -999.25 | -0.08 |
  | 1.0  | -999.25 | -999.25 | -0.08 |
  | 1.5  | -999.25 | -999.25 | -0.04 |
  | ...  | ...     | ...     | ...   |
  | 1.3  | -999.25 | -999.25 | -0.08 |
",8,8,2,3,well-logs,"[geophysics, go, golang, las, las-go, lasgo, well-logs]",0.0
171,laslibs,las-py,laslibs,https://github.com/laslibs/las-py,https://api.github.com/repos/las-py/laslibs,Python library for parsing standard well log files (Geophysical well logs),"# Las-py

## las-py is a zero-dependency Python library for parsing .Las file (Geophysical/Canadian well log files).

## Currently supports only version 2.0 of [LAS Specification](https://www.cwls.org/wp-content/uploads/2017/02/Las2_Update_Feb2017.pdf). For more information about this format, see the Canadian Well Logging Society [product page](https://www.cwls.org/products/)

- What's new in 1.1.0

  - Export to csv
  - Export to csv without rows containing null values
  - Bug fixes

- To Install


    ```sh
        $pip install las-py
    ```

- Usage


    ```python
        from las_py import Laspy
     ```
    ```python
        my_las = Laspy('path_to_las_file.las')
    ```

- Read data


    ```python
       data = my_las.data
       print(data)
       #[[2650.0, 177.825, -999.25, -999.25], [2650.5, 182.5, -999.25,-999.25], [2651.0,180.162, -999.25, -999.25], [2651.5, 177.825, -999.25, -999.25], [2652.0, 177.825, -999.25, -999.25] ...]
    ```
    ```python
        # get data with rows that has null value stripped
       data = my_las.data_stripped
       print(data)
       #[[2657.5, 212.002, 0.16665, 1951.74597], [2658.0, 201.44, 0.1966, 1788.50696], [2658.5, 204.314, 0.21004, 1723.21204], [2659.0, 212.075, 0.22888, 1638.328], [2659.5, 243.536, 0.22439, 1657.91699]...]
    ```

- Get the log headers


    ```python
        headers = my_las.header
        print(headers)
        # ['DEPTH', 'GR', 'NPHI', 'RHOB']
    ```

- Get the log headers descriptions


    ```python
        hds_and_desc = my_las.header_and_descr
        print(hds_and_desc)
        # {DEPTH': 'DEPTH', 'GR': 'Gamma Ray', 'NPHI': 'Neutron Porosity','RHOB': 'Bulk density'}
    ```

- Get a particular column, say Gamma Ray log


    ```python
        GR = my_las.column('GR')
        print(GR)
        # [-999.25, -999.25, -999.25, -999.25, -999.25, 122.03, 123.14, ...]
    ```
    ```python
        # get column with null values stripped
        GR = my_las.column_stripped('GR')
        print(GR)
        # [61.61, 59.99, 54.02, 50.87, 54.68, 64.39, 77.96, ...]
    ```
    > Note this returns the column, after all the data has been stripped off their null values, which means that valid data in a particular column would be stripped off if there is another column that has a null value at that particular row

- Get the Well Parameters

  ### Presents a way of accessing the details individual well parameters.

  ### The details include the following:

        1. descr - Description/ Full name of the well parameter
        2. units - Its unit measurements
        3. value - Value

  ```python
    start = my_las.well.STRT.value # 1670.0
    stop = my_las.well.STOP.value #  1669.75
    null_value = my_las.well.NULL.value #  -999.25
    # Any other well parameter present in the file, canbe gotten with the same syntax above
  ```

- Get the Curve Parameters

  ### Presents a way of accessing the details individual log columns.

  ### The details include the following:

        1. descr - Description/ Full name of the log column
        2. units - Unit of the log column measurements
        3. value - API value of the log column

  ```python
    NPHI = my_las.curve.NPHI.descr # 'Neutron Porosity'
    RHOB = my_las.curve.RHOB.descr # 'Bulk density'
    # This is the same for all log column present in the file
  ```

- Get the Parameters of the well

  ### The details include the following:

        1. descr - Description/ Full name of the log column
        2. units - Unit of the log column measurements
        3. value - API value of the log column

  ```python
    BHT = my_las.param.BHT.descr # 'BOTTOM HOLE TEMPERATURE'
    BHT_valaue = my_las.param.BHT.value # 35.5
    BHT_units = my_las.param.BHT.units # 'DEGC'
    # This is the same for all well parameters present in the file
  ```

- Get the number of rows and columns


    ```python
        rows = my_las.row_count # 4
        columns = my_las.column_count # 3081
    ```

- Get the version and wrap


    ```python
        version = my_las.version # '2.0'
        wrap = my_las.wrap # 'YES'
    ```

- Get other information

  ```python
      other = my_las.other
      print(other)
      # Note: The logging tools became stuck at 625 metres causing the data
      # between 625 metres and 615 metres to be invalid.
  ```

- Export to CSV

  ### This writes a csv file to the current working directory, with headers of the well and data section only.

  ```python
      my_las.to_csv('result')
      # result.csv has been created Successfully!
  ```

  > result.csv

  | DEPT | RHOB    | GR      | NPHI  |
  | ---- | ------- | ------- | ----- |
  | 0.5  | -999.25 | -999.25 | -0.08 |
  | 1.0  | -999.25 | -999.25 | -0.08 |
  | 1.5  | -999.25 | -999.25 | -0.04 |
  | ...  | ...     | ...     | ...   |
  | 1.3  | -999.25 | -999.25 | -0.08 |

  Or get the version of csv with null values stripped

  ```python
      my_las.to_csv_stripped('clean')
      # clean.csv has been created Successfully!
  ```

  > clean.csv

  | DEPT | RHOB  | GR   | NPHI  |
  | ---- | ----- | ---- | ----- |
  | 80.5 | 2.771 | 18.6 | -6.08 |
  | 81.0 | 2.761 | 17.4 | -6.0  |
  | 81.5 | 2.752 | 16.4 | -5.96 |
  | ...  | ...   | ...  | ...   |
  | 80.5 | 2.762 | 16.2 | -5.06 |

- ## Support
  las-py is an MIT-licensed open source project. You can help it grow by becoming a sponsor/supporter. Donate on [Patreon](https://www.patreon.com/bePatron?u=19152008)
",8,8,2,2,well-logs,"[canadian, csv, las, parsing, python-library, well-logs]",0.0
172,imranfadhil,logASCII_viewer,,https://github.com/imranfadhil/logASCII_viewer,https://api.github.com/repos/logASCII_viewer/imranfadhil,Upload well LAS (log ASCII) files and view the raw logs interactively in streamlit,"# logASCII_viewer
Upload well LAS (log ASCII) files and view the raw logs interactively in streamlit

![preview](/src/logASCII_viewer/data/preview.png)
",6,6,2,0,well-logs,"[drilling, lasio, log, log-ascii, petrophysics, plotly, streamlit, well, well-logs]",0.0
ashrafalaghbari,ProBHP,,https://github.com/ashrafalaghbari/ProBHP,https://api.github.com/repos/ProBHP/ashrafalaghbari,An AI-powered app for flowing bottom-hole pressure estimation  and analysis in oil wells with multiphase flow.,"# ProBHP: Intelligent FBHP Estimation and Analysis
[![Made with Python](https://img.shields.io/badge/Made%20with-Python%203.10.7-blue.svg)](https://www.python.org/)
## Description
ProBHP is an AI-powered system for accurate flowing bottom hole pressure (FBHP) estimation and analysis in oil wells. It utilizes advanced machine learning and Explainable AI techniques to optimize production, monitor fluid movements, and assess reservoir performance. ProBHP provides cost-effective solutions for FBHP estimation in single-phase and multiphase flow scenarios, overcoming the limitations of conventional methods. With superior accuracy and real-time monitoring, ProBHP enhances wellbore management and production optimization.

# Problem Statement & Motivation

The estimation of flowing bottom hole pressure (FBHP) in oil wells is crucial for monitoring fluid movements, optimizing production, quantifying reservoir performance, and understanding wellbore behavior. However, conventional methods for estimating FBHP are expensive and unreliable, especially when dealing with multiphase flow. Existing empirical correlations and mechanistic models developed in laboratory settings often yield inaccurate results when applied in the field. Additionally, obtaining FBHP using pre-installed permanent gauges in smart wells and well-testing analysis is costly and time-consuming, requiring constant calibration and maintenance. Therefore, there is a clear need for a more efficient and accurate approach.

# Data Collection:

To address the FBHP estimation problem, a dataset consisting of 206 data points was collected from various sources, including Govier and Fogarasi (1975) and Asheim (1986). These sources conducted BHP surveys by deploying down-hole pressure gauges just above the perforations to record the FBHP. The collected data serves as a valuable resource for developing and validating improved FBHP estimation methods.

# Model Development and Deployment

In this project, I have developed a model capable of accurately predicting FBHP using a Feedforward Neural Network (FFNN). To optimize the FFNN and obtain the optimal hyperparameters, I employed Bayesian optimization, which proved to be highly efficient in implementation. This approach allowed me to fine-tune the network and achieve superior performance in FBHP estimation.

The developed model has been successfully deployed and is capable of performing both online and batch predictions. This means that it can provide real-time FBHP estimates during ongoing operations, as well as handle large datasets for retrospective analysis. The deployment of the model enables continuous monitoring of FBHP and enhances decision-making in oil and gas well operations.
## View Notebooks in Colab

| Notebook | Colab Link |
| -------- | ---------- |
| Linear Regression & Decision Tree | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ashrafalaghbari/ProBHP/blob/main/notebooks/lr_dt.ipynb) |
| FeedForwad Neural Network-Bayesian Optimization | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ashrafalaghbari/ProBHP/blob/main/notebooks/FFNN.ipynb) |

![probhp](https://github.com/ashrafalaghbari/ProBHP/assets/98224412/ed09d777-7076-4b50-a016-7ed36c94e2b9)

# Explainable AI and Feature Importance:

To enhance the interpretability of the FBHP prediction model, I incorporated Explainable AI techniques using SHAP (SHapley Additive exPlanations) values. SHAP values provide insight into the contribution of each feature to the model's predictions. By understanding the feature importance, I gain valuable insights into why the model is making specific predictions.

The utilization of SHAP values allows me to interpret the results of the FBHP model accurately. It provides a transparent and understandable framework to comprehend the factors influencing FBHP and facilitates better decision-making in well operations.

This app allows you to input relevant parameters and receive accurate FBHP predictions in real-time or for batch analysis, depending on your requirements. The integration of Explainable AI ensures that you can also access feature importance information, gaining valuable insights into the model's decision-making process and enhancing your understanding of FBHP dynamics in oil wells.

![wkflow_bhp](https://github.com/ashrafalaghbari/ProBHP/assets/98224412/b15ea5c6-3c71-433b-9c07-f345021ad99a)

# Installation

Follow these steps to install and run the project locally:

Set up a virtual environment (optional but recommended):

```bash
python -m venv env
env\Scripts\activate.bat
```

```bash
git clone https://github.com/ashrafalaghbari/ProBHP.git
cd <project-directory>
pip install -r requirements.txt
streamlit run app.py
```

Access the web application by opening the following URL in your web browser:

```bash
http://localhost:8501
```
Follow the instructions on the web application to use the project.

<!-- If you prefer to use a Docker image, you can follow these additional steps:

Pull the Docker image from Docker Hub:
```bash
docker pull fbhpapp:0.1
```
Run the Docker container:
```bash
docker run -p 8501:8501 fbhpapp:0.1
```
Access the web application using the same URL as mentioned above. -->

# References
* [An Automated Flowing Bottom-Hole Pressure Prediction for a Vertical Well Having Multiphase Flow UsingComputational Intelligence Techniques](https://onepetro.org/SPESATS/proceedings-abstract/18SATS/All-18SATS/215548)

* [Prediction of pressure in different two-phase flow conditions: Machine learning applications](https://www.sciencedirect.com/science/article/abs/pii/S0263224120311775)

* [A new flowing bottom hole pressure prediction model using M5 prime decision tree approach](https://link.springer.com/article/10.1007/s40808-021-01211-7)

* [Real-time prognosis of flowing bottom-hole pressure in a vertical well for a multiphase flow using computational intelligence techniques](https://link.springer.com/article/10.1007/s13202-019-0728-4)

* [Machine learning models to predict bottom hole pressure in multi-phase flow in vertical oil production wells](https://onlinelibrary.wiley.com/doi/abs/10.1002/cjce.23526)

* [An Intelligent Solution to Forecast Pressure Drop in a Vertical Well Having Multiphase Flow Using Functional Network Technique](https://onepetro.org/SPEPATS/proceedings-abstract/18PATC/All-18PATC/215853)

# License

[MIT](https://github.com/ashrafalaghbari/ProBHP/blob/main/LICENSE)


# Contact

If you have any questions or encounter any issues running this project, please feel free to [open an issue](https://github.com/ashrafalaghbari/ProBHP/issues). I'll be happy to help!


",1,1,1,0,oil-and-gas,"[explainable-ai, feature-importance, flowing-pressure, multiphase-flow, oil-and-gas, production-optimization, shap]",00000,
davidcasr,notebooks-oil-and-gas,,https://github.com/davidcasr/notebooks-oil-and-gas,https://api.github.com/repos/notebooks-oil-and-gas/davidcasr,Exploration of data and technologies of the oil and gas sector,"# 🛢️ Notebooks Oil and Gas

Exploration of data and technologies of the oil and gas sector

01 - Introduction to Welly

## Installation

Create a virtual env
```
virtualenv venv
```

Install the requirements

```
pip install -r requirements.txt
```

Made with ❤️ by @davidcasr",1,1,1,0,oil-and-gas,"[oil-and-gas, python]",00000,
acsours,etl_coal_data,,https://github.com/acsours/etl_coal_data,https://api.github.com/repos/etl_coal_data/acsours,"Extract, transform, and load coal pollution data into SQL database. ","# ETL_Project

**Group members: Anna Sours, Anna Kantor, Andre Shearer, Lisa Caruana**

## Examination of International trends in coal electricity production, air pollution levels and health impacts by country (2000 - 2017)


**Goal:** Create a database that will allow users to examine connections between the number of coal power plants in a country (by megawatt hour), air pollution levels and recorded deaths from air pollution per 100,000 people between 2000 - 2017.

Our group identified this as a gap in available databases. While there is an abundance of data available on each individual component, there is a lack of databases that combine data on electricity generated by coal power plants, air pollution levels and mortality rates from air pollution.


### Project Proposal:
Questions we were trying to answer include:
1.	Is there a measurable change in health effects as regions move from coal to renewables? 
2.	What countries are increasing electricity generation by coal fired power plants, decreasing their use and/or staying constant?
3.	Are there any noticeable impacts on air pollution levels with changes in electricity generation by coal fired power plants?
4.	Are there any noticeable changes in mortality rates from air pollution with changes in coal fired electricity production?


### Extract:
We collected the data from the following websites:
* Coal Power Generation 
 1.	New coal plants by country (Source: Global Energy Monitor) (https://docs.google.com/spreadsheets/d/1W3pt5FhqitHwbVWvvgfRr0S6QfqfOuea9pt3-Mlxp5M/edit#gid=1682876416)   
 2.	Retired coal plants by country (Source: Global Energy Monitor) (https://docs.google.com/spreadsheets/d/1W3pt5FhqitHwbVWvvgfRr0S6QfqfOuea9pt3-Mlxp5M/edit#gid=1682876416)

* Air Pollution Levels
 Source: OECD Air Pollution Exposure databank (https://data.oecd.org/air/air-pollution-exposure.htm#indicator-chart)
 1.	OECD (2021), Air pollution exposure (indicator). doi: 10.1787/8d9dcc33-en (Accessed on 10 March 2021)
 2.	OECD (2021), Air pollution effects (indicator). doi: 10.1787/573e3faf-en (Accessed on 10 March 2021)
 3.	OECD (2021), Air and GHG emissions (indicator). doi: 10.1787/93d10cf7-en (Accessed on 10 March 2021) 
* Mortality Rates from Air Pollution 
 1.	Air Pollution Exposure and Effects (Source: Kaggle) (https://data.oecd.org/air/air-pollution-exposure.htm#indicator-chart)


### Transform:
The data cleaning and shaping were performed in Jupyter Notebook by using Pandas. The main part of cleaning and transformation included dropping unnessesary columns, renaming columns, creating and merging dataframes. All datasets were checked for duplicates and null values. However, we decided not to exclude null values from the resultset as it would be valuable data for future analysis.


### Load:
The final data was loaded into PostgreSQL database by using SQLAlchemy. We decided to use a relational database to maintain data integrity, optimize data storage by normalizing the data to related tables for futere flexibility and comparisons between the data.

",0,0,1,0,coal,"[air-pollution, coal, coal-electricity-production, coal-plants]",00000,
MosGeo,BPSMAutoToolbox,,https://github.com/MosGeo/BPSMAutoToolbox,https://api.github.com/repos/BPSMAutoToolbox/MosGeo,This is the repository for the Stanford BPSM Automation Toolbox. The library can be used for creating and simulating basin and petroleum system models for studying probabilistic interactions.,"# The BPSM Automation Toolbox for Probabilistic Interactions

The BPSM Automation toolbox is a library to automate the creation, modification, and running of models used in the Schlumberger PetroMod software. The code is written in MATLAB (Octave support has not been tested).

<div align=""center"">
    <img width=800 src=""https://github.com/MosGeo/BPSMAutoToolbox/blob/master/ReadmeFigures/Workflow.png"" alt=""TopImage"" title=""Image of particle pack""</img>
</div>

## Capabilities
- A template PetroMod project can be loaded into Matlab. 
- Lithologies can be duplicated, mixed, modified, and deleted. 
- Models can be duplicated, deleted, and simulated directly from Matlab. 
- 1D, 2D, and 3D models can be modified.
- Custom open simulator scripts can be ran (OpenSimulator license is required). Using this, results can be read back into Matlab.

## Getting started
- Create your PetroMod 2017.2 or newer ""template project"" (recent older versions should work too).
- Create your output script in python to export output if needed (see below).
- Simulate your template project and make sure everything works.
- See the ""main.m"" for the procedure to load your template project in Matlab, modify parameters, duplicate models and simulate them in Matlab. You can also read the report accomponied in the repository and the workshop material included.

## General tips
- Save your PetroMod project in a folder that does not require administrator privileges.
- It is better to create all the required lithologies in one go before updating the project as writing the lithology files takes relatively long time (a couple of seconds), i.e., do not update the project in a for loop.
- If you are using lithology mixing, it is better to make sure everything is consistant by creating some mixes manually and comparing it to the mixes created by Matlab.
- Some parameters are internally saved with different unit than the one that is displayed in the PetroMod GUI.

## Open Simulator scripts tips
- Open Simulator requires Python to be installed in the system. The version of required Python in 2018 or older is 2.7.
- Check out scripts folder in PetroMod script folder for example (e.g., ""C:\Program Files\Schlumberger\PetroMod 2016.2\scripts"")
- To activate your script, in the Simulator window, choose ""Output"", ""Open Simulator"" and select your script. Make sure your script run in the template project. Save the simulator window.
- Another option is to run the written script directly from matlab as given in the example file.

## Future plans
The basic framework is implemented now. Most needed functionalites can be automated in Matlab. Feature addition is on hold for now and new features will be added as needed. If you have suggestions, bugs, or feedback, please contact me through Github or Email. I would love to hear from you. What are the functionalities that you use the most? What are the things that you cannot do with it? Do you have a bug to report? You can create an Issue on GitHub or reach me directly at Mustafa.Geoscientist@outlook.com

## Referencing
Al Ibrahim, M. A., 2019, Petroleum system modeling of heterogeneous organic-rich mudrocks, PhD Thesis, Stanford University, p. 131-135.
",4,4,3,7,petroleum,"[basin-modeling, bpsm, geology, hydrocarbons, petroleum]",00000,
dianaceroallard,RatonBasin_Geothermal,,https://github.com/dianaceroallard/RatonBasin_Geothermal,https://api.github.com/repos/RatonBasin_Geothermal/dianaceroallard,Raton Basin Colorado well log data collection and geothermal modeling.,"### Raton Basin Colorado. Geothermal Data Collection. Preliminar Thermal Assessement from Oil an Gas Well Data

### MAIN GOALS

#### - Visualization of public geothermal data for Raton Basin 

#### - Extraction and analysis of public formation tops, electric logs, production data

#### - Classification of thermofacies using unsupervised machine learning algorithms 

#### - Build a geotermal conceptual model for the basin

#### - Geothermal economic assessment of the basin


        
        


",6,6,1,0,well-logs,"[colorado, geothermal, machine-learning, well-logs]",00000,
almersawi,log-reader,,https://github.com/almersawi/log-reader,https://api.github.com/repos/log-reader/almersawi,"Node.js application for parsing, viewing and make calculations on .las files","# logReader
### Configurations 
We use Node.js and create server of port: 3000
<br>

```javascript
app.listen(3000, ()=> {
        console.log('Listen to port: 3000');
    });
```

<br>
After connecting to mongoDB, which we use to store our parsing data

### Use las-js package from npm 

```javascript
// for parsing las files 
const Lasjs = require('las-js');
```

in the ""routes"" folder, you can find userRoutes.js where you can add and remove the routes you need. As example: 

```javascript
router.get('/', userController.getIndex);

router.get('/:filename/:header', userController.getLog);

router.get('/:id/multi/plot', userController.getMulti)
```

As we use ""MVC"" you can find your controllers in the ""controllers"" folder and code your desired function based on the router you choosed.

### Parsing the .las file
First, we upload the file to the ""uploads"" folder and then, with the help of las-js package and async function you can read the file, get your desired data. <br>

After that, and the most important, we changed the data from array format to ogject format to suit the model we have created on the ""models"" folder and save it to our mongoDB database.

```javascript
const finalData = {};
for (var i=0; i < headers.length; i++){
var rowArray = [];
data.forEach(row => {
rowArray.push(row[i]);
});
finalData[headers[i]] = rowArray;
                  }
```

Creating our object and save it to our database

```javascript
const myData = new Lasfile({
                    fileName: filename,
                    headers: headers,
                    headersDesc: headerAndDescr,
                    finalData: finalData
                        });
myData.save()
```

### Display logs

we create a dynamic router containg the filename stored in our database and the desired header to view.

```javascript
router.get('/:filename/:header', userController.getLog);
```

Depending on the header we can find the desired array of data, then render the ejs page with that information. <br>

to display logs, we use D3.js which allows us to represent our data on svg graph created by our own.

### Multi-plot
First, we render the ejs page with a data obtained from our data base. These data contain the object with keys equlas to the headers and each key has its own data of array.
```javascript
exports.getMulti= (req, res, next) => {

    const id = req.params.id;
    Lasfile.findById(id)
    .then(data => {
        const headers = data.headers;
        const depthArr = data.finalData.DEPT;
        res.render('multi-plot', {
            pageTitle: ""Multi Plot Area"",
            headers: headers,
            depthArr: depthArr,
            finalData: data.finalData
        });
    })
    .catch(err => {
        console.log(err);
    })

}
```

The most important part is to detect the selected arrays that you want to plot 

```javascript
$(document).ready(function() {
      $("":checked"").each(function(){
        selectedArray.push($(this).val());
      });
```
After that we send these data to D3.js to plot
",5,5,1,6,well-logs,"[lasfile, logging, nodejs, well-logs]",00000,
daeIy,PyNPEFA,,https://github.com/daeIy/PyNPEFA,https://api.github.com/repos/PyNPEFA/daeIy,A Python implementation of integrated prediction error filter analysis (INPEFA).,"# PyNPEFA
 Generate an integrated prediction error filter analysis (INPEFA) curve based on given data. This program uses the l1 trend filtering algorithm before making the prediction error filter, and implements CVXOPT, Spectrum, lasio, and SciPy package. Check requirements.txt to install the required packages.
 
 Well log LAS file example is obtained from http://www.kgs.ku.edu/WellLogs/kcc_logs_2019/1051308423.zip
 
 PyNPEFA implementation example can be seen in ./example.ipynb (Jupyter Notebook)
 
",5,5,0,0,well-logs,"[inpefa, l1-trend-filtering, prediction-error-filter, well-logs]",00000,
hydrospanner,lasio-test,,https://github.com/hydrospanner/lasio-test,https://api.github.com/repos/lasio-test/hydrospanner,use lasio to analyze and plot digital well log files,,5,5,2,1,well-logs,"[petroleum-engineering, python, well-logs]",00000,
dcslagel,las-util-cpp,,https://github.com/dcslagel/las-util-cpp,https://api.github.com/repos/las-util-cpp/dcslagel,LAS  (Log Ascii Standard v2.0) parser in c++: beta-level-software,"NAME
----
LAS-Util-Cpp - LAS parser in C++

TABLE-OF-CONTENTS
-----------------
- [DESCRIPTION](#description)
- [SYNOPSIS](#synopsis)
- [OPTIONS](#options)
- [INSTALL-COMPILE-AND-RUN](#install-compile-and-run)
- [EXAMPLES](#examples)
- [PROJECT-ROADMAP](#project-roadmap)
- [FEATURE-REQUEST](#feature-request)
- [BUGS](#bugs)
- [COPYRIGHT](#copyright)

[DESCRIPTION](#name)
------------

Caution: This is beta software!

basic LAS (Log Ascii Standard) well-log parser in c++

`lasUtil` partially reads a las formatted file. 

The parser currently parses and displays the following sections:
- Version
- Well-Information
- Curve
- Parameter
- Other

The current goal of LAS-Util are:
- Parse LAS header meta-data records(lines)
- Explore the LAS file format specifications
- Explore design decisions related to CPP-Lang

LAS file format versions are written and maintained by   
the Canadian Well Logging Society at    
https://www.cwls.org/products/

[SYNOPSIS](#name)
----------

Usage: lasUtil -f <las_filename> [-p <sections_to_print>]    
     
Sections to print:    
Specify which sections to display by listing the letters following '-p'    

|Letter  | Section  |
|--------|----------|
|v       | Version Information Section  |
|w       | Well Information Section  |
|c       | Curve Section
|p       | Log Parameter Section  |
|o       | Other Section  |
|a       | Drilling Data Section  |

[INSTALL-COMPILE-AND-RUN](#name)
-------------------------

Note: Currently validated with GNU's g++-9 compiler

```bash
git clone https://github.com/dcslagel/las-util-cpp
cd las-util-cpp/src  
make clean
make  
cd ..
./src/lasUtil -f examples/sample_2.0.las
```


[EXAMPLES](#name)
---------

* Display help   
`./src/lasUtil -h`

* Display all sections of a given LAS file    
`./src/lasUtil -f examples/sample_2.0.las`

* Display only the well information section of a given LAS file    
`./src/lasUtil -p w -f examples/sample_2.0.las`

* Display the version and well information sections of a given LAS file    
`./src/lasUtil -p vw -f examples/sample_2.0.las`


[OPTIONS](#name)
--------

`-f`
  LAS file to parse

`-p`
  options for displaying section information

`-h`
  display help


[PROJECT-ROADMAP](#name)
----------------
las-util-cpp's project road-map is managed in github milestones at:

https://github.com/dcslagel/las-util-cpp/milestones

1. The current work-in-progress milestone is 0.0.3:

https://github.com/dcslagel/las-util-cpp/milestone/5

- Goals:
  - Add Add a REPL command loop
  - Add additional tests


[FEATURE-REQUEST](#name)
----------------

To request and discuss a potential feature create an issue at:
  - https://github.com/dcslagel/las-util-cpp/issues


[BUGS](#name)
-----

- Functionality is very basic. 

- Report bugs by creating an issue at:
  - https://github.com/dcslagel/las-util-cpp/issues

[COPYRIGHT](#name)
------

Copyright (c) 2019, 2020 DC Slagel and contributors
",4,4,1,7,well-logs,"[las-files, las-util, log-ascii-standard, well-logs, welllogs]",00000,
dcslagel,las-util-django,,https://github.com/dcslagel/las-util-django,https://api.github.com/repos/las-util-django/dcslagel,LAS  (Log Ascii Standard v2.0)  web utilities and api in Django web framework : beta-level software,"NAME
----

LAS Util - LAS web tools in Python/Django

TABLE-OF-CONTENTS
-----------------
- [DESCRIPTION](#description)
- [DEPENDENCIES](#dependencies)
- [SYNOPSIS](#synopsis)
- [REST-API](#rest-api)
- [PROJECT-ROADMAP](#project-roadmap)
- [FEATURE-REQUEST](#feature-request)
- [BUGS](#bugs)
- [COPYRIGHT](#copyright)


[DESCRIPTION](#name)
-----------
Caution: This is beta software!

LAS (Log Ascii Standard - Version 2.0) web utilities in Python/Django.

The current version of LAS-Util is a stand-alone Django site/app combination
that runs on a Django development server.

LAS well log file format versions are written and maintained by    
the Canadian Well Logging Society at      
https://www.cwls.org/products/

`LAS-Util-Django` current functionality:
- Upload a LAS file that includes the VERSION and optionally: WELL, CURVE,
  PARAMETER and optional OTHER sections.
- Parse the VERSION, WELL, CURVE, PARAMETER and OTHER sections.
- Store the parsed meta-data in a SQLite database.
- Display a list of processed files with links to their details.
  Note: Currently LAS-Util renames the uploaded files to `las_file-[datetime]`
- Display detailed data in a table format.
- Provide api for listing uploaded LAS docs and details.
- Provide api for uploading LAS docs.
- Unit testing with data fixtures.
- Test coverage reporting.
- Responsive display on most devices.


LAS-Util has been tested with Django version 4.2.3

The default database is sqlite.

[DEPENDENCIES](#name)
------------

| Component             | Version    |
|-----------------------|------------|
| asgiref               | 3.7.2      |
| coverage              | 7.2.7      |
| Django                | 4.2.3      |
| Django-Rest-Framework | 3.14.0     |
| Pytz                  | 2023.3     |
| Sqlparse              | 0.4.4      |
| Sqlite3               | 3.39.5     |
| typing_extensions     | 4.7.1      |

Django and Django-Rest-Framework can be installed with
```
cd las-util-django/
pip install -r requirements.txt
```

[SYNOPSIS](#name)
---------

  ```bash
  # Setup:

  ## 1. Make workdir.
  mkdir workdir
  cd workdir

  ## 2. Create virualenv.
  python3 -m venv site-venv
  source site-env/bin/activate


  ## 3. Either clone this GitHub repository, or download a release package.

  ### Option 1: Clone the GitHub repository.
  git clone https://github.com/dcslagel/las-util-django

  ### Option 2: Download a release package (either a .zip or a .tar.gz package).
  ###   Packages can be found at: https://github.com/dcslagel/las-util-django/releases.

  ###   Example download cmds:
  ###     Note: Even though these paths say 'archive' rather than 'release' it looks like we
  ###           still need to go to the release url to find these paths.
  curl -L https://github.com/dcslagel/las-util-django/archive/v0.1.2tar.gz -o v0.1.2.tar.gz
  ### or
  wget https://github.com/dcslagel/las-util-django/archive/v0.1.2.tar.gz

  ###  Unpack release package with
  unzip v0.1.2.zip
  ### or (your tar cmd may require different flags).
  tar -xvf v0.1.2.tar.gz


  ## 4. Install python/django dependencies.
  cd las-util-django/
  pip install -r requirements.txt

  ## 5. Prep django database.
  ## cd las-util-django/src/
  cd src
  ## makemigrations should report no new migrations.
  python manage.py makemigrations
  python manage.py migrate


  ## 6. Run test suite: All tests should pass.
  ## If any test fails report test failure at:
  ##  https://github.com/dcslagel/las-util-django/issues
  ##    Include the release number or git commit of las_util_django
  ##    and the test failure text.
  python manage.py test

  ## 7. Run dev web server.
  python manage.py runserver
  ```

  then in a web browser browse to:  
  http://127.0.0.1:8000/upload/

  Select one of these LAS files to upload:
  - las-util-django/src/las_util/example_data/version.las
  - las-util-django/src/las_util/example_data/sample_2.0_well_section.las

  Click 'upload'    

  LAS-Util will:
  - parse the version and well sections and save them to the database

Select the 'Display-Files' menu item. The uploaded file will have the most recent date.

  The resulting data files will be displayed at:  
  http://127.0.0.1:8000/list/


[REST-API](#name)
--------

To upload a LAS doc use a post command:
```bash
# Cd to the example_data directory
cd las-util-django/src/las_util/example_data

# Run a GET in order to retrieve the csrftoken in a cookie.
curl -c cookie.txt http://127.0.0.1:8000/upload/ --silent -S --output /dev/null

# The crsf token is in the 7th field of the cookie.
# optionally: grep csrftoken cookie.txt | cut -f 7
token=$(awk '/.*csrftoken/ {print $7}' cookie.txt)


# Notes:
# In '-F' 'filename' is the name field of:
#   <input type=""file"" name=""filename"" required="""" id=""id_filename"">

#--------------------------------------------------------------------
curl http://127.0.0.1:8000/api/upload/ \
-v \
-X POST \
-F ""filename=@version.las"" \
-H ""X-CSRFToken: ${token}"" \
-H ""Cookie: csrftoken=${token}""
```

To retrieve uploaded LAS docs:
```bash
curl http://127.0.0.1:8000/api/list/
```

To retrieve details of a specific LAS doc in JSON format:

Syntax:    
```bash
curl http://127.0.0.1:8000/api/detail/[filename]    
```

Example:     
```bash
# first retrieve a filename from the pervious 'api/list' call.
# example: las_file-2019-08-29-21-41-42.las
curl http://127.0.0.1:8000/api/detail/las_file-2019-08-29-21-41-42.las
```


[PROJECT-ROADMAP](#name)
----------------
NOTE: This project is NOT being actively developed.

Here is the last roadmap before development stopped.    
`LAS-Util-Django`'s project road-map is managed in github milestones at:
- https://github.com/dcslagel/las-util-django/milestones

The current work-in-progress milestone is 0.1.3:
- https://github.com/dcslagel/las-util-django/milestone/6
- Goals:
  - Add initial parse and display functionality for the ~ASCII (data) section


[FEATURE-REQUEST](#name)
----------------
To request and discuss a potiential feature create an issue at:
- https://github.com/dcslagel/las-util-django/issues


[BUGS](#name)
----

- Functionality is basic.
- Report bugs by creating an issue at:    
  https://github.com/dcslagel/las-util-django/issues

[COPYRIGHT](#name)
---------

Copyright (c) 2019 DC Slagel and contributors
",4,4,1,10,well-logs,"[las-files, las-util, log-ascii-standard, well-logs]",00000,
iykekings,las-js,,https://github.com/iykekings/las-js,https://api.github.com/repos/las-js/iykekings,A zero-dependency JavaScript library for reading/parsing canadian well-log files (.Las files),"⚠️⚠️ This repo is no longer maintained and has been moved to [Las Libraries](https://github.com/laslibs/las-js) ⚠️⚠️

# las-js is a zero-dependency JavaScript library for parsing .Las file (Geophysical well log files).

### Currently supports only version 2.0 of LAS Specification. For more information about this format, see the Canadian Well Logging Society [web page](http://www.cwls.org/las/)

## How to use

- Installing

  > NPM

  ```sh
  $npm install las-js
  ```

  > Yarn

  ```sh
  $yarn add las-js
  ```

  > Browser

  ```html
  <script defer src=""https://cdn.jsdelivr.net/npm/las-js""></srcipt>
  ```

- Usage

  import/require las-js module

  > Node

  ```js
  // common js
  const { Las } = require('las-js');
  // esm
  import { Las } from 'las-js';
  const myLas = new Lasjs(path.resolve(__dirname, `./sample/example1.las`)));
  ```

> Browser

las-js adds a global class Lasjs or using esm

```js
import { Las } from 'las-js';
```

```js
const input = document.getElementById('file-input');
input.addEventListener('change', async e => {
  const file = e.target.files[0];
  const myLas = new Lasjs(file);
});
// or
const myLas = new Lasjs('https://raw.githubusercontent.com/iykekings/las-js/master/src/__test__/sample/A10.las'); // url - only on browser
```

- Read data

  > Use Laspy.data to get a 2-dimensional array containing the readings of each log,
  > Or Lasjs.dataStripped to get the same as above but with all rows containing null values stripped off

  ```js
  async function read() {
    try {
      const data = await myLas.data();
      console.log(data);
      /**
         [[2650.0, 177.825, -999.25, -999.25],
          [2650.5, 182.5, -999.25,-999.25],
          [2651.0,180.162, -999.25, -999.25],
          [2651.5, 177.825, -999.25, -999.25],
          [2652.0, 177.825, -999.25, -999.25] ...]
        */

      const dataStripped = await myLas.dataStripped();
      console.log(dataStripped);
      /**
       [[2657.5, 212.002, 0.16665, 1951.74597],
       [2658.0, 201.44, 0.1966, 1788.50696],
       [2658.5, 204.314, 0.21004, 1723.21204],
       [2659.0, 212.075, 0.22888, 1638.328],
       [2659.5, 243.536, 0.22439, 1657.91699]...]
       */
    } catch (error) {
      console.log(error);
    }
  }
  ```

- Get the log headers


    ```javascript
        // ...
        const headers = await myLas.header();
        console.log(headers);
        // ['DEPTH', 'GR', 'NPHI', 'RHOB']
       // ...
    ```

- Get the log headers descriptions


    ```Js
        //...
        const headerAndDescr = await myLas.headerAndDescr();
        console.log(headerAndDescr)
        // {DEPTH: 'DEPTH', GR: 'Gamma Ray', NPHI: 'Neutron Porosity', RHOB: 'Bulk density'}
        // ...
    ```

- Get a particular column, say Gamma Ray log


    ```Js
        // ...
        const gammaRay = await myLas.column('GR');
        console.log(gammaRay);
        // [-999.25, -999.25, -999.25, -999.25, -999.25, 122.03, 123.14, ...]
        // ...
    ```
    ```Js
        // ...
        // get column with null values stripped
        const gammaRay = await myLas.columnStripped('GR');
        console.log(gammaRay);
        // [61.61, 59.99, 54.02, 50.87, 54.68, 64.39, 77.96, ...]
        // ...
    ```
    > Note this returns the column, after all the data has been stripped off their null values, which means that valid data in a particular column would be stripped off if there is another column that has a null value at that particular row

- Get the Well Parameters

  ### Presents a way of accessing the details of individual well parameters.

  ### The details include the following:

        1. descr - Description/ Full name of the well parameter
        2. units - Its unit measurements
        3. value - Value

  ```Js
    // ...
    const well = await myLas.wellParams()
    const start = well.STRT.value // 1670.0
    const stop = well.STOP.value // 1669.75
    const null_value = well.NULL.value //  -999.25
    // Any other well parameter present in the file, can be gotten with the same syntax above
    // ...
  ```

- Get the Curve Parameters

  ### Presents a way of accessing the details of individual log columns.

  ### The details include the following:

        1. descr - Description/ Full name of the log column
        2. units - Unit of the log column measurements
        3. value - API value of the log column

  ```Js
    // ...
    const curve = await myLas.curveParams()
    const NPHI = curve.NPHI.descr // 'Neutron Porosity'
    const RHOB = curve.RHOB.descr // 'Bulk density'
    // This is the same for all log column present in the file
    // ...
  ```

- Get the Parameters of the well

  ### The details include the following:

        1. descr - Description/ Full name of the log column
        2. units - Unit of the log column measurements
        3. value - API value of the log column

  ```Js
    // ...
    const param = await myLas.logParams(); // 'BOTTOM HOLE TEMPERATURE'
    const BHT = param.BHT.descr // 'BOTTOM HOLE TEMPERATURE'
    const BHTValaue = param.BHT.value // 35.5
    const BHTUnits = param.BHT.units // 'DEGC'
    // This is the same for all well parameters present in the file
    // ...
  ```

- Get the number of rows and columns


    ```Js
        // ...
        const numRows = await myLas.rowCount() // 4
        const numColumns = await myLas.columnCount() // 3081
        // ...
    ```

- Get the version and wrap


    ```Js
        // ...
        const version = await myLas.version() // '2.0'
        const wrap = await myLas.wrap() // true
        // ...
    ```

- Get other information

  ```Js
      // ...
      const other = await myLas.other()
      console.log(other)
      // Note: The logging tools became stuck at 625 metres causing the data between 625 metres and 615 metres to be invalid.
      // ...
  ```

- Export to CSV

  ### For node, this writes a csv file to the current working directory, with headers of the well and data section only for node. For browser, this returns a File Blob, that can be downloaded by using _URL.createObjectURL_

  ```Js
      //...
      await myLas.toCsv('result')
      // result.csv has been created Successfully!
      //...
  ```

  > result.csv

  | DEPT | RHOB    | GR      | NPHI  |
  | ---- | ------- | ------- | ----- |
  | 0.5  | -999.25 | -999.25 | -0.08 |
  | 1.0  | -999.25 | -999.25 | -0.08 |
  | 1.5  | -999.25 | -999.25 | -0.04 |
  | ...  | ...     | ...     | ...   |
  | 1.3  | -999.25 | -999.25 | -0.08 |

  Or get the version of csv with null values stripped

  ```Js
      // ...
      await myLas.toCsvStripped('clean')
      // clean.csv has been created Successfully!
      // ...
  ```

  > clean.csv

  | DEPT | RHOB  | GR   | NPHI  |
  | ---- | ----- | ---- | ----- |
  | 80.5 | 2.771 | 18.6 | -6.08 |
  | 81.0 | 2.761 | 17.4 | -6.0  |
  | 81.5 | 2.752 | 16.4 | -5.96 |
  | ...  | ...   | ...  | ...   |
  | 80.5 | 2.762 | 16.2 | -5.06 |

- Browser and Node Supports

  > las-js is written in typescript and compiles to es6.

  - Browser
    Supports IE 10 and 11 - (doesn't yet support url)
    Doesn't support Opera Mini
  - Node
    Tested 0n 8, 10 and 12

- ## Support
  las-js is an MIT-licensed open source project. You can help it grow by becoming a sponsor/supporter.[Become a Patron!](https://www.patreon.com/bePatron?u=19152008)
",3,3,2,1,well-logs,"[csv, geophysics, las, las-js, parsing, well-logs]",00000,
RamySaleem,Machine-Predict-Lithologies-Using-Wireline-logs,,https://github.com/RamySaleem/Machine-Predict-Lithologies-Using-Wireline-logs,https://api.github.com/repos/Machine-Predict-Lithologies-Using-Wireline-logs/RamySaleem,"To identify lithologies, geoscientists use subsurface data such as wireline logs and petrophysical data. However, this process is often tedious, repetitive, and time-consuming. This project aims to use machine learning techniques to predict lithology from petrophysical logs, which are direct indicators of lithology.","# Machine Predict Lithologies Using Wireline logs
## 1.1 What is machine learning?
Machine Learning (ML) is the method of applying programmatic and statistical computer technology to analyse large datasets, and as a result, uncover new understandings. It is a focussed sub-section of Artificial Intelligence (AI), where a more extensive set of predictive modelling tools enable a computer program to learn by generalising from examples. For more information read https://www.golder.com/insights/machine-learning-and-the-growing-use-cases-for-geoscience-practices/.

## 1.2 Problem Statement:
Identifying subsurface lithologies or rock types is essential for all geoscientists in order to explore our subsurface resources, particularly in the oil and gas industry. Lithology refers to the type of rock that forms the subsurface, and it is classified into, for instance, sandstone, claystone, marl, limestone, and dolomite. Several subsurface data can be used to identify lithologies, such as wireline logs petrophysical data. However, it is often a tedious, repetitive and time-consuming task. This project will predict lithology from petrophysical logs using machine learning techniques (classification) and add to the solution of these problems since these logs are direct proxies of lithology.

## 1.3 What are the classification in machine learning?
The Classification algorithm is a Supervised Learning technique that is used to identify the category of new observations on the basis of training data. In Classification, a program learns from the given dataset or observations and then classifies new observation into a number of classes or groups. In machine learning, classification refers to a predictive modeling problem where a class label is predicted for a given example of input data.

## 1.4 Wireline Logs Datasets
The data consist of 118 wells dataset spans through the South and North Viking graben and penetrates a highly variable geology from the Permian evaporites in the south the the deeply buried Brent delta facies in the North. An investigation of the provided training data clearly shows that the lithologic record offshore Norway is dominated by shales and shaly sediments. This is followed by sandstones, limestones, marls and the tufts.

The provided dataset contains well logs, interpreted lithofacies and lithostratigraphy for 90+ released wells from offshore Norway. The well logs include the well name (WELL), the measured depth, x,y,z location for the wireline measurement as well as the well logs CALI, RDEP, RHOB, DHRO, SGR,  GR, RMED, RMIC, NPHI, PEF, RSHA, DTC, SP, BS, ROP, DTS, DCAL, MUDWEIGHT. An explanation of the abbreviations is shown in the figure below.

The las files of the petrophysical logs dataset can be found [here](https://zenodo.org/record/4351156#.YXhFTBrMJPb).

## Results

### Well-1
![Well number 1](https://i.imgur.com/XSdNzX3.png)

### Well-2

![Well_number_2](https://i.imgur.com/YOrMKvo.png)

### Well-3

![Well_number_3](https://i.imgur.com/6gUOQew.png)

## Conclusions

Considering this work, the key messages and conclusion of this work could be summarised as follows:
    
1. The project results show that the XGboost and Catboost classifiers recorded higher accuracy on the training and validation datasets.

2. The XGboost was recorded 93.2% on the training dataset and 77.6% on the validation dataset, while the Catboost was recorded 85.6% on the training dataset and 76.7% on the validation data. 

3. We have created the Ridge Classifier model to impute the missing values of the categorical columns. Moreover, we have applied the k-Fold grid and random search to explore the dataset besides exploring the hyperparameters of the decision tree and random forest models.

4. The subsurface prediction of lithology in geoscience engineering deals with sparse petrophysical datasets with tens of features. One approach for feature selection is using entirely data-driven techniques such as selecting the whole data after imputing the missing values. The other approach is to use the data with actual values without filling the missing values. We found here that the Gamma-ray (GR), neutron density () and well location (X_log, Y_log) features carry essential geological information and could be used as an alternative representation of lithology when predicting the lithology from petrophysical wireline logs, especially for the oil and gas industry, for example.

5. The machine learning-based model could provide a very efficient and fast proxy for complex and slow complete physical-based lithology prediction, which in application like optimisation, could be very helpful. However, two areas need further attention:
    
    5.1. When going from a more simplified model to a more complex model, accuracy will be increased. However, the research in ML application must have analysis about the trade-off between speed and accuracy.
    
    5.2. In the ML model developed in this work, it was assumed that the training dataset here is “petrophysical wireline logs” that was fed to the ML to make a prediction about the subsurface lithologies. In fact, that training dataset is the ‘basin-wide sample’ while we are considering making a prediction about the lithology. Here, the challenging statistical question comes that how much training dataset is representative of the subsurface lithology of the dataset?

6. In the framework of decision analysis, in this study, the value of machine learning in a particular predicting subsurface lithologies was analysed. This analysis will be helpful for the companies to predict the lithologies based on their wireline logs data.

## Future Work

• This intensive geological dataset can be used to explore more classic machine learning models with better hyperparameters tuning.

• Apply deep learning methodology to predict the lithology using the same dataset.

• Utilise the free google cloud GPU to model the data using artificial neural networks such as CNN using PyTorch library.

## References

• Machine Predicted Lithology competition  by Force and Xeek 2020 Machine learning .  https://xeek.ai/challenges/force-well-logs/overview

• (Bormann P., et al. 2020). 2020 FORCE Machine Learning Contest. https://github.com/bolgebrygg/Force-2020-Machine-Learning-competition.

• FORCE 2020 Well well log and lithofacies dataset for machine learning competition https://zenodo.org/record/4351156#.YXmi1Z7MJPb.


Acknowledgements 
=================
The work contained in this repositories contains work conducted during a PhD study undertaken as part of the Natural Environment Research Council (NERC) Centre for Doctoral Training (CDT) in Oil & Gas funded 50% through its National Productivity Investment Fund grant number NE/R01051X/1 and 50% by the University of Aberdeen through its PhD Scholarship Scheme. The support of both organisations is gratefully acknowledged. The work is reliant on Open-Source Python Libraries, particularly numpy, matplotlib, Scikit-learn, XGBoost and pandas and contributors to these are thanked, along with Jovian and GitHub for open access hosting of the Python scripts for the study.

![University of Aberdeen](https://pbs.twimg.com/profile_images/1572172791801061377/UPSWmPyN_400x400.jpg)

![NERC-CDT](https://nerc-cdt-oil-and-gas.ac.uk/wp-content/uploads/news/2015-news-NERC-funding.jpg)

![NERC](https://auracdt.hull.ac.uk/wp-content/uploads/2019/11/UKRI_NER_Council-Logo_Horiz-RGB.png)

![CDT](https://i.imgur.com/QDOhcN3.png)

",2,2,1,0,well-logs,"[catboost-model, classification, lithology, lithology-prediction, log-analysis, log-interpretation, los-classification, machine-learning, subsurface-data, well-logs, wire-line-logs, xboost]",00000,
RamySaleem,Reservoir-Quality-in-Producing-Sandstones,,https://github.com/RamySaleem/Reservoir-Quality-in-Producing-Sandstones,https://api.github.com/repos/Reservoir-Quality-in-Producing-Sandstones/RamySaleem,"This project will explore, analyse and visualise publicly available wells datasets from the United States offshore data centre, the USGS boreholes website - Bureau of Safety and Environmental Enforcement (BSEE) https://www.data.bsee.gov/Main/Default.aspx with a particular focus on the Gulf of Mexico (GOM) wells. This project will study sandstones quality as a reservoir, the production history of the operators on the Gulf of Mexico and a well summary report to highlight any possible problem. The reservoir quality analysis will examine relationships between average values of porosity, permeability, depth, temperature, pressure, thickness, age, and play type for data files from 2009 until 2019.The porosity plotted and shown in a wide range of plots as a function of permeability and burial depth. Also, the median (P50) porosity will be plotted against depth to examine the porosity trend. Moreover, this project will investigate the companies oil and gas production in the gulf of Mexico for the last five years. Lastly, the analysis will include an investigation of well summary reports of five wells. The project will include web scrapping to collect online well summary reports to generate a word cloud. The project results can be useful for specifying realistic distributions of parameters for both exploration risk evaluation and/or reservoir modelling by machine learning algorithms in the next project. ","# Exploratory-Data-Analysis
## Reservoir-Quality-in-Producing-Sandstones
This project will explore, analyse and visualise publicly available wells datasets from the United States offshore data centre, the USGS boreholes website - Bureau of Safety and Environmental Enforcement (BSEE) https://www.data.bsee.gov/Main/Default.aspx with a particular focus on the Gulf of Mexico (GOM) wells. This project will study sandstones quality as a reservoir, the production history of the operators on the Gulf of Mexico and a well summary report to highlight any possible problem. The reservoir quality analysis will examine relationships between average values of porosity, permeability, depth, temperature, pressure, thickness, age, and play type for data files from 2009 until 2019.The porosity plotted and shown in a wide range of plots as a function of permeability and burial depth. Also, the median (P50) porosity will be plotted against depth to examine the porosity trend. Moreover, this project will investigate the companies oil and gas production in the gulf of Mexico for the last five years. Lastly, the analysis will include an investigation of well summary reports of five wells. The project will include web scrapping to collect online well summary reports to generate a word cloud. The project results can be useful for specifying realistic distributions of parameters for both exploration risk evaluation and/or reservoir modelling by machine learning algorithms in the next project. 

• Collected & merged 100,000+ rows of Sandstone reservoirs data files dataset using the Pandas library.

• Analysed & Visualised sandstone reservoir dataset using Pandas, Numpy, Matplotlib, Plotly & Folium libraries

• Discovered insights about reservoir porosity & permeability, well operation problems and production performance


![Project summary](EDA-Ramy-05062021.gif)

For more information and summary presentation, please watch our video: 
""https://www.youtube.com/embed/Hsl8wnVqViY""


Acknowledgements 
=================
The work contained in this repositories contains work conducted during a PhD study undertaken as part of the Natural Environment Research Council (NERC) Centre for Doctoral Training (CDT) in Oil & Gas funded 50% through its National Productivity Investment Fund grant number NE/R01051X/1 and 50% by the University of Aberdeen through its PhD Scholarship Scheme. The support of both organisations is gratefully acknowledged. The work is reliant on Open-Source Python Libraries, particularly numpy, matplotlib, plotly and pandas and contributors to these are thanked, along with Jovian and GitHub for open access hosting of the Python scripts for the study.

![University of Aberdeen](https://pbs.twimg.com/profile_images/1572172791801061377/UPSWmPyN_400x400.jpg)

![NERC-CDT](https://nerc-cdt-oil-and-gas.ac.uk/wp-content/uploads/news/2015-news-NERC-funding.jpg)

![NERC](https://auracdt.hull.ac.uk/wp-content/uploads/2019/11/UKRI_NER_Council-Logo_Horiz-RGB.png)

![CDT](https://i.imgur.com/QDOhcN3.png)


",2,2,1,0,well-logs,"[data-analysis, data-analysis-python, data-visualisation, dataset, datasets, eda, lithology, lithology-logs, pandas, python, well-logs, wireline]",00000,
roguLINA,transformers_for_oil_gas,,https://github.com/roguLINA/transformers_for_oil_gas,https://api.github.com/repos/transformers_for_oil_gas/roguLINA,Robust representations of oil wells' intervals via sparse attention mechanism,"Robust representations of oil wells' intervals via sparse attention mechanism
=====

Code for experiments from the article of the same name. Include framework `trans_oil_gas` for dataset of well-intervals generation and training and testing Transformer-based (with Transformer, Informer, and Performer) Siamese and Triplet models. 

Installation of `trans_oil_gas`:
-----
1. Clone this repository
2. Install all necessary libraries via command in terminal: `pip install transformers_for_oil_gas/`
3. Use our framework via importing modules with names started with `utils_*` from `trans_oil_gas` 

Reproducing experiments from the article
-----
To reproduce all our experiments from the article ""Similarity learning via Transformers: representing time series from oil\&gas"":
1. Open `notebooks` folder.
2. Run jupyter notebook `all_models.ipynb`. It will train all models (Siamese and Triplet Transformer, Informer, and Performer).
3. Run experiments in other notebooks. It will use the pretrained models obtained in step 2. 

License
-----
The project is distributed under [MIT License](https://github.com/roguLINA/transformers_for_oil_gas/blob/main/License.txt).
",2,2,2,0,well-logs,"[deep-learning, informer, performer, representation-learning, similarity-learning, transformer, well-logs]",00000,
firasisme,GA-vs-NeuralNetwork,,https://github.com/firasisme/GA-vs-NeuralNetwork,https://api.github.com/repos/GA-vs-NeuralNetwork/firasisme,[MATLAB inside] Comparative research well log prediction: Genetic algorithm vs Neural Network ,"# GA-vs-NeuralNetwork
[![Website](https://img.shields.io/badge/website-visit-brightgreen)](https://firasisme.github.io/)

Comparative research well log prediction: Genetic algorithm vs Neural Network 

**Data**

[Ikpikpuk 1 Log Data (LAS)](https://pubs.usgs.gov/of/1999/ofr-99-0015/Wells/Ikpik1/LAS/IK1LAS.htm)

[Ikpikpuk 1 Drilling and Geologic Reports](https://pubs.usgs.gov/of/1999/ofr-99-0015/Wells/Ikpik1/PDF/IK1PDF.htm)

**Result**

![Image](https://github.com/firasisme/GA-vs-NeuralNetwork/blob/master/GA%20vs%20NN.png)

## Abstract
P-wave velocity is a very important parameter in exploration activities. P-wave velocity (Vp) can be determined from wireline logging data. Generally, the industry only does logging at certain depths which are considered to have prospects in order to save exploration costs. Missing wireline logging data will certainly be a serious problem because it requires complete and accurate data so that the chances of exploration success are high. A method is needed to estimate Vp using data other than sonic log. This research aims to estimate Vp based on available log data using Genetic Algorithm (GA) method and Neural Network (NN). The inversion process is carried out using the method in the Ikpikpuk1 well until the relationship of Vp is obtained with the gamma ray log, resistivity log and density log. The next process estimating Vp by blind test on the same well but the depth is different from inversion. The results showed that the Neural Network method was superior to the Genetic Algorithm method. In the three formations that are the object of research the Neural Network method is consistent because the estimation error is smaller than the Genetic Algorithm method.
## requirement for this code
- [MATLAB 2017 or later](https://www.mathworks.com/)
- 4GB RAM or more
- processor 2 cores or more

## References
DOI: [10.13140/RG.2.2.32222.59209](https://www.researchgate.net/publication/335907884_Estimation_of_P-Wave_Velocity_with_Genetic_Algorithm_and_Neural_Network_Approach_Based_on_Wireline_Logging_Data?channel=doi&linkId=5d82d90ca6fdcc8fd6f3b1ae&showFulltext=true)
",2,2,1,0,well-logs,"[genetic-algorithm, geophysical-inversions, inversion, neural-network, well-logs]",00000,
luisteran5296,Interpretacion-automatizada-de-registros-geofisicos-de-pozos,,https://github.com/luisteran5296/Interpretacion-automatizada-de-registros-geofisicos-de-pozos,https://api.github.com/repos/Interpretacion-automatizada-de-registros-geofisicos-de-pozos/luisteran5296,El objetivo de este proyecto es obtener un método tal que la computadora sea capaz de realizar una interpretación de registros geofísicos de manera automática y sin intervención humana alguna dado un set de datos que contenga registros geofísicos.,"<h1 align=""center"">Interpretación automatizada de registros geofísicos de pozos</h1> 
_______________________________________________________________________________________________________________________________


<h5 align=""center"">Luis Terán</h5> 

La aplicación web completa se encuentra disponible en:

</br><center> https://registro-pozos-turae.herokuapp.com/</center>


## Objetivo

El objetivo de este proyecto es obtener un método tal que la computadora sea capaz de realizar una interpretación de registros geofísicos de manera automática y sin intervención humana alguna dado un set de datos que contenga registros geofísicos.

## Desarrollo 

El procedimiento realizado por el programa es cómo se describe a continuación:

Lectura de datos
Al iniciar la ejecución del programa se solicitan diversos parámetros para elaborar la corrección e interpretación de los registros en cuestión, en caso de no ser ingresados se asignarán valores que no ejecutan ninguna corrección a los datos o bien, que permitan la ejecución del programa. Los parámetros solicitados son: 

- Archivo: Elección del archivo extensión LAS que contiene los registros geofísicos.
- FGR: Factor de corrección para el registro de Rayos gamma. Se recomienda usar las cartas de Schlumberger ""Schlumberger - Log Interpretation charts"", Sección 2-1, 2-2 y 2-3. El valor por defecto en caso de omisión es 1.
- FSP: Factor de corrección de para el registro de Potencial Espontáneo. Se recomienda usar las cartas de Schlumberger ""Schlumberger - Log Interpretation charts"", Sección 2-5, 2-6, 2-7, 2-8, 2-9, 2-10. El valor por defecto en caso de omisión es 1.
- FDEN: Factor de corrección para el registro de densidad. Se recomienda usar las cartas de Schlumberger ""Schlumberger - Log Interpretation charts"". El valor por defecto en caso de omisión es 0.
- FNPHI: Factor de corrección para el registro de Rayos gamma. Se recomienda usar las cartas de Schlumberger ""Schlumberger - Log Interpretation charts"". El valor por defecto en caso de omisión es 0.
- FRSOM: Factor de corrección para el registro de resistividad somera. Se recomienda usar las cartas de Schlumberger ""Schlumberger - Log Interpretation charts"", Sección 6-1,6-2,6-3,6-5,6-7. El valor por defecto en caso de omisión es 1.
- FRMED: Factor de corrección para el registro de resistividad media. Se recomienda usar las cartas de Schlumberger ""Schlumberger - Log Interpretation charts"", Sección 6-1,6-2,6-3,6-5,6-7. El valor por defecto en caso de omisión es 1.
- FRPROF: Factor de corrección de para el registro de resistividad profunda. Se recomienda usar las cartas de Schlumberger ""Schlumberger - Log Interpretation charts"", Sección 6-1,6-2,6-3,6-5,6-7. El valor por defecto en caso de omisión es 1.
- PMA: Densidad promedio de la matriz. El valor por defecto en caso de omisión es 2.45.
- PF: Densidad promedio de fluido. El valor por defecto en caso de omisión es 1.7.
- LMA, LFL: Lentitud promedio de la matriz y del fluido respectivamente.
- A: Factor de tortuosidad. El valor por defecto en caso de omisión es 1.
- M: Exponente de cementación. El valor por defecto en caso de omisión es 2.
- N: Exponente de saturación. El valor por defecto en caso de omisión es 2
- Método: Método para calcular el volumen de arcilla, El valor por defecto en caso de omisión es el método lineal.
- Nc: Número de litologías presentes, en caso de no describirse se determinará de forma automática.

## Identificación de curvas

Del archivo leído, dentro del cuadro descriptivo de este aspecto se extrae el nombre de las curvas, sus unidades respectivas y su descripción.
Posteriormente, se declara un diccionario que contiene posibles nombres, palabras clave o mnemónicos utilizados para describir el nombre de las curvas, así como las posibles unidades de las curvas y descripciones relacionadas con las curvas.
Para la identificación de las curvas se revisa por cada elemento en el diccionario la identificación de caracteres que se relacione con los nombres de las curvas extraídos de los archivos. 
La identificación se realiza con base en identificación de caracteres contenidos o bien, con el mnemónico de su clase. Por ejemplo:

Contenido del diccionario: “CAL”
Elementos que identificaría: “CALIPER”, “CALIP”, ”CAL” ,”CALI”, “RCALIPER”, “REGCALI”

En caso de no encontrar coincidencias por nombre, se repite el procedimiento anterior con las descripciones del archivo en comparación con las declaradas en el diccionario. Y en caso de no volver a encontrar coincidencias, se repite el proceso por tercera ocasión pero ahora con las unidades.

<img src=""images/1.jpg"" alt=""Figure 1"" style=""width: 600px;""/><p style=""text-align:center;font-size: 11px;"">Porción del contenido del diccionario de búsqueda</p>


## Revisión de datos disponibles y procedimiento a seguir

Una vez realizada la identificación de curvas, se realiza una revisión de que curvas se pudieron encontraron y cuales no pudieron ser identificadas, ya sea por falla del programa o porque no se encontraba el registro en el archivo LAS.
Con base en el registro de curvas identificadas se determina qué cálculos y gráficas se podrán realizar a lo largo del programa restante, así como los resultados que se entregarán.

## Corrección y cálculo de curvas
En la etapa de corrección de curvas, debido a la complicación que estas generaban, decidimos optar por la aplicación de correcciones de manera simplificada. Es decir, que el factor de corrección necesario sea obtenido de manera manual mediante aplicación de tablas para posteriormente aplicarse directamente a los registros del archivo

Posteriormente, el cálculo de las curvas de porosidad, saturación y volumen de arcilla se realizó de la siguiente manera:

Fórmula de temperatura de formación:

$ Tf = Ti = Ts + [(Tmax -Ts) / Dmax]*Di $

Fórmula de la múltiplicación del registros de resistividad por el factor de corrección:

$ Rmi = Rm[(Ts + C)/(Ti + C)] $
$ Rmfi = Rmf[(Ts + C)/(Ti + C)] $
$ Rmci = Rmc[(Ts + C)/(Ti + C)] $

Fórmula de resistividad de agua de fórmación:

$ Rweq = 10^(K*log(Rmfsq)+SP)/K; Rmfsq = Rmfe; K=61+0.133*Ti $

Fórmula para el cálculo del agua de formación:

$ Rw = \frac{Rweq + 0.131*10^{[1/log(BHT/19.99]-2.0}}{-0.5*Rweq+10^{[0.0426*log(BHT/50.8)]}} $

Fórmula para el índice de arcillosidad:

$ Igr = (GRlog - GRmin) / (GRmax - GRmin) $

Fórmula para el cálculo de volumen de arcilla de Clavier:

$ Vshale = 1.7*[3.38*(Igr+0.7)^{2}]^{0.5} $

Fórmula para el cálculo de volumen de arcilla de Steiber:

$ Vshale = Igr / (3.0 - 2.0*Igr) $

Fórmula para el cálculo de volumen de arcilla de Larionov:

$ Vshale = 0.083*(2^{3.7*Igr}-1) $

Fórmula para el cálculo de porosidad-densidad:

$ \Theta d = (\rho ma -\rho b)/(\rho ma -\rho fl) $


## Interpretación

Para realizar una interpretación, primero es necesario establecer las separaciones entre cada capa de la formación. Sin embargo, diferentes capas pueden pertenecer a una misma litología y repetirse a lo largo de la formación en diferentes intervalos. Para ello se hizo uso de las propiedades de los materiales de la siguiente manera:

Si utilizáramos dos propiedades de cada litología y las graficáramos en un plano bidimensional se verían de la siguiente manera:

<img src=""images/2.png"" alt=""Figure 1"" style=""width: 400px;""/><p style=""text-align:center;font-size: 11px;"">Representación de dos propiedades en un registro</p>

De la misma forma, debido a que las litologías comparten características similares que los describen, si utilizáramos una mayor cantidad de valores discretos como los incluidos en un archivo LAS, se verían de la siguiente manera:

<img src=""images/3.png"" alt=""Figure 1"" style=""width: 400px;""/><p style=""text-align:center;font-size: 11px;"">Representación de dos propiedades en un registro</p>

Y consecuentemente, si lográramos que el programa pudiera diferenciar los grupos y asignar cada punto a su respectiva profundidad podríamos establecer una clasificación de las litologías que existen.

Esto se realizó mediante algoritmos de Machine Learning de clasificación no supervisada utilizando dos enfoques diferentes:
- El primer enfoque (Algoritmo de K-means) parte de la idea de que se conoce el número de litologías presentes, y por lo tanto el número de agrupaciones de puntos, por lo que es necesario adaptar el modelo a la información establecida. Por esa misma razón, es posible obtener resultados más precisos.
- El segundo enfoque (Algoritmo de Mean Shift) toma la idea de que no se conoce el número de litologías presentes, por lo que automáticamente se determinará el número de agrupaciones, pero con menor precisión.
Previo a la aplicación de los algoritmos es necesario aplicar una normalización de los datos para fomentar la convergencia de los algoritmos. Esto es, que cada rango de variación de los registros sea de 0 – 1.

<img src=""images/4.png"" alt=""Figure 1"" style=""width: 400px;""/><p style=""text-align:center;font-size: 11px;"">Agrupación de las propiedades contenidas en el registro</p>

Asimismo, con las diferentes litologías que existen es natural que ciertas litologías compartan características con otra litología presente en la formación, produciendo un efecto de traslape.

<img src=""images/5.png"" alt=""Figure 1"" style=""width: 400px;""/><p style=""text-align:center;font-size: 11px;"">Traslape de puntos respecto a dos propiedades en el registro</p>

Sin embargo, podríamos tomar en cuenta otra propiedad (otro registro geofísico) determinante que diferencie una litología de la otra y graficarla. Esto generaría que se diferencien las litologías.

<img src=""images/6.png"" alt=""Figure 1"" style=""width: 400px;""/><p style=""text-align:center;font-size: 11px;"">Representación gráfica de puntos del registro utilizando tres propiedades</p>

En caso de volver a repetirse podríamos integrar nuevamente una propiedad más y diferenciar. Esto implicaría utilizar cuatro dimensiones lo que resulta complicado de interpretar visualmente para un ser humano, pero resulta factible para una computadora. Incluso se podrían manejar n cantidad de características para diferenciar litologías, por lo que se podría utilizar toda la información disponible para clasificar las litologías, y mientras mayor sea el número de características o registros que se tengan disponibles, será mayor la precisión para diferenciar. 

<img src=""images/7.png"" alt=""Figure 1"" style=""width: 600px;""/><p style=""text-align:center;font-size: 11px;"">Ejemplo de separación de la formación por capas</p>

Una vez realizado este proceso, es posible diferenciar cada una de las capas. No obstante, se desconoce aún a cuál litología pertenecen. Para determinar a qué litología pertenecen se reconstruyó el crossplot de densidad contra porosidad de neutrón.

Como paso inicial, se obtuvieron los valores promedio de las propiedades para cada agrupación obtenida, incluyendo aquellos que se repiten en intervalos separados. 

Para obtener el crossplot, manualmente se observaron aproximadamente doce puntos en la gráfica de cada una de las líneas y se realizó una interpolación polinomial de 5° grado. Una vez obtenidos los coeficientes de la curva, se remuestrearon las curvas con 45 puntos obteniendo la representación completa de las curvas.

<img src=""images/8.png"" alt=""Figure 1"" style=""width: 400px;""/><p style=""text-align:center;font-size: 11px;"">Crossplot de densidad contra porosidad de neutrón</p>

Posteriormente, los valores promedio de densidad y porosidad de neutrón de cada agrupación son colocados en el crossplot. Para identificar a qué curva pertenecen, se calcula la distancia euclidiana con cada uno de los puntos de muestreo de las curvas y se elige aquel que tiene una menor distancia al punto. Y así, se obtiene la información de a qué litología pertenece.

<img src=""images/9.png"" alt=""Figure 1"" style=""width: 400px;""/><p style=""text-align:center;font-size: 11px;"">Discretización de las curvas del crossplot</p>

Finalmente, la información obtenida es expuesta en un gráfico final. Antes de realizar dicho gráfico, debido a que el contenido del gráfico puede variar dependiendo si se cuenta con ciertos registros o no, primero se hace un conteo de gráficos a realizar y consecutivamente se van desplegando las imágenes respectivas en función de los registros o cálculos que se pudieron realizar.

## Herramientas utilizadas

Para el despliegue total del programa se utilizaron diferentes herramientas de trabajo, las cuales son descritas a continuación:

- Python: El procesamiento completo de los datos fue desarrollado en el lenguaje de programación Python, a este proceso interno se le conoce como “Back-end”. Las versiones de prueba y verificación del código fueron desarrolladas en Jupyter Notebook. Se utilizaron diversas bibliotecas que contribuyeron al procesamiento completo del programa, las cuales son:
       -Las: Es una librería encargada de la lectura de archivos .LAS, donde se divide la información en función de los diferentes segmentos que contiene el archivo, lo que facilita el acceso a datos específicos.
       -Pandas: Es una librería para el manejo y operación de datos (similar a Excel), esta librería permitió establecer los datos dentro de un marco organizado. Además permitió realizar los cálculos de manera rápida, sencilla y entendible para la lectura del código.
       -Numpy: Una librería matemática que permite agilizar el manejo de datos, así como realizar operaciones matemáticas tales como interpolaciones, manejo de matrices, ordenamiento de valores, etc.
       -Matplotlib: Es una librería que permite al usuario graficar con un alto grado de libertad.
       -Scikit-learn: Librería de Machine Learning para Python
- Flask: Es un framework de desarrollo para aplicaciones web y de escritorio. Esta herramienta permite conectar el proceso interno con la interfaz gráfica de usuario, es decir, el back-end con el front-end. Para el desarrollo del front-end o interfaz gráfica, se utilizó HTML y CSS dentro del marco referenciado por Flask, que a su vez conecta con el programa en Python. El framework permite el proceso de transferencia de lectura y recepción  de datos de un lenguaje a otro. Al mismo tiempo, este framework permite establecer la conexión con el formato de despliegue.
- Heroku: Es un servidor de despliegue de aplicaciones web gratuito. Permite utilizar el formato creado con Flask y generar un servidor de Python con los elementos necesarios instalados para que sea posible procesar los datos.

## Resultados

Para mostrar el funcionamiento del programa se usó como referencia el archivo “registros.las” otorgado por el profesor Jose Luis Ortiz el semestre pasado en la clase de “Registros Geofísicos de pozos”. El resultado obtenido para ese archivo fue el siguiente:

<img src=""images/10.png"" alt=""Figure 1"" style=""width: 600px;""/><p style=""text-align:center;font-size: 11px;"">Ejemplo del resultado arrojado por el programa</p>

Se optó por utilizar este archivo debido a que la interpretación de este archivo fue utilizado como forma de evaluación del segundo parcial. Posteriormente fueron verificadas las respuestas en clase, por lo que se cuenta con la respuesta correcta de la interpretación de los registros. 

<img src=""images/11.png"" alt=""Figure 1"" style=""width: 600px;""/><p style=""text-align:center;font-size: 11px;"">Resultados verdaderos del archivo utilizado</p>

Utilizando como criterio conocido que existen 3 litologías en la formación, el resultado obtenido por medio del programa fue el siguiente:

<img src=""images/12.png"" alt=""Figure 1"" style=""width: 600px;""/><p style=""text-align:center;font-size: 11px;"">Resultado arrojado por el programa para una formación de tres litologías</p>

Por otro lado, utilizando la clasificación automática se obtiene el siguiente resultado:

<img src=""images/13.png"" alt=""Figure 1"" style=""width: 600px;""/><p style=""text-align:center;font-size: 11px;"">Resultado arrojado por el programa para una clasificación automática</p>

Como se puede observar los resultados obtenidos tanto para la clasificación automática como para la clasificación con número de litologías asignado es muy similar a los resultados verdaderos. Y los límites entre litologías están casi definidos de manera precisa. Si bien esto es debido a que los datos utilizados son datos educativos y por lo tanto bien definidos, también significa que el método es factible a identificar litologías. 

Otro aspecto importante, es que el programa falla en identificar a la lutita. Esto es debido a que únicamente toma dos registros (densidad y porosidad de neutrón) para determinar a qué litología pertenece, y de acuerdo al gráfico realizado estos dos parámetros en ese intervalo coinciden con los de la curva de la dolomita, tanto los de interpretación automática como los de litologías identificadas.

<img src=""images/14.png"" alt=""Figure 1"" style=""width: 400px;""/><p style=""text-align:center;font-size: 11px;"">Crossplot del resultado del programa para tres litologías</p>

<img src=""images/15.png"" alt=""Figure 1"" style=""width: 400px;""/><p style=""text-align:center;font-size: 11px;"">Crossplot del resultado del programapa para clasificación automática</p>

<img src=""images/16.png"" alt=""Figure 1"" style=""width: 800px;""/><p style=""text-align:center;font-size: 11px;"">Resultado arrojado por el programa para una formación de cinco litologías</p>

## Conclusiones

El programa permite hacer la lectura, organización, corrección y cálculos en los datos de manera efectiva. Dentro del procedimiento realizado, se puede destacar lo siguiente:

El programa puede identificar curvas ya sea por nombre, unidades o descripción. Sin embargo, puede haber ciertas curvas que no hayan sido contempladas en el diccionario de identificación. Todas aquellas que no sean identificadas podrían agregarse posteriormente fácilmente al diccionario.

La corrección aún requiere en gran parte de cálculos y procedimientos realizados a mano, ya que resulta muy complejo programar todo el proceso de corrección se simplificó este al máximo de manera que únicamente se aplicaran factores de corrección y solo se ejecutaran aquellos cálculos con fórmula.

La interpretación ofrece resultados razonables. Estamos conscientes de que la interpretación realizada por el programa no es exacta, pero consideramos que una interpretación precisa va más allá de los alcances de este proyecto.


",1,1,1,0,well-logs,"[clustering, kmeans, machine-learning, mean-shift, petrophysics, well-logs]",00000,
MosGeo,LasToolbox,,https://github.com/MosGeo/LasToolbox,https://api.github.com/repos/LasToolbox/MosGeo,A LAS format toolbox,"# Las Toolbox

A LAS file format toolbox for reading Las files in Matlab. All information is loaded in a structured format including headers.

## Compatibility

Currently, the code can only be used with LAS version 2. LAS version 3 is not currently supported.
",1,1,2,1,well-logs,"[borehole, las, well-logs]",00000,
taiwofawumi,DS_python-well-logs-plot,,https://github.com/taiwofawumi/DS_python-well-logs-plot,https://api.github.com/repos/DS_python-well-logs-plot/taiwofawumi,Python custom function for plotting geological well logs,"# well-logs-plot
This is my python function that plots geological well logs in a slightly more physically appealing way. Only the first version! I will continue to add more features.

In this version, I focused on plotting my caliper in a way that makes it easy to identify hole issues. I have normalized the caliper log, but this is an optional step.
",1,1,1,0,well-logs,"[earth-science, geological-well-logs, python, well-logs]",00000,
Iron486,SPWLA_PDDA_SIG_machine_learning_competition,,https://github.com/Iron486/SPWLA_PDDA_SIG_machine_learning_competition,https://api.github.com/repos/SPWLA_PDDA_SIG_machine_learning_competition/Iron486,,"# SPWLA_PDDA_SIG_machine_learning_competition

The aim of this contest was the development of data-driven models to estimate reservoir properties, including **shale volume**, **porosity**, and **fluid saturation**, based on a common set of well logs like gamma ray, bulk density, neutron porosity, resistivity, and sonic. Log data from eight wells and from the same field were provided, together with the corresponding reservoir properties estimated by petrophysicists. The goal was building a data-driven model using the provided training dataset. Following that, the newly developed data-driven models was deployed on the test data set in order to predict the reservoir properties based on the well-log data.

The notebook called [Iron486_1.ipynb](https://github.com/Iron486/SPWLA_PDDA_SIG_machine_learning_competition/blob/main/Iron486_1.ipynb) is the one with the best root-mean-squared-error on test dataset.

The file named [report_Iron486_1.pdf](https://github.com/Iron486/SPWLA_PDDA_SIG_machine_learning_competition/blob/main/report_Iron486_1.pdf) is the report that includes the description of the methods and the obtained results.

In the folder called [Other results](https://github.com/Iron486/SPWLA_PDDA_SIG_machine_learning_competition/tree/main/Other_results) there are other notebook results,  using different hyperparameters and different techniques. 

[Iron486_1.csv](https://github.com/Iron486/SPWLA_PDDA_SIG_machine_learning_competition/blob/main/Iron486_1.csv) is the file with all the predicted values on the test dataset.


",0,0,1,0,well-logs,"[data-cleaning, ensemble-learning, machine-learning, missing-values, pipelines, regression-analysis, well-logs]",00000,
BeardedBeaver,dlas,,https://github.com/BeardedBeaver/dlas,https://api.github.com/repos/dlas/BeardedBeaver,Library for D programming language that provides tools to work with LAS (Well log ASCII Standart) files,"# dlas

Library for D programming language that provides tools to work with LAS (Well Log ASCII Standart) files.
-----------

Simple library for D to deal with well logs. 

## Information

Documentation is available [here](https://dlas.dpldocs.info/dlas.html) 

Author: Dmitriy Linev

License: MIT

## Features

  - Import LAS files version 1, 2 and 3

## Example

```D
auto loader = new Loader;
auto lasHeader = loader.loadLasHeader(""path/to/file.las"");
double start = lasHeader[""STRT""].value.get!double;
double stop = lasHeader[""STOP""].value.get!double;
double step = lasHeader[""STEP""].value.get!double;
int nsamples = (stop - start) / step; // computes amount of samples in LAS file
auto lasData = loader.loadLasData(""path/to/file.las"");
auto md = data[0][];  // data is a 2D array of doubles so all cool D stuff can be applied

```

## Package content

| Directory     | Contents                       |
|---------------|--------------------------------|
| `./source`    | Source code.                   |
| `./test`      | Unittest data.                 |

## Installation

dlas is available in dub. If you're using dub run `dub add dlas` in your project folder and dub will add dependency and fetch the latest version.",0,0,1,0,well-logs,"[dlang, las, well-logs]",00000,
dcslagel,las-util-php,,https://github.com/dcslagel/las-util-php,https://api.github.com/repos/las-util-php/dcslagel,LAS  (Log Ascii Standard)  web utilities in no-framework procedural php,"NAME
----
LAS Util - Log Ascii Standard 2.0 web tools in Php

TABLE-OF-CONTENTS
-----------------
- [DESCRIPTION](#description)
- [SYNOPSIS](#synopsis)
- [DEPENDENCIES](#dependencies)
- [PROJECT-ROADMAP](#project-roadmap)
- [REST-API](#rest-api)
- [FEATURE-REQUEST](#feature-request)
- [BUGS](#bugs)
- [COPYRIGHT](#copyright)

[DESCRIPTION](#name)
-----------

Caution: Las-Util-Php is beta software with limited functionality.

LAS (Log Ascii Standard) web utilities in non-framework PHP

The current goals of `las-util-php` are:
- Parse LAS 2.0 meta-data and data sections
- Explore software design issues related to non-framework PHP
- Explore responsive web design issue with data related web tools

This utility is based on the LAS file format specification   
maintained by the Canadian Well Logging Society at   
https://www.cwls.org/products/#products-las


LAS-Util current functionality:
- Upload a LAS file that includes only the VERSION, WELL and CURVE header
  sections
- Parse the sections and add them to the database
- Display a list of uploaded files
- Display details on a selected uploaded file
- Provide API for listing uploaded LAS docs and details
- Responsive multi-device display


LAS-Util has been tested with PHP 7.4.1

[SYNOPSIS](#name)
--------

 ```bash
# Setup:
git clone https://github.com/dcslagel/las-util-php
cd las-util-php/prj

# View makefile menu
make help

# Make uploads and database directories
make initdirs

# Make database, this depends on Sqlite3 being installed
make initdb

# start development web server
make run
```

In a web browser, browse to:    
http://localhost:7000/upload

Select the LAS file prj/example_data/sample_next.las to upload.
Sample_next.las has been verified that it will process correctly.

The sample_next.las currently is made up of the header sections: version, well
and curve.
Additional header sections and the '~A' data section will be added
in future iterations.

Click 'upload'

  LAS-Util will:
  - upload the file to a local uploads directory
  - parse the file's information and save it to the database

Select the 'Display LAS Files' menu item. The uploaded file will have the most recent date.

[DEPENDENCIES](#name)
------------

SQLite3


[PROJECT-ROADMAP](#name)
---------------
`LAS-Util-Php`'s project road-map is managed in github milestones at:    

https://github.com/dcslagel/las-util-php/milestones


[REST-API](#name)
--------

To retrieve uploaded LAS docs:
```bash
curl http://127.0.0.1:7000/api/list
```

To retrieve details of a specific LAS doc
Syntax:    
```bash
curl http://127.0.0.1:7000/api/detail?[filename]
```

Example:     
```bash
# first retrieve a filename from the previous 'api/list' call
# example: las_file-2019-08-29-21-41-42.las
curl http://127.0.0.1:7000/api/detail?las_file-2019-08-29-21-41-42.las
```


[FEATURE-REQUEST](#name)
----------------

To request and discuss a potential feature, create an issue at:
  - https://github.com/dcslagel/las-util-php/issues


[BUGS](#name)
----

- Functionality is limited to reading some fields from a LAS file containing
  only the v2.0 type sections.

- Report bugs by creating an issue at:
  - https://github.com/dcslagel/las-util-php/issues


[COPYRIGHT](#name)
------

Copyright (c) 2019, 2020 DC Slagel
",0,0,1,7,well-logs,"[las-files, las-util, log-ascii-standard, well-logs]",00000,
aifenaike,Acoustic-Log-Estimation_Using-ML,,https://github.com/aifenaike/Acoustic-Log-Estimation_Using-ML,https://api.github.com/repos/Acoustic-Log-Estimation_Using-ML/aifenaike,,"# Acoustic-Log-Estimation_Using-ML


In most hydrocarbon fields, it is not uncommon for some log records to be absent. This could be as a result of incomplete logging, bad borehole conditions, damaged instruments, or data loss. In any of such cases, it is important to devise means of estimating some vital logs where absent. This repository presents a machine learning technique that integrates several logs for P-wave prediction to minimize errors and uncertainties associated with traditional estimation methods. The well borehole diameter and several logging measurements such as resistivity (RT), bulk density (RHOB), porosity (NPHI), radioactivity content (GR), and photoelectric absorption factor (PEF) of the formation, are used as predictors. 

## Objective
To provide a blueprint for predicting the values of missing p-sonic logs where needed. This project can be tweaked with appropriate modifications of the algorithms to any area of well logging studies, where missing log values are needed. 

## Dataset
The data used in this research is from the open database for the Volve oil field in Stavanger at the southern end of the Norwegian sector in the North Sea. 

## Model Architecture
Four intelligent models were used in this work.
- Gradient Boosting
- Random Forest
- Extreme gradient boosting
- Linear Regression
",0,0,1,0,well-logs,"[acoustic-logs, gradient-boosting-machine, petrophysics, random-forest, well-logs]",00000,
aifenaike,Logio,,https://github.com/aifenaike/Logio,https://api.github.com/repos/Logio/aifenaike,"Logio is an Unsupervised Machine learning framework for well log visualization, and well-well depth correlation using logs.","[![Python](https://img.shields.io/badge/python-3-blue.svg)](https://python.org)
[![lasio](https://img.shields.io/badge/lasio-Lasio-brightgreen)](https://lasio.readthedocs.io/en/latest/)


# Logio
**Society of Petroleum Engineers, University of Ibadan Chapter**  
    *Submission for the SPE Lagos Section Hackathon*

Python package for well log analysis and visualization. 
Also, this package is an Unsupervised Machine learning framework for well-well depth correlation using logs.



![Well Log Plots](static/plotwell.png)

> [LAS Files](#las-files)  
> [Features](#features)  
> [Dependencies](#dependencies)  
> [Documentation](#documentation)  
> [Installation](#installation)  
> [Getting Started](#getting-started)  
> [Credits and References](#credits-and-references)  
> [Contributing](#contributing)  
> [Support](#support)  
> [Authors](#authors)  
---

### LAS Files

**LAS**, short for Log ASCII Standard (LAS) files, are generated in borehole operations such as geophysical, geological, or petrophysical logs. file contains  
Well log data saved in LAS file generally contains information, including its file **version**, **well description**, **physical rock curve** along with **data table** and **other information** related to the well data typically used in well log analysis. 

---

### Features

Here are a few things this package does well:

* Loads LAS data from various sources:
    - URL link (`https://example.com/.../.../path/to/lasfile.LAS`)
    - Local file (`path/to/lasfile.LAS` instead without `https`)
* Robust IO framework for loading data from flat files (CSV and delimited), Excel files, las files and JSON.
* Parsing well log data into any of the formats mentioned above.
* Hardcoded and flexible implementations for visualization of well logs and non-well log data, but in log format
* A novel system for well-to-well log correlation using dynamic depth warping techniques.
    - correlating well logs and obtaining the minimum-cost or ""best"" match.
---

### Dependencies

This project uses **Python 3** with dependencies provided in **[requirements.txt](requirements.txt)**. 

---

### Documentation

See the [Tutorials](./Tutorial) to explore the framework step-by-step in jupyter notebooks
and the [documentation](https://logio.readthedocs.io/en/latest/) for more details.

---

### Installation

Clone this repository using this command below on Terminal (Linux or Mac) or <a href=""https://en.wikipedia.org/wiki/Windows_Subsystem_for_Linux"" target=""_blank""><abbr title=""Windows Subsystem for Linux"">WSL</abbr></a> (Windows).
```sh
git clone https://gitlab.com/aifenaike/Logio.git
cd Logio
```

Python environment setup is recommended for using this project repository.  

You can [create the environment variable manually](https://docs.python.org/3/library/venv.html) by typing the commands below on Linux or MacOS (and also WSL console).

```sh
python -m venv venv
source venv/bin/activate
```
and for Windows.
```
python -m venv venv
venv/Scripts/activate
```

You can now proceed to install required packages by running
```sh
pip install -r requirements.txt
```
---

### Getting Started

```mermaid
graph TB
A(logio)--> B((core))
A --> C((logplot))
A --> D((dynamic_time_warping))
C --> E{PlotWell}
C --> F{LogPLot}
B --> G{Analysis}
D --> H{dtw}
```

Example Session:
Load and plot a well log from ```.las``` file
```python
# Import the packages
>>> from logio.core import Analysis
>>> from logio.logplot import PlotWell, LogPlot

# Read in your data from a .las file
>>> data = Analysis().read_file(filename=""data/15_9-F-11B.LAS"")

# Plot a GR log with a cutoff delineating shale from sand volumes
LogPlot(data).cutoff_plot(x=""GR"", y=""DEPTH"", x_cutoff=0.45,  y_range= (0,0),xscale='linear',labels= ['Sand', 'Shale'], 
                          fig_size = (4.5, 7),colors=['#964B00','#101010']) 
```

![Gamma Ray Cutoff Plot](static/cutoff_plot.png ""Gamma Ray Cutoff Plot"")


---

### Credits and References

 - [**Schlumberger** Log Interpretation Principles\Applications](https://www.slb.com/resource-library/book/log-interpretation-principles-applications)
 - [**lasio**](https://github.com/kinverarity1/lasio)
 - [**Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package**](https://cran.r-project.org/web/packages/dtw/vignettes/dtw.pdf)
 
---

## Contributing

Please see [`CONTRIBUTING.md`](CONTRIBUTING.md).

---

### Support

For support, email alexander.ifenaike@gmail.com

---

### Authors

Please see [`AUTHORS.md`](AUTHORS.md).

[back to top](#logio)



```python

```
",0,0,1,0,well-logs,"[data-visualization, distance-similarity, unsupervised-learning, well-logs]",00000,
arjavanb,Vs_Estimation,,https://github.com/arjavanb/Vs_Estimation,https://api.github.com/repos/Vs_Estimation/arjavanb,Estimation of Shear wave velocity in oil sands from other well logs,"# Data Science
I apply ML algorithms on public data. Source data is also available for download and practice

## Log_Estimation
Using machine learning algorithms and available well data, new well logs (e.g. Facies log, Shear sonic log) will be estimated
I am going to use a sample data set
I will use scikit-learn as my main Python library for machine learning purposes. However, for explatory data analyses and ploting, scientific library ecossystem will be utilized.
I will apply both regression and classification algorithms to estimate a contineous and discrete logs (i.e. Vs (shear wave velocity) and Facies log)
",0,0,2,0,well-logs,"[geophysics, petrophysics, regression, rock-physics, shearwave, well-logs]",00000,
JulioCMS1500,Electrofacies-classification,,https://github.com/JulioCMS1500/Electrofacies-classification,https://api.github.com/repos/Electrofacies-classification/JulioCMS1500,Electrofacies classification using supervised learning algortihms,"# Electrofacies Classification Using Supervised Leanring Algorithms

**Introduction**

In this work I combine statistical analysis like K-means clustering (K-mean) and Principal Components Analysis (PCA) as a pre-processing step to find specific patterns related to each depositional facie, solving as much as possible the multi-class problem. Feature selection is important to define and quantify which well logging measurements contribute mainly to pattern recognition. Tuning hyperparameters from each classification algorithm gives the best possible performance based on data structure, avoiding complications like overfitting, underfitting and multi-class. Finally, model evaluation using thresholds metrics measure the performance of KNN and SVM. 
Proper data preparation is needed before using these algorithms. All these analyses will be done using Python programming language which contains most of the algorithms and statistical tools needed to perform a complete analysis.

The following work is splited in three diferent codes ordered as:

**1.-** Data Analysis and Feature selection.\
**2.-** K-means & PCA.\
**3.-** KNN and SVM classification.
",0,0,1,1,well-logs,"[electrofacies, knn-classification, supervised-learning, svm-classifier, well-logs]",00000,
maribickpostanes,Displaying-Well-Log-Parameters--Plots-and-Animations,,https://github.com/maribickpostanes/Displaying-Well-Log-Parameters--Plots-and-Animations,https://api.github.com/repos/Displaying-Well-Log-Parameters--Plots-and-Animations/maribickpostanes,The objective of this GitHub repository is to develop code that can effectively display well log parameter plots and their animations.,,7,7,2,0,oilandgas,"[animation, datavisualization, drilling, jupyter-notebook, lasio, luxembourg, matplotlib, matplotlib-pyplot, oilandgas, petroleum-engineering, petrophysics, python, welllogging]",00000,
maribickpostanes,Interactive-Horizon-Surfaces-Plot,,https://github.com/maribickpostanes/Interactive-Horizon-Surfaces-Plot,https://api.github.com/repos/Interactive-Horizon-Surfaces-Plot/maribickpostanes,This open-source code repository provides a Python implementation for generating an interactive horizon surfaces plot using Plotly. The focus of this code is to visualize the Volve horizons in a 3D surface plot.,"# Interactive-Horizon-Surfaces-Plot
This open-source code repository provides a Python implementation for generating an interactive horizon surfaces plot using Plotly. The focus of this code is to visualize the Volve horizons in a 3D surface plot.

## Features

- Interactive visualization: Explore the horizon surfaces plot with zooming, panning, and hovering capabilities.
- Horizon visualization: Display multiple geological horizons including BCU, Hugin Fm Base, Hugin Fm Top, Shetland GP Top, and Ty Fm Top.
- Data integration: Utilize data from the Volve dataset for generating the horizon surfaces.
- Plot customization: Adjust plot properties, such as colorscales and axis titles, to customize the visualization.
- Easy-to-use: The code provides a straightforward interface for generating the horizon surfaces plot in Python.

## Note on Interactive Plot

Please note that the interactive plot may not be visible when viewing the Jupyter Notebook file directly on GitHub. However, an alternative HTML file (`Volve Horizon Surfaces Python Code.html`) is included in this repository, showcasing the interactive plot.

To view the interactive plot, follow these steps:

1. Download or clone the repository to your local machine.
2. Open the Jupyter Notebook file (`Volve Horizon Surfaces.ipynb`) using Jupyter Notebook or Jupyter Lab.
3. Run the code in the notebook to generate the interactive plot.
4. The generated plot will be displayed within the Jupyter Notebook interface.

Alternatively, you can directly view the interactive plot by opening the `Volve Horizon Surfaces.html` file in a web browser without running the Jupyter Notebook. Please note that running the code in a Jupyter Notebook environment is recommended to fully experience the interactivity and functionality of the plot.

## Contributing

Contributions to this open-source project are welcome. If you have any suggestions, improvements, or bug fixes, feel free to open an issue or submit a pull request.

![Volve Horizon Surfaces Screenshot](https://github.com/maribickpostanes/Interactive-Horizon-Surfaces-Plot/assets/127098659/ddb1e2a6-777f-4348-821d-b37e14e8b515)
",6,6,1,0,oilandgas,"[datavisualization, digital, geosciences, griddata, horizon, html, interactiveplot, jupyter-notebook, northsea, numpy, oil-and-gas-industry, oilandgas, pandas, pathlib, petroleum-engineering, plotly, python, scipyinterpolate, subsurface, volve]",00000,
maribickpostanes,Simple-Production-Data-Plotting,,https://github.com/maribickpostanes/Simple-Production-Data-Plotting,https://api.github.com/repos/Simple-Production-Data-Plotting/maribickpostanes,This GitHub repository provides sample Python code for creating a simple production data plot using Jupyter Notebook.,,3,3,1,0,oilandgas,"[datavisualization, equinor, jupyter-notebook, northsea, oilandgas, production-data, python, volve]",00000,
maribickpostanes,Interactive-Well-Trajectory-Plot,,https://github.com/maribickpostanes/Interactive-Well-Trajectory-Plot,https://api.github.com/repos/Interactive-Well-Trajectory-Plot/maribickpostanes,This open-source code repository provides a Python implementation for generating an interactive well trajectory plot using Plotly. The focus of this code is to visualize the trajectory of Well 15/9-F-5 from the Volve dataset.,"# Interactive-Well-Trajectory-Plot
This open-source code repository provides a Python implementation for generating an interactive well trajectory plot using Plotly. The focus of this code is to visualize the trajectory of Well 15/9-F-5 from the Volve dataset.

## Features
- Interactive visualization: Explore the well trajectory plot with zooming, panning, and hovering capabilities.
- Well trajectory visualization: Display the path of Well 15/9-F-5 in a 3D plot for a comprehensive view of the wellbore path.
- Data integration: Utilize the well trajectory data specifically from the Volve dataset.
- Plot customization: Adjust plot properties, such as colors and line styles, to customize the visualization.
- Easy-to-use: The code provides a straightforward interface for generating the well trajectory plot in Python.

## Note on Interactive Plot
The interactive plot may not be visible when viewing the Jupyter Notebook file directly on GitHub. However, an alternative HTML file (`Well Trajectory Python Code.html`) is included in this repository, showcasing the interactive plot.

To view the interactive plot, follow these steps:

1. Download or clone the repository to your local machine.
2. Open the Jupyter Notebook file (`Well Trajectory.ipynb`) using Jupyter Notebook or Jupyter Lab.
3. Run the code in the notebook to generate the interactive plot.
4. The generated plot will be displayed within the Jupyter Notebook interface.

Alternatively, you can directly view the interactive plot by opening the `Well Trajectory.html` file in a web browser without running the Jupyter Notebook. Please note that running the code in a Jupyter Notebook environment is recommended to fully experience the interactivity and functionality of the plot.

## Contributing
Contributions to this open-source project are welcome. If you have any suggestions, improvements, or bug fixes, feel free to open an issue or submit a pull request.

![Well Trajectory Screenshot](https://github.com/maribickpostanes/Interactive-Well-Trajectory-Plot/assets/127098659/c0d09567-d733-4d2f-8458-6b7c5b605cd8)
",3,3,1,0,oilandgas,"[beautifulsoup, bs4, datavisualization, digital, drilling, html, jupyter-notebook, northsea, numpy, oil-and-gas-industry, oilandgas, pandas, petroleum-engineering, plotly, python, realtimedata, volve, wellboretrajectory, witsml, xml]",00000,
ActiveMargins,ProductionMapping,,https://github.com/ActiveMargins/ProductionMapping,https://api.github.com/repos/ProductionMapping/ActiveMargins,Mapping production curves with unsupervised classification of time series curves,"# ProductionMapping
Mapping production curves with unsupervised classification of time series curves
",2,2,1,0,oilandgas,"[geology, geospatial-analysis, oilandgas, timeseries, unsupervised-clustering]",00000,
ashrafalaghbari,odc,,https://github.com/ashrafalaghbari/odc,https://api.github.com/repos/odc/ashrafalaghbari,"A Python Module for Outliers Detection, Visualization and Treatment in Oil Well Datasets","# Outlier Detection and Treatment with ODC

OilDataCleaner (ODC) is a Python module designed for detecting anomalies in time series data related to the oil industry. With its data cleaning techniques, ODC is an essential tool for anyone working with oil data. Whether you're a researcher, analyst, or engineer, ODC can help you identify unusual patterns and outliers in your data, enabling you to make more informed decisions.

## Installation
To use this module:

1. Clone or download this repository to your local machine.

2. Install the required packages by running the following command in your command prompt or terminal:

```bash
pip install -r requirements.txt
```
This will install all the necessary dependencies for using this module.

3. Import the necessary packages and the DetectOutliers class in your Python script:

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from odc import DetectOutliers
```
4. Load your dataset using pandas and create an instance of the DetectOutliers class:

```python
df = pd.read_csv(""F_14.csv"", parse_dates=[""DATEPRD""], index_col=""DATEPRD"")
clean = DetectOutliers(df)
```

5. Detect outliers in the desired variable:

```python
# Detecting outliers in the 'ON_STREAM_HRS' variable
ON_STREAM_HRS_outliers = clean.detect_outliers_in_time('ON_STREAM_HRS', 'BORE_OIL_VOL')
print(""Outliers detected in ON_STREAM_HRS variable:\n"", ON_STREAM_HRS_outliers)

```
Output:

    Outliers detected in ON_STREAM_HRS variable:
     DATEPRD
    2010-10-31    25.00000
    2012-09-15     0.95833
    2013-10-27    24.30833
    2014-10-26    25.00000
    Name: ON_STREAM_HRS, dtype: float64

6. Plot the outliers:

```python
clean.plot_outliers(ON_STREAM_HRS_outliers)
```
Output:

![test_4_0](https://user-images.githubusercontent.com/98224412/236640490-ae183998-86ec-4911-8df2-1c56d0892211.png)


7. Treat the outliers:

```python
df['ON_STREAM_HRS'] = clean.treat_outliers_in_time()
```
8. Check that the outliers were treated:

```python
outliers_after_treatment = clean.detect_outliers_in_time('ON_STREAM_HRS', 'BORE_OIL_VOL')
print(""Outliers detected in ON_STREAM_HRS variable after treatment:\n"", outliers_after_treatment)
```
Output:

    Outliers detected in ON_STREAM_HRS variable after treatment:
     No outliers detected.



For more examples of using this module, please refer to the `data_cleaning.ipynb` notebook in this repository.


## License

This project is licensed under the [MIT License](https://github.com/ashrafalaghbari/odc/blob/main/LICENSE) - see the LICENSE file for details.


## Contributing

Contributions are always welcome!

See [contributing.md](https://github.com/ashrafalaghbari/odc/blob/main/contributing.md) for ways to get started.

## Contact

If you have any questions or encounter any issues running `odc module`, please feel free to [open an issue](https://github.com/ashrafalaghbari/odc/issues). I'll be happy to help!



",2,2,1,0,oilandgas,"[anamoly-detection, oil-analysis, oil-wells, oilandgas, outliers-treatment, petroleum-dataset, python]",00000,
enriquemezav,spwlaunisc_PythonAppliedOG,,https://github.com/enriquemezav/spwlaunisc_PythonAppliedOG,https://api.github.com/repos/spwlaunisc_PythonAppliedOG/enriquemezav,"Creado con fines educativos , con el objetivo de demostrar que Python es una excelente herramienta para la visualización y análisis de datos petroleros.","# Python Aplicado a la Industria del O&G

[![Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/enriquemezav/spwlaunisc_PythonAppliedOG/master?filepath=notebook/ws_spwlaunisc.ipynb)
[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/enriquemezav/spwlaunisc/blob/master/notebook/ws_spwlaunisc.ipynb)
[![CCO](https://img.shields.io/github/license/softwareunderground/awesome-open-geoscience.svg)](https://github.com/enriquemezav/spwlaunisc_PythonAppliedOG/blob/master/LICENSE)

Hola, somos un grupo de estudiantes miembros del Capítulo Estudiantil de la **Society of Petrophysicists and Well Log Analysts** en la *Universidad Nacional de Ingeniería* ([SPWLA UNI SC](https://www.linkedin.com/company/spwlaunisc/)); organizamos este curso de introducción a la programación en Python en colaboración con el grupo de investigación TRM de acceso abierto y gratuito al público, con el objetivo de mostrar su aplicación en la industria de Oil & Gas.

<H1 align=""center""><img src=""https://i.ibb.co/0GKk29s/Dise-o-sin-t-tulo.png"" width = 800></H1>

## Cómo usar este notebook

- Ejecute el código utilizando los cuadernos de Jupyter disponibles en el directorio [notebooks](notebook) de este repositorio. 

- Ejecute versiones ejecutables de estos cuadernos utilizando [Google Colab](http://colab.research.google.com): [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/enriquemezav/spwlaunisc/blob/master/notebook/ws_spwlaunisc.ipynb)

- Inicie un servidor de cuadernos en vivo con estos cuadernos usando [binder](https://beta.mybinder.org/): [![Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/enriquemezav/spwlaunisc_PythonAppliedOG/master?filepath=notebook/ws_spwlaunisc.ipynb)

## Sobre el notebook

El libro presenta las bibliotecas centrales esenciales para trabajar con datos en Python: particularmente NumPy , Pandas , Matplotlib , SciPy y Lasio. Así también, se usaron conjuntos de datos abiertos disponibles como:

| Base de datos abierta | Referente | Propietario |  
|:---:|:---:|:---:|   
| Exploración | [The University of Kansas](https://www.ku.edu/) | [Kansas Geological Survey](http://www.kgs.ku.edu/PRS/Scans/Log_Summary/index.html) |  
| Producción | [Campo Volve](https://www.equinor.com/en/how-and-why/digitalisation-in-our-dna/volve-field-data-village) | [Zenodo](https://zenodo.org/) **(Alfonso Reyes)** |

## Índice
(Nota: a veces, la renderización del cuaderno de GitHub puede ser lenta o complicada. Si tiene problemas con los siguientes enlaces, intente ver el material en [nbviewer](https://nbviewer.jupyter.org/github/enriquemezav/spwlaunisc_PythonAppliedOG/blob/master/notebook/ws_spwlaunisc.ipynb).

**Índice del cuaderno**

1. Fundamentos de programación en Python.
2. Principales bibliotecas en Python.
3. Conjunto de datos abiertos de Exploración.
4. Conjunto de datos abiertos de Producción.

## Licencia

Este material se publica bajo la licencia [CC0](LICENSE) ""Sin derechos reservados"" y, por lo tanto, usted es libre de reutilizar, modificar, construir y mejorar este material para cualquier propósito. Lea más sobre CC0 [aquí](https://creativecommons.org/share-your-work/public-domain/cc0/).

[![CC0](http://mirrors.creativecommons.org/presskit/buttons/88x31/svg/cc-zero.svg)](https://creativecommons.org/publicdomain/zero/1.0/)
",0,0,1,0,oilandgas,"[oilandgas, python, spwlaunisc, workshop]",00000,
maribickpostanes,Interactive-Seismic-Data-Section-Plot,,https://github.com/maribickpostanes/Interactive-Seismic-Data-Section-Plot,https://api.github.com/repos/Interactive-Seismic-Data-Section-Plot/maribickpostanes,This repository contains a Python script that uses the Plotly library to create an interactive web-based application for plotting seismic data sections from SEG-Y files.,"# Interactive-Seismic-Data-Section-Plot
This repository contains a Python script that uses the Plotly library to create an interactive web-based application for plotting seismic data sections from SEG-Y files.

## Note on the Uploaded Github Files
The uploaded python code file (`Seismic Data Section Plot.ipynb`) is too big to display on Github. You can download the files from this repository and open it on Jupyter notebook or VS Studio Code. An alternative HTML file (`Seismic Data Section Plot.html`) is included in this repository, which showcases the interactive plot without the need to run the Jupyter Notebook. Additionally, the file (`Seismic Data Section Plot (Reversed).ipynb`) has been uploaded and updated to reverse the y-axis time values, as per the corrections of certain people. Thank you for your feedback.

## Data Credit and Source Code
Equinor ASA (formerly Statoil) and the former Volve license partners, ExxonMobil Exploration & Production Norway AS and Bayerngas Norge AS, are credited for providing the VOLVE dataset under CC BY 4.0 license.

The code for this practice coding was adapted from the code found on Equinor's GitHub page, linked here: https://github.com/equinor/segyio-notebooks/blob/master/notebooks/pylops/01_seismic_inversion.ipynb. I used the Plotly library to make the generated seismic data section plot interactive, instead of using Matplotlib.

## Contributing
Contributions to this open-source project are welcome. If you have any suggestions, improvements, or bug fixes, feel free to open an issue or submit a pull request.

Thank you so much for visiting and taking the time to read this GitHub repository. I hope this can contribute to other's work. Your support and interest are sincerely appreciated.

https://github.com/maribickpostanes/Interactive-Seismic-Data-Section-Plot/assets/127098659/cd25084f-634e-4b6f-8372-7ddc2c8e0557
",2,2,1,0,oil-and-gas-industry,"[datavisualization, geosciences, interactiveplot, jupyter-notebook, northsea, numpy, oil-and-gas-industry, pandas, petroleum-engineering, plotly, python, seg-y, segyio, seismic, seismic-data-section, volve]",00000,
maribickpostanes,Philippine-Historical-Petroleum-Production,,https://github.com/maribickpostanes/Philippine-Historical-Petroleum-Production,https://api.github.com/repos/Philippine-Historical-Petroleum-Production/maribickpostanes,This repository contains Python code for plotting historical petroleum production data in the Philippines from 1979 to 2021 using Plotly library. ,"# Philippine-Historical-Petroleum-Production
This repository contains Python code for plotting historical petroleum production data in the Philippines from 1979 to 2021. The code uses the Plotly library to create interactive plots that visualize the data over time. This can be helpful for understanding how petroleum production has changed over the years, and for identifying potential problems or opportunities.

## Features
- Interactive visualization: Explore the historical petroleum production data plot with zooming, panning, and hovering capabilities.
- Data visualization: Display different aspects of the petroleum production data, such as the total production, the production of different types of petroleum, or the production by oilfield, gas field and condensate field.
- Data customization: Adjust plot properties, such as colors, fonts, and titles, to customize the visualization.
- Easy to use: The code is easy to use and does not require any special knowledge of Python.

## Note on Interactive Plot
The interactive plot may not be visible when viewing the Jupyter Notebook file directly on GitHub. An alternative HTML file (`Petprod Philippines.html`) is included in this repository, which showcases the interactive plot without the need to run the Jupyter Notebook.

## Data Credits
The data set for this repository was sourced from the Department of Energy in the Philippines, available on their website: https://www.doe.gov.ph/energy-information-resources?q=energy-resources/petroleum-statistics. Accessed August 21, 2023.

## Contributing
Contributions to this open-source project are welcome. If you have any suggestions, improvements, or bug fixes, feel free to open an issue or submit a pull request.

Thank you so much for visiting and taking the time to read this GitHub repository. I hope you found it informative and learned something new. Your support and interest are sincerely appreciated. I hope that these plots will be a valuable resource for anyone who is interested in understanding the petroleum industry in the Philippines.

![1-oilprod](https://github.com/maribickpostanes/Philippine-Historical-Petroleum-Production/assets/127098659/c693e492-004e-4a4f-a9a7-890d883531c3)

![1-gasprod](https://github.com/maribickpostanes/Philippine-Historical-Petroleum-Production/assets/127098659/a403a170-eb47-4ad6-b0fb-bb95ae4c2ce0)

![1-condensateprod](https://github.com/maribickpostanes/Philippine-Historical-Petroleum-Production/assets/127098659/09f62cab-e530-470d-b06c-4aa042991cba)
",1,1,1,0,oil-and-gas-industry,"[oil-and-gas-industry, petroleum-production-engineering, philippines, plotly, productiondata, python, pythonprogramming]",00000,
luthfigeo,Well-Log-Data-Availability,,https://github.com/luthfigeo/Well-Log-Data-Availability,https://api.github.com/repos/Well-Log-Data-Availability/luthfigeo,To check log data availability in a LAS file,,2,2,1,0,well-logs,"[lasio, petrophysics, well-logs]",00000,
luthfigeo,Litho-Classification,,https://github.com/luthfigeo/Litho-Classification,https://api.github.com/repos/Litho-Classification/luthfigeo,Handle classification within volcanic formation using supervised learning.,"# Volcanic facies Classification using Random Forest

Handle classification within volcanic formation using Random Forest. Another method is also included to be compared. In this case I'm dealing with Tuff, Igneous Rock (Andesite, Basalt), Sedimentary sequences (Tuffaceous Sandstone, Shale), and Conglomerates.

It's better to see the documentation in the notebook first because the code are not made to run as a whole process. I also attached a support to be used in data preparation. The file ""Lithology Labelling"" can help you to label the data, combine what you see in the LAS file with the lithology from core description, cutting, or mudlog.
",2,2,2,0,well-logs,"[facies-classification, label, lithology, petrophysics, random-forest, well-logs]",00000,
equinor,lcm,equinor,https://github.com/equinor/lcm,https://api.github.com/repos/lcm/equinor,Lost Circulation Material,"# Lost Circulation Material Optimizer

![CI](https://github.com/equinor/lcm/workflows/CI/badge.svg)

Web application for creating, comparing, and optimize blending of lost circulation material used to bridge fractures and stop losses in rock formations during petroleum drilling.

This repository is the result from the merger of the two summer intern projects from 2020.

- Team Blend <https://github.com/equinor/LCMLibrary-Blend>
- Team Bridge <https://github.com/equinor/LCMLibrary-Bridge>

![plot](bridge-plot.png)

Deployed environments:
 - [Test](https://proxy-lost-circulation-material-test.radix.equinor.com)
 - [Production](https://lost-circulation-material.app.radix.equinor.com)

## Requirements

- docker
- docker-compose

## Running

1. Create a copy of `.env-template` called `.env` and populate it with values.
2. Build the containers

    ```sh
    docker-compose build
    ```

3. Start the project

    ```sh
    docker-compose up
    ```

## Contributing

Pull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.

Please make sure to update tests as appropriate.

## Operational runbook

[RUNBOOK](runbook.md)

## Interpolating new fraction data

Bridge data from products on a different scale than the one defined at `api/calculators/bridge.py:45` can be added to
the LCM optimizer as long as the data gets interpolated into the same scale.

That can be done like this;

1. Add a file at `./api/test_data/interpolate_input.csv`
2. Have the first column be called ""Size"" and have 101 measuring points of the products
3. Add one column for each product, where the header is the name of the product.
    ```csv
    Size,Prod1,Prod2
    0.01,0,0
    0.011482,0,0
    ...
    10000,100,100
    ```
4. Run `docker-compose build api && docker-compose run api python calculators/fraction_interpolator.py`
5. One result file for each product will be created in `./api/test_data/`

## Radix
Two different environments in Radix are used: one for test (deploys from branch ""test"") and one for production (deploy from branch ""master"")

## License

[MIT](LICENSE)
",4,4,2,15,petroleum,"[equinor, flask, genetic-algorithm, petroleum, python, typescript]",00000,
Ayberk-Uyanik,SGR_Logs_Interpreter,,https://github.com/Ayberk-Uyanik/SGR_Logs_Interpreter,https://api.github.com/repos/SGR_Logs_Interpreter/Ayberk-Uyanik,"Workflows developed for to display and interpret conventional well logs, depositional environments, sequence boundaries, heat flows, clay types, fracture intensities, and operational records","An open source well log interpretation tool benefitting from data science and visualisation libraries of Python programming language, Plotly in particular, allowing users to display and interpret conventional well logs, depositional environments, sequence boundaries, heat flows, clay types, fracture intensities, and operational records to better understand subsurface geology which is crucial for any exploration project.
",2,2,1,0,well-logs,"[geological-data, geology, geoscience, geosciences, jupyter-notebook, open-source, plotly, python, well-logs, welllog, welllogging, welllogs]",00000,
luthfigeo,Log-Alias,,https://github.com/luthfigeo/Log-Alias,https://api.github.com/repos/Log-Alias/luthfigeo,Synchronize log alias to be a single mnemonic,"# Log-Alias
Synchronize log alias to be a single mnemonic

Usually we deal with broad range of well log mnemonics, this brief codes (and bunch of log aliases) tried to uniform the log alias into one specific name. It also merge similar log type but has different mnemonic and interval within the same well. Further development should need a normalization for each log before the merging begin because different mnemonics might be due to different service company or tools in which the measurement could be different. These codes could be applied for single well or multi well.

**Input:**
A csv file consist of log measurements (and well name if you want to run multiwell), each column consist of one log. I recommend using lasio to read LAS file and convert it to dataframe, later you can save it as csv to be inputted in these code, or, you can directly use the dataframe as input.
",1,1,2,0,well-logs,"[mnemonics, petrophysics, well-logs]",00000,
gwallison,skytruth_reformat,,https://github.com/gwallison/skytruth_reformat,https://api.github.com/repos/skytruth_reformat/gwallison,Reformat the SkyTruth scrape of FracFocus website (2011-May 2013) so it can be merged with later FracFocus data.,,0,0,2,0,oil-and-gas,"[chemicals, environmental-monitoring, fracfocus, fracking, oil-and-gas]",00000,
gwallison,FF-POC,,https://github.com/gwallison/FF-POC,https://api.github.com/repos/FF-POC/gwallison,Proof-of-concept of code to make the FracFocus chemical disclosures into a usuable database.,,1,1,2,0,oil-and-gas,"[chemicals, environmental-monitoring, fracfocus, fracking, oil-and-gas]",00000,
imayachita,log-analysis,,https://github.com/imayachita/log-analysis,https://api.github.com/repos/log-analysis/imayachita,Data Science in Geoscience: Wireline Log Analysis,"# log-analysis

Wireline Log Analysis

Requirements: <br>
```pip install -r requirements.txt```

<b> Part 1: </b> Clustering well log data <br>
<b> Part 2: </b> Predict Porosity and Permeability with Neural Networks

<b> Plotting the log data: </b> <br>
![image1](wireline_log.jpg)

<b> Prediction results on porosity and permeability: </b> <br>
The model was trained on the first 150 data and predicted the rest of the data <br>
![image2](wireline_log_predicted.jpg)
",5,5,1,3,oil-and-gas,"[clustering, geoscience, neural-networks, oil-and-gas]",00000,
derrickturk,shiny-aRpsDCA,,https://github.com/derrickturk/shiny-aRpsDCA,https://api.github.com/repos/shiny-aRpsDCA/derrickturk,Interactive aRpsDCA demo with Shiny,"# aRpsDCA interactive demo

As seen at https://derrickturk.shinyapps.io/shiny-aRpsDCA.

Uses the aRpsDCA library for decline curve analysis, found on CRAN at https://cran.r-project.org/web/packages/aRpsDCA and GitHub at https://github.com/derrickturk/aRpsDCA.

Licensed under the GPL v2. If you'd like to use anything here under a different license, please contact me.

(c) 2015 [dwt](http://www.github.com/derrickturk) | [terminus data science, LLC](http://www.terminusdatascience.com)
",3,3,2,0,petroleum,"[decline-curve-analysis, petroleum, petroleum-engineering, shiny]",00000,
luthfigeo,Synthetic-Sonic,,https://github.com/luthfigeo/Synthetic-Sonic,https://api.github.com/repos/Synthetic-Sonic/luthfigeo,Generating synthetic DT log,"# Synthetic-Sonic
Generating synthetic DT log using several different ML method.
",0,0,2,0,well-logs,"[machine-learning, petrophysics, sonic, well-logs]",00000,
