repo,user,organization,url (HTML),url (API),description,readme,stargazer count,watcher count,subscriber count,open issue count,topic (search),topics,NAICS Code
frappe,erpnext,frappe,https://github.com/frappe/erpnext,https://api.github.com/repos/erpnext/frappe,Free and Open Source Enterprise Resource Planning (ERP),"<div align=""center"">
    <a href=""https://erpnext.com"">
        <img src=""https://raw.githubusercontent.com/frappe/erpnext/develop/erpnext/public/images/erpnext-logo.png"" height=""128"">
    </a>
    <h2>ERPNext</h2>
    <p align=""center"">
        <p>ERP made simple</p>
    </p>

[![CI](https://github.com/frappe/erpnext/actions/workflows/server-tests.yml/badge.svg?branch=develop)](https://github.com/frappe/erpnext/actions/workflows/server-tests.yml)
[![UI](https://github.com/erpnext/erpnext_ui_tests/actions/workflows/ui-tests.yml/badge.svg?branch=develop&event=schedule)](https://github.com/erpnext/erpnext_ui_tests/actions/workflows/ui-tests.yml)
[![Open Source Helpers](https://www.codetriage.com/frappe/erpnext/badges/users.svg)](https://www.codetriage.com/frappe/erpnext)
[![codecov](https://codecov.io/gh/frappe/erpnext/branch/develop/graph/badge.svg?token=0TwvyUg3I5)](https://codecov.io/gh/frappe/erpnext)
[![docker pulls](https://img.shields.io/docker/pulls/frappe/erpnext-worker.svg)](https://hub.docker.com/r/frappe/erpnext-worker)

[https://erpnext.com](https://erpnext.com)

</div>

ERPNext as a monolith includes the following areas for managing businesses:

1. [Accounting](https://erpnext.com/open-source-accounting)
1. [Warehouse Management](https://erpnext.com/distribution/warehouse-management-system)
1. [CRM](https://erpnext.com/open-source-crm)
1. [Sales](https://erpnext.com/open-source-sales-purchase)
1. [Purchase](https://erpnext.com/open-source-sales-purchase)
1. [HRMS](https://erpnext.com/open-source-hrms)
1. [Project Management](https://erpnext.com/open-source-projects)
1. [Support](https://erpnext.com/open-source-help-desk-software)
1. [Asset Management](https://erpnext.com/open-source-asset-management-software)
1. [Quality Management](https://erpnext.com/docs/user/manual/en/quality-management)
1. [Manufacturing](https://erpnext.com/open-source-manufacturing-erp-software)
1. [Website Management](https://erpnext.com/open-source-website-builder-software)
1. [Customize ERPNext](https://erpnext.com/docs/user/manual/en/customize-erpnext)
1. [And More](https://erpnext.com/docs/user/manual/en/)

ERPNext is built on the [Frappe Framework](https://github.com/frappe/frappe), a full-stack web app framework built with Python & JavaScript.

## Installation

<div align=""center"" style=""max-height: 40px;"">
    <a href=""https://frappecloud.com/erpnext/signup"">
        <img src="".github/try-on-f-cloud-button.svg"" height=""40"">
    </a>
    <a href=""https://labs.play-with-docker.com/?stack=https://raw.githubusercontent.com/frappe/frappe_docker/main/pwd.yml"">
      <img src=""https://raw.githubusercontent.com/play-with-docker/stacks/master/assets/images/button.png"" alt=""Try in PWD"" height=""37""/>
    </a>
</div>

> Login for the PWD site: (username: Administrator, password: admin)

### Containerized Installation

Use docker to deploy ERPNext in production or for development of [Frappe](https://github.com/frappe/frappe) apps. See https://github.com/frappe/frappe_docker for more details.

### Manual Install

The Easy Way: our install script for bench will install all dependencies (e.g. MariaDB). See https://github.com/frappe/bench for more details.

New passwords will be created for the ERPNext ""Administrator"" user, the MariaDB root user, and the frappe user (the script displays the passwords and saves them to ~/frappe_passwords.txt).


## Learning and community

1. [Frappe School](https://frappe.school) - Learn Frappe Framework and ERPNext from the various courses by the maintainers or from the community.
2. [Official documentation](https://docs.erpnext.com/) - Extensive documentation for ERPNext.
3. [Discussion Forum](https://discuss.erpnext.com/) - Engage with community of ERPNext users and service providers.
4. [Telegram Group](https://erpnext_public.t.me) - Get instant help from huge community of users.


## Contributing

1. [Issue Guidelines](https://github.com/frappe/erpnext/wiki/Issue-Guidelines)
1. [Report Security Vulnerabilities](https://erpnext.com/security)
1. [Pull Request Requirements](https://github.com/frappe/erpnext/wiki/Contribution-Guidelines)
1. [Translations](https://translate.erpnext.com)


## License

GNU/General Public License (see [license.txt](license.txt))

The ERPNext code is licensed as GNU General Public License (v3) and the Documentation is licensed as Creative Commons (CC-BY-SA-3.0) and the copyright is owned by Frappe Technologies Pvt Ltd (Frappe) and Contributors.

By contributing to ERPNext, you agree that your contributions will be licensed under its GNU General Public License (v3).

## Logo and Trademark Policy

Please read our [Logo and Trademark Policy](TRADEMARK_POLICY.md).
",14931,14931,598,1590,retail,"[accounting, asset-management, crm, distribution, erp, erpnext, frappe, hacktoberfest, healthcare, hrms, integrations, localization, manufacturing, point-of-sale, procurement, profit, project-management, python, retail, support]",44-45
shopizer-ecommerce,shopizer,,https://github.com/shopizer-ecommerce/shopizer,https://api.github.com/repos/shopizer/shopizer-ecommerce,Shopizer java e-commerce software,"# Shopizer 3.X (for java 1.8 +) (tested with Java 11)



[![last_version](https://img.shields.io/badge/last_version-v3.2.5-blue.svg?style=flat)](https://github.com/shopizer-ecommerce/shopizer/tree/3.2.5)
[![Official site](https://img.shields.io/website-up-down-green-red/https/shields.io.svg?label=official%20site)](http://www.shopizer.com/)
[![Docker Pulls](https://img.shields.io/docker/pulls/shopizerecomm/shopizer.svg)](https://hub.docker.com/r/shopizerecomm/shopizer)
[![stackoverflow](https://img.shields.io/badge/shopizer-stackoverflow-orange.svg?style=flat)](http://stackoverflow.com/questions/tagged/shopizer)
[![CircleCI](https://circleci.com/gh/shopizer-ecommerce/shopizer.svg?style=svg)](https://circleci.com/gh/shopizer-ecommerce/shopizer)


Java open source e-commerce software

Headless commerce and Rest api for ecommerce

- Catalog
- Shopping cart
- Checkout
- Merchant
- Order
- Customer
- User

Shopizer Headless commerce consists of the following components:

- Spring boot Java / Spring boot backend
- Angular administration web application
- React JS front end application



See the demo: [**New demo on the way 2023]
-------------------
Headless demo Available soon

1.  Run from Docker images:

From the command line:

```
docker run -p 8080:8080 shopizerecomm/shopizer:latest
```
       
2. Run the administration tool

⋅⋅⋅ Requires the java backend to be running

```
docker run \
 -e ""APP_BASE_URL=http://localhost:8080/api"" \
 -p 82:80 shopizerecomm/shopizer-admin
```


3. Run react shop sample site

⋅⋅⋅ Requires the java backend to be running

```
docker run \
 -e ""APP_MERCHANT=DEFAULT""
 -e ""APP_BASE_URL=http://localhost:8080""
 -p 80:80 shopizerecomm/shopizer-shop-reactjs
```

API documentation:
-------------------

https://app.swaggerhub.com/apis-docs/shopizer/shopizer-rest-api/3.0.1#/

Get the source code:
-------------------
Clone the repository:
     
	 $ git clone git://github.com/shopizer-ecommerce/shopizer.git
	 
	 $ git clone git://github.com/shopizer-ecommerce/shopizer-admin.git
	 
	 $ git clone git://github.com/shopizer-ecommerce/shopizer-shop-reactjs.git

If this is your first time using Github, review http://help.github.com to learn the basics.

You can also download the zip file containing the code from https://github.com/shopizer-ecommerce for each of the the projects above

To build the application:
-------------------

1. Shopizer backend


From the command line:

	$ cd shopizer
	$ mvnw clean install
	$ cd sm-shop
	$ mvnw spring-boot:run

2. Shopizer admin

Form compiling and running Shopizer admin consult the repo README file

3. Shop sample site

Form compiling and running Shopizer admin consult the repo README file


### Access the application:
-------------------

Access the headless web application at: http://localhost:8080/swagger-ui.html


The instructions above will let you run the application with default settings and configurations.
Please read the instructions on how to connect to MySQL, configure an email server and configure other subsystems


### Documentation:
-------------------

Documentation available <https://shopizer-ecommerce.github.io/documentation/>

Api documentation <https://app.swaggerhub.com/apis-docs/shopizer/shopizer-rest-api/3.0.1#/>

ChatOps <https://shopizer.slack.com>  - Join our Slack channel <https://communityinviter.com/apps/shopizer/shopizer>

More information is available on shopizer web site here <http://www.shopizer.com>

### Participation:
-------------------

If you have interest in giving feedback or for participating to Shopizer project in any way
Feel to use the contact form <http://www.shopizer.com/contact.html> and share your email address
so we can send an invite to our Slack channel

### How to Contribute:
-------------------
Fork the repository to your GitHub account

Clone from fork repository
-------------------

       $ git clone https://github.com/yourusername/shopizer.git

Build application according to steps provided above

Synchronize lastest version with the upstream
-------------------

      $ git remote add upstream https://github.com/yourusername/shopizer.git
	  $ git pull upstream 3.2.5

Create new branch in your repository
-------------------

	   $ git checkout -b branch-name


Push your changes to Shopizer
-------------------

Please open a PR (pull request) in order to have your changes merged to the upstream


",3257,3257,295,457,retail,"[apache2, cloud, e-commerce, java, retail, shopizer, springboot, springframework, springmvc]",44-45
microsoft,forecasting,microsoft,https://github.com/microsoft/forecasting,https://api.github.com/repos/forecasting/microsoft,Time Series Forecasting Best Practices & Examples,"# Forecasting Best Practices 

Time series forecasting is one of the most important topics in data science. Almost every business needs to predict the future in order to make better decisions and allocate resources more effectively.

This repository provides examples and best practice guidelines for building forecasting solutions. The goal of this repository is to build a comprehensive set of tools and examples that leverage recent advances in forecasting algorithms to build solutions and operationalize them. Rather than creating implementations from scratch, we draw from existing state-of-the-art libraries and build additional utilities around processing and featurizing the data, optimizing and evaluating models, and scaling up to the cloud. 

The examples and best practices are provided as [Python Jupyter notebooks and R markdown files](examples) and [a library of utility functions](fclib). We hope that these examples and utilities can significantly reduce the “time to market” by simplifying the experience from defining the business problem to the development of solutions by orders of magnitude. In addition, the example notebooks would serve as guidelines and showcase best practices and usage of the tools in a wide variety of languages.

## Cleanup notice (2020-06-23)

> We've carried out a cleanup of large obsolete files to reduce the size of this repo. If you had cloned or forked it previously, please delete and clone/fork it again to avoid any potential merge conflicts.


## Content

The following is a summary of models and methods for developing forecasting solutions covered in this repository. The [examples](examples) are organized according to use cases. Currently, we focus on a retail sales forecasting use case as it is widely used in [assortment planning](https://repository.upenn.edu/cgi/viewcontent.cgi?article=1569&context=edissertations), [inventory optimization](https://en.wikipedia.org/wiki/Inventory_optimization), and [price optimization](https://en.wikipedia.org/wiki/Price_optimization). To enable high-throughput forecasting scenarios, we have included examples for forecasting multiple time series with distributed training techniques such as Ray in Python, parallel package in R, and multi-threading in LightGBM. Note that html links are provided next to R examples for best viewing experience when reading this document on our [`github.io`](https://microsoft.github.io/forecasting/) page.

| Model                                                                                             | Language | Description                                                                                                 |
|---------------------------------------------------------------------------------------------------|----------|-------------------------------------------------------------------------------------------------------------|
| [Auto ARIMA](examples/grocery_sales/python/00_quick_start/autoarima_single_round.ipynb)           | Python   | Auto Regressive Integrated Moving Average (ARIMA) model that is automatically selected                      |
| [Linear Regression](examples/grocery_sales/python/00_quick_start/azure_automl_single_round.ipynb) | Python   | Linear regression model trained on lagged features of the target variable and external features             |
| [LightGBM](examples/grocery_sales/python/00_quick_start/lightgbm_single_round.ipynb)              | Python   | Gradient boosting decision tree implemented with LightGBM package for high accuracy and fast speed          |
| [DilatedCNN](examples/grocery_sales/python/02_model/dilatedcnn_multi_round.ipynb)                 | Python   | Dilated Convolutional Neural Network that captures long-range temporal flow with dilated causal connections |
| [Mean Forecast](examples/grocery_sales/R/02_basic_models.Rmd) [(.html)](examples/grocery_sales/R/02_basic_models.nb.html)                                | R        | Simple forecasting method based on historical mean                                                          |
| [ARIMA](examples/grocery_sales/R/02a_reg_models.Rmd) [(.html)](examples/grocery_sales/R/02a_reg_models.nb.html)                                              | R        | ARIMA model without or with external features                                                               |
| [ETS](examples/grocery_sales/R/02_basic_models.Rmd) [(.html)](examples/grocery_sales/R/02_basic_models.nb.html)                                              | R        | Exponential Smoothing algorithm with additive errors                                                        |
| [Prophet](examples/grocery_sales/R/02b_prophet_models.Rmd) [(.html)](examples/grocery_sales/R/02b_prophet_models.nb.html)                                       | R        | Automated forecasting procedure based on an additive model with non-linear trends                           |

The repository also comes with AzureML-themed notebooks and best practices recipes to accelerate the development of scalable, production-grade forecasting solutions on Azure. In particular, we have the following examples for forecasting with Azure AutoML as well as tuning and deploying a forecasting model on Azure.

| Method                                                                                                    | Language | Description                                                                                                |
|-----------------------------------------------------------------------------------------------------------|----------|------------------------------------------------------------------------------------------------------------|
| [Azure AutoML](examples/grocery_sales/python/00_quick_start/azure_automl_single_round.ipynb)              | Python   | AzureML service that automates model development process and identifies the best machine learning pipeline |
| [HyperDrive](examples/grocery_sales/python/03_model_tune_deploy/azure_hyperdrive_lightgbm.ipynb)          | Python   | AzureML service for tuning hyperparameters of machine learning models in parallel on cloud                 |
| [AzureML Web Service](examples/grocery_sales/python/03_model_tune_deploy/azure_hyperdrive_lightgbm.ipynb) | Python   | AzureML service for deploying a model as a web service on Azure Container Instances                        |


## Getting Started in Python

To quickly get started with the repository on your local machine, use the following commands.

1. Install Anaconda with Python >= 3.6. [Miniconda](https://conda.io/miniconda.html) is a quick way to get started.

2. Clone the repository
    ```
    git clone https://github.com/microsoft/forecasting
    cd forecasting/
    ```

3. Run setup scripts to create conda environment. Please execute one of the following commands from the root of Forecasting repo based on your operating system.

    - Linux
    ```
    ./tools/environment_setup.sh
    ```

    - Windows
    ```
    tools\environment_setup.bat
    ```

    Note that for Windows you need to run the batch script from Anaconda Prompt. The script creates a conda environment `forecasting_env` and installs the forecasting utility library `fclib`.

4. Start the Jupyter notebook server
    ```
    jupyter notebook
    ```
    
5. Run the [LightGBM single-round](examples/grocery_sales/python/00_quick_start/lightgbm_single_round.ipynb) notebook under the `00_quick_start` folder. Make sure that the selected Jupyter kernel is `forecasting_env`.

If you have any issues with the above setup, or want to find more detailed instructions on how to set up your environment and run examples provided in the repository, on local or a remote machine, please navigate to the [Setup Guide](./docs/SETUP.md).

## Getting Started in R

We assume you already have R installed on your machine. If not, simply follow the [instructions on CRAN](https://cloud.r-project.org/) to download and install R.

The recommended editor is [RStudio](https://rstudio.com), which supports interactive editing and previewing of R notebooks. However, you can use any editor or IDE that supports RMarkdown. In particular, [Visual Studio Code](https://code.visualstudio.com) with the [R extension](https://marketplace.visualstudio.com/items?itemName=Ikuyadeu.r) can be used to edit and render the notebook files. The rendered `.nb.html` files can be viewed in any modern web browser.

The examples use the [Tidyverts](https://tidyverts.org) family of packages, which is a modern framework for time series analysis that builds on the widely-used [Tidyverse](https://tidyverse.org) family. The Tidyverts framework is still under active development, so it's recommended that you update your packages regularly to get the latest bug fixes and features.

## Target Audience
Our target audience for this repository includes data scientists and machine learning engineers with varying levels of knowledge in forecasting as our content is source-only and targets custom machine learning modelling. The utilities and examples provided are intended to be solution accelerators for real-world forecasting problems.

## Contributing
We hope that the open source community would contribute to the content and bring in the latest SOTA algorithm. This project welcomes contributions and suggestions. Before contributing, please see our [Contributing Guide](CONTRIBUTING.md).

## Reference

The following is a list of related repositories that you may find helpful.

|                                                                                                            |                                                                                                 |
|------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------|
| [Deep Learning for Time Series Forecasting](https://github.com/Azure/DeepLearningForTimeSeriesForecasting) | A collection of examples for using deep neural networks for time series forecasting with Keras. |
| [Microsoft AI Github](https://github.com/microsoft/ai)                                                     | Find other Best Practice projects, and Azure AI designed patterns in our central repository.    |



## Build Status

| Build         | Branch  | Status  |
| -  | -  | - |
| **Linux CPU** | master  | [![Build Status](https://dev.azure.com/best-practices/forecasting/_apis/build/status/cpu_unit_tests_linux?branchName=master)](https://dev.azure.com/best-practices/forecasting/_build/latest?definitionId=128&branchName=master)   |
| **Linux CPU** | staging | [![Build Status](https://dev.azure.com/best-practices/forecasting/_apis/build/status/cpu_unit_tests_linux?branchName=staging)](https://dev.azure.com/best-practices/forecasting/_build/latest?definitionId=128&branchName=staging) |
",2622,2622,104,0,retail,"[artificial-intelligence, automl, azure-ml, best-practices, deep-learning, demand-forecasting, dilated-cnn, forecasting, hyperparameter-tuning, jupyter-notebook, lightgbm, machine-learning, model-deployment, prophet, python, r, retail, tidyverse, time-series]",44-45
doublechaintech,scm-biz-suite,doublechaintech,https://github.com/doublechaintech/scm-biz-suite,https://api.github.com/repos/scm-biz-suite/doublechaintech,"供应链中台系统基础版，集成零售管理, 电子商务, 供应链管理,  财务管理, 车队管理, 仓库管理, 人员管理, 产品管理, 订单管理, 会员管理, 连锁店管理, 加盟管理, 前端React/Ant Design, 后端Java Spring+自有开源框架，全面支持MySQL, PostgreSQL, 全面支持国产数据库南大通用GBase 8s,通过REST接口调用，前后端完全分离。","# 请使用SCMR1来建立环境，项目将有重大更新！！

# DaaS新特性TeaQL，用于支持大型复杂关联应用

```java

    Task task =Q.task(orderId) // 根据订单找到一个任务
                .selectAll() // 取所有字段, 但是不包含子列表
                .selectDropOffTaskItemList( // 选择所有的卸车任务
                    Q.dropOffTaskItem() // 定制卸车任务
                        .selectProduct() // 选择产品
                        .selectCustomOrder( // 卸车任务上面还关联了一个订单
                            Q.customOrder() // 定制订单选择
                                .selectAll() // 选择订单所有字段
                                .selectCustomOrderItemList() // 选择订单下面的订单项
                                .selectDeliveryOrderAssetList( // 选择订单子列表下面的相关资产列表
                                    Q.deliveryOrderAsset() // 定制订单资产列表
                                        .selectAsset( // 选择资产对象
                                            Q.asset() // 定制资产选择
                                                .selectAssetStatus() // 状态要加上，便于显示资产状态
                                                .where( // 把不合法的资产过滤出去
                                                    Asset.ASSET_STATUS_PROPERTY,
                                                    QueryOperator.NOT_EQUAL,
                                                    AssetStatus.INVALID))))).execute(ctx);
```
TeaQL是双链团队新发明的基于各种编程语言的应用语言，为高科维护的大型应用开发提供帮助。

# 集成供应链套件（全部源码）

高度可定制零售供应链中台基础系统，中台管理界面可通过javascript高阶函数定制，Java后台主要通过**增加方法**或者**重写**已有的大量方法来灵活定制。

本系统代码是用自研知识图谱和因果网络处理系统自动生成高可维护源代码。

传统开发方式无法开发如此大规模的应用系统并且灵活变更，我们使用了自研的云端开发工具DaaS（Development as a Service）开发了这个平台，目前已有多家公司采用，请参见[DaaS Start Kit](https://github.com/doublechaintech/daas-start-kit)， 这个项目的模型文件 retailscm.xml 也在里面（运行该模型需要注册）。注意：运行本项目无需DaaS，直接参考部署手册。

智能化开发的简单例子，请参见[医生排班系统](https://github.com/doublechaintech/his-biz-suite)


针对生鲜供应链, 请访问：
* https://demo.doublechaintech.com/admin/freshchain/index.html


| 角色        | 用户名           | 密码         |
| ------------- |:-------------:|:-------------------:|
| 平台        | 13900000001           | admin123         |
| 商户1        | 13900000002           | admin123         |
| 商户2        | 13900000003           | admin123         |

主要特征如下:

* 生鲜供应链是基于点对点多商户模式构建的系统
* 商户之间的关系是平等的， 即使核心商户也是这样，
* 每个商户自己的组织结构，有采购目录和销售目录,通过这种方式，可以把商户之间的信息流、商流、物流链接起来，形成一个供应链
* 传统供应链系统就像SAP基于核心企业作为使用对象，上下游合作企业并没有供应链账号可用
* 本供应链是使得上下游企业也可以通过管理完成的人力资源、产品信息交换，库存等。可以通过定制本系统实现不同的应用。


## 目录

- [集成供应链套件](#集成供应链套件)
  - [目录](#目录)
  - [业务涵盖范围](#业务涵盖范围)
  - [核心特性](#核心特性)
    - [架构](#架构)
      - [前端架构](#前端架构)
      - [后端架构](#后端架构)
      - [权限管理](#权限管理)
      - [部署架构](#部署架构)
    - [项目概念 来自于 数据库设计](#项目概念-来自于-数据库设计)
  - [如何使用](#如何使用)
  - [许可](#许可)
  - [技术支持](#技术支持)
  - [兼容性，以下环境测试通过](#兼容性以下环境测试通过)
  - [二次开发](#二次开发)
  - [部署](#部署)
  - [在线演示系统(不支持IE，最好使用Chrome）](#在线演示系统不支持ie最好使用chrome)
    - [总部及分支机构运营](#总部及分支机构运营)
    - [业务扩展](#业务扩展)
    - [落地业务运营](#落地业务运营)
    - [采购和供应商管理](#采购和供应商管理)
    - [仓配一体化管理](#仓配一体化管理)
    - [人力资源管理](#人力资源管理)
    - [用户权限管理](#用户权限管理)
  - [组成部分](#组成部分)
    - [bizcore: 服务器端核心代码项目， Business Core](#bizcore-服务器端核心代码项目-business-core)
    - [bizui：中台集成界面项目， Business UI，不是闭嘴！](#bizui中台集成界面项目-business-ui不是闭嘴)
    - [数据中台（独立产品在本项目应用）](#数据中台独立产品在本项目应用)
    - [配套的数据大屏](#配套的数据大屏)
    - [核心功能](#核心功能)
    - [额外数据库支持](#额外数据库支持)
  - [参与本项目](#参与本项目)
 

## 业务涵盖范围

* 参考下面的图

![概念关系](/doc/retailscm-concept-tree.jpg)


## 核心特性
* 本系统包括前端、后端、数据大屏，数据结构和基础数据都是通过自研软件开发开发
* 高度可定制，增加、修改、屏蔽界面和后台功能不必更改生成代码，只是需要在custom层加入代码即可，看后文解释
* 以零售平台为核心开发，集成五流：信息流，商流，物流，资金流，人才流。

### 架构


#### 前端架构
![前端架构](/doc/front-arch.jpg)

#### 后端架构
![后端架构](/doc/backend-arch.jpg)

#### 权限管理
![权限管理](/doc/iam-arch.jpg)
#### 部署架构

![部署架构](/doc/deploy-arch.jpg)


### 项目概念 来自于 [数据库设计](/doc/retailscm-database-design.pdf)

![ScreenShot](/doc/word-cloud.png)

* 桑基图, 展现概念血缘

![ScreenShot](/doc/retailscm-sankey.jpg)

通过 https://demo.doublechaintech.com/admin/design/retail_design.html 可以看到交互式查看模型



<img width=""760"" alt=""retail-san-key"" src=""./doc/newsankey.png"">


## 如何使用
* 在此基础上定制业务系统，可以在这个系统基础上通过前后台增加代码来定制业务系统
* 建立数据中台，这样源代码几乎不需要改动，本系统提供了大量的接口，可以让业务系统通过API插入数据，可以视作一个独立的微服务
* 用于教学和练习

## 许可

除Logo，登录页和主页图片以外，本系统源代码为Apache 2 License，可以用于商用目的

## 技术支持

本系统提供商业化系统支持和基于Issue的免费的技术支持

## 兼容性，以下环境测试通过

* 硬件环境：Amd64/ARM64/华为鲲鹏服务器
* 操作系统环境：CentOS 6+/Ubuntu 16.04+
* 数据库：MySQL 5.7+， GBase 8s，Informix 11， PostgreSQL 9.3+
* 缓存系统: Redis 3.2+
* 事件流系统: Kafka

## 二次开发
* Java开发手册请参见：https://kdocs.cn/l/sUdwkkyZD?f=130
* 数据库设计文档(共计94页）: [数据库设计](/doc/retailscm-database-design.pdf)
* 包含顺序图的详细设计文档(共计200多页）：[详细设计](/doc/retailscm-detail-design.pdf)


## 部署

如果要自行部署，请参照 [部署指南](/DEPLOYMENT.md)

## 在线演示系统(不支持IE，最好使用Chrome）


系统平台框架天然支持一个用户多个App，下面为18种角色建立了演示账户，每种角色都可以登录，代表不同用户管理的不同资源。
* 最新支持数据搜索
* 功能分组

### 总部及分支机构运营

| 角色        | 用户名           | 密码         |
| ------------- |:-------------:|:-------------------:|
|零售商店全国中心|13900000001|admin123|
|零售商店省中心|13900000002|admin123|
|零售商店城市服务中心|13900000003|admin123|

演示地址：https://demo.doublechaintech.com/admin/retailscm/index.html


### 业务扩展

| 角色        | 用户名           | 密码         |
| ------------- |:-------------:|:-------------------:|
|城市的合作伙伴|13900000004|admin123|
|潜在客户|13900000005|admin123|


演示地址：https://demo.doublechaintech.com/admin/retailscm/index.html

### 落地业务运营

| 角色        | 用户名           | 密码         |
| ------------- |:-------------:|:-------------------:|
|零售商店|13900000006|admin123|
|零售商店的会员|13900000007|admin123|


演示地址：https://demo.doublechaintech.com/admin/retailscm/index.html


### 采购和供应商管理
| 角色        | 用户名           | 密码         |
| ------------- |:-------------:|:-------------------:|
|消费者的订单|13900000008|admin123|
|产品供应商|13900000009|admin123|
|供应订单|13900000010|admin123|
|零售商店的订单|13900000011|admin123|


演示地址：https://demo.doublechaintech.com/admin/retailscm/index.html

### 仓配一体化管理
| 角色        | 用户名           | 密码         |
| ------------- |:-------------:|:-------------------:|
|仓库|13900000012|admin123|
|货架|13900000013|admin123|
|运输车队|13900000014|admin123|
|运输任务|13900000015|admin123|
|会计凭证|13900000017|admin123|


演示地址：https://demo.doublechaintech.com/admin/retailscm/index.html


### 人力资源管理
| 角色        | 用户名           | 密码         |
| ------------- |:-------------:|:-------------------:|
|工资等级|13900000018|admin123|
|员工|13900000020|admin123|


演示地址：https://demo.doublechaintech.com/admin/retailscm/index.html

### 用户权限管理

| 角色        | 用户名           | 密码         |
| ------------- |:-------------:|:-------------------:|
|用户域|13900000021|admin123|

演示地址：https://demo.doublechaintech.com/admin/retailscm/index.html

## 组成部分
### bizcore: 服务器端核心代码项目， Business Core

后端 Java/Spring/Redis/MySQL/ArrangoDB/Kafka
Java 源代码在bizcore/WEB-INF/ 下


![ScreenShot](/doc/backend.png)
````
caf_core_src: 通用框架库核心代码，包含技术框架，基础设施
caf_custom_src: 通用框架库，主要包含配置文件
retailscm_core_src: 零售业务核心代码
retailscm_custom_src: 零售业务定制代码，如果要定制，在此处增加类，继承retail_core_src的相应类，可以调用或者重写父类，core里面的类提供大量可以重用的方法。
````

### bizui：中台集成界面项目， Business UI，不是闭嘴！
前端 React/dvajs/antd/echarts/Redux


![ScreenShot](/doc/home-screen.jpg)
![ScreenShot](/doc/root-app.jpg)


### 数据中台（独立产品在本项目应用,使用了图计算等很多新技术，助做经营决策可以，不能当成报表、结算等数据来源）

* 新一代图计算实时引擎
* 数据分析结果管理与分享
* 支持客户端通过API调用数据，实现闭环反馈
* 实时推送和订阅
演示地址：https://demo.doublechaintech.com/admin/dmos/ 
* 用户名/密码: 13900000005/admin123
* 浏览任何数据集->菜单选择->常用功能->候选数据集


![ScreenShot](/doc/dmos000.jpg)
![ScreenShot](/doc/dmos001.jpg)
![ScreenShot](/doc/dmos002.jpg)
![ScreenShot](/doc/dmos003.jpg)

### 配套的数据大屏

![ScreenShot](/doc/datascreen.png)


### 核心功能
* 平台管理，平台鸟瞰视图
* 业务开拓管理（CRM），管理联系，销售进度，完成合伙人转化
* 小超会员管理，会员管理订单，支付，邮寄地址等
* 人力资源管理（HRM），可以管理入职，绩效考核，工资发放，经验，教育经历，培训考试记录

### 未来发展
* 渐进模式，从单个核心企业的中心化SaaS模式，到多核心企业多中心SaaS
* 应用区块链技术，扩展到多中心
* 变更控制：所有数据变更，走控制流程，数据来源可以解释
* 更强的基础分析工作台：支持多个维度分析单个列表的数据。



### 额外数据库支持

目前已经支持PGSQL9.5+和GBASE 8S

## 参与本项目
<img src=""/doc/philip-wechat-qr.jpeg"" alt=""WechatQRCode"" title=""WechatQRCode"" width=""150""  />

* 探讨供应链发展趋势
* 探讨供应链相关技术和产品
* 探讨分享市场机遇
* 验证时请输入供应链相关的名词: scm, gyl, 供应链

## 升级计划
* 本项目于2017年开发，2019年初开源，开发团队在四年里获取了更进一步的供应链知识，模型需求更新
* 模型上像对等供应链发展
* 业务操作将提高操作的跟踪性
* 升级到新的基础平台
* JSON定义动态前端


",2243,2243,96,10,retail,"[antd, digital-transformation, echarts, ecommerce, erp, fleet-management, headless-commerce, java, knowledge-graph, mysql, oracle-atg-alternative, ordermanagement, redis, retail, scm, spring, supply-chain, tms, warehouse-management]",44-45
pbatard,Fido,,https://github.com/pbatard/Fido,https://api.github.com/repos/Fido/pbatard,A PowerShell script to download Windows or UEFI Shell ISOs,"Fido: A PowerShell download script for Microsoft Windows and UEFI Shell ISOs
============================================================================

[![Licence](https://img.shields.io/badge/license-GPLv3-blue.svg?style=flat-square)](https://www.gnu.org/licenses/gpl-3.0.en.html)
[![Github stats](https://img.shields.io/github/downloads/pbatard/Fido/total.svg?style=flat-square)](https://github.com/pbatard/Fido/releases)

Description
-----------

Fido is a PowerShell script that is primarily designed to be used in [Rufus](https://github.com/pbatard/rufus), but that
can also be used in standalone fashion, and whose purpose is to automate access to the official Microsoft Windows retail
ISO download links as well as provide convenient access to [bootable UEFI Shell images](https://github.com/pbatard/UEFI-Shell).

This script exists because, while Microsoft does make retail ISO download links freely and publicly available (at least
for Windows 8 through Windows 11), up until recent releases, most of these links were only available after forcing users
to jump through a lot of unwarranted hoops that created an exceedingly counterproductive, if not downright unfriendly,
consumer experience, that greatly detracted from what people really want (direct access to ISO downloads).

As to the reason one might want to download Windows __retail__ ISOs, as opposed to the ISOs that are generated by
Microsoft's own Media Creation Tool (MCT), this is because using official retail ISOs is currently the only way to
assert with absolute certainty that the OS content has not been altered. Indeed, because there only exists a single
master for each of them, Microsoft retail ISOs are the only ones you can obtain an official SHA-1 for (from MSDN, if you
have access to it, or from sites [such as this one](https://msdn.rg-adguard.net/public.php)) allowing you to be 100%
sure that the image you are using has not been corrupted and is safe to use.

This, in turn, offers assurance that the content __YOU__ are using to install your OS, which it is indeed critical to
validate beforehand if you have the slightest concern about security, does match, bit for bit, the one that Microsoft
released.

On the other hand, regardless of the manner in which Microsoft's Media Creation Tool produces its content, because no
two MCT ISOs are ever the same (due to MCT always regenerating the ISO content on the fly) it is currently impossible to
validate with absolute certainty whether any ISO that was generated by the MCT is safe to use. Especially, unlike what
is the case for retail ISOs, it is impossible to tell whether an MCT ISO may have been corrupted after generation.

Hence the need to provide users with a much easier and less restrictive way to access official retail ISOs...

License
-------

[GNU General Public License version 3.0](https://www.gnu.org/licenses/gpl-3.0) or later.

How it works
------------

The script basically performs the same operation as one might perform when visiting the following URL (that is, in the
case of Windows 10, provided that you have also changed your `User-Agent` browser string, since, the Microsoft web
servers detect that you are using a version of Windows that is the same as the one you are trying to download, they
may redirect you __away__ from the page that allows you to obtain a direct ISO download link):

https://www.microsoft.com/en-us/software-download

After checking basic access to the Microsoft software downloads website the script first queries the web API from the
Microsoft servers, to request the language selection available for the version of Windows selected, and then requests
the actual download links, for all the architectures available for that language + version.

Requirements
------------

Windows 8 or later with PowerShell. Windows 7 is __not__ supported.

Commandline mode
----------------

Fido supports commandline mode whereas, whenever one of the following options is provided, a GUI is not instantiated
and you can instead generate the ISO download from within a PowerShell console or script.

Note however that, as of 2023.05, Microsoft has removed access to older releases of Windows ISOs and as a result, the
list of releases that can be downloaded from Fido has had to be reduced to only the latest for each version.

The options are:
- `Win`: Specify Windows version (e.g. _""Windows 10""_). Abbreviated version should work as well (e.g `-Win 10`) as long
   as it is unique enough. If this option isn't specified, the most recent version of Windows is automatically selected.
   You can obtain a list of supported versions by specifying `-Win List`.
- `Rel`: Specify Windows release (e.g. _""21H1""_). If this option isn't specified, the most recent release for the chosen
   version of Windows is automatically selected. You can also use `-Rel Latest` to force the most recent to be used.
   You can obtain a list of supported versions by specifying `-Rel List`.
- `Ed`: Specify Windows edition (e.g. _""Pro/Home""_). Abbreviated editions should work as well (e.g `-Ed Pro`) as long
   as it is unique enough. If this option isn't specified, the most recent version of Windows is automatically selected.
   You can obtain a list of supported versions by specifying `-Ed List`.
- `Lang`: Specify Windows language (e.g. _""Arabic""_). Abbreviated or part of a language (e.g. `-Lang Int` for
   `English International`) should work as long as it's unique enough. If this option isn't specified, the script attempts
   to select the same language as the system locale.
   You can obtain a list of supported languages by specifying `-Lang List`.
- `Arch`: Specify Windows architecture (e.g. _""x64""_). If this option isn't specified, the script attempts to use the same
   architecture as the one from the current system.
- `GetUrl`: By default, the script attempts to automatically launch the download. But when using the `-GetUrl` switch,
   the script only displays the download URL, which can then be piped into another command or into a file.

Examples of a commandline download:

```
PS C:\Projects\Fido> .\Fido.ps1 -Win 10
No release specified (-Rel). Defaulting to '21H1 (Build 19043.985 - 2021.05)'.
No edition specified (-Ed). Defaulting to 'Windows 10 Home/Pro'.
No language specified (-Lang). Defaulting to 'English International'.
No architecture specified (-Arch). Defaulting to 'x64'.
Selected: Windows 10 21H1 (Build 19043.985 - 2021.05), Home/Pro, English International, x64
Downloading 'Win10_21H1_EnglishInternational_x64.iso' (5.0 GB)...
PS C:\Projects\Fido> .\Fido.ps1 -Win 10 -Rel List
Please select a Windows Release (-Rel) for Windows 10 (or use 'Latest' for most recent):
 - 21H1 (Build 19043.985 - 2021.05)
 - 20H2 (Build 19042.631 - 2020.12)
 - 20H2 (Build 19042.508 - 2020.10)
 - 20H1 (Build 19041.264 - 2020.05)
 - 19H2 (Build 18363.418 - 2019.11)
 - 19H1 (Build 18362.356 - 2019.09)
 - 19H1 (Build 18362.30 - 2019.05)
 - 1809 R2 (Build 17763.107 - 2018.10)
 - 1809 R1 (Build 17763.1 - 2018.09)
 - 1803 (Build 17134.1 - 2018.04)
 - 1709 (Build 16299.15 - 2017.09)
 - 1703 [Redstone 2] (Build 15063.0 - 2017.03)
 - 1607 [Redstone 1] (Build 14393.0 - 2016.07)
 - 1511 R3 [Threshold 2] (Build 10586.164 - 2016.04)
 - 1511 R2 [Threshold 2] (Build 10586.104 - 2016.02)
 - 1511 R1 [Threshold 2] (Build 10586.0 - 2015.11)
 - 1507 [Threshold 1] (Build 10240.16384 - 2015.07)
PS C:\Projects\Fido> .\Fido.ps1 -Win 10 -Rel 20H2 -Ed Edu -Lang Fre -Arch x86 -GetUrl
https://software-download.microsoft.com/db/Win10_Edu_20H2_v2_French_x32.iso?t=c48b32d3-4cf3-46f3-a8ad-6dd9568ff4eb&e=1629113408&h=659cdd60399584c5dc1d267957924fbd
```

Additional Notes
----------------

Because of its intended usage with Rufus, this script is not designed to cover every possible retail ISO downloads.
Instead we mostly chose the ones that the general public is likely to request. For instance, we currently have no plan
to add support for LTSB/LTSC Windows ISOs downloads.

If you are interested in such downloads, then you are kindly invited to visit the relevant download pages from Microsoft
such as [this one](https://www.microsoft.com/evalcenter/evaluate-windows-10-enterprise) for LTSC versions.
",2038,2038,62,3,retail,"[edk2, iso, powershell-script, retail, uefi, uefi-shell, ui, windows, windows-10, windows-8-1, windows11]",44-45
brakmic,BlockchainStore,,https://github.com/brakmic/BlockchainStore,https://api.github.com/repos/BlockchainStore/brakmic,:moneybag: Retail Store that runs on Ethereum ,"## Retail Store on Blockchain

### About

This is a [Smart Contract](https://github.com/brakmic/BlockchainStore/blob/master/contracts/Store.sol) that runs on [Ethereum](https://www.ethereum.org/)

It is written in [Solidity](https://solidity.readthedocs.io/en/develop/) and represents a retail store. It supports customer and product registrations. Every registered customer owns a shopping cart to collect products before checking out.

### Dapp

In this early version there's no proper web interface available and you'll have to use `truffle console` to access the contract. In future I'll provide a web-app written in Angular 4.x. The ultimate goal is to not only produce a web-site but a complete **web-platform** behind it. *Embedding* a real-world business model into something like a DApp implies certain functionalities:

* **database** [*you certainly don't want to store your customers personal data on the blockchain*]

* **error handling** [*there's no error-handling in Ethereum but your business isn't Ethereum*]

* **transactions** [*Ethereum transactions aren't your business transactions*]

* **unavoidable updates** [*no code is eternal*]

* **automatic backups** [*I'm repeating myself...see databases above*]

* **backend APIs** [*for example: detailed product infos, currency conversions, geo-locations etc.*]

...and many other things.

Giving customers an interface where they can add or remove products to/from their shopping carts is important but not the ultimate goal. The shopping experience on the UI **and** a sophisticated business logic in the backend must both exist to support each other. As long as we can't put a *non-public & fast* database on Ethereum we'll have to maintain it somewhere else. And to achieve this goal our Dapp will rely on backend APIs.

Currently, a simple demo to play around with [web3](https://github.com/ethereum/wiki/wiki/JavaScript-API)-API is available. To get the above demo working please follow these steps:

* Compile the contracts with `truffle compile`
* Then move the newly created *build* folder to *src*
* Now you can boot the app via `npm run start:hmr`

### Tokens

Store [Tokens](https://github.com/brakmic/BlockchainStore/blob/master/contracts/Tokens/BaseStoreToken.sol#L9) will soon be supported. One could use them to purchase goods in the store or for *initial coin offerings*. For example: you're planning to open a store that deals with certain popular goods but you're unsure how many potential customers are out there. Now you could simply buy some ethers or other coins to finance your store (to pay goods in advance, hire a dev to code a proper Dapp for your customers etc.). Now everything depends on how successful your business will be. You may or may not be able to sustain it.

As we all know there are always certain risks to take care of and that's why people try to convince other people to support their business ideas. So, you decide to sell shares of your nascent business to interested parties. You create a proper business info material, for example a web-site that describes your business, how it should look like, what are potential risks etc. You generate a certain amount of tokens based on a price that could be fixed or not. Let's say you sell *1 MyStoreToken* for *0.001 ETH*. Additionally you can determine certain limits and how long your ICO will last. Of course there's no obligation to create all of your tokens in advance. You could easily define a dynamic token supply that depends on incoming ETHs.

Until the first working version gets out you can test the current development in `truffle console`:

Declare a reference variable for deployed token contract.

> `var token;`

Get its reference via JS Promise.

> `BaseStoreToken.deployed().then(d => token = d);`

Display initial token supply.

> `token.initialSupply();`

Transfer 10 tokens from default address to web3.eth.accounts[1]

> `token.transfer(web3.eth.accounts[1], 10);`

### API

| Name  | Group  | Signature  | Usage  | Returns |
|:-|:-|:-|:-|:---|
| **transferOwnership**  | owner  | address   | store.transferOwnership(new_owner_address)  |   |
| **registerProduct**   | owner  | uint256, bytes32, bytes32, uint, uint   | store.registerProduct(id, name, description, price, default_amount)   | **bool** |
| **deregisterProduct**   | owner   | uint256   | store.deregisterProduct(id)   | **bool**  |
| **getProduct**  | customer  | uint256  | store.getProduct(id)  | (**bytes32** *name*, **bytes32** *description*, **uint256** *price*, **uint256** *default_amount*)  |
| **registerCustomer**  | owner   | address, bytes32, uint256  | store.registerCustomer(cust_address, cust_name, cust_balance)  |  **bool**  |
| **deregisterCustomer**  | owner   | address   | store.deregisterCustomer(cust_address)  | **bool**   |
| **insertProductIntoCart**  | customer  | uint256  | store.insertProductIntoCart(prod_id)  | (**bool** *success*, **uint256** *position_in_prod_mapping*)  |
| **removeProductFromCart**  | customer  | uint  | store.removeProductFromCart(prod_position_in_mapping)  | *fires an event on successful removal* |
| **getCart**  | customer  |   | store.getCart()  | (**uint256[] memory** *product_ids*, **uint256** *completeSum*)  |
| **checkoutCart**  | customer  |   | store.checkoutCart()  | **bool**  |
| **emptyCart** | customer | | store.emptyCart() | **bool** |
| **getBalance**  | customer  |   | store.getBalance()  | **uint256** |
| **renameStoreTo**  | owner  | bytes32  | store.renameStoreTo(new_store_name)  | **bool**  |

### Usage

First, activate a local test-blockchain with `testrpc`. If you don't have it just type `npm install -g ethereumjs-testrpc` and let NPM install it for you.

Second, go into the root of this project and execute `truffle compile` and `truffle migrate` (when changing the code during live-testing use `truffle migrate --reset` instead).

Third, jump into truffle's console with `truffle console`. Now you can use the local Blockchain to play with the Store :smile:

### Interactive Testing

I've created this project to learn a bit about Solidity & Ethereum. Expect no sophisticated code here. *And lots of bugs.*

**Here's how I interact with it:**

First, we'll need two addresses: a **customer** and a **seller**. By default *testrpc* registers ten Ethereum accounts at our disposal.

*For more information about the namespace web3.eth consult [truffle docs](http://truffleframework.com/docs/) and also Ethereum [JavaScript API](https://github.com/ethereum/wiki/wiki/JavaScript-API).*

> `var seller = web3.eth.accounts[0];`

> `var customer = web3.eth.accounts[1];`

We also need a reference to our Store.

> `var store;`

We get this reference asynchronously by executing this snippet.

> `Store.deployed().then(d => store = d);`

Now we register a new Customer with a certain amount of money. The original signature of [registerCustomer](https://github.com/brakmic/BlockchainStore/blob/master/contracts/Store.sol#L158) in Solidity differs a bit from the one used below. This is because we want to execute this API from our `seller` account. All available API calls can be expanded by using similar options that let Ethereum know which account should pay for the execution of the code. As you already know the **smart contracts** don't get executed for free. You have to pay the miners. You can also set the amount of `gas` that can be used. More information regarding these options can be found [here](http://truffleframework.com/docs/getting_started/contracts).

> `store.registerCustomer(customer, ""Harris"", 100, {from: seller});`

Our customers will hopefully buy some of our products. Now let's register one by using `registerProduct`. Note that I'm not using `{from: seller}` here. By default **truffle** executes transactions under the first available account address. Only when we explicitely want to have a transaction being executed under a different address, like in the **shopping cart checkout** below, we'll have to provide it.

> `store.registerProduct(0, ""t-shirt"", ""lacoste"", 40, 1);`

Now, as a customer we take a T-Shirt with id == 0 and put it into our cart.

> `store.insertProductIntoCart(0, {from: customer});`

Let's see what's in the cart. Note that we don't execute a **transaction** here. A transaction would try to change the state on the blockchain that makes no sense in this case. Instead we execute a *.call()* that returns the product ids and total sum.

> `store.getCart.call({from: customer});`

We also want to take care of proper event handling...

> `var allStoreEvents = store.allEvents().watch({}, '');`

...by registering an event handler that'll siphon them all.

> `allStoreEvents.watch(function(err, res) { console.log(""Error: "" + err); console.log(""Event: "" + res.event); });`

Let's try to **check out**. :smile:

> `store.checkoutCart({from: customer});`

Finally, let's see our balance after the checkout.

> `store.getBalance.call({from: customer});`

### Automatic Testing

The [tests](https://github.com/brakmic/BlockchainStore/blob/master/test/TestStore.sol#L7) are written in Solidity. Simply enter `truffle test` in your console.


### Thanks

Many thanks to the nice Ethereum community from [reddit](https://www.reddit.com/r/ethereum/comments/6ik0yb/learning_solidity_a_simple_storesmartcontract/).

Special thanks to [cintix](https://www.reddit.com/user/cintix) for the advice regarding [unbounded iterations](https://www.reddit.com/r/ethereum/comments/6ik0yb/learning_solidity_a_simple_storesmartcontract/dj70kww/).

### List of used images

[Cash Register](https://pixabay.com/en/cash-register-register-retail-sale-576181/) - CC0 Public Domain

[Ethereum Logo](https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/a2bcf4f8-9a5d-4f85-873b-cf94ce537eb0.png) - free for non-commercial use via Google Filter Settings

### License

[MIT](https://github.com/brakmic/BlockchainStore/blob/master/LICENSE)
",448,448,32,0,retail,"[bitcoin, blockchain, ethereum, ethereum-dapp, retail, smart-contracts, solidity, stores]",44-45
Tencent,tdesign-miniprogram-starter-retail,Tencent,https://github.com/Tencent/tdesign-miniprogram-starter-retail,https://api.github.com/repos/tdesign-miniprogram-starter-retail/Tencent,TDesign - 微信小程序 - 零售行业模板,"<p align=""center"">
  <a href=""https://tdesign.tencent.com/"" target=""_blank"">
    <img alt=""TDesign Logo"" width=""200"" src=""https://tdesign.gtimg.com/site/TDesign.png"">
  </a>
</p>

<p align=""center"">
  <a href=""https://img.shields.io/github/stars/Tencent/tdesign-miniprogram-starter-retail"">
    <img src=""https://img.shields.io/github/stars/Tencent/tdesign-miniprogram-starter-retail"" alt=""License"">
  </a>  
  <a href=""https://github.com/Tencent/tdesign-miniprogram-starter-retail/issues"">
    <img src=""https://img.shields.io/github/issues/Tencent/tdesign-miniprogram-starter-retail"" alt=""License"">
  </a>  
  <a href=""https://github.com/Tencent/tdesign-miniprogram-starter-retail/LICENSE"">
    <img src=""https://img.shields.io/github/license/Tencent/tdesign-miniprogram-starter-retail"" alt=""License"">
  </a>
  <a href=""https://www.npmjs.com/package/tdesign-miniprogram"">
    <img src=""https://img.shields.io/npm/v/tdesign-miniprogram.svg?sanitize=true"" alt=""Version"">
  </a>
  <a href=""https://www.npmjs.com/package/tdesign-miniprogram"">
    <img src=""https://img.shields.io/npm/dw/tdesign-miniprogram"" alt=""Downloads"">
  </a>
</p>

# TDesign 零售行业模版示例小程序

TDesign 零售模版示例小程序采用 [TDesign 企业级设计体系小程序解决方案](https://tdesign.tencent.com/miniprogram/overview) 进行搭建，依赖 [TDesign 微信小程序组件库](https://github.com/Tencent/tdesign-miniprogram)，涵盖完整的基本零售场景需求。

## :high_brightness: 预览

<p>请使用微信扫描以下二维码：</p>

 <img src=""https://we-retail-static-1300977798.cos.ap-guangzhou.myqcloud.com/retail-mp/common/qrcode.jpeg"" width = ""200"" height = ""200"" alt=""模版小程序二维码"" align=center />

## :pushpin: 项目介绍

### 1. 业务介绍

零售行业模版小程序是个经典的单店版电商小程序，涵盖了电商的黄金链路流程，从商品->购物车->结算->订单等。小程序总共包含 28 个完整的页面，涵盖首页，商品详情页，个人中心，售后流程等基础页面。采用 mock 数据进行展示，提供了完整的零售商品展示、交易与售后流程。页面详情：

<img src=""https://cdn-we-retail.ym.tencent.com/tsr/tdesign-starter-readmeV1.png"" width = ""650"" height = ""900"" alt=""模版小程序页面详情"" align=center />



主要页面截图如下：

<p align=""center"">
    <img alt=""example-home"" width=""200"" src=""https://cdn-we-retail.ym.tencent.com/tsr/example/v1/home.png"" />
    <img alt=""example-sort"" width=""200"" src=""https://cdn-we-retail.ym.tencent.com/tsr/example/v2/sort.png"" />
    <img alt=""example-cart"" width=""200"" src=""https://cdn-we-retail.ym.tencent.com/tsr/example/v1/cart.png"" />
    <img alt=""example-user-center"" width=""200"" src=""https://cdn-we-retail.ym.tencent.com/tsr/example/v1/user-center.png"" />
    <img alt=""example-goods-detail"" width=""200"" src=""https://cdn-we-retail.ym.tencent.com/tsr/example/v1/goods-detail.png"" />
    <img alt=""example-pay"" width=""200"" src=""https://cdn-we-retail.ym.tencent.com/tsr/example/v1/pay.png"" />
    <img alt=""example-order"" width=""200"" src=""https://cdn-we-retail.ym.tencent.com/tsr/example/v1/order.png"" />
    <img alt=""example-order-detail"" width=""200"" src=""https://cdn-we-retail.ym.tencent.com/tsr/example/v2/order.png"" />
</p>



### 2. 项目构成

零售行业模版小程序采用基础的 JavaScript + WXSS + ESLint 进行构建，降低了使用门槛。

项目目录结构如下：

```
|-- tdesign-miniprogram-starter
    |-- README.md
    |-- app.js
    |-- app.json
    |-- app.wxss
    |-- components	//	公共组件库
    |-- config	//	基础配置
    |-- custom-tab-bar	//	自定义 tabbar
    |-- model	//	mock 数据
    |-- pages
    |   |-- cart	//	购物车相关页面
    |   |-- coupon	//	优惠券相关页面
    |   |-- goods	//	商品相关页面
    |   |-- home	//	首页
    |   |-- order	//	订单售后相关页面
    |   |-- promotion-detail	//	营销活动页面
    |   |-- usercenter	//	个人中心及收货地址相关页面
    |-- services	//	请求接口
    |-- style	//	公共样式与iconfont
    |-- utils	//	工具库
```

### 3. 数据模拟

零售小程序采用真实的接口数据，模拟后端返回逻辑，在小程序展示完整的购物场景与购物体验逻辑。

### 4. 添加新页面

1. 在 `pages `目录下创建对应的页面文件夹
2. 在 `app.json` 文件中的 ` ""pages""` 数组中加上页面路径
3. [可选] 在 `project.config.json` 文件的 `""miniprogram-list""` 下添加页面配置

## :hammer: 构建运行

1. `npm install`
2. 小程序开发工具中引入工程
3. 构建 npm

## :art: 代码风格控制

`eslint` `prettier`

## :iphone: 基础库版本

最低基础库版本`^2.6.5`

## :dart: 反馈

企业微信群
TDesign 团队会及时在企业微信大群中同步发布版本、问题修复信息，也会有一些关于组件库建设的讨论，欢迎微信或企业微信扫码入群交流：

<img src=""https://oteam-tdesign-1258344706.cos.ap-guangzhou.myqcloud.com/site/doc/TDesign%20IM.png"" width = ""200"" height = ""200"" alt=""模版小程序页面详情"" align=center />


邮件联系：tdesign@tencent.com

## :link: TDesign 其他技术栈实现

- 移动端 小程序 实现：[mobile-miniprogram](https://github.com/Tencent/tdesign-miniprogram)
- 桌面端 Vue 2 实现：[web-vue](https://github.com/Tencent/tdesign-vue)
- 桌面端 Vue 3 实现：[web-vue-next](https://github.com/Tencent/tdesign-vue-next)
- 桌面端 React 实现：[web-react](https://github.com/Tencent/tdesign-react)

## :page_with_curl: 开源协议

TDesign 遵循 [MIT 协议](https://github.com/Tencent/tdesign-miniprogram-starter-retail/LICENSE)。
",423,423,11,5,retail,"[miniprogram, retail, tdesign, template, tencent, wechat]",44-45
papyrussolution,OpenPapyrus,,https://github.com/papyrussolution/OpenPapyrus,https://api.github.com/repos/OpenPapyrus/papyrussolution,"Sophisticated ERP, CRM, Point-Of-Sale, etc. Open source now. This system is developed since 1996.","# OpenPapyrus

www.petroglif.ru

# En

### The enterprise management system OpenPapyrus

The developed system for management of small and medium enterprises. It includes a very extensive functionality of ERP, CRM, Point-Of-Sale.
Perfectly operates in the following business segments:

* Wholesale
* Retail
* Pharmacy
* Restaurants and cafes
* Beauty salons
* Fitness clubs

One of the best in its class systems in the Russian market.
It supports a large number of types of equipment and is able to interact with a variety of popular systems.

Programming Language: C ++

Operating system: Windows XP or higher

Language: Russian 
Experimental translation phase: English, German, Dutch, Portuguese

Integration with services, equipment and systems:
* EGAIS
* EDI systems
* Cash register module Set-Retail
* Frontol cash register module
* Pepsico iSales
* IP-telephony server Asterisk
* Universal hardware driver ATOL
* Fiscal registrars Shtrif-FR
* Fiscal registrars Viki Print
* Fiscal registrars Pirit
* Electronic scales CAS
* Electronic scales DIGI
* Electronic scales Mettler-Toledo
* Electronic scales Bizerba
* Electronic scales Shtrih-Print
* Datamax label printers (DPL and ELTRON description languages)
* Zebra label printers (ZPL description language)
* UDS customer loyalty system

features.pdf: detailed (but, alas, not full) features description.

ppmanual.pdf: Documentation (big, but also not very full).

OPpyJobSrvr_x.x.xx.xxxxx.exe: optional server component JobServer, ensuring the execution of works on schedule
  and able to ensure the functioning of mobile devices, as well as (for high-enterprises)
  a 3-tier server. To familiarize with the system at the initial stage is not necessary.

To install from scratch:

* First setup OPpyServer_x.x.xx.xxxxx.exe
* Then setup OPpyClient_x.x.xx.xxxxx.exe

You can start working. With 2 attached distribution database:

sample - a small demo database

empty - empty database for actual use

The name for authorization in both databases: 'master' without a password.

To update release run OPpyUpdate_x.x.xx.xxxxx.exe

The functionality is similar to a fully proprietary versions Papyrus, but the database is not compatible
because of different encryption keys.

Support paid.

### Last release links

* [OpenPapyrus Server](http://uhtt.ru/dispatcher/dc/download?key=openpapyrus-setup-server)
* [OpenPapyrus Client](http://uhtt.ru/dispatcher/dc/download?key=openpapyrus-setup-client)
* [OpenPapyrus Update](http://uhtt.ru/dispatcher/dc/download?key=openpapyrus-setup-update)
* [OpenPapyrus JobServer](http://uhtt.ru/dispatcher/dc/download?key=openpapyrus-setup-jobserver)

* [OpenPapyrus Features (pdf)](http://uhtt.ru/dispatcher/dc/download?key=papyrus-features)
* [OpenPapyrus Manual (pdf)](http://uhtt.ru/dispatcher/dc/download?key=papyrus-manual)

-------
### Screenshots

![Ware Edit Dialog](https://github.com/papyrussolution/OpenPapyrus/blob/master/ManWork/Pict/PNG/dlg-goods.png)

![Person Edit Dialog](https://github.com/papyrussolution/OpenPapyrus/blob/master/ManWork/Pict/PNG/dlg-person.png)

![Pos Pane Dialog](https://github.com/papyrussolution/OpenPapyrus/blob/master/ManWork/Pict/PNG/sh-pospane.PNG)
-------

# Ru

### Система управления предприятием OpenPapyrus

Развитая система для управления малым и средним предприятием. Включает очень обширный функционал ERP, CRM, Point-Of-Sale.
Превосходно работает в следующих сегментах бизнеса:

* Оптовая торговля
* Розничная торговля
* Аптечный бизнес
* Рестораны и кафе
* Салоны красоты
* Фитнес-клубы

Одна из лучших систем аналогичного класса на российском рынке.
Поддерживает большое количество типов оборудования и умеет взаимодействовать со множеством популярных систем.

Язык программирования: C++

Операционная система: Windows XP или выше

Язык: Русский

Экспериментальная фаза перевода: English, German, Dutch, Portuguese

Интеграция с сервисами, оборудованием и системами:
* ЕГАИС
* Меркурий (ВЕТИС)
* Честный Знак
* Системы EDI
* Кассовый модуль Сет-Ритейл
* Кассовый модуль Фронтол
* Pepsico iSales
* Сервер ip-телефонии Asterisk
* Универсальный драйвер оборудования Атол
* Фискальные регистраторы Штриф-ФР
* Фискальные регистраторы Viki Print
* Фискальные регистраторы Pirit
* Электронные весы CAS
* Электронные весы DIGI
* Электронные весы Mettler-Toledo
* Электронные весы Bizerba
* Электронные весы Штрих-Принт
* Принтеры этикеток Datamax (языки описания DPL и ELTRON)
* Принтеры этикеток Zebra (язык описания ZPL)
* Система лояльности клиентов UDS

features.pdf: подробное (но, увы, не полное) описание возможностей.

ppmanual.pdf: документация (большая, но тоже очень не полная).

OPpyJobSrvr_x.x.xx.xxxxx.exe: опциональный серверный компонент JobServer, обеспечивающий выполнение работ по расписанию
  и способный обеспечивать функционирование мобильных устройств, а также (для высоконагруженных предприятий)
  как 3-tier сервер. Для ознакомления с системой на начальном этапе не нужен.

Для инсталляции с нуля:

* Сначала установите OPpyServer_x.x.xx.xxxxx.exe
* Затем установите OPpyClient_x.x.xx.xxxxx.exe

Можно начинать работать. С дистрибутивом прилагаются 2 базы данных:

sample - небольшая демонстрационная база

empty - пустая база данных для реального использования

Имя для авторизации в обеих базах данных: master без пароля.

Для обновления релиза запустите OPpyUpdate_x.x.xx.xxxxx.exe

Функциональность полностью аналогична проприетарной версии Papyrus, но базы данных не совместимы
из-за разных ключей шифрования.

Поддержка платная.


",220,220,8,10,retail,"[accounting, bizerba, business, c-plus-plus, cpp, crm, datamax, egais, erp, frontol, inventory, invoice, orders, point-of-sale, restorant, retail, russian, setretail, wholesale, windows]",44-45
apinprastya,sultan,,https://github.com/apinprastya/sultan,https://api.github.com/repos/sultan/apinprastya,Minimarket Point Of Sales (POS) software writen in C++ with Qt framework,"# Sultan POS
Minimarket POS (Point Of Sales) software writen in C++ with Qt Framework. The main target of Sultan POS is minimarket and able to run on Raspberry Pi.

## Feature
* Networked (uses websocket)
* Multi prices
* Item categories
* Supliers
* Multi user and permission
* Databases SQLite / MySQL
* Sales report
* Item sales report
* Money report
* Margin calculation using average
* Customer database
* Customer reward
* Customer credit
* Purchase and purchase return
* Cashier and sold return
* Stock Card
* Box / Package item
* Export / import database to file and Google drive
* And many

## Compile
* Please check [Wiki Compile](https://github.com/apinprastya/sultan/wiki/Compile)

## Screenshot
* Please check [Wiki Screenshot](https://github.com/apinprastya/sultan/wiki/Screenshot)

## Contact
Any question/support please contact me apin.klas@gmail.com

## Note
Please consider to give a star when clone this repository

## License
GPL. See [LICENSE](https://github.com/apinprastya/sultan/blob/master/LICENSE)

## Contribution
Any contribution are welcome
",195,195,20,42,retail,"[kasir, minimarket-pos, point-of-sale, pos, qt-framework, retail, sultan-pos]",44-45
practical-data-science,ecommercetools,,https://github.com/practical-data-science/ecommercetools,https://api.github.com/repos/ecommercetools/practical-data-science,"EcommerceTools is a Python data science toolkit for ecommerce, marketing science, and technical SEO analysis and modelling and was created by Matt Clarke.","# EcommerceTools

![EcommerceTools](https://github.com/practical-data-science/ecommercetools/blob/master/banner.png?raw=true)

EcommerceTools is a data science toolkit for those working in technical ecommerce, marketing science, and technical seo and includes a wide range of features to aid analysis and model building. The package is written in Python and is designed to be used with Pandas and works within a Jupyter notebook environment or in standalone Python projects. 

#### Installation

You can install EcommerceTools and its dependencies via PyPi by entering `pip3 install ecommercetools` in your terminal, or `!pip3 install ecommercetools` within a Jupyter notebook cell. 

---

### Modules

- [Transactions](#Transactions)
- [Products](#Products)
- [Customers](#Customers)
- [Advertising](#Advertising)
- [Operations](#Operations)
- [Marketing](#Marketing)
- [NLP](#NLP)
- [SEO](#SEO)
- [Reports](#Reports)
---

### Transactions

1. #### Load sample transaction items data

If you want to get started with the transactions, products, and customers features, you can use the `load_sample_data()` function to load a set of real world data. This imports the transaction items from widely-used Online Retail dataset and reformats it ready for use by EcommerceTools. 

```python
from ecommercetools import utilities

transaction_items = utilities.load_sample_data()
transaction_items.head()
```

<table border=""1"" class=""dataframe"">
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>order_id</th>
      <th>sku</th>
      <th>description</th>
      <th>quantity</th>
      <th>order_date</th>
      <th>unit_price</th>
      <th>customer_id</th>
      <th>country</th>
      <th>line_price</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>536365</td>
      <td>85123A</td>
      <td>WHITE HANGING HEART T-LIGHT HOLDER</td>
      <td>6</td>
      <td>2010-12-01 08:26:00</td>
      <td>2.55</td>
      <td>17850.0</td>
      <td>United Kingdom</td>
      <td>15.30</td>
    </tr>
    <tr>
      <th>1</th>
      <td>536365</td>
      <td>71053</td>
      <td>WHITE METAL LANTERN</td>
      <td>6</td>
      <td>2010-12-01 08:26:00</td>
      <td>3.39</td>
      <td>17850.0</td>
      <td>United Kingdom</td>
      <td>20.34</td>
    </tr>
    <tr>
      <th>2</th>
      <td>536365</td>
      <td>84406B</td>
      <td>CREAM CUPID HEARTS COAT HANGER</td>
      <td>8</td>
      <td>2010-12-01 08:26:00</td>
      <td>2.75</td>
      <td>17850.0</td>
      <td>United Kingdom</td>
      <td>22.00</td>
    </tr>
    <tr>
      <th>3</th>
      <td>536365</td>
      <td>84029G</td>
      <td>KNITTED UNION FLAG HOT WATER BOTTLE</td>
      <td>6</td>
      <td>2010-12-01 08:26:00</td>
      <td>3.39</td>
      <td>17850.0</td>
      <td>United Kingdom</td>
      <td>20.34</td>
    </tr>
    <tr>
      <th>4</th>
      <td>536365</td>
      <td>84029E</td>
      <td>RED WOOLLY HOTTIE WHITE HEART.</td>
      <td>6</td>
      <td>2010-12-01 08:26:00</td>
      <td>3.39</td>
      <td>17850.0</td>
      <td>United Kingdom</td>
      <td>20.34</td>
    </tr>
  </tbody>
</table>

2. #### Create a transaction items dataframe

The `utilities` module includes a range of tools that allow you to format data, so it can be used within other EcommerceTools functions. The `load_transaction_items()` function is used to create a Pandas dataframe of formatted transactional item data. When loading your transaction items data, all you need to do is define the column mappings, and the function will reformat the dataframe accordingly. 

```python
import pandas as pd
from ecommercetools import utilities

transaction_items = utilities.load_transaction_items('transaction_items_non_standard_names.csv',
                                 date_column='InvoiceDate',
                                 order_id_column='InvoiceNo',
                                 customer_id_column='CustomerID',
                                 sku_column='StockCode',
                                 quantity_column='Quantity',
                                 unit_price_column='UnitPrice'
                                 )
transaction_items.to_csv('transaction_items.csv', index=False)
print(transaction_items.head())
```

<table>
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>order_id</th>
      <th>sku</th>
      <th>description</th>
      <th>quantity</th>
      <th>order_date</th>
      <th>unit_price</th>
      <th>customer_id</th>
      <th>country</th>
      <th>line_price</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>536365</td>
      <td>85123A</td>
      <td>WHITE HANGING HEART T-LIGHT HOLDER</td>
      <td>6</td>
      <td>2010-12-01 08:26:00</td>
      <td>2.55</td>
      <td>17850.0</td>
      <td>United Kingdom</td>
      <td>15.30</td>
    </tr>
    <tr>
      <th>1</th>
      <td>536365</td>
      <td>71053</td>
      <td>WHITE METAL LANTERN</td>
      <td>6</td>
      <td>2010-12-01 08:26:00</td>
      <td>3.39</td>
      <td>17850.0</td>
      <td>United Kingdom</td>
      <td>20.34</td>
    </tr>
    <tr>
      <th>2</th>
      <td>536365</td>
      <td>84406B</td>
      <td>CREAM CUPID HEARTS COAT HANGER</td>
      <td>8</td>
      <td>2010-12-01 08:26:00</td>
      <td>2.75</td>
      <td>17850.0</td>
      <td>United Kingdom</td>
      <td>22.00</td>
    </tr>
    <tr>
      <th>3</th>
      <td>536365</td>
      <td>84029G</td>
      <td>KNITTED UNION FLAG HOT WATER BOTTLE</td>
      <td>6</td>
      <td>2010-12-01 08:26:00</td>
      <td>3.39</td>
      <td>17850.0</td>
      <td>United Kingdom</td>
      <td>20.34</td>
    </tr>
    <tr>
      <th>4</th>
      <td>536365</td>
      <td>84029E</td>
      <td>RED WOOLLY HOTTIE WHITE HEART.</td>
      <td>6</td>
      <td>2010-12-01 08:26:00</td>
      <td>3.39</td>
      <td>17850.0</td>
      <td>United Kingdom</td>
      <td>20.34</td>
    </tr>
  </tbody>
</table>

3. #### Create a transactions dataframe

The `get_transactions()` function takes the formatted Pandas dataframe of transaction items and returns a Pandas dataframe of aggregated transaction data, which includes features identifying the order number. 

```python
import pandas as pd
from ecommercetools import customers

transaction_items = pd.read_csv('transaction_items.csv')
transactions = transactions.get_transactions(transaction_items)
transactions.to_csv('transactions.csv', index=False)
print(transactions.head())
```

<table>
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>order_id</th>
      <th>order_date</th>
      <th>customer_id</th>
      <th>skus</th>
      <th>items</th>
      <th>revenue</th>
      <th>replacement</th>
      <th>order_number</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>536365</td>
      <td>2010-12-01 08:26:00</td>
      <td>17850.0</td>
      <td>7</td>
      <td>40</td>
      <td>139.12</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>536366</td>
      <td>2010-12-01 08:28:00</td>
      <td>17850.0</td>
      <td>2</td>
      <td>12</td>
      <td>22.20</td>
      <td>0</td>
      <td>2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>536367</td>
      <td>2010-12-01 08:34:00</td>
      <td>13047.0</td>
      <td>12</td>
      <td>83</td>
      <td>278.73</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>536368</td>
      <td>2010-12-01 08:34:00</td>
      <td>13047.0</td>
      <td>4</td>
      <td>15</td>
      <td>70.05</td>
      <td>0</td>
      <td>2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>536369</td>
      <td>2010-12-01 08:35:00</td>
      <td>13047.0</td>
      <td>1</td>
      <td>3</td>
      <td>17.85</td>
      <td>0</td>
      <td>3</td>
    </tr>
  </tbody>
</table>

---

### Products

#### 1. Get product data from transaction items

```python
products_df = products.get_products(transaction_items)
products_df.head()
```

<table>
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>sku</th>
      <th>first_order_date</th>
      <th>last_order_date</th>
      <th>customers</th>
      <th>orders</th>
      <th>items</th>
      <th>revenue</th>
      <th>avg_unit_price</th>
      <th>avg_quantity</th>
      <th>avg_revenue</th>
      <th>avg_orders</th>
      <th>product_tenure</th>
      <th>product_recency</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>10002</td>
      <td>2010-12-01 08:45:00</td>
      <td>2011-04-28 15:05:00</td>
      <td>40</td>
      <td>73</td>
      <td>1037</td>
      <td>759.89</td>
      <td>1.056849</td>
      <td>14.205479</td>
      <td>10.409452</td>
      <td>1.82</td>
      <td>3749</td>
      <td>3600</td>
    </tr>
    <tr>
      <th>1</th>
      <td>10080</td>
      <td>2011-02-27 13:47:00</td>
      <td>2011-11-21 17:04:00</td>
      <td>19</td>
      <td>24</td>
      <td>495</td>
      <td>119.09</td>
      <td>0.376667</td>
      <td>20.625000</td>
      <td>4.962083</td>
      <td>1.26</td>
      <td>3660</td>
      <td>3393</td>
    </tr>
    <tr>
      <th>2</th>
      <td>10120</td>
      <td>2010-12-03 11:19:00</td>
      <td>2011-12-04 13:15:00</td>
      <td>25</td>
      <td>29</td>
      <td>193</td>
      <td>40.53</td>
      <td>0.210000</td>
      <td>6.433333</td>
      <td>1.351000</td>
      <td>1.16</td>
      <td>3746</td>
      <td>3380</td>
    </tr>
    <tr>
      <th>3</th>
      <td>10123C</td>
      <td>2010-12-03 11:19:00</td>
      <td>2011-07-15 15:05:00</td>
      <td>3</td>
      <td>4</td>
      <td>-13</td>
      <td>3.25</td>
      <td>0.487500</td>
      <td>-3.250000</td>
      <td>0.812500</td>
      <td>1.33</td>
      <td>3746</td>
      <td>3522</td>
    </tr>
    <tr>
      <th>4</th>
      <td>10123G</td>
      <td>2011-04-08 11:13:00</td>
      <td>2011-04-08 11:13:00</td>
      <td>0</td>
      <td>1</td>
      <td>-38</td>
      <td>0.00</td>
      <td>0.000000</td>
      <td>-38.000000</td>
      <td>0.000000</td>
      <td>inf</td>
      <td>3620</td>
      <td>3620</td>
    </tr>
  </tbody>
</table>

#### 2. Calculate product consumption and repurchase rate


```python
repurchase_rates = products.get_repurchase_rates(transaction_items)
repurchase_rates.head(3).T
```

<table>
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>sku</th>
      <td>10002</td>
      <td>10080</td>
      <td>10120</td>
    </tr>
    <tr>
      <th>revenue</th>
      <td>759.89</td>
      <td>119.09</td>
      <td>40.53</td>
    </tr>
    <tr>
      <th>items</th>
      <td>1037</td>
      <td>495</td>
      <td>193</td>
    </tr>
    <tr>
      <th>orders</th>
      <td>73</td>
      <td>24</td>
      <td>29</td>
    </tr>
    <tr>
      <th>customers</th>
      <td>40</td>
      <td>19</td>
      <td>25</td>
    </tr>
    <tr>
      <th>avg_unit_price</th>
      <td>1.05685</td>
      <td>0.376667</td>
      <td>0.21</td>
    </tr>
    <tr>
      <th>avg_line_price</th>
      <td>10.4095</td>
      <td>4.96208</td>
      <td>1.351</td>
    </tr>
    <tr>
      <th>avg_items_per_order</th>
      <td>14.2055</td>
      <td>20.625</td>
      <td>6.65517</td>
    </tr>
    <tr>
      <th>avg_items_per_customer</th>
      <td>25.925</td>
      <td>26.0526</td>
      <td>7.72</td>
    </tr>
    <tr>
      <th>purchased_individually</th>
      <td>0</td>
      <td>0</td>
      <td>9</td>
    </tr>
    <tr>
      <th>purchased_once</th>
      <td>34</td>
      <td>17</td>
      <td>22</td>
    </tr>
    <tr>
      <th>bulk_purchases</th>
      <td>73</td>
      <td>24</td>
      <td>20</td>
    </tr>
    <tr>
      <th>bulk_purchase_rate</th>
      <td>1</td>
      <td>1</td>
      <td>0.689655</td>
    </tr>
    <tr>
      <th>repurchases</th>
      <td>39</td>
      <td>7</td>
      <td>7</td>
    </tr>
    <tr>
      <th>repurchase_rate</th>
      <td>0.534247</td>
      <td>0.291667</td>
      <td>0.241379</td>
    </tr>
    <tr>
      <th>repurchase_rate_label</th>
      <td>Moderate repurchase</td>
      <td>Low repurchase</td>
      <td>Low repurchase</td>
    </tr>
    <tr>
      <th>bulk_purchase_rate_label</th>
      <td>Very high bulk</td>
      <td>Very high bulk</td>
      <td>High bulk</td>
    </tr>
    <tr>
      <th>bulk_and_repurchase_label</th>
      <td>Moderate repurchase_Very high bulk</td>
      <td>Low repurchase_Very high bulk</td>
      <td>Low repurchase_High bulk</td>
    </tr>
  </tbody>
</table>

---

### Customers

#### 1. Create a customers dataset

```python
from ecommercetools import customers

customers_df = customers.get_customers(transaction_items)
customers_df.head()
```

<table>
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>customer_id</th>
      <th>revenue</th>
      <th>orders</th>
      <th>skus</th>
      <th>items</th>
      <th>first_order_date</th>
      <th>last_order_date</th>
      <th>avg_items</th>
      <th>avg_order_value</th>
      <th>tenure</th>
      <th>recency</th>
      <th>cohort</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>12346.0</td>
      <td>0.00</td>
      <td>2</td>
      <td>1</td>
      <td>0</td>
      <td>2011-01-18 10:01:00</td>
      <td>2011-01-18 10:17:00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>3701</td>
      <td>3700</td>
      <td>20111</td>
    </tr>
    <tr>
      <th>1</th>
      <td>12347.0</td>
      <td>4310.00</td>
      <td>7</td>
      <td>7</td>
      <td>2458</td>
      <td>2010-12-07 14:57:00</td>
      <td>2011-12-07 15:52:00</td>
      <td>351.14</td>
      <td>615.71</td>
      <td>3742</td>
      <td>3377</td>
      <td>20104</td>
    </tr>
    <tr>
      <th>2</th>
      <td>12348.0</td>
      <td>1797.24</td>
      <td>4</td>
      <td>4</td>
      <td>2341</td>
      <td>2010-12-16 19:09:00</td>
      <td>2011-09-25 13:13:00</td>
      <td>585.25</td>
      <td>449.31</td>
      <td>3733</td>
      <td>3450</td>
      <td>20104</td>
    </tr>
    <tr>
      <th>3</th>
      <td>12349.0</td>
      <td>1757.55</td>
      <td>1</td>
      <td>1</td>
      <td>631</td>
      <td>2011-11-21 09:51:00</td>
      <td>2011-11-21 09:51:00</td>
      <td>631.00</td>
      <td>1757.55</td>
      <td>3394</td>
      <td>3394</td>
      <td>20114</td>
    </tr>
    <tr>
      <th>4</th>
      <td>12350.0</td>
      <td>334.40</td>
      <td>1</td>
      <td>1</td>
      <td>197</td>
      <td>2011-02-02 16:01:00</td>
      <td>2011-02-02 16:01:00</td>
      <td>197.00</td>
      <td>334.40</td>
      <td>3685</td>
      <td>3685</td>
      <td>20111</td>
    </tr>
  </tbody>
</table>

#### 2. Create a customer cohort analysis dataset


```python
from ecommercetools import customers

cohorts_df = customers.get_cohorts(transaction_items, period='M')
cohorts_df.head()
```

<table>
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>customer_id</th>
      <th>order_id</th>
      <th>order_date</th>
      <th>acquisition_cohort</th>
      <th>order_cohort</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17850.0</td>
      <td>536365</td>
      <td>2010-12-01 08:26:00</td>
      <td>2010-12</td>
      <td>2010-12</td>
    </tr>
    <tr>
      <th>7</th>
      <td>17850.0</td>
      <td>536366</td>
      <td>2010-12-01 08:28:00</td>
      <td>2010-12</td>
      <td>2010-12</td>
    </tr>
    <tr>
      <th>9</th>
      <td>13047.0</td>
      <td>536367</td>
      <td>2010-12-01 08:34:00</td>
      <td>2010-12</td>
      <td>2010-12</td>
    </tr>
    <tr>
      <th>21</th>
      <td>13047.0</td>
      <td>536368</td>
      <td>2010-12-01 08:34:00</td>
      <td>2010-12</td>
      <td>2010-12</td>
    </tr>
    <tr>
      <th>25</th>
      <td>13047.0</td>
      <td>536369</td>
      <td>2010-12-01 08:35:00</td>
      <td>2010-12</td>
      <td>2010-12</td>
    </tr>
  </tbody>
</table>


#### 3. Create a customer cohort analysis matrix

```python
from ecommercetools import customers

cohort_matrix_df = customers.get_cohort_matrix(transaction_items, period='M', percentage=True)
cohort_matrix_df.head()
```

<table>
  <thead>
    <tr style=""text-align: right;"">
      <th>periods</th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
    </tr>
    <tr>
      <th>acquisition_cohort</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2010-12</th>
      <td>1.0</td>
      <td>0.381857</td>
      <td>0.334388</td>
      <td>0.387131</td>
      <td>0.359705</td>
      <td>0.396624</td>
      <td>0.379747</td>
      <td>0.354430</td>
      <td>0.354430</td>
      <td>0.394515</td>
      <td>0.373418</td>
      <td>0.500000</td>
      <td>0.274262</td>
    </tr>
    <tr>
      <th>2011-01</th>
      <td>1.0</td>
      <td>0.239905</td>
      <td>0.282660</td>
      <td>0.242280</td>
      <td>0.327791</td>
      <td>0.299287</td>
      <td>0.261283</td>
      <td>0.256532</td>
      <td>0.311164</td>
      <td>0.346793</td>
      <td>0.368171</td>
      <td>0.149644</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>2011-02</th>
      <td>1.0</td>
      <td>0.247368</td>
      <td>0.192105</td>
      <td>0.278947</td>
      <td>0.268421</td>
      <td>0.247368</td>
      <td>0.255263</td>
      <td>0.281579</td>
      <td>0.257895</td>
      <td>0.313158</td>
      <td>0.092105</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>2011-03</th>
      <td>1.0</td>
      <td>0.190909</td>
      <td>0.254545</td>
      <td>0.218182</td>
      <td>0.231818</td>
      <td>0.177273</td>
      <td>0.263636</td>
      <td>0.238636</td>
      <td>0.288636</td>
      <td>0.088636</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>2011-04</th>
      <td>1.0</td>
      <td>0.227425</td>
      <td>0.220736</td>
      <td>0.210702</td>
      <td>0.207358</td>
      <td>0.237458</td>
      <td>0.230769</td>
      <td>0.260870</td>
      <td>0.083612</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>


```python
from ecommercetools import customers

cohort_matrix_df = customers.get_cohort_matrix(transaction_items, period='M', percentage=False)
cohort_matrix_df.head()
```

<table>
  <thead>
    <tr style=""text-align: right;"">
      <th>periods</th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
    </tr>
    <tr>
      <th>acquisition_cohort</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2010-12</th>
      <td>948.0</td>
      <td>362.0</td>
      <td>317.0</td>
      <td>367.0</td>
      <td>341.0</td>
      <td>376.0</td>
      <td>360.0</td>
      <td>336.0</td>
      <td>336.0</td>
      <td>374.0</td>
      <td>354.0</td>
      <td>474.0</td>
      <td>260.0</td>
    </tr>
    <tr>
      <th>2011-01</th>
      <td>421.0</td>
      <td>101.0</td>
      <td>119.0</td>
      <td>102.0</td>
      <td>138.0</td>
      <td>126.0</td>
      <td>110.0</td>
      <td>108.0</td>
      <td>131.0</td>
      <td>146.0</td>
      <td>155.0</td>
      <td>63.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>2011-02</th>
      <td>380.0</td>
      <td>94.0</td>
      <td>73.0</td>
      <td>106.0</td>
      <td>102.0</td>
      <td>94.0</td>
      <td>97.0</td>
      <td>107.0</td>
      <td>98.0</td>
      <td>119.0</td>
      <td>35.0</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>2011-03</th>
      <td>440.0</td>
      <td>84.0</td>
      <td>112.0</td>
      <td>96.0</td>
      <td>102.0</td>
      <td>78.0</td>
      <td>116.0</td>
      <td>105.0</td>
      <td>127.0</td>
      <td>39.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>2011-04</th>
      <td>299.0</td>
      <td>68.0</td>
      <td>66.0</td>
      <td>63.0</td>
      <td>62.0</td>
      <td>71.0</td>
      <td>69.0</td>
      <td>78.0</td>
      <td>25.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>



#### 4. Create a customer ""retention"" dataset


```python
from ecommercetools import customers

retention_df = customers.get_retention(transactions_df)
retention_df.head()
```

<table>
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>acquisition_cohort</th>
      <th>order_cohort</th>
      <th>customers</th>
      <th>periods</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2010-12</td>
      <td>2010-12</td>
      <td>948</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2010-12</td>
      <td>2011-01</td>
      <td>362</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2010-12</td>
      <td>2011-02</td>
      <td>317</td>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2010-12</td>
      <td>2011-03</td>
      <td>367</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2010-12</td>
      <td>2011-04</td>
      <td>341</td>
      <td>4</td>
    </tr>
  </tbody>
</table>

#### 5. Create an RFM (H) dataset

This is an extension of the regular Recency, Frequency, Monetary value (RFM) model that includes an additional parameter ""H"" for heterogeneity. This shows the number of unique SKUs purchased by each customer. While typically unassociated with targeting, this value can be very useful in identifying which customers should probably be buying a broader mix of products than they currently are, as well as spotting those who may have stopped buying certain items. 


```python
from ecommercetools import customers

rfm_df = customers.get_rfm_segments(customers_df)
rfm_df.head()
```

<table>
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>customer_id</th>
      <th>acquisition_date</th>
      <th>recency_date</th>
      <th>recency</th>
      <th>frequency</th>
      <th>monetary</th>
      <th>heterogeneity</th>
      <th>tenure</th>
      <th>r</th>
      <th>f</th>
      <th>m</th>
      <th>h</th>
      <th>rfm</th>
      <th>rfm_score</th>
      <th>rfm_segment_name</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>12346.0</td>
      <td>2011-01-18 10:01:00</td>
      <td>2011-01-18 10:17:00</td>
      <td>3700</td>
      <td>2</td>
      <td>0.00</td>
      <td>1</td>
      <td>3701</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>111</td>
      <td>3</td>
      <td>Risky</td>
    </tr>
    <tr>
      <th>1</th>
      <td>12350.0</td>
      <td>2011-02-02 16:01:00</td>
      <td>2011-02-02 16:01:00</td>
      <td>3685</td>
      <td>1</td>
      <td>334.40</td>
      <td>1</td>
      <td>3685</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>111</td>
      <td>3</td>
      <td>Risky</td>
    </tr>
    <tr>
      <th>2</th>
      <td>12365.0</td>
      <td>2011-02-21 13:51:00</td>
      <td>2011-02-21 14:04:00</td>
      <td>3666</td>
      <td>3</td>
      <td>320.69</td>
      <td>2</td>
      <td>3666</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>111</td>
      <td>3</td>
      <td>Risky</td>
    </tr>
    <tr>
      <th>3</th>
      <td>12373.0</td>
      <td>2011-02-01 13:10:00</td>
      <td>2011-02-01 13:10:00</td>
      <td>3686</td>
      <td>1</td>
      <td>364.60</td>
      <td>1</td>
      <td>3686</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>111</td>
      <td>3</td>
      <td>Risky</td>
    </tr>
    <tr>
      <th>4</th>
      <td>12377.0</td>
      <td>2010-12-20 09:37:00</td>
      <td>2011-01-28 15:45:00</td>
      <td>3690</td>
      <td>2</td>
      <td>1628.12</td>
      <td>2</td>
      <td>3730</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>111</td>
      <td>3</td>
      <td>Risky</td>
    </tr>
  </tbody>
</table>


#### 6. Create a purchase latency dataset


```python
from ecommercetools import customers 

latency_df = customers.get_latency(transactions_df)
latency_df.head()
```

<table>
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>customer_id</th>
      <th>frequency</th>
      <th>recency_date</th>
      <th>recency</th>
      <th>avg_latency</th>
      <th>min_latency</th>
      <th>max_latency</th>
      <th>std_latency</th>
      <th>cv</th>
      <th>days_to_next_order</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>12680.0</td>
      <td>4</td>
      <td>2011-12-09 12:50:00</td>
      <td>3388</td>
      <td>28</td>
      <td>16</td>
      <td>73</td>
      <td>30.859898</td>
      <td>1.102139</td>
      <td>-3329.0</td>
      <td>Order overdue</td>
    </tr>
    <tr>
      <th>1</th>
      <td>13113.0</td>
      <td>24</td>
      <td>2011-12-09 12:49:00</td>
      <td>3388</td>
      <td>15</td>
      <td>0</td>
      <td>52</td>
      <td>12.060126</td>
      <td>0.804008</td>
      <td>-3361.0</td>
      <td>Order overdue</td>
    </tr>
    <tr>
      <th>2</th>
      <td>15804.0</td>
      <td>13</td>
      <td>2011-12-09 12:31:00</td>
      <td>3388</td>
      <td>15</td>
      <td>1</td>
      <td>39</td>
      <td>11.008261</td>
      <td>0.733884</td>
      <td>-3362.0</td>
      <td>Order overdue</td>
    </tr>
    <tr>
      <th>3</th>
      <td>13777.0</td>
      <td>33</td>
      <td>2011-12-09 12:25:00</td>
      <td>3388</td>
      <td>11</td>
      <td>0</td>
      <td>48</td>
      <td>12.055274</td>
      <td>1.095934</td>
      <td>-3365.0</td>
      <td>Order overdue</td>
    </tr>
    <tr>
      <th>4</th>
      <td>17581.0</td>
      <td>25</td>
      <td>2011-12-09 12:21:00</td>
      <td>3388</td>
      <td>14</td>
      <td>0</td>
      <td>67</td>
      <td>21.974293</td>
      <td>1.569592</td>
      <td>-3352.0</td>
      <td>Order overdue</td>
    </tr>
  </tbody>
</table>



#### 7. Customer ABC segmentation

```python
from ecommercetools import customers

abc_df = customers.get_abc_segments(customers_df, months=12, abc_class_name='abc_class_12m', abc_rank_name='abc_rank_12m')
abc_df.head()
```


<table>
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>customer_id</th>
      <th>abc_class_12m</th>
      <th>abc_rank_12m</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>12346.0</td>
      <td>D</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>12347.0</td>
      <td>D</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>12348.0</td>
      <td>D</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>12349.0</td>
      <td>D</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>12350.0</td>
      <td>D</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table>

#### 8. Predict customer AOV, CLV, and orders

EcommerceTools allows you to predict the AOV, Customer Lifetime Value (CLV) and expected number of orders via the Gamma-Gamma and BG/NBD models from the excellent Lifetimes package. By passing the dataframe of transactions from `get_transactions()` to the `get_customer_predictions()` function, EcommerceTools will fit the BG/NBD and Gamma-Gamma models and predict the AOV, order quantity, and CLV for each customer in the defined number of future days after the end of the observation period.

```python
customer_predictions = customers.get_customer_predictions(transactions_df, 
                                                          observation_period_end='2011-12-09', 
                                                          days=90)
customer_predictions.head(10)
```

<table border=""1"" class=""dataframe"">
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>customer_id</th>
      <th>predicted_purchases</th>
      <th>aov</th>
      <th>clv</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>12346.0</td>
      <td>0.188830</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>12347.0</td>
      <td>1.408736</td>
      <td>569.978836</td>
      <td>836.846896</td>
    </tr>
    <tr>
      <th>2</th>
      <td>12348.0</td>
      <td>0.805907</td>
      <td>333.784235</td>
      <td>308.247354</td>
    </tr>
    <tr>
      <th>3</th>
      <td>12349.0</td>
      <td>0.855607</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>4</th>
      <td>12350.0</td>
      <td>0.196304</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>5</th>
      <td>12352.0</td>
      <td>1.682277</td>
      <td>376.175359</td>
      <td>647.826169</td>
    </tr>
    <tr>
      <th>6</th>
      <td>12353.0</td>
      <td>0.272541</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>7</th>
      <td>12354.0</td>
      <td>0.247183</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>8</th>
      <td>12355.0</td>
      <td>0.262909</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>9</th>
      <td>12356.0</td>
      <td>0.645368</td>
      <td>324.039419</td>
      <td>256.855226</td>
    </tr>
  </tbody>
</table>
---

### Advertising

#### 1. Create paid search keywords


```python
from ecommercetools import advertising

product_names = ['fly rods', 'fly reels']
keywords_prepend = ['buy', 'best', 'cheap', 'reduced']
keywords_append = ['for sale', 'price', 'promotion', 'promo', 'coupon', 'voucher', 'shop', 'suppliers']
campaign_name = 'fly_fishing'

keywords = advertising.generate_ad_keywords(product_names, keywords_prepend, keywords_append, campaign_name)
keywords.head()
```

<table>
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>product</th>
      <th>keywords</th>
      <th>match_type</th>
      <th>campaign_name</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>fly rods</td>
      <td>[fly rods]</td>
      <td>Exact</td>
      <td>fly_fishing</td>
    </tr>
    <tr>
      <th>1</th>
      <td>fly rods</td>
      <td>[buy fly rods]</td>
      <td>Exact</td>
      <td>fly_fishing</td>
    </tr>
    <tr>
      <th>2</th>
      <td>fly rods</td>
      <td>[best fly rods]</td>
      <td>Exact</td>
      <td>fly_fishing</td>
    </tr>
    <tr>
      <th>3</th>
      <td>fly rods</td>
      <td>[cheap fly rods]</td>
      <td>Exact</td>
      <td>fly_fishing</td>
    </tr>
    <tr>
      <th>4</th>
      <td>fly rods</td>
      <td>[reduced fly rods]</td>
      <td>Exact</td>
      <td>fly_fishing</td>
    </tr>
  </tbody>
</table>


#### 2. Create paid search ad copy using Spintax

```python
from ecommercetools import advertising

text = ""Fly Reels from {Orvis|Loop|Sage|Airflo|Nautilus} for {trout|salmon|grayling|pike}""
spin = advertising.generate_spintax(text, single=False)

spin
```


    ['Fly Reels from Orvis for trout',
     'Fly Reels from Orvis for salmon',
     'Fly Reels from Orvis for grayling',
     'Fly Reels from Orvis for pike',
     'Fly Reels from Loop for trout',
     'Fly Reels from Loop for salmon',
     'Fly Reels from Loop for grayling',
     'Fly Reels from Loop for pike',
     'Fly Reels from Sage for trout',
     'Fly Reels from Sage for salmon',
     'Fly Reels from Sage for grayling',
     'Fly Reels from Sage for pike',
     'Fly Reels from Airflo for trout',
     'Fly Reels from Airflo for salmon',
     'Fly Reels from Airflo for grayling',
     'Fly Reels from Airflo for pike',
     'Fly Reels from Nautilus for trout',
     'Fly Reels from Nautilus for salmon',
     'Fly Reels from Nautilus for grayling',
     'Fly Reels from Nautilus for pike']

---

### Operations

#### 1. Create an ABC inventory classification

```python
inventory_classification = operations.get_inventory_classification(transaction_items)
inventory_classification.head()
```

<table>
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>sku</th>
      <th>abc_class</th>
      <th>abc_rank</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>10002</td>
      <td>A</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>10080</td>
      <td>A</td>
      <td>2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>10120</td>
      <td>A</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>10123C</td>
      <td>A</td>
      <td>4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>10123G</td>
      <td>A</td>
      <td>4</td>
    </tr>
  </tbody>
</table>


---
### Marketing

#### 1. Get ecommerce trading calendar

```python
from ecommercetools import marketing

trading_calendar_df = marketing.get_trading_calendar('2021-01-01', days=365)
trading_calendar_df.head()
```

<table>
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>date</th>
      <th>event</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2021-01-01</td>
      <td>January sale</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2021-01-02</td>
      <td></td>
    </tr>
    <tr>
      <th>2</th>
      <td>2021-01-03</td>
      <td></td>
    </tr>
    <tr>
      <th>3</th>
      <td>2021-01-04</td>
      <td></td>
    </tr>
    <tr>
      <th>4</th>
      <td>2021-01-05</td>
      <td></td>
    </tr>
  </tbody>
</table>


#### 2. Get ecommerce trading events


```python
from ecommercetools import marketing

trading_events_df = marketing.get_trading_events('2021-01-01', days=365)
trading_events_df.head()
```


<table>
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>date</th>
      <th>event</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2021-01-01</td>
      <td>January sale</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2021-01-29</td>
      <td>January Pay Day</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2021-02-11</td>
      <td>Valentine's Day [last order date]</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2021-02-14</td>
      <td>Valentine's Day</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2021-02-26</td>
      <td>February Pay Day</td>
    </tr>
  </tbody>
</table>



---

### NLP

#### 1. Generate text summaries
The `get_summaries()` function of the `nlp` module takes a Pandas dataframe containing text and returns a machine-generated summary of the content using a Huggingface Transformers pipeline via PyTorch. To use this feature, first load your Pandas dataframe and import the `nlp` module from `ecommercetools`.

```python
import pandas as pd
from ecommercetools import nlp 

pd.set_option('max_colwidth', 1000)
df = pd.read_csv('text.csv')
df.head()
```

Specify the name of the Pandas dataframe, the column containing the text you wish to summarise (i.e. `product_description`), and specify a column name in which to store the machine-generated summary. The `min_length` and `max_length` arguments control the number of words generated, while the `do_sample` argument controls whether the generated text is completely unique (`do_sample=False`) or extracted from the text (`do_sample=True`).

```python
df = nlp.get_summaries(df, 'product_description', 'sampled_summary', min_length=50, max_length=100, do_sample=True)
df = nlp.get_summaries(df, 'product_description', 'unsampled_summary', min_length=50, max_length=100, do_sample=False)
df = nlp.get_summaries(df, 'product_description', 'unsampled_summary_20_to_30', min_length=20, max_length=30, do_sample=False)
```

Since the model used for text summarisation is very large (1.2 GB plus), this function will take some time to complete. Once loaded, summaries are generated within a second or two per piece of text, so it is advisable to try smaller volumes of data initially.


### SEO

#### 1. Discover XML sitemap locations
The `get_sitemaps()` function takes the location of a `robots.txt` file (always stored at the root of a domain), and returns the URLs of any XML sitemaps listed within. 

```python
from ecommercetools import seo

sitemaps = seo.get_sitemaps(""http://www.flyandlure.org/robots.txt"")
print(sitemaps)

```

#### 2. Get an XML sitemap
The `get_dataframe()` function allows you to download the URLs in an XML sitemap to a Pandas dataframe. If the sitemap contains child sitemaps, each of these will be retrieved. You can save the Pandas dataframe to CSV in the usual way. 

```python
from ecommercetools import seo

df = seo.get_sitemap(""http://flyandlure.org/sitemap.xml"")
print(df.head())
```


<table>
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>loc</th>
      <th>changefreq</th>
      <th>priority</th>
      <th>domain</th>
      <th>sitemap_name</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>http://flyandlure.org/</td>
      <td>hourly</td>
      <td>1.0</td>
      <td>flyandlure.org</td>
      <td>http://www.flyandlure.org/sitemap.xml</td>
    </tr>
    <tr>
      <th>1</th>
      <td>http://flyandlure.org/about</td>
      <td>monthly</td>
      <td>1.0</td>
      <td>flyandlure.org</td>
      <td>http://www.flyandlure.org/sitemap.xml</td>
    </tr>
    <tr>
      <th>2</th>
      <td>http://flyandlure.org/terms</td>
      <td>monthly</td>
      <td>1.0</td>
      <td>flyandlure.org</td>
      <td>http://www.flyandlure.org/sitemap.xml</td>
    </tr>
    <tr>
      <th>3</th>
      <td>http://flyandlure.org/privacy</td>
      <td>monthly</td>
      <td>1.0</td>
      <td>flyandlure.org</td>
      <td>http://www.flyandlure.org/sitemap.xml</td>
    </tr>
    <tr>
      <th>4</th>
      <td>http://flyandlure.org/copyright</td>
      <td>monthly</td>
      <td>1.0</td>
      <td>flyandlure.org</td>
      <td>http://www.flyandlure.org/sitemap.xml</td>
    </tr>
  </tbody>
</table>


#### 3. Get Core Web Vitals from PageSpeed Insights
The `get_core_web_vitals()` function retrieves the Core Web Vitals metrics for a list of sites from the Google PageSpeed Insights API and returns results in a Pandas dataframe. The function requires a a Google PageSpeed Insights API key. 

```python
from ecommercetools import seo

pagespeed_insights_key = ""xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx""
urls = ['https://www.bbc.co.uk', 'https://www.bbc.co.uk/iplayer']
df = seo.get_core_web_vitals(pagespeed_insights_key, urls)
print(df.head())
```

#### 4. Get Google Knowledge Graph data
The `get_knowledge_graph()` function returns the Google Knowledge Graph data for a given search term. This requires the use of a Google Knowledge Graph API key. By default, the function returns output in a Pandas dataframe, but you can pass the `output=""json""` argument if you wish to receive the JSON data back. 

```python
from ecommercetools import seo

knowledge_graph_key = ""xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx""
knowledge_graph = seo.get_knowledge_graph(knowledge_graph_key, ""tesla"", output=""dataframe"")
print(knowledge_graph)
```

#### 5. Get Google Search Console API data
The `query_google_search_console()` function runs a search query on the Google Search Console API and returns data in a Pandas dataframe. This function requires a JSON client secrets key with access to the Google Search Console API. 

```python
from ecommercetools import seo

key = ""google-search-console.json""
site_url = ""http://flyandlure.org""
payload = {
    'startDate': ""2019-01-01"",
    'endDate': ""2019-12-31"",
    'dimensions': [""page"", ""device"", ""query""],
    'rowLimit': 100,
    'startRow': 0
}

df = seo.query_google_search_console(key, site_url, payload)
print(df.head())

```


<table>
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>page</th>
      <th>device</th>
      <th>query</th>
      <th>clicks</th>
      <th>impressions</th>
      <th>ctr</th>
      <th>position</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>http://flyandlure.org/articles/fly_fishing_gea...</td>
      <td>MOBILE</td>
      <td>simms freestone waders review</td>
      <td>56</td>
      <td>217</td>
      <td>25.81</td>
      <td>3.12</td>
    </tr>
    <tr>
      <th>1</th>
      <td>http://flyandlure.org/</td>
      <td>MOBILE</td>
      <td>fly and lure</td>
      <td>37</td>
      <td>159</td>
      <td>23.27</td>
      <td>3.81</td>
    </tr>
    <tr>
      <th>2</th>
      <td>http://flyandlure.org/articles/fly_fishing_gea...</td>
      <td>DESKTOP</td>
      <td>orvis encounter waders review</td>
      <td>35</td>
      <td>134</td>
      <td>26.12</td>
      <td>4.04</td>
    </tr>
    <tr>
      <th>3</th>
      <td>http://flyandlure.org/articles/fly_fishing_gea...</td>
      <td>DESKTOP</td>
      <td>simms freestone waders review</td>
      <td>35</td>
      <td>200</td>
      <td>17.50</td>
      <td>3.50</td>
    </tr>
    <tr>
      <th>4</th>
      <td>http://flyandlure.org/</td>
      <td>DESKTOP</td>
      <td>fly and lure</td>
      <td>32</td>
      <td>170</td>
      <td>18.82</td>
      <td>3.09</td>
    </tr>
  </tbody>
</table>

##### Fetching all results from Google Search Console

To fetch all results, set `fetch_all` to `True`. This will automatically paginate through your Google Search Console data and return all results. Be aware that if you do this you may hit Google's quota limit if you run a query over an extended period, or have a busy site with lots of `page` or `query` dimensions. 

```python
from ecommercetools import seo

key = ""google-search-console.json""
site_url = ""http://flyandlure.org""
payload = {
    'startDate': ""2019-01-01"",
    'endDate': ""2019-12-31"",
    'dimensions': [""page"", ""device"", ""query""],
    'rowLimit': 25000,
    'startRow': 0
}

df = seo.query_google_search_console(key, site_url, payload, fetch_all=True)
print(df.head())

```

##### Comparing two time periods in Google Search Console

```python
payload_before = {
    'startDate': ""2021-08-11"",
    'endDate': ""2021-08-31"",
    'dimensions': [""page"",""query""],    
}

payload_after = {
    'startDate': ""2021-07-21"",
    'endDate': ""2021-08-10"",
    'dimensions': [""page"",""query""],    
}

df = seo.query_google_search_console_compare(key, site_url, payload_before, payload_after, fetch_all=False)
df.sort_values(by='clicks_change', ascending=False).head()
```


#### 6. Get the number of ""indexed"" pages
The `get_indexed_pages()` function uses the ""site:"" prefix to search Google for the number of pages ""indexed"". This is very approximate and may not be a perfect representation, but it's usually a good guide of site ""size"" in the absence of other data. 

```python
from ecommercetools import seo

urls = ['https://www.bbc.co.uk', 'https://www.bbc.co.uk/iplayer', 'http://flyandlure.org']
df = seo.get_indexed_pages(urls)
print(df.head())
```

<table>
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>url</th>
      <th>indexed_pages</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2</th>
      <td>http://flyandlure.org</td>
      <td>2090</td>
    </tr>
    <tr>
      <th>1</th>
      <td>https://www.bbc.co.uk/iplayer</td>
      <td>215000</td>
    </tr>
    <tr>
      <th>0</th>
      <td>https://www.bbc.co.uk</td>
      <td>12700000</td>
    </tr>
  </tbody>
</table>


#### 7. Get keyword suggestions from Google Autocomplete
The `google_autocomplete()` function returns a set of keyword suggestions from Google Autocomplete. The `include_expanded=True` argument allows you to expand the number of suggestions shown by appending prefixes and suffixes to the search terms. 

```python
from ecommercetools import seo

suggestions = seo.google_autocomplete(""data science"", include_expanded=False)
print(suggestions)

suggestions = seo.google_autocomplete(""data science"", include_expanded=True)
print(suggestions)
```

<table>
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>term</th>
      <th>relevance</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>data science jobs</td>
      <td>650</td>
    </tr>
    <tr>
      <th>1</th>
      <td>data science jobs chester</td>
      <td>601</td>
    </tr>
    <tr>
      <th>2</th>
      <td>data science course</td>
      <td>600</td>
    </tr>
    <tr>
      <th>3</th>
      <td>data science masters</td>
      <td>554</td>
    </tr>
    <tr>
      <th>4</th>
      <td>data science salary</td>
      <td>553</td>
    </tr>
    <tr>
      <th>5</th>
      <td>data science internship</td>
      <td>552</td>
    </tr>
    <tr>
      <th>6</th>
      <td>data science jobs london</td>
      <td>551</td>
    </tr>
    <tr>
      <th>7</th>
      <td>data science graduate scheme</td>
      <td>550</td>
    </tr>
  </tbody>
</table>

#### 8. Retrieve robots.txt content
The `get_robots()` function returns the contents of a robots.txt file in a Pandas dataframe so it can be parsed and analysed. 

```python
from ecommercetools import seo

robots = seo.get_robots(""http://www.flyandlure.org/robots.txt"")
print(robots)
```

<table>
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>directive</th>
      <th>parameter</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>User-agent</td>
      <td>*</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Disallow</td>
      <td>/signin</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Disallow</td>
      <td>/signup</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Disallow</td>
      <td>/users</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Disallow</td>
      <td>/contact</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Disallow</td>
      <td>/activate</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Disallow</td>
      <td>/*/page</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Disallow</td>
      <td>/articles/search</td>
    </tr>
    <tr>
      <th>8</th>
      <td>Disallow</td>
      <td>/search.php</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Disallow</td>
      <td>*q=*</td>
    </tr>
    <tr>
      <th>10</th>
      <td>Disallow</td>
      <td>*category_slug=*</td>
    </tr>
    <tr>
      <th>11</th>
      <td>Disallow</td>
      <td>*country_slug=*</td>
    </tr>
    <tr>
      <th>12</th>
      <td>Disallow</td>
      <td>*county_slug=*</td>
    </tr>
    <tr>
      <th>13</th>
      <td>Disallow</td>
      <td>*features=*</td>
    </tr>
  </tbody>
</table>

#### 9. Get Google SERPs
The `get_serps()` function returns a Pandas dataframe containing the Google search engine results for a given search term. Note that this function is not suitable for large-scale scraping and currently includes no features to prevent it from being blocked.

```python
from ecommercetools import seo

serps = seo.get_serps(""data science blog"")
print(serps)
```

<table>
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>title</th>
      <th>link</th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>10 of the best data science blogs to follow - ...</td>
      <td>https://www.tableau.com/learn/articles/data-sc...</td>
      <td>10 of the best data science blogs to follow. T...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Best Data Science Blogs to Follow in 2020 | by...</td>
      <td>https://towardsdatascience.com/best-data-scien...</td>
      <td>14 Jul 2020 — 1. Towards Data Science · Joined...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Top 20 Data Science Blogs And Websites For Dat...</td>
      <td>https://medium.com/@exastax/top-20-data-scienc...</td>
      <td>Top 20 Data Science Blogs And Websites For Dat...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Data Science Blog – Dataquest</td>
      <td>https://www.dataquest.io/blog/</td>
      <td>Browse our data science blog to get helpful ti...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>51 Awesome Data Science Blogs You Need To Chec...</td>
      <td>https://365datascience.com/trending/51-data-sc...</td>
      <td>Blog name: DataKind · datakind data science bl...</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Blogs on AI, Analytics, Data Science, Machine ...</td>
      <td>https://www.kdnuggets.com/websites/blogs.html</td>
      <td>Individual/small group blogs · Ai4 blog, featu...</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Data Science Blog – Applied Data Science</td>
      <td>https://data-science-blog.com/</td>
      <td>... an Bedeutung – DevOps for Data Science. De...</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Top 10 Data Science and AI Blogs in 2020 - Liv...</td>
      <td>https://livecodestream.dev/post/top-data-scien...</td>
      <td>Some of the best data science and AI blogs for...</td>
    </tr>
    <tr>
      <th>8</th>
      <td>Data Science Blogs: 17 Must-Read Blogs for Dat...</td>
      <td>https://www.thinkful.com/blog/data-science-blogs/</td>
      <td>Data scientists could be considered the magici...</td>
    </tr>
    <tr>
      <th>9</th>
      <td>rushter/data-science-blogs: A curated list of ...</td>
      <td>https://github.com/rushter/data-science-blogs</td>
      <td>A curated list of data science blogs. Contribu...</td>
    </tr>
  </tbody>
</table>


#### Create an ABCD classification of Google Search Console data
The `classify_pages()` function returns an ABCD classification of Google Search Console data. This calculates the cumulative sum of clicks and then categorises pages using the ABC algorithm (the first 80% are classed A, the next 10% are classed B, and the final 10% are classed C, with the zero click pages classed D). 




```python
from ecommercetools import seo

key = ""client_secrets.json""
site_url = ""example-domain.co.uk""
start_date = '2022-10-01'
end_date = '2022-10-31'

df_classes = seo.classify_pages(key, site_url, start_date, end_date, output='classes')
print(df_classes.head())

df_summary = seo.classify_pages(key, site_url, start_date, end_date, output='summary')
print(df_summary)

```

                                                    page  clicks  impressions    ctr  position  clicks_cumsum  clicks_running_pc  pc_share class  class_rank
    0  https://practicaldatascience.co.uk/machine-lea...    3890        36577  10.64     12.64           3890           8.382898  8.382898     A           1
    1  https://practicaldatascience.co.uk/data-scienc...    2414        16618  14.53     14.30           6304          13.585036  5.202138     A           2
    2  https://practicaldatascience.co.uk/data-scienc...    2378        71496   3.33     16.39           8682          18.709594  5.124558     A           3
    3  https://practicaldatascience.co.uk/data-scienc...    1942        14274  13.61     15.02          10624          22.894578  4.184984     A           4
    4  https://practicaldatascience.co.uk/data-scienc...    1738        23979   7.25     11.80          12362          26.639945  3.745367     A           5
      

    class  pages  impressions  clicks   avg_ctr  avg_position  share_of_clicks  share_of_impressions
    0     A     63       747643   36980  5.126349     22.706825             79.7                  43.7
    1     B     46       639329    4726  3.228043     31.897826             10.2                  37.4
    2     C    190       323385    4698  2.393632     38.259368             10.1                  18.9
    3     D     36         1327       0  0.000000     25.804722              0.0                   0.1



---

### Reports
The Reports module creates weekly, monthly, quarterly, or yearly reports for customers and orders and calculates a range of common ecommerce metrics to show business performance.

#### 1. Customers report
The `customers_report()` function takes a formatted dataframe of transaction items (see above) and a desired frequency (D for daily, W for weekly, M for monthly, Q for quarterly) and calculates aggregate metrics for each period. 

The function returns the number of orders, the number of customers, the number of new customers, the number of returning customers, and the acquisition rate (or proportion of new customers). For monthly reporting, I would recommend a 13-month period so you can compare the last month with the same month the previous year. 

```python
from ecommercetools import reports

df_customers_report = reports.customers_report(transaction_items, frequency='M')
print(df_customers_report.head(13))
```

#### 2. Transactions report
The `transactions_report()` function takes a formatted dataframe of transaction items (see above) and a desired frequency (D for daily, W for weekly, M for monthly, Q for quarterly) and calculates aggregate metrics for each period. 

The metrics returned are: customers, orders, revenue, SKUs, units, average order value, average SKUs per order, average units per order, and average revenue per customer. 

```python
from ecommercetools import reports

df_orders_report = reports.transactions_report(transaction_items, frequency='M')
print(df_orders_report.head(13))
```

",174,174,7,6,retail,"[customer, customers, ecommerce, marketing, marketing-analytics, marketing-tools, retail, seo, seo-optimization, seotools]",44-45
jsbots,AutoFish,,https://github.com/jsbots/AutoFish,https://api.github.com/repos/AutoFish/jsbots,An easy-to-use fishing bot for games with wow-like fishing logic. ,"<p align=""center""> <img src=""app/img/logo.png"" width=""400px""> </p>
<div align=""center"">

 <img alt=""GitHub package.json version"" src=""https://img.shields.io/github/package-json/v/olesgeras/autofish""> [![GitHub license](https://img.shields.io/github/license/olesgeras/AutoFish)](https://github.com/olesgeras/AutoFish/blob/4c5f0fdb5af0f1378f3318d563c5738fa7580e2f/LICENSE)
<a href=""https://youtu.be/A3W8UuVIZTo""><img alt="""" src=""https://img.shields.io/youtube/views/A3W8UuVIZTo?style=social""></a>

</div>

## Table of Contents :page_with_curl:

- [Fishing bot](#fishing-bot-fish)
- [Disclaimer](#disclaimer-warning)
- [Guide](#guide-blue_book)
- [Threshold](#threshold-red_circle)
- [Fishing zone](#fishing-zone-dart)
- [Applying Lures](#applying-lures-pushpin)
- [Interactive key](#interactive-key)
- [Soulbound items](#soulbound-items-auto-confirmation-large_blue_diamond)
- [Download](#download-open_file_folder)

AutoFish Premium:
- [Features](#autofish-premiumcrown)
- [Remote Control](#telegram-remote-control-iphone)
- [Arduino Control](#arduino-control-joystick)
- [Multiple Windows](#multiple-windows-rocket)
- [Sound Detection](#sound-detection-loud_sound)
- [Mammoth Selling](#mammoth-selling-elephant)
- [Random camera/character movements](#random-cameracharacter-movements-robot)
- [AFK Fishing](#afk-fishing-sleeping)


## Fishing bot :fish:

This is a fishing bot designed for wow-like fishing logic (when a model of a bobber has red/blue feather and plunging animation, for example it can work even with Minecraft). It is built using the [Electron](https://github.com/electron/electron)  framework and leverages the [keysender](https://github.com/Krombik/keysender) library and [nut.js](https://github.com/nut-tree/nut.js) library to analyze the screen and automate the fishing process in a manner that mimics human behavior.

This bot is capable of simultaneously handling one or multiple game windows, enabling efficient fishing across different instances. Additionally, it incorporates the use of [tesseract.js](https://github.com/naptha/tesseract.js) for loot analysis.

**Features:**
- Fishing lures support.
- Loot filtering support.
- Automated confirmation for soulbound items.
- Intentional ""miss"" functionality.
- Randomized log out/log in functionality.
- Advanced automation with various elements of randomness, including random sleep intervals, random reactions, random delays after catching, randomized mouse speed and curvature, and random bobber highlighting.

<p align=""center"">
<img src=""guide_img/UI.jpg"" width=""640"">
</p>

For more detailed review you can watch <img src=""https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/YouTube_full-color_icon_%282017%29.svg/1280px-YouTube_full-color_icon_%282017%29.svg.png"" width=""20""> [AutoFish 1.12](https://youtu.be/A3W8UuVIZTo)

This is commonly referred to as a ""pixel bot."" It operates without modifying the game's memory or utilizing vision libraries like OpenCV. Instead, it employs a simpler approach: it analyzes the game window for concentrated red colors and monitors that area for any changes. As the bobber moves and gently sways, the bot follows its motion. However, when the bobber suddenly jerks, the bot clicks on it and catches the fish.

## Disclaimer :warning:

**This project was solely developed for educational purposes, aiming to explore the feasibility of creating a functional gaming bot using web-development technologies only. The software provided should never be used with real-life applications, games and servers**

If you still want to use it outside educational sandbox, it is crucial to understand and accept the associated risks. You assume full responsibility for any outcomes that may arise, as no one else can be held accountable. It's essential to acknowledge that this software is not designed to be ""undetectable"" in any way, nor was it ever intended for such purposes as stated above. As a result, **no guarantees or assurances can be made regarding the functionality or outcomes of the bot**.

## Guide :blue_book:

1. Launch the game.
2. Are you using **Filter** feature?
   - Yes: Turn off **Auto Loot** in the game, set **UI scale** in the game to default, turn on **Open loot window at mouse** in the game.
   - No: Turn on **Auto Loot** in the game.
3. Assign your 'fishing' and 'lures' keys in the game and assign the same keys for the bot.
4. Find a place to fish.
5. Set up your **Fishing Zone** by clicking the **Set Fishing Zone** button. Adjust the size and position of the **Fishing Zone** window to exclude any reddish or bluish elements (depending on the switch you selected). Keep in mind that the **Fishing Zone** functions as an overlay, so it will also recognize colors from your character and the game's user interface, including texts and health bars.
6. Press the Start button and refrain from using your mouse and keyboard (if you require the bot to function in the background, consider using a **virtual machine** such as VirtualBox or VMware Player).
7. To stop the bot, press your designated stop key (default: space).


### Hints and Issues

- The bot has been exclusively tested with the default user interface (UI) and default UI scale, without any addons. Therefore, before using the bot, ensure that all addons are turned off and the UI scale is set to default. This is particularly important for fishing addons like Fishing Buddy and others. Disable any UI features they provide.
- The bot will make 5 attempts to cast and find the bobber (default: 5). If it fails, it will stop the application.
- If you use Filter feature in different from English languages for the first time, wait until the bot downloads the data for your language. Also read about [Soulbound items](#soulbound-items-auto-confirmation-large_blue_diamond).
- With older games the bot might run into ""Interrupted"" loop: it's when the bot will confuse the previous bobber for the current one and after the bobber dissappeared the bot will think that it caught the fish and will recast, after that the whole process will repeat again and again. To bypass this issue use **Manual Threshold** or increase **Cast Animation Delay** value to make the bot wait until the previous bobber will disappear completely.  
- If the bot doesn't move/press/clicks your mouse, try to launch it as administrator.
- The bot works properly only with 1 monitor, so if you use multiple monitors, launch the bot and the game on **the primary one**.
- Sound Detection feature might not work with some audio devices, in that case you need to switch to another device (e.g. you are using headphones and sound detection doesn't work, then plug in speakers and test again).
- You can turn off all the ""sleeping"" and random values to make the bot work **2-3 times** faster.
- Don't fish near other players, the bot might confuse their bobber for yours.

## Threshold :red_circle:

*Since 2.0.0 Threshold value is obsolete and available only in Manual mode*

The Threshold value represents an RGB value of a red or blue color, depending on the switch used. It serves as a color threshold below which the bot will ignore all corresponding colors.

For instance, if the red switch is used and the threshold is set to 60, the bot will only recognize colors that are redder than this threshold value. This allows the bot to focus on the reddest parts of the screen, such as a red feather on the bobber.

Increasing the threshold value will result in the bot recognizing fewer red colors, while decreasing the threshold value will cause the bot to recognize more red colors on the screen.

### Errors (Manual mode only)

Are you encountering an error?

> Yes, it says: *Found red/blue colors before casting. Change your Fishing Zone or increase the Threshold value or change the fishing place.*

Adjust the position or size of your **Fishing Zone** to exclude any reddish or bluish elements. If you are certain there are none, you may need to increase the **Threshold** or switch to the other color.

> No, but the bot recasts all the time and can't find the bobber. (as says in the log)

It indicates that you may have set the **Threshold** value too high. Try decreasing it. If the error persists, there might be an issue with the size or position of your **Fishing Zone**.

> No, but the bot clicks too early before fish is even hooked.

There are a couple of solutions for this. First, try switching to the 1st person view. If that doesn't help, navigate to the **Advanced Settings** and adjust either the **Bobber Sensitivity** (disable the Auto adjust Sensitivity and Density option) or the **Bobber Density** (particularly if it's not Retail) values in the **Critical** section.

> No, the bot finds the bobber (as says in the log) but it doesn't react to the bobber being hooked.

Then you do the opposite to the previous issue: navigate to the **Advanced Settings** and decrease either the **Bobber Sensitivity** (particularly if it's Retail) or the **Bobber Density** (especially if it's not Retail) values in the **Critical** section. In the case of Vanilla (splash), you can decrease the Splash Color value.""

### Dynamic Threshold

<p align=""center"">
<img src=""guide_img/dynamicth.jpg"" align=""center"">
</p>

The bot incorporates a simple mechanism to adjust the threshold value. After a certain number of failed attempts to find the bobber (default: 5), the bot will decrease the threshold by the provided value (default: 5). It will then make another set of attempts to find the bobber, continuing to decrease the threshold until it reaches a value of Threshold < 20. This feature is particularly useful in adverse weather conditions when the color or brightness of the bobber is significantly reduced, enabling the bot to continue functioning effectively.

## Fishing Zone :dart:

The Fishing Zone is an adjustable area in the water where your bobber may land. The bot specifically searches for the bobber within this designated area.

The Fishing Zone is displayed as an overlay window, meaning that the bot will also recognize the colors of your character and user interface.

To assist you in assessing the presence of red or blue colors within the Fishing Zone, there is a ""Check"" button. This feature relies on the Threshold value. If, upon pressing the Check button, the window turns red, close the window, gradually increase the Threshold value, and test again. Repeat this process until the window turns green.

A general guideline to follow is that the better you can perceive the red or blue feather, the better the bot will be able to detect it as well. Consider the following tips:

- Optimize your video settings to enhance visibility, excluding weather effects.
- Disable weather effects to prevent the bot from confusing rain or fog with bobber movements. However, note that extremely harsh weather conditions, such as blizzards, may significantly reduce the bot's efficiency. In such cases, you can either find an alternative fishing spot, switch between blue and red feathers, or wait for better weather.
- Keep in mind that different camera directions can affect the brightness, size, and visibility of the red feather on the bobber, subsequently impacting the bot's performance. While the specific location may not be critical, the direction and positioning are crucial.
- Although camera position is not highly significant, in dark or snowy areas, a closer view of the bobber might improve visibility. Generally, a normal third-person view is ideal.
- Adjust gamma, brightness, and contrast settings to enhance the brightness and vibrancy of the bobber.
- In very dark zones, consider using alternative bobbers with distinctive red or blue colors instead of the default one.

Watch <img src=""https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/YouTube_full-color_icon_%282017%29.svg/1280px-YouTube_full-color_icon_%282017%29.svg.png"" width=""20""> [AutoFish 1.12](https://youtu.be/A3W8UuVIZTo) for video explanation.

## Applying Lures :pushpin:
For **Retail** and **Classic/Vanilla** type of fishing you need to use a macro that will apply the lures onto your fishpole and **assign that macro to **Lures Key** option**:

**retail**:

```
/use *lure_name*
/use *fishpole_name*
```

**Classic**:

```
/equip *fishpole_name*
/use *lure_name*
/use 16
```

**Vanilla**:

```
/script UseAction(*your lures key*);
/script PickupInventoryItem(16);
/script ReplaceEnchant();
```

## Interactive key

In **Retail**-like games You can use interactive key to catch your fish, if you want to use it with the bot, turn on Int. Key option and assign the same key you use for interactive key in the game.

<p align=""center"">
<img src=""guide_img/intkey.jpg"" align=""center"">
</p>

To make the interactive key work, you use this commands (write them in the chat and press enter, one by one):
```
/console SoftTargetInteractArc 2  - This will allow you to interact with the bobber no matter which way you are facing.
/console SoftTargetInteractRange 30  - This increases the interaction range to 30 yards. Adjust to your needs
```

## Soulbound items auto-confirmation :large_blue_diamond:

If the item requires confirmation on looting, the bot will confirm it automatically. **This won't work with AutoLoot turned on**, so if you need such items always use **whitelist**. **This feature also doesn't work with any other language except English**. As a solution (both if you need AutoLoot on and if your WoW isn't in English) use [AutoLooter](https://www.curseforge.com/wow/addons/autolooter) instead.


## AutoFish Premium	:crown:

AutoFish Premium is the same application only with some additional features available for supporters of the project.

**Additional Premium Features:**
- Remote control via Telegram.
- Control with Arduino Board (complete hardware emulation).
- Multiple game windows support (up to 4).
- Sound Detection.
- Mammoth Selling (junk selling).
- Profiles support.
- AFK Fishing Mode (auto-focusing the window only when catching/casting).

<p align=""center"">
<img src=""guide_img/Premium_UI.jpg"" width=""640"">
</p>

## Arduino Control :joystick:

*This feature is available only for [Premium version](https://www.buymeacoffee.com/jsbots/e/96734) of the app*

<p align=""center"">
<img src=""guide_img/arduino.jpg"" align=""center"">
</p>

Watch <img src=""https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/YouTube_full-color_icon_%282017%29.svg/1280px-YouTube_full-color_icon_%282017%29.svg.png"" width=""20""> [Arduino Control Test Video](https://youtu.be/yE-qARS73oo)

The bot is able to connect to your Arduino Board and use it to emulate a mouse/keyboard device, it will look like a real keyboard or mouse to the OS and the game. What you need to do to make it possible:

1. Get an Arduino with an ATmega32U4 on board (any cheap copies for 2-3$ will do too, you can find them on Chinese online markets).
2. Connect it to your computer.
3. Install [Arduino IDE](https://www.arduino.cc/en/software).
4. Click **New Sketch** and replace everything there with this sketch: [Arduino Sketch](https://github.com/jsbots/Clicker/blob/main/clicker_sketch/clicker_sketch.ino).
5. Click **Tools** -> **Port** and choose the port your Arduino Board connected to.
6. Click **Sketch** -> **Upload** and wait until the code uploads to your board.
7. Launch AutoFish, click **Advanced Settings** turn on **Use Arduino Board** option and choose the port your Arduino Board connected to, press **Connect** button.

## Telegram remote control :iphone:

*This feature is available only for [Premium version](https://www.buymeacoffee.com/jsbots/e/96734) of the app*

Watch <img src=""https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/YouTube_full-color_icon_%282017%29.svg/1280px-YouTube_full-color_icon_%282017%29.svg.png"" width=""20""> [Telegram remote control Test Video](https://youtu.be/aKulvFK6ubg)

1. Get the token from [BotFather](https://t.me/BotFather) by using /newbot command and following the instruction. Imagine some long and random name for the bot so that someone won't accidentally join your bot and gain control over your fishing process.
2. Paste the token to **Telegram token** input field in **Remote Control** section in the **Advanced Settings** and press enter.

<p align=""center"">
<img src=""guide_img/tmtoken.jpg"" align=""center"">
</p>

3. Press **Connect** button and wait until the name of the button changes to either **done** or **error* (*might take awhile*).
4. Open the bot in your Telegram and either press /start or write /start command.
5. If evertyhing is OK, the bot will reply with:

<p align=""center"">
<img src=""guide_img/tmmenu.jpg"" width=""416px"" align=""center"">
</p>

6. Now set your **Chat Zone** as on the screenshot below by pressing **Set Chat Zone** button on the main window of the AutoFish.

7. If you want to make the bot notify you about any errors or whipser messeges, *you need to start it from Telegram* (*not by pressing Start on the bot's interface*). Whisper detection will work much better and reliable if you **turn off all the other chat messages**.

## Multiple Windows :rocket:

*This feature is available only for [Premium version](https://www.buymeacoffee.com/jsbots/e/96734) of the app*

Watch <img src=""https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/YouTube_full-color_icon_%282017%29.svg/1280px-YouTube_full-color_icon_%282017%29.svg.png"" width=""20""> [Multiple Windows Test Video](https://youtu.be/ih-xoQcByz8)

The Multiple Windows feature enables you to fish simultaneously in multiple game windows, with support for up to four windows. The bot will seamlessly switch between the game windows as needed for casting and catching fish. This feature enhances your fishing efficiency by allowing you to manage multiple fishing spots or engage in multi-character fishing activities.

Watch [this](https://youtu.be/o1i_cgZzuHc?t=33) if you wonder how it looks like.

## Sound Detection :loud_sound:

*This feature is available only for [Premium version](https://www.buymeacoffee.com/jsbots/e/96734) of the app*

Watch <img src=""https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/YouTube_full-color_icon_%282017%29.svg/1280px-YouTube_full-color_icon_%282017%29.svg.png"" width=""20""> [Sound Detection Test Video](https://youtu.be/ZggOy8tA32A)

<p align=""center"">
<img src=""guide_img/sound-detection.jpg"" align=""center"">
</p>

Sound Detection is an alternative to pixel recognition logic. The bot will hook the bobber only after ""splash"" sound and won't rely on checking the animation of the bobber plunging.

With both Int. Key and Sound Detection turned on you can be completely independent from Threshold and Fishing Zone options. If you don't use Int. key or the game doesn't support it, the bot still needs to find a bobber first but checking will be done by sound recognition if you turn on Sound Detection option.

Before using sound detection turn off Music and Ambient Sounds in the game, leave only Sound Effects. Your volume should be at normal/default level. Try to find a place secluded from the sounds made by other players to avoid false detections.

You can also use AFK Fishing Mode in DX12 now, with Int.Key + Sound Detection the bot will focus the window only when it needs to cast and when it detects splash sound (turn on Sound in Background for that).

**Warning!** Sound Detection feature might not work with some audio devices, in that case you need to switch to another device (e.g. you are using headphones and sound detection doesn't work, then plug in speakers and test again).

## Mammoth Selling :elephant:

*This feature is available only for [Premium version](https://www.buymeacoffee.com/jsbots/e/96734) of the app*

Watch <img src=""https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/YouTube_full-color_icon_%282017%29.svg/1280px-YouTube_full-color_icon_%282017%29.svg.png"" width=""20""> [Mammoth Selling Test Video](https://youtu.be/zY2LqAPszdg)

<p align=""center"">
<img src=""guide_img/mammoth.jpg"" align=""center"">
</p>

As an alternative to filtering you can use a trader on your mammoth mount to sell all the junk items during the fishing. The bot will summon your mount, target your trader, interact with it by using interaction key in the game, unsummon the mount and go on fishing.

Because of the novelty of the interaction key this feature is available only for Retail.

Depends on the mount the name of your trader might be different, so change the default value.

## Random Camera/Character Movements :robot:

*This feature is available only for [Premium version](https://www.buymeacoffee.com/jsbots/e/96734) of the app*

Watch <img src=""https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/YouTube_full-color_icon_%282017%29.svg/1280px-YouTube_full-color_icon_%282017%29.svg.png"" width=""20""> [Random Camera/Character Movements Test Video](https://youtu.be/o1hU3fNn4uk)

<p align=""center"">
<img src=""guide_img/rngMove.jpg"" align=""center"">
</p>

The bot will randomly move and change your camera view from time to time within the given value. It will also balance itself every n minutes (default: 5) and return to the initial camera and character position.  

## AFK Fishing :sleeping:

*This feature is available only for [Premium version](https://www.buymeacoffee.com/jsbots/e/96734) of the app*

Watch <img src=""https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/YouTube_full-color_icon_%282017%29.svg/1280px-YouTube_full-color_icon_%282017%29.svg.png"" width=""20""> [AFK Fishing Test Video](https://youtu.be/lQi6fSxMyL0)

To facilitate the use of the bot when you have a single monitor and cannot run it in the background or set up a virtual machine, the AFK Fishing mode is available. This mode enables the bot to focus on the game window only during the casting and fish-catching process. After that, it will automatically switch back to the previous window using the Alt + Tab keys.

With AFK Fishing mode, you can engage in other activities such as watching videos, browsing the internet, or reading a book while the bot continues to monitor the bobber in the background. It's important to note that this mode requires DirectX11 (turned on in the game) for proper functionality.

## Download :open_file_folder:

<p align=""center"">
<a href=""https://www.buymeacoffee.com/jsbots""><img src=""https://img.buymeacoffee.com/button-api/?text=Buy me a coffee&emoji=&slug=jsbots&button_colour=FFDD00&font_colour=000000&font_family=Bree&outline_colour=000000&coffee_colour=ffffff"" /></a>
</p>

AutoFish 2.2.1 beta Public: [Download](https://www.buymeacoffee.com/jsbots/e/95380) or download it from **Releases** section.

AutoFish 2.3.0 beta Premium: [Download](https://www.buymeacoffee.com/jsbots/e/96734)

The software is open-source, allowing you to clone the repository, review the code, and launch it directly from the command-line interface if you have concerns about downloading the executable file.

If you choose to download the executable file, it functions as a setup file that installs the bot in the following directory: *c:/users/your_user/App Data/Local/random_folder/*. Additionally, a shortcut with a randomly generated name will be created on your desktop.

If you wish to uninstall the bot, you can do so through the Windows Settings. The uninstaller will have the same name as the shortcut on your desktop.

Please note that if you download a new setup file, it is necessary to uninstall the previous version of AutoFish before proceeding. The application and folder names are generated randomly during each installation, preventing automatic installation of a new version in the previous folder.
",142,142,8,5,retail,"[bot, desktop-automation, electron, fishbot, fishing, javascript, javscript-bot, retail, wow, wrath-of-the-lich-king]",44-45
UplandsDynamic,simple-stock-management,,https://github.com/UplandsDynamic/simple-stock-management,https://api.github.com/repos/simple-stock-management/UplandsDynamic,Server component of the Simple Stock Management stock & inventory web app. Designed for small businesses & non-profits.,"# Simple Stock Management - Server Component

## Security Advisory

Please note, this application has not been audited for security and may contain vulnerabilities that could expose data contained on the host system to unauthorized manipulation or disclosure. Deploy at your own risk.

## About

This a demo/prototype repository for the server component of the Simple Stock Management stock and inventory system. It is built using web technologies, with a client/server architecture. The repository for the frontend app component is at: https://github.com/Aninstance/simple-stock-management-frontend

The system allows ""stores"" to request transfers of stock (""order"") from a central stock repository (""warehouse""). Stock is adjusted for the ""Warehouse Account"" and the ""Store Account"" as stock transfers are ""ordered"". Email notifications are sent to the ""warehouse"" administrator(s) and the ordering ""store manager"".

This project offers a web frontend that connects to a RESTful API backend. Data is stored in either a SQLite, mySQL or PostgreSQL (recommended) database.

## Support & Project Status

A regularly patched, proprietary licensed application-as-a-service version, fully maintained for subscribers and clients, is available upon request (limited availability) and is currently priced at £10.00/month.

A one-off installation service for this GPL licensed version is also available.

The GPL licensed version of this project offered here is *not guaranteed* to be regularly maintained. It is made available here for demo/prototype purposes only, and should not be used in production (i.e. a ""live"" working environment) unless the administrator regularly patches project dependencies (i.e. PYPI & npm packages) with upstream security updates as and when released by vendors.

If you would like to avail of the proprietary subscription to the application-as-a-service, or request other bespoke work on this project, please email to discuss: ssm@uplandsdynamic.com.

## Key Technologies for Server Component

Key technologies include: Python 3.7; Django; Django-rest-framework; Django_q (for asynchronous processes); Javascript; HTML5; CSS3;

## Live Demo

There is a live demo, available here:

https://frontend.ssm.webapps.uplandsdynamic.com

There are two test users - one for the warehouse administrator, the other for a 'store manager'. Credentials are:

Adminstrator:
Username: test_admin
Password: jduejHje(89K

Manager:
Username: test_manager
Password: jduejHje(89K

## Screenshots

![Screenshot 1](./meta/img/screenshot_1.png?raw=true)
![Screenshot 2](./meta/img/screenshot_2.png?raw=true)
![Screenshot 4](./meta/img/screenshot_4.png?raw=true)

![Screenshot 6](./meta/img/screenshot_6.png?raw=true)
![Screenshot 7](./meta/img/screenshot_7.png?raw=true)
![Screenshot 8](./meta/img/screenshot_8.png?raw=true)
![Screenshot 5](./meta/img/screenshot_5.png?raw=true)
![Screenshot 9](./meta/img/screenshot_9.png?raw=true)
![Screenshot 10](./meta/img/screenshot_10.png?raw=true)

## Key features

- Administrator may add, edit and delete stock from database.
- Store managers may request transfers (""order"") stock from the ""warehouse"".
- Dynamic search of stock lines (SKU and description).
- Configurable pagination of results table.
- Transfer requests of stock lines are loaded to a ""truck"" (i.e. like ""adding to a basket/cart"" in an e-commerce system), before the request is submitted.
  - The ""truck"" retains the transfer data until the ""Request truck dispatch"" button is clicked. The truck data is retained across sessions (meaning the data remains in the truck even if the user logs out, then resumes their transfer at a later time).
  - Once the ""Request truck dispatch"" button is clicked, the transfer request process will complete. The truck empties and a single email containing a summary of the successful transfers - and any failures - is dispatched to both the requesting user and the warehouse administrator. Warehouse quantities are immediately adjusted accordingly, both in the ""Warehouse"" and ""Store"" accounts.
- A ""Stock take"" feature compiles and emails detailed reports, consisting of:

  - For every unique stock line in a ""Store Account"" (see screenshot #10, below, for an example report):

    - SKU
    - Stock description
    - Units of opening stock
    - Units of closing stock
    - Change in stock units since last stock take
    - Number of units transferred since last stock take
    - Number of units recorded sold since last stock take
    - Number of units recorded as shrinkage since last stock take
    - Differential for units of unrecorded history since last stock take (i.e. unrecorded sales, unrecorded transfers, unrecorded loss)
    - Current transfer value of a unit
    - Current retail price of a unit
    - Total value of units recorded sold since last stock take
    - Total value of units recorded as shrinkage since last stock take
    - Total value of units transferred since last stock take
    - Total value differential of units with unrecorded history since last stock take, at present xfer price
    - Total value differential of units with unrecorded history since last stock take, at present retail price (i.e. unrecorded sales, unrecorded transfers, shrinkage)
    - Current total held stock transfer value at present xfer price
    - Current total held stock retail value at present retail price

  - Overview of the entire ""Store Account"" (see screenshot #10, below, for an example report):

    - Units of opening stock
    - Units of closing stock
    - Units of stock transferred since last stock take
    - Units of stock recorded sold since last stock take
    - Units of stock recorded as shrinkage since last stock take
    - Change in stock holding owing to unrecorded unit history since last stock take (i.e. unrecorded sales, unrecorded transfers, unrecorded loss)
    - Value of stock recorded sold since last stock take
    - Value of stock recorded as shrinkage since last stock take
    - Total value differential of units with unrecorded history since last stock take at current transfer value
    - Total value differential of units with unrecorded history since last stock take at current retail value (i.e. unrecorded sales, unrecorded transfers, unrecorded loss)
    - Total value of transfers since last stock take (at actual xfer prices)
    - All time total value of transfers (at actual xfer prices)
    - Value of held stock at current transfer price
    - Value of held stock at current retail price

- Automated removal of obsolete stock line records (lines with zero units of held stock) from the Store accounts following a successful stock take process
- Historical retention of previous stock take data (not currently exposed on the UI)

## Installation & usage (on Linux systems)

__Below are basic steps to install and run a demonstration of the app on an Linux Ubuntu 20.04 server. They do not provide for a secure installation, such as would be required if the app was publicly available. Steps should be taken to security harden the environment if using in production.__

### Brief installation instructions

- First, clone the repository to your file system.

- Ensure you have access to a current version of PostgreSQL (either locally installed, or remote).

- Ensure gunicorn is installed on your system.

- Create a system user under which to run the application (e.g. `django`). Recursively change ownership of the application directory and all its sub directories to that user, then switch to operate as that user.

- Change into the application's root directory.

- Install a python virtual environment on your system and make that your python source.

- Run `pip3 install -r requirements.txt`.

- Copy `StockManagement/settings.DEFAULT.py` to `StockManagement/settings.py`.

- Edit `StockManagement/settings.py` according to your environment. Be sure to add the URL of your frontend web client to the CORS_ORIGIN_WHITELIST list property.

- Create the PostgreSQL database and user, as defined.

- Create a directory named `secret_key` in the application's root directory and change its ownership to the application user (as created above).

- Change permissions on the `secret_key` directory so only the user running the application can read it, e.g.: `chmod 0700 secret_key`.

- As root (using sudo), create the log directory and file, e.g.:

  - `sudo mkdir -p /var/log/django;`
  - `sudo touch /var/log/django/ssm.log`

- Change ownership of the log directory and its log file to the user running the app, e.g.:

  - `sudo chown -R django /var/log/django/`

- Create a systemd unit file to run the gunicorn service at `/etc/systemd/system/gunicorn.service`, then enable and start start the systemd service (details of how to do this is outwith the scope of this document, but if you need further advice feel free to get in touch).

- Create a systemd unit file to run the django_q service (which manages long running operations, such as 'stock taking') at `/etc/systemd/system/djangoq.service`. Enable and start the systemd service (details of how to do this is outwith the scope of this document, but if you need further advice feel free to get in touch).

- Install a web server (recommended Nginx) to operate as a reverse proxy and create an appropriate configuration file to connect to the unix socket created by gunicorn (as defined above). See the official Nginx and Django documentation for configuration examples.

- Create the database tables, using the commands:

  - `python manage.py makemigrations;`
  - `python manage.py makemigrations stock_control;`
  - `python manage.py makemigrations accounts;`
  - `python manage.py migrate`.

- If running for the first time (i.e. your persistent database folder is empty), define a superuser by issuing the following commands from the application's root directory `python manage.py createsuperuser`.

- In the application's root directory, run `python manage.py collectstatic`, to add the static files to the appropriate directory (ensure the path to the `static` directory has been correctly configured in your web server configuration).



- Now visit the app's administration area in your web browser (e.g. `https://your.domain.tld/admin`).

- If running for the first time, create an `administrators` group and add a new user to it, as follows:

  - Click `add` next to `Groups` in the `Authentication & Authorization` section.
  - Name the new group `administrators`.
  - Under `Available permissions`, scroll to the bottom and select all the `spm_app` permissions, clicking the arrow on the right to add these to the `Chosen permissions` pane (you may hold `shift` to select multiple at once). Once done, click `Save`.
  - Create 2 additional user accounts; one for an application `administrator` (responsible for managing the `warehouse`), the other a regular user (a manager of a client `shop`). Ensure *both* users are assigned the `staff` status.
  - Assign the `administrator` user to the `administrators` group, by: navigating to `Home > Users > username`; scrolling down to the `Permissions` section; selecting `administrators` from the `Available groups` box; and double-clicking it. This moves the group to the `Chosen groups` pane. Then, scroll to the bottom of the page and click `Save`.

- If running for the first time, it's also necessary to initialise the provided frontend client by creating a single stock line from the admin dashboard (this only needs to be done if there are zero stock lines in the database - all subsequent stock items may be added directly from the provided frontend client by users with the `administrators` privilege). To do this:
  
  - Navigate to `Home > Stock_Control > Stock datas` (accessible via the sidebar)
  - Tap the `Add Stock Data` button (top right of the screen)
  - Complete and submit the form.
  
- Click `LOG OUT` (top right)

- Login to the [web client](https://github.com/Aninstance/simple-stock-management-frontend) using the administrator user you created. Begin using Simple Stock Management.

### Update Instructions

- From the application's root directory, run `git pull`. 
- Then, run `pip3 install -r requirements.txt`.
- Then, restart the gunicorn server: `systemctl restart gunicorn.service djangoq.service`.

## Brief UI instructions

Please see the repository for the frontend client, at https://github.com/Aninstance/simple-stock-management-frontend

Note: The above guide is not definitive and is intended for users who know their way around Ubuntu server and Django.

*Users would need to arrange database backups and to secure the application appropriately when used in a production environment.*

## Development Roadmap

- No new features planned at present. To request a change or additional functionality, or to file a bug, please open a github issue and/or contact dan@uplandsdynamic.com.

## Authors

- Dan Bright (Uplands Dynamic), dan@uplandsdynamic.com
",101,101,11,7,retail,"[charity, database, django, inventory, inventory-management, inventory-management-system, non-profit, python3, react, retail, retail-data, shopwired, small-business, stock-control, stock-inventory, stock-management, stock-management-system, warehouse, warehouse-stock]",44-45
surgieboi,zyla,,https://github.com/surgieboi/zyla,https://api.github.com/repos/zyla/surgieboi,The fastest full-stack headless commerce platform in the world.,"![Zyla](zyla.jpg)

# Zyla

The fastest full-stack headless commerce platform in the world.

# Overview

Zyla is the fastest end-to-end headless commerce stack that blends cutting-edge content management and commerce tools that can accept payments and ship worldwide on deployment.

# Requirements

* [Dola](https://dola.me) merchant account 
* [Vercel](https://vercel.com)

# Getting Started

1. Go to [zyla.rocks](https://zyla.rocks) and click `Deploy Now`
2. Select your [Starter Kit](#starter-kits)
3. Finalize your project details
4. Deploy!

Once your project has been deployed on Vercel, you'll receive a public URL with an SSL automatically applied.

To add your own domain, read the [Custom Domains](https://vercel.com/docs/custom-domains) guide on Vercel.

Note, domains typically get applied within minutes; however, it may take 24-48 hours for your DNS to fully resolve.

# Starter Kits

Each starter kit comes integrated with [BEP](https://bep.life) and [Dola](https://dola.me), and preconfigured to be 1-click deployable on Vercel using `Deploy Now`.

### BEP

B.E.P. or Backendless Ecommerce Platform turns any website into a shop with just 1-line of code, no backend or CMS required. Plus, it's free, minus standard processing fees (2.9% + $0.30, per transaction).

### Dola

Dola is a 1-click checkout and digital wallet. Unlike Apple, Google, or Shop Pay, Dola works on every browser and device, and is platform agnostic.

### Starter Kits

* GraphCMS
    * [Gatsby](https://github.com/dolapay/bep-examples/tree/main/with-graphcms-gatsby)
    * [NextJS](https://github.com/dolapay/bep-examples/tree/main/with-graphcms-next)
    * [NuxtJS](https://github.com/dolapay/bep-examples/tree/main/with-nuxt-graphcms)

Additional CMS integrations coming soon!

# Examples

### Zyla's Swag Store

![Zyla](assets/zyla.jpg)

Zyla's swag store uses Zyla's [GraphCMS & NextJS](https://github.com/dolapay/bep-examples/tree/main/with-graphcms-next) to sell rare Pokemon cards.

[View Store](https://swag.zyla.rocks/)

# Browser Compatibility

- last 2 Chrome versions
- last 2 Firefox versions
- last 2 Edge versions
- modern browsers

# Contribute

If you like the idea behind BEP and want to become a contributor - do not hesitate and check our list of the active issues or contact us directly via zyla@dola.me or [join our Discord](https://discord.gg/9ZbKMHa).

If you have discovered a :ant: or have a feature suggestion, feel free to create an issue on Github.
",79,79,8,0,retail,"[commerce, ecommerce, payments, retail]",44-45
Shreerang,Vue-Nuggets,,https://github.com/Shreerang/Vue-Nuggets,https://api.github.com/repos/Vue-Nuggets/Shreerang,E-commerce UI Nuggets based on Vue,"![Logo](src/assets/Vue_Nuggets.png)

## Library of UI components for e-commerce sites built using VueJs

## NPM Package 📦 📦 📦
You can now download the **ecommerece-ui-nuggets** from [NPM](https://www.npmjs.com/package/ecommerce-ui-nuggets)

## Installing the Package
Install the package.
``` javascript
npm install ecommerce-ui-nuggets
```

## Usage
``` javascript
import { Component_Name } from 'ecommerce-ui-nuggets'
```

## Demo Site 📡
https://ecommerce-vue-nuggets.herokuapp.com/

## Storybook (WIP) 📕
https://shreerang.github.io/Vue-Nuggets

## 2019 RoadMap 🛣️
https://github.com/Shreerang/Vue-Nuggets/wiki/Roadmap

## You can buy me a KoFi ☕
Buy me a [cup of Coffee](https://ko-fi.com/shreerangp) if you like my work and if you use any of these components

## What is this❓
I am creating a library of UI components using VueJs targeted towards e-commerce sites..✌️

## Why am I doing this? 🤔
I was doing a comparative study of various e-commerce sites; large and small and I realized 💭 that a lot of things are common to these sites. These things are something that can be componentized 🗃️ and used across sites. ℹ️

These comonents are currently being built using [Vue](https://vuejs.org/). I might consider building the same components using [React](https://reactjs.org/) as well. At some point I might even consider creating [Web Components](https://www.webcomponents.org/) for this.

## Unit Tests 💉
I will try and keep the test coverage for all of these components at a 100% at all times. 😃 Currently it is very low. 😢 (I promise this will improve)

## Browser Support 🌎
**Chrome**, **Firefox** and **Safari** are 100% supported! Not everything will ne supported on IE11 and Edge!

## Accessibility ♿
Web Accessibility is extrememly important! I will make all the efforts to implement and test the components for web accessibility.

### List of components 📇

## 1. Quantity Selector (Dev Complete) 🍰
### Usage
``` javascript
import { QuantitySelector } from 'ecommerce-ui-nuggets';

components: {
    QuantitySelector,
}

<QuantitySelector />
```

### Properties
| Property Name      | Default Value | Required |  Type   |
| ------------------ | :-----------: | :------: | :-----: |
| count              |       1       |    No    | Number  |
| maxCount           |       6       |    No    | Number  |
| isCountEditable    |     true      |    No    | Boolean |
| iconDimensions     |      15       |    No    | Number  |
| minusIconFillColor |     #000      |    No    | String  |
| plusIconFillColor  |     #000      |    No    | String  |

Count of the component is emitted to the parent copmponent using a custom event `get-count`

## 2. Star Rating (WIP) 🏗️
### Usage
``` javascript
import { StarRating } from 'ecommerce-ui-nuggets';

components: {
    StarRating,
}

<StarRating :rating=""3.5"" />
```

### Properties

| Property Name  |      Default Value      | Required |      Type      |
| -------------- | :---------------------: | :------: | :------------: |
| totalStars     |            5            |    No    |     Number     |
| rating         |           NA            |   Yes    |     Number     |
| noRatingMsg    | Be the first to review! |    No    |     String     |
| totalReviews   |           NA            |    No    | Number/ String |
| fillColor      |         #42b983         |    No    |     String     |
| baseColor      |          #CCC           |    No    |     String     |
| iconDimensions |           20            |    No    |     Number     |

## 3. Responsive Product Grid (Dev Complete) 🍰
I initially started off building a Grid based on what made most sense at that time; defining the number of cells and the number of columns based on the breakpoints. Ans hence the component was this way.

### Usage
``` javascript
<Grid :cells=""10"" />

<Grid :cells=""10"" :columns=""5"" />

<Grid :cells=""17"" :columns=""{xs: 2, sm: 3, md: 4, lg: 5}"" />
```
I soon realized that this is not what developers would want out of the Grid component. Use cases like a Product Grid (Search/ Browse page), ""Suggested Products"" or ""Similar Products"" like scroll, Product Page and so on are all truely data driven. So there is the CSS grid and then the **data** that is the most critical part!

So I have updated the implementation and the usage is no longer going to look like it appears above!

### Usage
``` javascript
import { Grid, GridItem } from 'ecommerce-ui-nuggets';

components: {
    Grid,
    GridItem
}

/* Use the following when you want to build a product scroll like feature */
<Grid :columns=""7"">
    <GridItem v-for=""(item, index) in dataGrid"" :key=""index"">
        <p>Content in each cell goes here!</p>
    </GridItem>
</Grid>

/* Use the following when you want to build a search/ browse page like feature */
<Grid :columns=""{xs: 2, sm: 3, md: 4, lg: 5}"">
    <GridItem v-for=""(item, index) in dataGrid"" :key=""index"">
        <p>Content in each cell goes here!</p>
    </GridItem>
</Grid>
```

### Notes
1. I am making use of 4 CSS break-points, prescribed by [Twitter Bootstrap](https://getbootstrap.com/docs/4.1/layout/overview/).
2. ```css
   /* Extra small devices (portrait phones, less than 576px) */
   @media all and (max-width: 575.98px) {
   	...;
   }

   /* Small devices (landscape phones, less than 768px) */
   @media all and (max-width: 767.98px) {
   	...;
   }

   /* Medium devices (tablets, less than 992px) */
   @media (max-width: 991.98px) {
   	...;
   }

   /* Large devices (desktops, less than 1200px) */
   @media (min-width: 992px) {
   	...;
   }

   /* Extra large devices (large desktops)
   No media query since the extra-large breakpoint has no upper bound on its width */
   ```

3. The columns prop can accept a number or an object in the following format
    ```javascript
    {
        xs: 2,
        sm: 3,
        md: 4,
        lg: 5
    }
    ```
4. The keys **xs**, **sm**, **md** and **lg** correspond to the 4 break points described above.
5. The values for each of these keys is the number of columns that you would like to display for that break point.
6. The dashed border on each cell is just representative. Actual component does not have this border.
7. The number displayed inside of the cell is also represtative. Actual component does not have this number.

### Properties
| Property Name |        Default Value         | Required |       Type       |
| ------------- | :--------------------------: | :------: | :--------------: |
| columns       | {xs: 2, sm: 2, md: 3, lg: 4} |    No    | Number or Object |

## 4. Alerts (errors and warnings) Global/page-level and inline (Dev Complete) 🍰

### Usage
``` javascript
import { Alert } from 'ecommerce-ui-nuggets';

components: {
    Alert,
}

<Alert />
```

### Properties
| Property Name | Default Value | Required |  Type   |
| ------------- | :-----------: | :------: | :-----: |
| alertType     |     error     |    No    | String  |
| alertPosition |    global     |    No    | String  |
| isActive      |     false     |    No    | Boolean |
| persistFor    |      NA       |    No    | Number  |

### Notes
1. Persistance of the global alert signifies the time for which the alert will be shown and then it will disappear! The time is expected in seconds! The component will convert it to milli-seconds.
2. In case there is a global error and an inline field level error on the screen, the global error message will always take presidence. Meaning, the screen will scroll to the global error message first.

## 5. Bag Count (Dev Complete) 🍰
### Usage
```javascript
import { BagCount } from 'ecommerce-ui-nuggets';

components: {
    BagCount,
}

<BagCount />
```

### Properties
| Property Name  |                                                                                                                                                                                                                   Default Value                                                                                                                                                                                                                    | Required |  Type  |
| -------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :------: | :----: |
| iconDimensions |                                                                                                                                                                                                                         20                                                                                                                                                                                                                         |    No    | Number |
| iconPath       | M177.91,55.377h-22.589v-1.368C155.311,24.25,131.091,0,101.302,0C71.503,0,47.292,24.25,47.292,54.009v1.368H24.704L11.495,202.614h179.624L177.91,55.377L177.91,55.377z M101.302,6.624c19.687,0,36.619,12.105,43.761,29.232c-9.448-14.137-25.5-23.478-43.761-23.478c-18.231,0-34.313,9.34-43.77,23.507C64.713,18.729,81.635,6.624,101.302,6.624z M57.297,55.377c4.406-20.263,22.481-35.485,44.024-35.485c21.582,0,39.618,15.222,44.024,35.485H57.297z |    No    | String |
| fillColor      |                                                                                                                                                                                                                      #42b983                                                                                                                                                                                                                       |    No    | String |
| fontSize       |                                                                                                                                                                                                                         80                                                                                                                                                                                                                         |    No    | Number |
| fontColor      |                                                                                                                                                                                                                        #FFF                                                                                                                                                                                                                        |    No    | String |
| bagCount       |                                                                                                                                                                                                                         NA                                                                                                                                                                                                                         |    No    | Number |

### Notes
1. The `iconPath` property tells the component about the path the SVG icon needs to draw! This opens up a lot of options for the developers.
2. When the bag is empty, the bag icon can be different, from when the bag has items! This can be achieved by passing different path values to the `iconPath` property based on the bag count.
3. Since empty bag can have a different SVG path compared to when the bag count is greater than 0, the `bagCount` property is not required.

## 6. Variance Selector - Color & Size Picker (WIP) 🚧
### Usage
``` javascript
import { VarianceSelector } from 'ecommerce-ui-nuggets';

components: {
    VarianceSelector,
}

/* Use the following when you want to build a size picker */
<VarianceSelector labelName=""Size"" labelDefaultValue=""Please select a size"" :varianceData=""[{ name: 'Small', value: 'S', availability: false },{ name: 'Medium', value: 'M', availability: false },{ name: 'Large', value: 'L' },]"" />

/* Use the following when you want to build a color picker */
<VarianceSelector labelName=""Color"" labelDefaultValue=""Please select a color"" shape=""circle"" :varianceData=""[{ name: 'Small', value: 'S', availability: false },{ name: 'Medium', value: 'M', availability: false },{ name: 'Large', value: 'L' },]"" />
```

### Properties
| Property Name     |           Default Value            | Required |               Type                |
| ----------------- | :--------------------------------: | :------: | :-------------------------------: |
| labelName         |                Type                |    No    |              String               |
| labelDefaultValue | Please select one of the following |    No    |              String               |
| varianceData      |                 NA                 |    No    |               Array               |
| shape             |               square               |    No    | String oneOf ['square', 'circle'] |

Selected value of the component is emitted to the parent copmponent using a custom event `get-selected-variant`

### Notes
⚠️ Some edge cases are still being worked upon! ⚠️

1. Variance Selector is one of size or color picker.
2. The label name, and its default value are configurable.
3. There are 2 supported shapes - square (for size picker) and circle (for color picker)
4. This component is extremely data driven and hence value of `varianceData` defines the component. Pay special attention to #5 and #6 below.
5. The `varianceData` prop is an Array or objects that would look something like this in case of the **size-picker**.

``` javascript
[
	{ name: 'Xtra Small', value: 'XS' },
	{ name: 'Small', value: 'S', availability: false },
	{ name: 'Medium', value: 'M', availability: false },
	{ name: 'Large', value: 'L' },
	{ name: 'Xtra Large', value: 'XL' },
	{ name: 'Double XL', value: 'XLL' },
];
```

-   Here, the `value` will always be shown within the box and the `name`, if available; will be shown in place of `labelDefaultValue` when it is selected.
-   If `name` is not available we will show the `value` in place of `labelDefaultValue` when selected.
-   The `availability` key needs to passed as `false` only when the product in that particular size is unavailable, so that the component can style the unavailable sized box accordingly.
-   When the `availability` key is not present or when is passed as `true` we will assume that the size is available.

6. The `varianceData` prop is an Array or objects that would look something like this in case of the **color-picker**.

```javascript
[
	{ name: 'Black/Red', image: { background: swatchImg, position: '-0px 0' } },
	{ name: 'Black/Pink', availability: false, image: { background: swatchImg, position: '-56px 0' } },
	{ name: 'Black/Tan', availability: false, image: { background: swatchImg, position: '-112px 0' } },
	{ name: 'Black/Tan/Purple', value: 'BTP', image: { background: swatchImg, position: '-168px 0' } },
	{ name: 'Magnta Multi', value: 'MM', image: { background: swatchImg, position: '-224px 0' } },
];
```

-   Here, `name` will always be shown in place of `labelDefaultValue`, even if `value` is available. In case `name` is unavaialble `value` will be shown in place of `labelDefaultValue` when selected.
-   The important thing to note here is the `image` object. This object expects 2 keys - `background` which is the route to the image and `position` which is required to define the background position of the image.

## 7. Layout switcher (Planned) 🔮

## 8. Image Carousel (Planned) 🔮

## 9. Modal (Planned) 🔮

## 10. Infinite Scroll (Planned) 🔮

## 11. Scroll to Top (Dev Complete) 🍰
### Usage
```javascript
import { ScrollToTop } from 'ecommerce-ui-nuggets';

components: {
    ScrollToTop,
}

<ScrollToTop />
```

### Properties
| Property Name  |                                                                                                           Default Value                                                                                                           | Required |  Type  |
| -------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :------: | :----: |
| iconDimensions |                                                                                                                20                                                                                                                 |    No    | Number |
| iconPath       | M5.906,34.998c-1.352,1.338-3.541,1.338-4.893,0c-1.35-1.338-1.352-3.506,0-4.846l19.54-19.148c1.352-1.338,3.543-1.338,4.895,0l19.539,19.148c1.352,1.34,1.352,3.506,0,4.846c-1.352,1.338-3.541,1.338-4.893,0L23,19.295L5.906,34.998z |    No    | String |
| fillColor      |                                                                                                              #42b983                                                                                                              |    No    | String |
| iconViewBox    |                                                                                                         0 0 46.001 46.001                                                                                                         |    No    | String |

## Note ⚠️
If you would like to suggest more components. Please add a issue on Github and use tag component-request.",78,78,4,6,retail,"[ecommerce-site, retail, ui-nuggets, vue, vue-components, vuejs2]",44-45
Gigacore,react-shopping-cart,,https://github.com/Gigacore/react-shopping-cart,https://api.github.com/repos/react-shopping-cart/Gigacore,A simple shopping cart SPA written in React & Redux,"# React Shopping Cart
A simple shopping cart SPA with an ability to persist cart items on page refresh. 

**Demo:** https://www.gigacore.in/demos/shopping-cart/

### Under the hood

* React
* Redux
* ES6
* No DOM manipulation libraries used (such as jQuery)
* SASS
* Redux-Persist to store cart data on page refresh using localStorage.

### Get it runnin'!
* git clone
* npm i
* npm start

#### The MIT License (MIT)
MIT © 2017 Santhosh Sundar",77,77,5,8,retail,"[ecommerce, es6, react, redux, retail, shopping-cart]",44-45
intelligent-retail,smart-store,intelligent-retail,https://github.com/intelligent-retail/smart-store,https://api.github.com/repos/smart-store/intelligent-retail,,"# Smart Store sample application

このリポジトリは、 [Smart Store リファレンスアーキテクチャー](https://news.microsoft.com/ja-jp/2019/01/29/blog-smart-store/) に基づくサンプル実装です。

:warning: _This document supports in Japanese only for now, sorry._

## Key Features

このサンプル実装には以下の機能が含まれています。

- 統合商品マスタ: `/src/item-service`
- 在庫管理: `/src/stock-service`
- Box管理サービス: `/src/box-service`
- POSサービス: `/src/pos-service`
- Boxクライアントアプリ: `/src/client-app`

## Getting Started

Azure のリソースのデプロイおよびプロビジョニングについては、 [セルフペースドハンズオン資料](/docs/self-paced-handson.md) をご参照ください。
",76,76,23,11,retail,"[azure, azure-functions, cosmosdb, retail]",44-45
moqui,PopCommerce,moqui,https://github.com/moqui/PopCommerce,https://api.github.com/repos/PopCommerce/moqui,"POP Commerce is an eCommerce and ERP application suite for retail and wholesale organizations. POP Commerce is based on Moqui Framework, Mantle Business Artifacts, and Simple Screens.","## POP (Plain-Old-Product) Commerce - Retail and Wholesale ERP and eCommerce

[![license](http://img.shields.io/badge/license-CC0%201.0%20Universal-blue.svg)](https://github.com/moqui/PopCommerce/blob/master/LICENSE.md)
[![build](https://travis-ci.org/moqui/PopCommerce.svg)](https://travis-ci.org/moqui/PopCommerce)
[![release](http://img.shields.io/github/release/moqui/PopCommerce.svg)](https://github.com/moqui/PopCommerce/releases)
[![commits since release](http://img.shields.io/github/commits-since/moqui/PopCommerce/v2.2.0.svg)](https://github.com/moqui/PopCommerce/commits/master)
[![downloads](http://img.shields.io/github/downloads/moqui/PopCommerce/total.svg)](https://github.com/moqui/PopCommerce/releases)

[![Discourse Forum](https://img.shields.io/badge/moqui%20forum-discourse-blue.svg)](https://forum.moqui.org)
[![Google Group](https://img.shields.io/badge/google%20group-moqui-blue.svg)](https://groups.google.com/d/forum/moqui)
[![LinkedIn Group](https://img.shields.io/badge/linked%20in%20group-moqui-blue.svg)](https://www.linkedin.com/groups/4640689)
[![Stack Overflow](https://img.shields.io/badge/stack%20overflow-moqui-blue.svg)](http://stackoverflow.com/questions/tagged/moqui)

POP Commerce is a simple eCommerce application and an admin app to administer the eCommerce site and manage various 
aspects of a retail or wholesale business, from customer service and fulfillment to accounting. The goal is for POP 
Commerce to include all functionality a general retail business might need.

While meant to be a usable and functional, the primary purpose is to be a easy to customize starting point for custom 
eCommerce sites and a demonstration of the use of Moqui Framework and Mantle Business Artifacts entities and services 
for building eCommerce applications.

POP Commerce is based on the Moqui Framework and Mantle Business Artifacts projects. It reuses various ERP screens from 
the SimpleScreens project. 

### Running POP Commerce

To run POP Commerce you need Moqui Framework, POP Commerce itself, and the components it depends on. Moqui supports a 
few methods for setup and deployment as described in the documentation here:

<http://www.moqui.org/docs/framework/Run+and+Deploy>

The easiest way to try POP Commerce is with the binary distribution available on GitHub:

<https://github.com/moqui/PopCommerce/releases>

If you don't have gradle or ant installed you can use this command line to run Java directly:

    $ java -jar moqui.war

### Build and Run Locally

To get and locally run the latest POP Commerce you'll need JDK 8 or later (OpenJDK or Oracle), and either a git client or you can 
use the binary download link on GitHub.

Java can be downloaded here (make sure to use the Download button under the **JDK** column, NOT the under the JRE column):

<http://www.oracle.com/technetwork/java/javase/downloads/index.html>

The following instructions use the Gradle Wrapper to build. You can optionally download and install Gradle 
(from <http://www.gradle.org/downloads>) and use **gradle** instead of **./gradlew** in the example commands.

To download Moqui/Mantle/PopCommerce source and build/run locally use the following steps:

#### Step 1: Download Moqui Framework

Zip: <https://github.com/moqui/moqui-framework/archive/master.zip>

Git: <https://github.com/moqui/moqui-framework.git>

From either source you should put the contents in a **moqui** directory for the next steps. If you use the Zip download 
change the directory name from **moqui-framework-master** to **moqui**. If you clone the Git repository clone it into 
a **moqui** directory. 

#### Step 2: Download POP Commerce and Dependencies

This is easy with the dependency configuration per component, and the Gradle get component tasks. With Gradle Wrapper 
you don't need to install Gradle separately to do this. The PopCommerce component is configured by default in the Moqui 
addons.xml file, so just run:

    $ ./gradlew getComponent -Pcomponent=PopCommerce

If you downloaded the zip archive for Moqui Framework this will download the zip archives for PopCommerce and each 
component it depends on. If you cloned from the git repository this will clone all components from their repositories. 

#### Step 3: Build and Load Data

From the **moqui** directory run:
 
    $ ./gradlew load

This will build Moqui and load seed and demo data from all components into an embedded H2 database.

#### Step 4: Run Moqui

From the **moqui** directory run:
 
    $ java -jar moqui.war

#### Step 5: Access the POP Commerce applications

For the eCommerce application, in your browser go to:

<http://localhost:8080/popc>

Or for the admin application go to:

<http://localhost:8080/vapps/PopcAdmin>

Use the button in the lower-left corner of the screen login as John Doe.

### Setup Commands Quick Reference

Java 8 JDK is required (OpenJDK or Oracle): <http://www.oracle.com/technetwork/java/javase/downloads/index.html>

Here are command line steps for initial checkout, setup, and run:

    $ git clone git@github.com:moqui/moqui-framework.git moqui
    $ cd moqui
    $ ./gradlew getComponent -Pcomponent=PopCommerce
    $ ./gradlew load
    $ java -jar moqui.war

Here are steps for a basic update (for development with clean out and rebuild of database):

    $ cd moqui
    $ ./gradlew cleanAll gitPullAll
    $ ./gradlew load
    $ java -jar moqui.war

To access the eCommerce app go to something like <http://localhost:8080/popc> in a web browser. To access the admin app 
go to <http://localhost:8080/vapps/PopcAdmin>.
",72,72,33,5,retail,"[accounting, application, crm, ecommerce-application, erp, erp-application, inventory, java, mantle, mantle-business, moqui, moqui-framework, pop-commerce, purchasing, retail, sales, web, webapp, wholesale]",44-45
enghwa,ecommerce-shopfront-on-aws,,https://github.com/enghwa/ecommerce-shopfront-on-aws,https://api.github.com/repos/ecommerce-shopfront-on-aws/enghwa,"A high availability, API-first reference architecture for ecommerce using AWS services","# How to build a multi-region highly available active-active architecture

This self-paced workshop takes to step by step **Building Multi-Region Active-Active (and Active-Passive) solution with morden architecture and polyglot persistent databases**. The web application is a Bookstore, and it can be used as a reference architecture for highly available ecommerce store.

![Bookstore](images/bookstore.png)

Key points on implementation:

 * Multi-region Active – Active (Active – Passive) Architecture
 * Modern Web Application using Amplify and ReactJS
 * Modern Application architecture using Serverless and Container
 * Polyglot Persistence database architecture with DynamoDB and Aurora MySQL

<!-- Functionality
=============
 * The content marketing and blogging framework 
 * Ecommerce flow to purchase, manage the cart, checkout, order history and best seller products
 * It is a Cloud-Ready  -->

<!-- Highly Available
================
 * The application can failover to another region with RTO and RPO of less than 15 minutes
 * **RTO:** Recovery Time Objective – the targeted duration of time and a service level within which a business process must be restored after a disaster.
 * **RPO:** Recovery Point Objective –  the maximum targeted period in which data might be lost from a service due to a major incident. -->

Architecture Overview
=====================

![Architecture diagram](images/architecture_diagram.png)

The architeure includes the following flow:

1. An Wordpress layer for the book blog posts, using AWS Fargate and Aurora MySQL. (Module 1-1)
2. An UI layer built using HTML, Javascript (ReactJS) and CSS and hosted directly from AWS S3. An API layer built using Node.js running on AWS Lambda and exposed via Amazon API Gateway. A data layer storing book and order information in DynamoDB and Elasticache. (Module 1-2)
3. Build the same Wordpress layer in the secondary region. (Module 2-1)
4. Configure the replication across the regions for Aurora MySQL, S3, and DynamoDB. (Module 2-2)
5. Deploy the same UI and API layer in Singare. (Module 2-3)
6. Configure API gateway Custom Domain Name and Route53 Health Check for the failover testing. (Module 3)

For the purposes of this workshop, our failover is focused on the path from our application through API Gateway. 

<!-- The backend components are replicated to the second region so that it can be
failovered in the event of a disaster. All data in DynamoDB, S3, Aurora MySQL will be
replicated from the primary region to the secondary region ensures that our
application data will be available when we failover. -->

<details><summary>Application, Database, Infrastructure componets</summary>

**Application components**

* Web application blueprint – We include a React web application pre-integrated out-of-the-box with tools such as ReactJS Bootstrap, Redux, React Router, internationalization, and more.
* Serverless service backend – Amazon API Gateway powers the interface layer between the frontend and backend, and invokes serverless compute with AWS Lambda.  
* Authentication - Amazon Cognito to allow the application to authenticate users and authorize access to
the API layer. *Note* We will only use a single region for Amazon Cognito, as this serves as a reference implementation for authentication. In real-world deployment, this can be a social media authentication, eg: Amazon Cognito, Auth0, Facebook, Google etc.

**Database components**

* Product catalog/shopping cart - Amazon DynamoDB offers fast, predictable performance for the key-value lookups needed in the product catalog, as well as the shopping cart and order history. In this implementation, we have unique identifiers, titles, descriptions, quantities, locations, and price.
* Top sellers list - Amazon ElastiCache for Redis reads order information from Amazon DynamoDB Streams, creating a leaderboard of the “Top 20” purchased or rated books.
* Blog information - Amazon Aurora is a MySQL-compatible relational database that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. In this implementation, it includes book information for Blog posts.

**Infrastructure components**

* Continuous deployment code pipeline – AWS CodePipeline and AWS CodeBuild help you build, test, and release your application code. 
* Serverless web application – Amazon CloudFront and Amazon S3 provide a globally-distributed application. 
* Health check and routing - AWS Route53 is used for DNS and allows us to perform
health checks on our primary region, and upon detecting an issue,
automatically switching to the secondary region using Route53 DNS updates.

</details>

Implementation Instructions
===========================

This workshop is broken up into multiple modules. In each, we will walk
through a high level overview of how to implement or test a part of this architecture. 

### Recorded Video walk-through of the labs

[Lab 1-1a](https://youtu.be/NezXEBoDYZY)

[Lab 1b-2](https://youtu.be/CzmhoHMXIsw)

[Lab 2](https://youtu.be/pbj3wHvWssc)

[Lab 3](https://youtu.be/aYCk7XOZ2wk)

<!-- You will expand sections for detailed command or console instructions. -->

**Region Selection**
We have designed this workshop to use Ireland as the Primary Region and Singapore as Secondary Region.
Please check before creating resources to ensure you are in the correct region:
* Primary Region : `eu-west-1` (Ireland)
* Secondary Region : `ap-southeast-1` (Singapore)

### Modules 
0. [Prepare prerequisites](0_Prerequisities/README.md)
1. [Build a bookstore on Primary region](1_PrimaryRegion/README.md)
2. [Build a bookstore on Secondary region](2_SecondaryRegion/README.md)
3. [Configure Routing](3_Route53Configuration/README.md)
4. [Test failover](4_TestingFailover/README.md)
5. [Cleaning Up](5_Cleanup/README.md)

**Important Note**

In order to conduct this workshop you need, 
1. your own **laptop** (not a tablet) with Chrome or Firefox browsers. You corporate laptop may have a problem accessing domain .xyz.
2. **an AWS Account** with access to use IAM, S3, DynamoDB, Lambda and API Gateway The instructions in this workshop assume **only one student is using a given AWS account at a time**. If you try sharing an account with another student, you'll run into naming conflicts for certain resources - we do not recommend this as there may be unpredictable results or difficult to identify configuration issues.
3. **AWS Account**: Also if you use the corporate account, you might have permission (IAM) or SCP problems. We need **Administrator access** for the workshop. Please create a new AWS account with IAM Administrator access. 
4. **VPC**: Also if your corporate account already has 5 VPCs in Ireland and Singapore, you might experience to hit the default **VPC limit of 5**. 
5. If your laptop's **security policy blocks any 3rd party cookies (required by Cloud9)**, pair up with someone else who has a laptop which is not blocked.

To start the workshop you need the AWS Command Line Interface(CLI). The front end application uses Amplify and requires nodejs and npm. To avoid spending time on configuring your laptop, we will use [AWS Cloud9](https://aws.amazon.com/cloud9/) as our IDE. It has AWS CLI preconfigured. Follow the instruction [here to launch a AWS Cloud9 IDE](0_Prerequisities/README.md) before you start the lab.
Cloud9 will be deployed in a VPC in Ireland. If you don't have any VPC (including **the default VPC**), please create a default VPC using the [VPC console in Ireland](https://eu-west-1.console.aws.amazon.com/vpc/home?region=eu-west-1#vpcs:sort=VpcId). 

### Let's start!
Start the lab: [Prepare prerequisites](0_Prerequisities/README.md)
",61,61,8,18,retail,"[aws, aws-acm-certificate, aws-amplify, aws-amplify-react, aws-api-gateway, aws-aurora, aws-cloudfront, aws-dynamodb, aws-fargate-application, aws-lambda, aws-route53, bookstore, ecommerce-platform, ecommerce-website, headless-wordpress, microservices, react, retail, storefront, typescript]",44-45
xiaoshengxianjun,retailMall,,https://github.com/xiaoshengxianjun/retailMall,https://api.github.com/repos/retailMall/xiaoshengxianjun,一个基于vue开发的零售商城项目示例，采用了vux-UI框架,"# retailMall

> 一个零售类商城项目，包含商品列表，详情，分类，购物车，收藏，地址管理，个人信息等页面，下载直接运行即可查看详情，想图方便的，直接使用或者简单修改就可以。

### 运行效果示例图
![index](https://github.com/xiaoshengxianjun/retailMall/blob/master/demo/demo1.png)
![index](https://github.com/xiaoshengxianjun/retailMall/blob/master/demo/demo2.png) 
![index](https://github.com/xiaoshengxianjun/retailMall/blob/master/demo/demo3.png)
![index](https://github.com/xiaoshengxianjun/retailMall/blob/master/demo/demo4.png)
![index](https://github.com/xiaoshengxianjun/retailMall/blob/master/demo/demo5.png)

## Build Setup

``` bash
# install dependencies
npm install

# serve with hot reload at localhost:8080
npm run dev

# build for production with minification
npm run build

# build for production and view the bundle analyzer report
npm run build --report

# run unit tests
npm run unit

# run e2e tests
npm run e2e

# run all tests
npm test
```

For a detailed explanation on how things work, check out the [guide](http://vuejs-templates.github.io/webpack/) and [docs for vue-loader](http://vuejs.github.io/vue-loader).
",55,55,1,1,retail,"[retail, vue, vux, wx]",44-45
vitorfs,bloodhound,,https://github.com/vitorfs/bloodhound,https://api.github.com/repos/bloodhound/vitorfs,,"# bloodhound
",50,50,5,2,retail,"[crawler, retail]",44-45
RasaHQ,retail-demo,RasaHQ,https://github.com/RasaHQ/retail-demo,https://api.github.com/repos/retail-demo/RasaHQ,Rasa's retail starter pack,"# Retail Example Bot

This is a sample retail bot to help provide starter content, examples of how to implement particular features, and sample use cases. Built using Rasa 2.3.1

## Install dependencies

Run:
```bash
pip install -r requirements.txt
```

## Run the bot

Use `rasa train` to train a model.

Then, to run, first set up your action server in one terminal window:
```bash
rasa run actions
```

In another window, run the duckling server (for entity extraction):
```bash
docker run -p 8000:8000 rasa/duckling
```

Then talk to your bot by running:
```
rasa shell --debug
```

Note that `--debug` mode will produce a lot of output meant to help you understand how the bot is working
under the hood. To simply talk to the bot, you can remove this flag.


## Overview of the files

`data/stories.yml` - contains stories

`data/nlu.yml` - contains NLU training data


`data/rules.yml` - contains the rules upon which the bot responds to queries

`actions/actions.py` - contains custom action/api code

`domain.yml` - the domain file, including bot response templates

`config.yml` - training configurations for the NLU pipeline and policy ensemble

`tests/test_stories.yml` - end-to-end test stories


## Things you can ask the bot

1. Check the status of an order
2. Return an item
3. Cancel an item
4. Search a product inventory for shoes
5. Subscribe to product updates

The bot can handle switching forms and cancelling a form, but not resuming a form after switching yet.

The main flows have the bot retrieving or changing information in a SQLite database (the file `example.db`). You can use `initialize.db` to change the data that exists in this file.

For the purposes of illustration, the bot has orders for the following email addresses:

- `example@rasa.com`
- `me@rasa.com`
- `me@gmail.com`

And these are the shoes that should show as in stock (size, color):

```
inventory = [(7, 'blue'),
             (8, 'blue'),
             (9, 'blue'),
             (10, 'blue'),
             (11, 'blue'),
             (12, 'blue'),
             (7, 'black'),
             (8, 'black'),
             (9, 'black'),
             (10, 'black')
            ]
```

## Testing the bot

You can test the bot on test conversations by running  `rasa test`.
This will run [end-to-end testing](https://rasa.com/docs/rasa/user-guide/testing-your-assistant/#end-to-end-testing) on the conversations in `tests/test_stories.yml`.

Note that if duckling isn't running when you do this, you'll see some failures.

## Rasa X Deployment

To [deploy this bot](https://rasa.com/docs/rasa/user-guide/how-to-deploy/), it is highly recommended to make use of the
[one line deploy script](https://rasa.com/docs/rasa-x/installation-and-setup/one-line-deploy-script/) for Rasa X. As part of the deployment, you'll need to set up [git integration](https://rasa.com/docs/rasa-x/installation-and-setup/integrated-version-control/#connect-your-rasa-x-server-to-a-git-repository) to pull in your data and
configurations, and build or pull an action server image.


## Action Server Image

You will need to have docker installed in order to build the action server image. If you haven't made any changes to the action code, you can also use
the [public image on Dockerhub](https://hub.docker.com/repository/docker/cdesmar/retail-demo) instead of building it yourself.

It is recommended to use an [automated CI/CD process](https://rasa.com/docs/rasa/user-guide/setting-up-ci-cd) to keep your action server up to date in a production environment.


## :gift: License
Licensed under the GNU General Public License v3. Copyright 2021 Rasa Technologies
GmbH. [Copy of the license](https://github.com/RasaHQ/retail-demo/blob/main/LICENSE).
Licensees may convey the work under this license. There is no warranty for the work.
",37,37,5,8,retail,"[bot, chatbot, chatbot-example, python, rasa, rasa-starter-pack, retail]",44-45
samirsaci,ml-forecast-features-eng,,https://github.com/samirsaci/ml-forecast-features-eng,https://api.github.com/repos/ml-forecast-features-eng/samirsaci,Machine Learning for Retail Sales Forecasting — Features Engineering,"## Machine Learning for Retail Sales Forecasting — Features Engineering 📈
*Understand the impacts of additional features related to stock-out, store closing date or cannibalization on a Machine Learning model for sales forecasting*

<p align=""center"">
  <img align=""center"" src=""https://miro.medium.com/max/1280/1*C6QjcwkJGUmw0sot8pd4Cw.png"">
</p>

Based on the feedbacks of the last Makridakis Forecasting Competitions, Machine Learning models can **reduce the forecasting error by 20% to 60% compared to
benchmark statistical models.**

Their major advantage is the capacity to include external features that heavily impact the variability of your sales.

For example, e-commerce cosmetics sales are driven by ***special events** (promotions) and on how you **advertise a reference on the website** (first page, second page, …).

This process called **features engineering** is based on **analytical concepts and business insights** to understand what could drive your sales.

### Article
In this [Article](https://www.samirsaci.com/machine-learning-for-retail-sales-forecasting-features-engineering/), will try to understand the impact of 
several features on the accuracy of a model using the M5 Forecasting competition dataset.

### Experiment
Based on business insights or common sense, we will add additional features, built with existing ones, to help our model to capture all the key factors 
impacting your customer demand.

<p align=""center"">
  <img align=""center"" src=""https://miro.medium.com/max/700/1*PBsf-z8n_DrMaCEtaKkhXQ.png"">
</p>

### Data set
This analysis will be based on the M5 Forecasting dataset of Walmart stores sales records ([Link](
https://www.kaggle.com/c/m5-forecasting-accuracy)).

## Code
1. Create a folder Data in your directory where the notebook is located
2. Download all the files of the kaggle forecasting competition ([Link](
https://www.kaggle.com/c/m5-forecasting-accuracy)).
3. Launch the notebook

## About me 🤓
Senior Supply Chain Engineer with an international experience working on Logistics and Transportation operations. \
Have a look at my portfolio: [Data Science for Supply Chain Portfolio](https://samirsaci.com) \
Data Science for Warehousing📦, Transportation 🚚 and Demand Forecasting 📈 

",36,36,2,2,retail,"[demand-forecasting, feature-engineering, machine-learning, python, retail, sales-forecasting]",44-45
Kkthnx-Wow,KkthnxUI,Kkthnx-Wow,https://github.com/Kkthnx-Wow/KkthnxUI,https://api.github.com/repos/KkthnxUI/Kkthnx-Wow,KkthnxUI for Dragonflight World of Warcraft.,"![commits](https://img.shields.io/github/last-commit/Kkthnx-WoW/KkthnxUI/master) [![issues](https://img.shields.io/github/issues/Kkthnx-Wow/KkthnxUI)](https://github.com/Kkthnx-Wow/KkthnxUI/issues)   
![h4fl1au](https://user-images.githubusercontent.com/1692977/31845157-13107948-b5cc-11e7-926d-67e669b8ca69.png)   

KkthnxUI is a simplistic user interface that holds onto the information and functionality, while still keeping most of the good looks.
It can be used for any class or role. 

## Join the community
There are thousands of users, but most are content to simply download and use the interface without further ado. If you wish to get more involved though, have some questions you can't find answers to anywhere else or simply just wish to stop by and say hello, we have both a [discord](https://discordapp.com/) server and a Facebook page. 

[![discord](https://img.shields.io/discord/885347380231286855?label=Discord)](https://discord.gg/Rc9wcK9cAB)  

[![twitter](https://img.shields.io/twitter/follow/KkthnxUI)](twitter.com/KkthnxUI)   

[![twitch](https://img.shields.io/twitch/status/KkthnxTV)](https://www.twitch.tv/kkthnxtv)   
   
## Buy me a coffee
Donations are welcome, but not required to use the UI at all. Donations help me further my development and fuel my gaming! Donations will never be a requirement to use the UI! If you would like to donate, you can do so down below.

[![paypal](https://img.shields.io/badge/PayPal-Donate-blue)](https://www.paypal.me/kkthnx)   

[![patreon](https://img.shields.io/badge/Patreon-Subscribe-orange)](https://www.patreon.com/kkthnx)   

[![steam](https://img.shields.io/badge/Steam-Wishlist-lightgrey)](http://store.steampowered.com/wishlist/id/Kkthnx)
",33,33,4,3,retail,"[discord, donations, gaming, kkthnx, kkthnxui, patreon, paypal, retail, shadowlands, warcraft, wow]",44-45
Ashwinbicholiya,scanpay,,https://github.com/Ashwinbicholiya/scanpay,https://api.github.com/repos/scanpay/Ashwinbicholiya,self checkout app ,"### ScanPay(Self Checkout Application)

### Show some :heart: and star the repo to support the project

<a href=""https://www.linkedin.com/in/ashwin-bicholiya-9938481a0/"">
  <img align=""left"" alt=""Ashwin's Linkdein"" width=""28px"" src=""https://cdn.jsdelivr.net/npm/simple-icons@v3/icons/linkedin.svg"" />
</a>
<a href=""https://www.instagram.com/ashwinbicholiya/?hl=en"">
  <img align=""left"" alt=""Ashwin's Instagram"" width=""28px"" src=""https://cdn.jsdelivr.net/npm/simple-icons@v3/icons/instagram.svg"" />
</a>
<br/>

### About ScanPay

Mobile application that allows a customer conveniently buy and complete the payment for products without the need to go through the cashier checkout point.

### Gif & Some Screenshot
<img width=""180"" height=""320"" src=""https://user-images.githubusercontent.com/47949413/95316775-e0484b80-08b1-11eb-92c2-c763122d3cbd.gif"" /><img width=""180"" height=""320"" src=""https://user-images.githubusercontent.com/47949413/95170311-e23add80-07d1-11eb-960c-2d7ace0604f2.jpg""/><img  width=""180"" height=""320"" src=""https://user-images.githubusercontent.com/47949413/95170334-e7982800-07d1-11eb-9bd1-15c3b3e7ca1b.jpg""/><img width=""180"" height=""320"" src=""https://user-images.githubusercontent.com/47949413/95170348-ee269f80-07d1-11eb-9ef6-6601f24ff710.jpg""/>
<img width=""180"" height=""320"" src=""https://user-images.githubusercontent.com/47949413/95170359-f252bd00-07d1-11eb-9509-a2b7ec4a6811.jpg""/><img width=""180"" height=""320"" src=""https://user-images.githubusercontent.com/47949413/95170371-f7177100-07d1-11eb-8d3d-39888e4001b9.jpg""/><img  width=""180"" height=""320"" src=""https://user-images.githubusercontent.com/47949413/95170394-fe3e7f00-07d1-11eb-927d-06d7ab77cc7d.jpg""/><img  width=""180"" height=""320"" src=""https://user-images.githubusercontent.com/47949413/95170397-ff6fac00-07d1-11eb-97c2-690505eb9bf5.jpg""/>
<img width=""180"" height=""320"" src=""https://user-images.githubusercontent.com/47949413/95170415-04346000-07d2-11eb-8ce3-7c4788b4b0ef.jpg""/><img  width=""180"" height=""320"" src=""https://user-images.githubusercontent.com/47949413/95170428-0991aa80-07d2-11eb-9e47-ea4f7a58e4aa.jpg""/>

### License
    MIT License

    Copyright (c) 2020 ashwin bicholiya

    Permission is hereby granted, free of charge, to any person obtaining a copy
    of this software and associated documentation files (the ""Software""), to deal
    in the Software without restriction, including without limitation the rights
    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
    copies of the Software, and to permit persons to whom the Software is
    furnished to do so, subject to the following conditions:

    The above copyright notice and this permission notice shall be included in all
    copies or substantial portions of the Software.

    THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
    SOFTWARE.

## Getting Started
For help getting started with Flutter, view our online
[documentation](http://flutter.io/).
<br/>
For help on editing plugin code, view the [documentation](https://flutter.io/platform-plugins/#edit-code).
",28,28,3,0,retail,"[android-app, barcode-scanner, firebase, firebase-auth, firestore, flutter, flutter-apps, hacktoberfest, payment-gateway, razorpay, retail, scanner, self-checkout, selfcheckout, user-interface]",44-45
OpenXE-org,OpenXE,OpenXE-org,https://github.com/OpenXE-org/OpenXE,https://api.github.com/repos/OpenXE/OpenXE-org,OpenXE - Free Open Source ERP based on Xentral,"# OpenXE - The free ERP

![OpenXE overview](https://github.com/openxe-org/OpenXE/blob/master/www/themes/new/images/login_screen_picture.jpg ""OpenXE"")

OpenXE ist eine umfassende webbasierte Anwendung zur Abwicklung aller kaufmännischen Prozesse. Zu den Funktionen gehören unter Anderem:

* Erstellung von Angeboten
* Auftragsabwicklung
* Rechnungsstellung
* Bestellung
* Lagerverwaltung
* Kundenkommunikation
* Aufgaben- und Terminverwaltung
* Zeitabrechnung

# Neue Community-Seite: https://openxe.org/

An alle Interessenten:

Dieses Projekt startet mit einer leistungsfähigen Software, aber gerade am Anfang ist es schwierig, die potenziellen Anwender abzuschätzen. Das ist jedoch eine wichtige Information und auch Motivation für alle, die sich aktiv am Projekt beteiligen. Wir bitten daher, alle Interessenten für OpenXE, sich hier anzumelden. Die Anmeldung ist kostenfrei und mit keinerlei Verpflichtungen versehen. Ausserdem habt Ihr den Vorteil, Euch bei Fragen direkt im Communitybereich zu melden oder selber aktiv mitzugestalten.

Wir freuen uns über Eure Teilnahme, egal ob als stiller Mitleser oder aktiver User.

# Letzte Änderungen: Ticket System

Neu in [V.1.3](https://github.com/openxe-org/OpenXE/releases/tag/V.1.3):
Neuimplementierung des Xentral 20 Enterprise Ticketsystems mit vielen Verbesserungen, z.B.:
- Anhänge werden auch bei ausgehenden Nachrichten gespeichert
- Verbesserte Kommentarfunktion
- Unterstützung mehrerer Empfänger und CC
- Verbesserte Stapelverarbeitung

# Installation

[Hier gehts zur Server Installation](SERVER_INSTALL.md)

[Hier gehts zur OpenXE Installation](INSTALL.md)

OpenXE ist freie Software, lizensiert unter der EGPL 3.1.
Diese Software ist eine Ableitung und Veränderung von WaWision ERP. WaWision ERP wurde von embedded projects GmbH entwickelt und steht unter der EGPLv3.1-Lizenz als Open Source Software. Informationen zu WaWision und der Open-Source Version findet man unter http://www.wawision.de

",28,28,9,43,retail,"[accounting, billing, business, crm, erp, invoicing, manufacturing, openxe, procurement, retail, xentral]",44-45
FerasBasha,Forecasting-Retail-Sales-Using-Google-Trends-and-Machine-Learning,,https://github.com/FerasBasha/Forecasting-Retail-Sales-Using-Google-Trends-and-Machine-Learning,https://api.github.com/repos/Forecasting-Retail-Sales-Using-Google-Trends-and-Machine-Learning/FerasBasha,Author: Feras Al-Basha; Research Director: Yossiri Adulyasak; Research Director: Laurent Charlin; MSc in Global Supply Chain Management - Mémoire/Thesis; HEC Montréal.,"## Forecasting Retail Sales Using Google Trends and Machine Learning 

| <img width=""736"" alt=""KelloggsFrostedFlakes in Texas"" src=""https://user-images.githubusercontent.com/39706513/103062604-b6332b00-457c-11eb-8928-4df8e6eb2adc.png""> | 
|:--:| 
| *Example of Predictions Using XGBoost for Kellogg's Frosted Flakes Sales in Texas (Breakfast At The Frat)* |

### Summary

The historical Covid-19 pandemic has demonstrated the impact and essential significance that e-commerce has on the life of individuals during unprecedented times. Although, not all consumers are able to easily transition to e-commerce shopping due to multiple reasons, in particular in developing economies, many shoppers in advanced economies have relied on digital purchases for their needs and desires. Hence, the significant growth in online business has led to a structural change in the retail industry, presenting novel challenges and opportunities in demand forecasting to provide the right product, at the right place, in the right time, for the right price. 

The primary objective of this experiment is to propose a methodological framework to incorporate external data, in particular from  [Google Trends](https://trends.google.com/trends/explore), in retail sales forecasting by leveraging modern machine learning techniques. In order to investigate the predictive power of Google Trends we use the [Brazilian e-commerce by Olist](https://www.kaggle.com/olistbr/brazilian-ecommerce) as well as the [Breakfast at the Frat by dunnhumby](https://www.dunnhumby.com/source-files/) public datasets, to conduct a quantitative experiment in which we compare the predictive performance on sales forecasts of the following models: a) the Seasonal Autoregressive Integrated Moving Average (SARIMA) model, b) The Facebook Prophet tool (FBProphet), c) The Extreme Gradient Boosting algorithm (XGBoost), and d) a recurrent neural network with long short-term memory (LSTM). To measure forecasting accuracy, various performance metrics are used, and the performance of all forecasting models is benchmarked against a naïve model. 

Findings suggest that there is no statistically significant difference in the predictions made by a model that uses only real-world data as data input and a model that includes real-world data and Google Trends as data input. Nevertheless, forecasting accuracy improves when real-world data and Google Trends are combined to predict the sales of some retail products available in the real-world data. Generally, results imply that models making sales predictions that are closer to the mean and with lower forecast error, have a positive impact on inventory performance. Please note that the performance implications of the forecasting errors in the inventory management process are evaluated using a simulation tool and not part of this repository. 

The field continues to expand with research on new machine learning driven forecasting algorithms. Therefore, comparative studies provide an understanding of the progress being made with new approaches relative to previous ones and serve in testing out old approaches that succeeded in previous experiments, on new datasets and scenarios. 

The source code of the experiment is made available to the public and can be adapted in future projects.

### Prediction Task  

- The prediction task for the Brazilian e-commerce dataset is to forecast the weekly number of sales transactions by product category. The scope of sales transactions from the Brazilian e-commerce dataset are limited to the Sao Paolo region and for the top 7 selling product categories. Thus, the Brazilian e-commerce dataset is split into 7 separate datasets and each forecasting model (SARIMA, FBProphet, XGBoost, LSTM) is trained and tested 7 times, once for each product category

- The prediction task for Breakfast at the Frat dataset is to forecast the weekly number of units sold of 4 items across 3 stores. Therefore, the Breakfast at the Frat dataset is split into 12 separate datasets and each forecasting model (SARIMA, FBProphet, XGBoost, LSTM) is trained and tested 12 times, once for each product and store combination. The data used from the Breakfast at the Frat dataset include sales history, promotional, product, manufacturer and store information. 

### Data Input and Perfomrance Comparison Framework

<div align=""center"">
</div>

| ![Capture](https://user-images.githubusercontent.com/39706513/102944008-9110c080-4487-11eb-9bc8-2339261d1e39.PNG) | 
|:--:|

### Experiment Setup 

- The experiment utilizes configuration and parameter files to pre-process data and determine parameter values required to run the forecasting models. 

<div align=""center"">
  Experiment Conceptual Diagram
</div>

|![ExperimentDesign_MSc](https://user-images.githubusercontent.com/39706513/101991375-4bc7e400-3c7a-11eb-968f-00dbf5d85617.png) | 
|:--:|
| *[MLflow](https://mlflow.org/docs/latest/tracking.html) is used to track parameters and performance metrics* |


### Project Structure 

```
├── README.md          <- The top-level README for developers using this project.
│
├── conf               <- Configurations folder for the project.
│   ├── catalog.yml    <- Contains paths to reference datasets.
│   └── params.yml     <- Contains parameters for the experiment and models.
│
├── data
│   ├── 01_raw         <- The original, immutable data dump.
│   ├── 02_processed   <- The final, canonical data sets for modeling.
│   ├── 03_external    <- Google Trends data.
│   ├── 04_results     <- Saved predictions for each model.
│   └── 05_extra       <- Additional outputs.
│
│
├── notebooks          <- Jupyter notebooks.
│   ├── breakfast      <- Contains the notebooks relevant for Breakfast at The Frat.
│   └── olist          <- Contains the notebooks relevant for Olist.
│
├── mlruns             <- Experiment Tracking logs.
│
├── references         <- Data dictionaries, manuals, and all other explanatory materials.
│
├── environment.yml    <- The conda environment file for reproducing the analysis environment.
│
├── src                <- Additional code for this project.
│   ├── utils.py       <- Main functions for use in this project.
│   ├── metrics.py     <- Metrics for use in this project.
│   ├── scalers.py     <- Custom classes to scale the data.
│   └── sarima.py      <- A Scikit-Learn SARIMA model wrapper for easier modelling.
```

To create the environment, do:

```
conda env create -f environment.yml
```

To activate the environment, do:

```
conda activate <env name>
```

### Configuration files

##### catalog.yml

Here are a few examples of paths that are used to reference the datasets. There are two major branches, olist and breakfast.

```yaml
olist:
    base_dir: ""data/01_raw""
    tables:
        customers:     ""olist_customers_dataset.csv""
        products:
    ...
    ...

breakfast:
    base_dir: ""data/01_raw""
    xlsx_fname: ""dunnhumby_breakfast.xlsx""
    sheet_names: 
        transactions: ""dh Transaction Data""
    ...
    ...
```

##### params.yml

The experiment dates defined in this configuration file.   

```yaml
olist:
    experiment_dates: 
        train_start: '2017-01-01'
        test_start: '2018-01-07'
    ...
    ...

breakfast:
    dataset:
        store_ids: 
            2277: 'OH'       
            2277: 'OH'       
            389: 'KY'         
            25229: 'TX' 
        upc_ids:
            1600027527: 'cold_cereal'
            3800031838: 'cold_cereal'
    ...
    ...
```

##### model parameters

For the XGBoost model, there are a few parameters that need to be specified:

- `window_size` is used to create the number of lagged values as input for the model. A value of 52 will create 52 lags of the target column. 
- `avg_units` is used to create rolling averages using the lag-1 column to avoid leakage. It represents a list of rolling average features. A value of 2 will create a two time steps rolling-average, while a value of 16 will create a rolling-average of 16 time steps. Each will represent a column in the dataset.
- `gtrends_window_size` is used to cerate the number of lagged values for the google trends series. Each google trend series will be created usign this value.
- `search_iter` is used to specify how many rounds of hyperparameter search to perform using Hyperopt.

For the LSTM model, there are different parameters that need to be specified:

- `window_size` is used to create the number of lagged values as input for the model. A value of 52 will create 52 lags of the target column. 
- `gtrends_window_size` is used to cerate the number of lagged values for the google trends series. This value must be the same as the window_size as the LSTM expects the same number of dimensions for each feature.
- `dropout` is a hyperparameter that is specified in advance. It is used to reduce overfitting but could be included in the hyperparameter search if desired.
- `units_strategy` is used to determine how to select the number of hidden units for each LSTM layer. A stable strategy will keep the number of units constant accross layers. A decrease strategy will halve the number of units per layer. For a three layer model with initial number of units set to 50, the stable strategy will assign 50 units for each layer while the decrease strategy will set 50 for the first layer, 25 for the second layer and 16 for the third layer.
- `optimizers` the optimizer to use. 
- `loss` the loss to use.
- `search_iter` the number of random search iterations to perform during hyperparameter tunning.

Here is an example.  

```yaml
xgb:
    window_size: 52
    avg_units:
        - 2
        - 4
        - 8
        - 16
        - 26
    gtrends_window_size: 12
    search_iter: 100

lstm:
    window_size: 52
    gtrends_window_size: 52    # <- must match window_size
    dropout: 0.1
    units_strategy: 'decrease' # <- options are {decrease, stable}
    optimizers: 'adam'
    loss: 'mape'
    search_iter: 20
```
",26,26,4,0,retail,"[google-trends, machine-learning, retail, supply-chain-analytics, time-series-forecast]",44-45
BestBuyAPIs,bestbuy-cli,BestBuyAPIs,https://github.com/BestBuyAPIs/bestbuy-cli,https://api.github.com/repos/bestbuy-cli/BestBuyAPIs,🏷  Download data from the Best Buy Catalog API in bulk from the command line,"# bestbuy-cli

[![npm][npm-image]][npm-url]
[![travis][travis-image]][travis-url]
[![standard][standard-image]][standard-url]

[npm-image]: https://img.shields.io/npm/v/bestbuy-cli.svg?style=flat-square
[npm-url]: https://www.npmjs.com/package/bestbuy-cli
[travis-image]: https://img.shields.io/travis/BestBuyAPIs/bestbuy-cli.svg?style=flat-square
[travis-url]: https://travis-ci.org/BestBuyAPIs/bestbuy-cli
[standard-image]: https://img.shields.io/badge/code%20style-standard-brightgreen.svg?style=flat-square
[standard-url]: http://npm.im/standard

Download data from the Best Buy Catalog API in bulk from the command line.

Get an API key at [developer.bestbuy.com](https://developer.bestbuy.com).

![Best Buy CLI Tool](https://cdn.rawgit.com/BestBuyAPIs/bestbuy-cli/master/images/download-all-stores.gif)

## Install

```
npm install --global bestbuy-cli
```

Can't install node? [Download a standalone executable](https://github.com/BestBuyAPIs/bestbuy-cli/releases) from the GitHub Releases page!

## Usage

```bash
>bestbuy --help

Best Buy Bulk Download Tool (https://github.com/BestBuyAPIs/bestbuy-cli)

Usage: bestbuy [resource] [options]

    Examples:
      bestbuy categories
      bestbuy products --query ""active=true"" --show ""name,sku"" --output products.json
      bestbuy stores --format xml --output stores.xml
      bestbuy stores --query ""region=GA&storeType='outlet center'"" --output stores.json

    resource              resource to download: products, categories, stores
    --query, -q           use a custom query to filter the results (ampersand separated)
    --show, -s            fields to show (comma separated)
    --sort, -r            sort results by fields (comma separated)
    --key, -k             Best Buy API key (default: ""BBY_API_KEY environment variable"")
    --format, -f          format of the response as json, xml, csv or tsv (default: ""json"")
    --output, -o          name of file to send output (optional; If not present, out will go to stdout)
    --bare, -b            newline delimited - each item on own line without extra cruft (default: false)
    --version, -v         show version information
    --help, -h            show help

```
Visit the [Best Buy API Documentation](https://developer.bestbuy.com/documentation) for more details on writing custom queries.

## License

[MIT](LICENSE.md)
",23,23,8,1,retail,"[api, best, bestbuy, bulk, buy, catalog, cli, command, download, executable, line, node, products, retail, retailer, stores, tool]",44-45
Smile-SA,elasticsuite-for-retailer,Smile-SA,https://github.com/Smile-SA/elasticsuite-for-retailer,https://api.github.com/repos/elasticsuite-for-retailer/Smile-SA,"Smile ElasticSuite for Retailers - Magento2 toolkit for Retailers : Store Locator, Availability and Prices per store, etc...","## News

### Compatibility

Due to all new paradigms introduced by Magento 2.1 (entity manager, forms based on UI components, staging for the EE edition, ...), the **required minimum version of Magento for using this module is Magento 2.1**

If your project is based on Magento 2.1.x you can start working with ElasticSuite for Retailer today using the latest **1.2.0-alpha1 release**.

### Requirements

The module requires:

- [ElasticSuite](https://github.com/Smile-SA/elasticsuite) > 2.11.*
- [Offer](https://github.com/Smile-SA/magento2-module-offer) > 2.0.*
- [Seller](https://github.com/Smile-SA/magento2-module-seller) > 2.0.*
- [Retailer](https://github.com/Smile-SA/magento2-module-retailer) > 2.0.*
- [Store Locator](https://github.com/Smile-SA/magento2-module-store-locator) > 2.2.*
- [Retailer Offer](https://github.com/Smile-SA/magento2-module-retailer-offer) > 2.0.*
- [Store Delivery](https://github.com/Smile-SA/magento2-module-store-delivery) > 2.0.*

It's a toolkit module to install the RetailerSuite modules.

### How to use

1. Install the module via Composer:


ElasticSuite Version   | Module Version
-----------------------|------------------------------------------------------------------------
ElasticSuite **2.1.x** |Latest release: ```composer require smile/elasticsuite-for-retailer:""^1.4""```
ElasticSuite **2.3.x** |Latest release: ```composer require smile/elasticsuite-for-retailer:""^1.4""```
ElasticSuite **2.6.x** |Latest release: ```composer require smile/elasticsuite-for-retailer:""^1.4""```
ElasticSuite **2.7.x** |Latest release: ```composer require smile/elasticsuite-for-retailer:""~1.5.0""```
ElasticSuite **2.8.x** |Latest release: ```composer require smile/elasticsuite-for-retailer:""~1.6.0""```
ElasticSuite **2.9.x** |Latest release: ```composer require smile/elasticsuite-for-retailer:""~2.2.0""```
ElasticSuite **2.11.x** |Latest release: ```composer require smile/elasticsuite-for-retailer:""~2.3.0""```
ElasticSuite **2.11.x** |Latest release: ```composer require smile/elasticsuite-for-retailer:""~2.4.0""```

2. Enable it:

``` bin/magento module:enable Smile_Offer ```

``` bin/magento module:enable Smile_Seller ```

``` bin/magento module:enable Smile_Retailer ```

``` bin/magento module:enable Smile_StoreLocator ```

``` bin/magento module:enable Smile_RetailerOffer ```

``` bin/magento module:enable Smile_StoreDelivery ```

3. Optionnal: Drop old SMILE_RETAILER_ADDRESS_RETAILER_ID unique key

_if you already used older retailersuite modules on your projects, and you want to upgrade it,_
_before upgrading, you will have to DROP your current UNIQUE KEY from table smile_retailer_address : SMILE_RETAILER_ADDRESS_RETAILER_ID_
_This is necessary in order to get a db_schema.xml working correctly._

``` ALTER TABLE smile_retailer_address DROP INDEX SMILE_RETAILER_ADDRESS_RETAILER_ID ```

4. Install the module and rebuild the DI cache:

``` bin/magento setup:upgrade ```

## How to configure

> Stores > Configuration > Elasticsuite > Elastic Suite for Retailer

Navigation mode: Retailer/Drive   
    * Drive mode: the customer will only see the catalog of the chosen retailer in Front Office.    
    * Retail mode: the customer will browse the Web catalog by default.   
Display offers on product page: Yes/No (When enabled, offers of all stores will be displayed on product page.)

## What is ElasticSuite for Retailers ?

This package is a suite of several modules aiming to help merchants dealing with retail and omnichannel mechanics in Magento 2 (Store Locator, Product availability/price per store, pickup in store, etc...).

The package relies on our custom Search and Merchandising tool called ElasticSuite which can be found [here](https://github.com/Smile-SA/elasticsuite).

For more information, read <strong>the User Guide ""Smile ElasticSuite for retailer""</strong>. It's available [here](https://github.com/vipra93/elasticsuite-for-retailer/blob/master/doc/static/User%20Guide%20Smile%20ElasticSuite%20for%20Retailer%20Magento%202%20v1.pdf).

## Who is developping ElasticSuite for Retailer ?

<p align=""center"">
    <a href=""http://www.smile-oss.com""><img alt=""SmileLab"" src=""https://github.com/Smile-SA/elasticsuite/raw/master/doc/static/smilelab-logo.png"" /></a>
</p>

SmileLab is the innovation and experimentation department of Smile. Smile is the **European leader of Open Source** and also a four-times **Europe Partner of the the Year** (2010-2014) and two-times **Spirit of Excellence** (2015-2016) awarded by Magento.

Our multidisciplinary team brings together experts in technology, innovation, and new applications.

Together we explore, invent, and test technologies of the future, to better serve our clients.

## Main Features

### Current version

The current **2.4.0** version has been focused on compatibility with Magento 2.4.6:

<br/>
Some functionnalities example:

* **Store Locator :**

    This feature allows you to **create and manage your shops** in Magento's back-office. The module comes by default with several pre-configured attributes such as Shop Address, GPS coordinates, Opening Hours, etc...

    Once you have created all your shops, your customers will be able to navigate through them in the Front-Office on a **map**.

![Stores Map](doc/static/shop-map.png)

Each of your stores has also a **Shop detail page** and eventually a **Contact Shop page** if you enable this option on the store.

Your customer will be able to choose his favorite shop and this will keep it during all his navigation through your website.

<br/>

* **Store Offers:**

    This features let you create **specific offers for a given product and a given shop**: you'll be able to define the price and/or the availability for a product in each shop.

<p align=""center"">
    <img alt=""Offer Step one"" src=""doc/static/offer-step-one.png"" />
    <img alt=""Offer Step two"" src=""doc/static/offer-step-two.png"" />
</p>

   You will be able to enable an option to filter the navigation of the customer to the products available in his favorite shop:

<p align=""center"">
    <img alt=""Shop Availability"" src=""doc/static/shop-availability.png"" />
</p>

<br/>

The customer will even have the possibility to see **product's availability in the other shops** on the product detail page:

<p align=""center"">
    <img alt=""Shop Offer List"" src=""doc/static/shop-offer-list.png"" />
</p>

* **In Store delivery:**

    This feature allow the customer to choose between stores for the shipping address of his order.

<p align=""center"">
    <img alt=""Store Delivery"" src=""doc/static/home-store-delivery.png"" />
</p>

This is handled during checkout via a Store chooser in a popin.


<p align=""center"">
    <img alt=""Store Delivery"" src=""doc/static/store-delivery-chooser.png"" />
</p>

### And more to come!

The next versions that will be coming will include the following features:

* **Shops in autocomplete:**

    We plan to add the shops to the autocomplete box results for faster access.

* **And many more!**

    We will announce and integrate more features to the roadmap soon.

## Documentation

Documentation is available [here](https://github.com/Smile-SA/elasticsuite-for-retailer/wiki).

",18,18,13,6,retail,"[magento2, magento2-extension, merchandising, omnichannel, retail, store-locator]",44-45
compumatrix,portal,,https://github.com/compumatrix/portal,https://api.github.com/repos/portal/compumatrix,The Retail Distribution Network Portal for Cryptocurencies,"# portal
A Retail Distribution Network for Various Cryptocurencies
",18,18,22,0,retail,"[bitcoin, bitshares, fusion, lightning, network, retail]",44-45
databricks-industry-solutions,product-search,databricks-industry-solutions,https://github.com/databricks-industry-solutions/product-search,https://api.github.com/repos/product-search/databricks-industry-solutions,Semantic product search on Databricks,"![image](https://raw.githubusercontent.com/databricks-industry-solutions/.github/main/profile/solacc_logo_wide.png)

[![CLOUD](https://img.shields.io/badge/CLOUD-ALL-blue?logo=googlecloud&style=for-the-badge)](https://cloud.google.com/databricks)
[![POC](https://img.shields.io/badge/POC-10_days-green?style=for-the-badge)](https://databricks.com/try-databricks)

## LLM Product Search Accelerator

The purpose of this solution accelerator is to show how large language models (LLMs) and their smaller brethren can be used to enable product search.  Unlike product search used in most sites today that rely upon keyword matches, LLMs enable what is commonly referred to as a semantic search where the *conceptual similarities* in words come into play.

A model's knowledge of the *conceptual similarity* between words comes from being exposed to a wide range of documents and from those documents learning that certain words tend to have close relationships to one another.  For example, one document may discuss the importance of play for *children* and use the term *child* teaching the model that *children* and *child* have some kind of relationship.  Other documents may use these terms in similar proximity and other documents discussing the same topics may introduce the term *kid* or *kids*.  It's possible that in some documents all four terms pop-up but even if that never happens, there may be enough overlap in the words surrounding these terms that the model comes to recognize a close association between all these terms.

Many of the LLMs available from the open source community come available  as pre-trained models where these word associations have already been learned from a wide range of publicly available  information. With the knowledge these models have already accumulated, they can be used to search the descriptive text for products in a product catalog for items that seem aligned with a search term or phrase supplied by a user. Where the products featured on a site tend to use a more specific set of terms that have their own patterns of association reflecting the tone and style of the retailer or the suppliers they feature, these models can be exposed to additional data specific to the site to shape its understanding of the language being used.  This *fine-tuning* exercise can be used to tailor an off-the-shelf model to the nuances of a specific product catalog, enabling even more effective search results.

In this solution accelerator, we will show both versions of this pattern using an off-the-shelf model and one tuned to a specific body of product text. We'll then tackle the issues related to model deployment so that users can see how a semantic search capability can easily be deployed through their Databricks environment.

___
<tim.lortz@databricks.com> 
<mustafaali.sezer@databricks.com> 
<peyman@databricks.com>
<bryan.smith@databricks.com>
___


<img src='https://github.com/databricks-industry-solutions/product-search/raw/main/images/inference.png' width=800>

___

&copy; 2023 Databricks, Inc. All rights reserved. The source in this notebook is provided subject to the Databricks License [https://databricks.com/db-license-source].  All included or referenced third party libraries are subject to the licenses set forth below.

| library                                | description             | license    | source                                              |
|----------------------------------------|-------------------------|------------|-----------------------------------------------------|
|  WANDS | Wayfair product search relevance data | MIT  | https://github.com/wayfair/WANDS   |
| langchain | Building applications with LLMs through composability | MIT  |   https://pypi.org/project/langchain/ |
| chromadb | An open source embedding database |  Apache |  https://pypi.org/project/chromadb/  |
| sentence-transformers | Compute dense vector representations for sentences, paragraphs, and images | Apache 2.0 |https://pypi.org/project/sentence-transformers/ |

## Getting started

Although specific solutions can be downloaded as .dbc archives from our websites, we recommend cloning these repositories onto your databricks environment. Not only will you get access to latest code, but you will be part of a community of experts driving industry best practices and re-usable solutions, influencing our respective industries. 

<img width=""500"" alt=""add_repo"" src=""https://user-images.githubusercontent.com/4445837/177207338-65135b10-8ccc-4d17-be21-09416c861a76.png"">

To start using a solution accelerator in Databricks simply follow these steps: 

1. Clone solution accelerator repository in Databricks using [Databricks Repos](https://www.databricks.com/product/repos)
2. Attach the `RUNME` notebook to any cluster and execute the notebook via Run-All. A multi-step-job describing the accelerator pipeline will be created, and the link will be provided. The job configuration is written in the RUNME notebook in json format. 
3. Execute the multi-step-job to see how the pipeline runs. 
4. You might want to modify the samples in the solution accelerator to your need, collaborate with other users and run the code samples against your own data. To do so start by changing the Git remote of your repository  to your organization’s repository vs using our samples repository (learn more). You can now commit and push code, collaborate with other user’s via Git and follow your organization’s processes for code development.

The cost associated with running the accelerator is the user's responsibility.


## Project support 

Please note the code in this project is provided for your exploration only, and are not formally supported by Databricks with Service Level Agreements (SLAs). They are provided AS-IS and we do not make any guarantees of any kind. Please do not submit a support ticket relating to any issues arising from the use of these projects. The source in this project is provided subject to the Databricks [License](./LICENSE). All included or referenced third party libraries are subject to the licenses set forth below.

Any issues discovered through the use of this project should be filed as GitHub Issues on the Repo. They will be reviewed as time permits, but there are no formal SLAs for support. 
",16,16,1,2,retail,"[embedding-models, model-serving, rcg, retail, semantic-search, vector-store]",44-45
wardz,Diminish,,https://github.com/wardz/Diminish,https://api.github.com/repos/Diminish/wardz,[WoW] Diminishing returns tracker for players crowd controls.,"# Diminish

Diminish attaches icons to unit frames or nameplates, displaying time left until a [diminishing returns](https://wow.gamepedia.com/Diminishing_returns) category expires for that unit. As well as how far the category is diminished. *(Green border = 50% duration, yellow = 25%, red = 0%)*
  
Diminish can track DRs on both mobs and players if you enable PvE mode in the options.

**Note:** Diminish is no longer actively maintained. Feel free to fork this addon, but be wary it contains a ~9 year old codebase with lots of hacky methods added over time. Some code might rely on deprecated Blizzard functions that's marked for removal in a future patch.
(The DR spell database itself has moved over to [DRList-1.0](https://github.com/wardz/drlist-1.0) which is still updated every now and then).

## Configuration

Type `/diminish` to open the options panel. You may also skin the icons using [Masque](https://www.curseforge.com/wow/addons/masque).

DR icons for each unit frame is separately configurable. **Certain unit frames are default disabled for tracking or only enabled for certain zones.**

You can manually set icons used for a DR category by right clicking one of the checkboxes under ""Enabled Categories"" section.

## Notes

- Diminish not working with an unitframe addon? Go to options and enable ""Anchor to UIParent"", then position the icons wherever you want. For Party frames, try disabling ""Use Raid-Style Party Frames"" under WoW's interface options.
- Supports tracking max 5 party/raid frames.

## Links

Installing from source is not always guaranteed to work. You should download a packaged version here instead:

- [Curseforge Download](https://www.curseforge.com/wow/addons/diminish)
- [WoWInterface Download](https://www.wowinterface.com/downloads/info23628-DiminishDRTracker.html)
- [Github Download](https://github.com/wardz/diminish/releases/latest) (Choose binary)
- [Submit Bugs](https://github.com/wardz/diminish/issues)

## License

Copyright (C) 2023 Wardz | [MIT License](https://opensource.org/licenses/mit-license.php).
",13,13,2,2,retail,"[addon, classic, lua, retail, tbc, world-of-warcraft, wow]",44-45
VadimMustyatsa,naakcii,,https://github.com/VadimMustyatsa/naakcii,https://api.github.com/repos/naakcii/VadimMustyatsa,"Сервис экономии на закупках в торговых сетях Республики Беларусь за счёт формирования списка покупок из товаров, находящихся на акции.","**НаАкции.Бел** – это сервис экономии на закупках в торговых сетях за счёт формирования списка покупок преимущественно (или целиком) из товаров на акции (то есть тех, которые продаются торговыми сетями с каким-то из видов скидки). На текущий момент сервис представляет собой веб-приложение, с помощью которого можно сформировать такой список покупок и отправиться с ним в магазин. Однако для полноценного закупка необходима интеграция с учётными системами торговых сетей, которая запланирована на ближайшее время.

Описание самого сервиса можно найти [здесь](https://github.com/VadimMustyatsa/naakcii/wiki/Модель-сервиса-НаАкции.Бел). А описание технологического стека и инструментария [здесь](https://github.com/VadimMustyatsa/naakcii/wiki/Технологический-стек-и-инструментарий).

Структура репозитория:
* **api** – директория, в которой содержатся сервисы Spring, документация по ним доступна [через Swagger](http://178.124.206.42:8080/api/swagger-ui.html); 
* **features** – директория, в которой содержатся целевые сценарии поведения сервиса, написанные на языке Gherkin, а также вспомогательные материалы для них;
* **front** – директория, в которой содержится фронтэндовая часть приложения, написанная на Angular 4;
* **tests** – директория, в которой содержатся тесты, проверяющие приложение (на текущий момент там содержатся лишь E2E-тесты на Protractor, проверяющие реализацию целевых сценариев поведения сервиса, описанных в файлах директории features; unit-тесты для бэкэндовой части содеражтся в директории api);
* **wiki** – директория для хранения графических файлов вики проекта (на текущий момент там находятся файлы, иллюстрирующие модель сервиса).
",13,13,7,0,retail,"[angular, bdd, digital-service, economy, material-design, odd, protractor, retail, spring, startup]",44-45
MarshalOfficial,SaleSystem,,https://github.com/MarshalOfficial/SaleSystem,https://api.github.com/repos/SaleSystem/MarshalOfficial,A Powerful Sale System Backend Built on .Net Standard Library,"# SaleSystem
A Powerful multi-platform Sale System Backend Built on .Net Standard Library & SQL Express
* You can consume this sale system Backend, develop your own UI platform (Web, Desktop and mobile) and enjoy 
* Horizon : Stock and Product Management Module, Purchase and Sale Modules, CRM Module, Coupon and Discount Management, Necessary Reports, web api 

",11,11,1,0,retail,"[backend, crm, dotnetcore, net-core, netcore, pos, retail, sale, salesystem, warehouse, warehouse-management]",44-45
holonk,retail-calendar,holonk,https://github.com/holonk/retail-calendar,https://api.github.com/repos/retail-calendar/holonk,A configurable merchandising and retail calendar generator.,"# Retail Calendar

A configurable merchandising and retail calendar generator. For a given year and configuration, generates merchandising calendar along with Gregorian boundaries. It can be used for generating all calendar types, including NRF 4-5-4 calendar, 4-4-5 calendar or Gregorian calendar.

## Installation

Using npm:

```shell
 $ npm i --save retail-calendar
```

## Usage:

Instantiate RetailCalendarFactory with preferred options.

```javascript
const {
  WeekCalculation,
  WeekGrouping,
  LastDayOfWeek,
  LastMonthOfYear,
  RetailCalendarFactory,
} = require('retail-calendar')

const calendar = new RetailCalendarFactory(
  {
    weekCalculation: WeekCalculation.LastDayNearestEOM,
    weekGrouping: WeekGrouping.Group454,
    lastDayOfWeek: LastDayOfWeek.Saturday,
    lastMonthOfYear: LastMonthOfYear.January,
  },
  2017,
)
```

A retail calendar either has 52 or 53 weeks.

```javascript
calendar.numberOfWeeks
calendar.year // Given year, 2017 in this case
```

A retail calendar always has 12 months.

```javascript
calendar.months
calendar.months[0].monthOfYear // Starts from 1, up to 12
```

A retail calendar has **quarter of year** and **week of quarter**

Both Month and Week has quarter information
```javascript
calendar.months[0].quarterOfYear // from 1 to 4
calendar.weeks[0].quarterOfYear // from 1 to 4
```

The week also has week of quarter:
```javascript
calendar.weeks[0].weekOfQuarter
```

Each month consist of complete weeks. Month boundaries are always at the end of week. End of week is given in `lastDayOfWeek` option. Number of weeks for each month is defined in `weekGrouping` option.

```javascript
// Week grouping is 454 which means
calendar.months[0].numberOfWeeks // 4
calendar.months[1].numberOfWeeks // 5
calendar.months[2].numberOfWeeks // 4

calendar.months[3].numberOfWeeks // 4
calendar.months[4].numberOfWeeks // 5
calendar.months[5].numberOfWeeks // 4
// Each quarter repeats the same pattern.

// Each month has an array of weeks it contains.
calendar.months[0].weeks
```

Each month start and end boundaries expressed in Gregorian calendar.

```javascript
// Date time of beginning of month.
// In this case Feb 5, 2017 12:00 AM
calendar.months[0].gregorianStartDate
// Date time of end of month.
// In this case March 5, 2017 11:59.999 PM
calendar.months[0].gregorianEndDate
```

All weeks in a retail calendar are also directly accessible.
Each week also has boundaries available in Gregorian calendar.

```javascript
calendar.weeks // All weeks in calendar.
calendar.weeks[0].gregorianStartDate // Date
calendar.weeks[0].gregorianEndDate // Date
```

### 53 week years

Based on given configuration, a year may contain 53 weeks.

#### Restated

⚠ *previous versions of this library used the LeapYearStrategy option to configure restated behavior. This is no longer supported. All 53 week calendars are not restated* ⚠

#### Dropping Last Week

LAST week of year is ""dropped"" for 53-week years. This is the default behavior.

```javascript
// Last week of year has no month.
calendar.weeks[52].weekOfYear // 52
calendar.weeks[52].weekOfMonth // -1
calendar.weeks[52].monthOfYear // -1

// First month starts from 1st week
calendar.months[0].weeks[0].weekOfYear // 0
```
⚠ *previous versions of this library returned weekOfYear as -1, this is no longer the case. Last week's weekOfYear is 52* ⚠

#### Add to Penultimate Month

If `addLeapWeekToPenultimateMonth` is `true`,

extra week is ""added"" to the ELEVENTH month

```javascript
// AddToPenultimateMonth calendar example for 445 Calendar.
// 11th Month has 5 weeks instead of 4
calendar.months[10].weeks.length //5
```

⚠ *previous versions of this library supported `leapYearStrategy` option value `LeapYearStrategy.AddToPenultimateMonth` for the same behavior. `leapYearStrategy` is no longer supported* ⚠

### Options

#### LastDayOfWeek

Positive integer from 1 (Monday) to 7 (Sunday). Identifies on which day of week the calendar weeks end. Years, months, weeks of the retail calendar always ends on this day of week.

#### WeekCalculation

Identifies which method to use when calculating end of the retail calendar year.

See [4-4-5 Calendar](https://en.wikipedia.org/wiki/4%E2%80%934%E2%80%935_calendar) article for how both of these methods work.

- WeekCalculation.LastDayNearestEOM: Use the last end of retail week, nearest the end of last Gregorian month in the year.

- WeekCalculation.LastDayBeforeEOM: Use the last end of retail week, before the end of last Gregorian month in the year.

- WeekCalculation.LastDayBeforeEomExceptLeapYear: Use the last end of retail week, before the end of last Gregorian month in the year. If next year is leap year (has 53 weeks), make this year leap year by moving end of this year by 1 week forward.

- WeekCalculation.FirstBOWOfFirstMonth: Use the first, beginning of week day, of the start month as the start day of year.

#### LastMonthOfYear

Specifies the month the year ends. Plan year is the Gregorian year of the first month.

For example if last month is January and given year is 2017. The last month of retail calendar is January 2018
If last month is December and given year is 2017. The last month of retail calendar is December 2017.

#### WeekGrouping

Specifies how many weeks each month has in a quarter.

- `WeekGrouping.Group454`: 1st month has 4 weeks, 2nd has 5, 3rd has 4. Repeats for each quarter.
- `WeekGrouping.Group544`: 1st month has 5 weeks, 2nd has 4, 3rd has 4. Repeats for each quarter.
- `WeekGrouping.Group445`: 1st month has 4 weeks, 2nd has 4, 3rd has 5. Repeats for each quarter.

If `addLeapWeekToPenultimateMonth` is set to `true`, then the penultimate month will not abide this rule, as it will have an extra week.

#### LeapYearStrategy [Removed]

`enum`

If the year is a leap year (in the context of a retail calendar that means it has 53 weeks)

* And `LeapYearStrategy.Restated` is selected, the first week is not included in any month.
* And `LeapYearStrategy.DropLastWeek` is selected, the last week is not included in any month.
* And `LeapYearStrategy.AddToPenultimateMonth` is selected, the extra week is added to the 11th month

This option has no effect on 52 week years.

#### restated [Removed]
⚠ *This option has been superseded by [LeapYearStrategy](#LeapYearStrategy)* ⚠

`boolean`. If true, in leap years, first week is not included in any month. Otherwise, in leap years, last week is not included in any month.

Has no effect on 52 week years.
",11,11,3,5,retail,"[4-5-4-calendar, calendar, financial-calendar, gregorian-calendar, merchandise, nrf-calendar, retail]",44-45
NijatZeynalov,New-product-demand-forecasting-via-Content-based-learning-for-multi-branch-stores,,https://github.com/NijatZeynalov/New-product-demand-forecasting-via-Content-based-learning-for-multi-branch-stores,https://api.github.com/repos/New-product-demand-forecasting-via-Content-based-learning-for-multi-branch-stores/NijatZeynalov,New product demand forecasting via Content based learning for multi-branch stores: Ali and Nino Use Case,"# New product demand forecasting via Content based learning for multi-branch stores: Ali and Nino Use Case

---
## 1. Overview and motivation

Forecasting sales is a challenging task when you’re forecasting sales of a new product because you have no past performance on which to base your estimates.

Classic demand forecasting methods assume the availability of sales data for a certain historical period, which is obviously not the case when concerning a new product. Most research papers and approaches are limited either to a specific category of goods or use sophisticated marketing methods.


The proposed work is based on Ali and Nino multi-branch book store's sales data. Dataset is contains 23.345 books with over 90k unique customers per month and more than 170 orders per day. Although sales dataset was generated artifically, the approach can be apply to real cases and even small improvement in forecast accuracy can lead to significant increase of company profit and customers can buy quicker and at lower price.

The general objective of this work is to propose a forecasting method based on machine learning models to forecast the demand of new products satisfying the following conditions:

* does not depend on the type of goods;

* can work without history of sales;

* can work with large data set;

* no need for marketing research.

__and most importantly, it will help to find answers to the following questions of the company:__

__* I have never sold this product before, how will the demand be?__

__* If I order this product, how many units should I send to which branch?__


## 2. Methodology

### 2.1. Problem statement

The proposed work satisfies initial conditions because it meets all the requirements – forecasting demand without any marketing research and to work independent of the type of goods and historical data, so the goal of the work has been achieved. Community can use this model for predicting software development which could successfully work with very complex problems of predicting new good demand. This work can be used by companies’ analysts for optimizing sales assortment, planning and logistic optimization, as well as, can be useful for increasing the accuracy of other prediction systems as part of committee.

### 2.2. Data

As mentioned early, as a train data, 23.345 books have been scraped from Ali and Nino book-store website. Then, sales data has been generated based on each branch's characteristics (location, possible most sold books, the number of potential customers and etc.). Holidays, weekend-weekdays, customer purchasing power that varies based on various influences (COVID 19 etc.) taken into account and the dataset was designed accordingly.

### 2.3. Techniques

The proposed work can be divided into two main parts:

__| Content-based filtering__

The goal behind content-based filtering is to classify products, learn what the product look likes, look up those products in the database, and then recommend similar things. When the representative add new book's characteristics (name, genre, author name, collection, language, publishing year and etc.) which has never been sold in the store, the system returns 3 most similiar book.  

__|| Sales forecasting__

Given similiar books are the input of the sales forecasting algorithm which is try to determines the number of sales by predicting consumer behavior with these books from past transactions. 


## 3. Implementation

### 3.1. High-level design

![arthitecture](https://user-images.githubusercontent.com/31247506/204340965-6ca7eba7-d12d-4f8b-9b58-b8e94a056269.jpg)

### 3.2 Technical implementation

1. After the user adds new book with its features, our content filtering model comes into play. It utilizes properties and the metadata of a particular book to suggest other items with similar characteristics.  We use the cosine function to compute the similarity score between books, where each book will have a similarity score with every other book in our dataset.

Cosine similarity architecture

![Cosine similarity](https://miro.medium.com/max/1518/1*LF62aNT2XqWioSu4Beoi5Q.png)

2. When we determine most similiar three books, we use Temporal Fusion Transformer demand forecasting algorithm in order to predict future 3 month sales of these books over the all branches. It is a large model and will therefore perform much better with more data. Bu ghe biggest advantages of TFT are versatility and interpretability. In other words, the model works with multiple time series, with all sorts of inputs (even categorical variables).

Top level architecture of TFT, along with its main components

![Top level architecture of TFT, along with its main components (Source)](https://miro.medium.com/max/4800/1*7rXe_MVn5QI9oLP2vrMdvQ.webp)


3. The last step is to build the Fast API. When the user sends a book features to the uvicorn server which interacts with the API to trigger the prediction model. As a result, the model returns the result that is shown to the user in a JSON format. After creating the API, I create a Docker Image in which the app will the running.


![Top level architecture of API (Source)](https://miro.medium.com/max/1400/1*GvRd2gpkuUkg_x78N4QqaA.webp)

Fast API provides a complete dashboard for interacting with our API.

![Result](https://user-images.githubusercontent.com/31247506/208299045-b89de2ca-5047-48fc-8174-e4e702c0a6e0.png)

By looking at the API Response body section, we can see its result as follow:

```
[{""filial_ad"":""Eli ve Nino-3"",""kitab_say"":49}, {""filial_ad"":""Eli ve Nino-6"",""kitab_say"":23}, {""filial_ad"":""Eli ve Nino-8"",""kitab_say"":16}, {""filial_ad"":""Eli ve Nino-9"",""kitab_say"":9}]

```
As we expected, it shows how many units predicted to sold in the next 3 months for each branch.


Now it is time to deploy it into a Docker container. The idea behind containerization is that it will make our API portable and able to run uniformly and consistently across AWS, in a more secured way.

### 3.3. Infra

I am hosting the system on AWS ECS which is one of the best tools to easily deploy containerized applications from the docker hub registry. Everything that I used in this project is supposed to be without any additional charge, so anyone can use AWS free tier to follow along. 


### 3.4. Performance (Throughput, Latency)

The system meet the throughput and latency requirements as much as possible. Content filtering module app. takes 0.1-0.2 seconds and sales forecasting module takes 7-8 seconds for per request.


## 4. Appendix


### 4.1. References

I am adding references that I have consulted for my methodology.

[Step-by-step Approach to Build Your Machine Learning API Using Fast API](https://towardsdatascience.com/step-by-step-approach-to-build-your-machine-learning-api-using-fast-api-21bd32f2bbdb)

[Fast API, Docker and AWS ECS to Deploy ML Model](https://www.analyticsvidhya.com/blog/2022/09/fast-api-docker-and-aws-ecs-to-deploy-machine-learning-model/)

[Time Series Made Easy in Python](https://unit8co.github.io/darts/)

[Content-Based Recommendation System](https://studymachinelearning.com/content-based-recommendation-system/)

[How to forecast sales of a new product](https://www.bdc.ca/en/articles-tools/marketing-sales-export/sales/forecasting-sales-of-new-products)
",10,10,1,1,retail,"[aws-ecs, book-store-app, content-based-recommendation, cosine-similarity, darts, docker-image, fastapi, product-demand-forecast, retail, temporal-fusion-transformer, time-series]",44-45
wardz,DRList-1.0,,https://github.com/wardz/DRList-1.0,https://api.github.com/repos/DRList-1.0/wardz,[WoW] Library for crowd control diminishing returns categorization data.,"# DRList-1.0 (Diminishing Returns Database)

World of Warcraft library for providing player diminishing returns categorization.

## Contents

- [About](#about)
- [Install Manually](#manual-install)
- [Install With BigWigsMods Packager](#usage-with-bigwigsmods-packager)
- [Upgrading From DRData to DRList](#upgrading-from-drdata-to-drlist)
- [Example usage for Retail/TBC/Wotlk](https://github.com/wardz/DRList-1.0/wiki/Example-Usage-Retail-&-TBC)
- [Example usage for Classic](https://github.com/wardz/DRList-1.0/wiki/Example-Usage-Classic)
- [API Documentation](https://wardz.github.io/DRList-1.0/)
- [List of DR categories](https://github.com/wardz/DRList-1.0/wiki/DR-Categories)

### About

Library that contains (hopefully) the most up to date [diminishing returns](https://wow.gamepedia.com/Diminishing_returns) categorization. This is purely the diminishing return data itself with API's to determine if a spellID has a diminishing return, if it diminishes in PvE and the category it diminishes in. You will have to keep track of actual DR timers yourself.

**This addon is a rewrite of [DRData-1.0](https://www.wowace.com/projects/drdata-1-0) by Adirelle which is no longer maintained.**
DRList is updated to seamlessly support all World of Warcraft live clients. (Classic, TBC, Wotlk, Mainline)

___

### Manual Install

Requires [LibStub](https://www.curseforge.com/wow/addons/libstub).

- [Curseforge Downloads](https://wow.curseforge.com/projects/drlist-1-0)
- [Github Downloads](https://github.com/wardz/DRList-1.0/releases)

1. Unzip file into `WoW/Interface/AddOns/YourAddon/Libs/`.
2. Add an entry for `Libs/DRList-1.0/DRList-1.0.xml` into your addon's [TOC](https://wowpedia.fandom.com/wiki/TOC_format) file.

### Usage with BigWigsMods Packager

Requires [LibStub](https://www.curseforge.com/wow/addons/libstub).

1. Add an entry for `Libs/DRList-1.0/DRList-1.0.xml` into your addon's [TOC](https://wowpedia.fandom.com/wiki/TOC_format) file.
2. Add this repository to the packager's externals list. Preferably with the 'latest' tag to avoid using master branch, but both works aslong as you load the xml file.

_**.pkgmeta file:**_

```yaml
externals:
  Libs/DRList-1.0:
    url: https://github.com/wardz/DRList-1.0
    tag: latest
```

### Upgrading from DRData to DRList

- Any occurances of `DRData` must be renamed to `DRList`. Easiest is to just change the LibStub call so your DRData variable redirects to DRList.
- There's quite a few new DR categories added. Depending on how your addon is coded you might need to account for this. ([Category list](https://github.com/wardz/DRList-1.0/wiki/DR-Categories))
- For accessing raw data tables you will now need to add the current expansion as an extra table property.
  E.g `DRData.categoryNames` to `DRList.categoryNames.retail` or `DRList.categoryNames.classic`. The only exception for this is
  the spell list table.
- Calls to `IterateProviders` must be replaced with [IterateSpellsByCategory](https://github.com/wardz/DRList-1.0/blob/620a36fc1ccbfb399ead1b874b9a0fc648113b9c/DRList-1.0/DRList-1.0.lua#L347-L356). **Providers are obsolete.**
- For Classic Era (vanilla) you need to mostly use spell names instead of spell IDs. See [here](https://wardz.github.io/DRList-1.0/#Lib:GetCategoryBySpellID) for more details.

### Contributing

- [Submit a pull request.](https://github.com/wardz/DRList-1.0/pulls)
  I recommend creating a symlink between your WoW addons folder and DRList-1.0.
- [Report bugs or missing spells.](https://github.com/wardz/drlist-1.0/issues)
- [Help translate.](https://www.curseforge.com/wow/addons/drlist-1-0/localization)

### License

Copyright (C) 2023 Wardz | [MIT License](https://opensource.org/licenses/mit-license.php).
",10,10,3,0,retail,"[addon, classic, library, lua, retail, world-of-warcraft]",44-45
kerberos-io,heatmap,kerberos-io,https://github.com/kerberos-io/heatmap,https://api.github.com/repos/heatmap/kerberos-io,Creating a heatmap based on video recordings ,"# Heatmap

This repository implements the concept of a heatmap for video recordings, which highlights the occurences of objects of interests that are moving in a static scene. A scene can be a sportfield, a public place, a shop, a factory or any physical place in this world. Objects can be persons, cars, boxes in a factory, players on a field, or anything else that create differences in a static scene.

<img width=""851"" alt=""Screenshot 2022-01-25 at 22 04 12"" src=""https://user-images.githubusercontent.com/1546779/151059451-1ac20282-592e-40f0-8f8f-96346cceae5c.png"">

> A heatmap is a graphical representation of data that uses a system of color-coding to represent different values. 

Heatmaps are produced by processing one or more recordings of a static scene, and tracking one or more object of interests. As illustrated before, scenes and objects are of your own interest and will vary from usecase to usecase.

## The implementation

Heatmapping is a post-process, which means that before you can create any heatmap you will need a pipeline of recordings or images. Heatmaps are generated over time and can bring huge value, they provide insights of a specific scene at a specific time (a retail store is more crowdy when having discounts).

Creating this pipeline or continuous flow of data can be challenging to develop. You might need to scale over multiple scenes (stores, fields, etc) and multiple cameras in a scene can be installed. To solve this, and this goes beyond this repository, we will use the [Kerberos Enterprise Suite stack](https://doc.kerberos.io/enterprise/first-things-first/). This stack will allow us to easily onboard and scale video cameras, and attach them to a video pipeline, so we will get new recordings served as soon they become available. Bottom line Kerberos Enterprise will help us to focus on our usecase and not the complexity of scale and onboarding IP cameras.

Easily explained, this is what the project is doing.

1. A Kafka broker connection of [Kerberos Enterprise](https://doc.kerberos.io/enterprise/first-things-first/) is established, and new recordings are received.
2. The relevant recording is downloaded from [Kerberos Vault](https://doc.kerberos.io/vault/first-things-first/) and stored in memory 
3. The recording is opened and gets read frame by frame.
4. A detector for example MobileNetSSD recognizes a human, car, or other objects. 
5. BoundingBoxes get placed around the object of interest. 
6. Non max surpression merges overlapping bbox into one.
7. The coordinates of the bbox get saved and converted using a homography transformation. 

## Dependencies

| Packages|
|--------|
| python 3.8 |
| OpenCV |
| datetime |
| numpy |
| collections - Defaultdict |
| pandas |
| imutils |

## Examples

This repository has been executed and tested in different environments

### Sports 

Using video recordings from a paddel playingfield we are trying to map the locations players have walked. First we capture there movement and later on we convert the coordinates and plot them onto an image of the playingfield. This will give us insight in where people mostly walk and what areas are left unattended.

![Tracking objects in OpenCV and MobileNet](media/tracker.png)

![Padelfield 2d map](media/paddelfield_2d_points.jpeg)

### Retail shop

To be completed
",9,9,4,1,retail,"[analytics, heatmap, mobilenet, opencv, padel, python, retail, tensorflow, videoanalytics]",44-45
cointastical,Physical-Locations-Bitcoin,,https://github.com/cointastical/Physical-Locations-Bitcoin,https://api.github.com/repos/Physical-Locations-Bitcoin/cointastical,Physical Stores where you can Buy or Sell bitcoin,"# Physical-Locations-Bitcoin
Physical Stores where you can Buy or Sell bitcoin

There are a number of foreign currency exchanges and other places where there is a ""storefront""/branch and an actual teller or other staff where you can just walk up and do a bitcoin buy and/or sell.

Some of the following stores require KYC / Identity verification. When we learn which do require KYC, and which do not (i.e., [**NO KYC**]), we will update this list accordingly.

[Updated: 14 Jan, 2023. Note — there are some countries, such as Turkey, Ukraine, and others where there are numerous additional locations but we have no business names or location information on them.]

**Physical stores and Trading Spaces**
--------------------------------------

**Europe, Middle East, and Africa:**

- BitKonan's [Bitcoin Store](https://www.bitcoinpit.de/bitcoin-store) (💵) Split & Zagreb, Croatia
- [House of Nakamoto](https://www.thehouseofnakamoto.com/en/standort) (💵) Vienna, Austria
- [bitcoin.wien](https://www.bitcoin.wien/contact/us/) (💵) Vienna, Austria [Previously BitBuy.at]
- [ComproEuros](https://comproeuro.it/) (💵) Roverto (TN), Bologna and Pordenone, Italy
- [BitBase](https://bitbase.es/tiendas-bitcoin) (💵) Spain, and Portugal 
- [Le Comptoir des Cybermonnaies](https://www.lecomptoirdescybermonnaies.fr/acheter-vendre-cryptomonnaies-comptoir) Bordeaux, France
- [CoinsFera](https://www.coinsfera.com/) (💵) [Istanbul](https://www.coinsfera.com/en/branches?branch=istanbul), Turkey, and [Dubai](https://www.coinsfera.com/en/branches?branch=dubai), UAE
- [Sirius Coin](https://www.siriuscoin.com/) (💵) Istanbul, Turkey
- [Cointral](https://cointral.com/our-branches) (💵) Istanbul, Turkey, and Dubai, UAE
- [AltcoinTurk Trader Base](https://np.reddit.com/r/Altcointurk/comments/c033pd) (💵) Istanbul, Turkey
- [NakitCoins](https://nakitcoins.com/) (💵) Istanbul, Turkey and [partner locations](https://nakitcoins.com/partners-locator) in Turkey
- [Wistrades](https://twitter.com/FxUndisputed) (💵) Nairobi, Kenya
- [Digital Money LTD](https://dmexchange.com/) London, UK, Kampala, Uganda, and (soon) Nairobi, Kenya

**North America:**

- [Luxolo](https://luxolo.io/) (💵) Portland, ME
- [Cryptospace](https://www.cryptospaceus.com/otc) (💵) San Pedro, CA
- [Bitcoin Exchange](https://twitter.com/tweetest1423/status/14436116073685647360) (💵) San Diego (Pacific Beach), CA
- [Maine Bitcoin LLC](https://maine-bitcoin.com/office-lewiston) (💵) Lewiston, ME [Appointment required for face-to-face trade]
- [BitLiquid](https://bit-liquid.com/contact#33eb8977-7de6-41b4-a017-4ec6a198f98b) (💵) St. Louis Park, MN & Las Vegas, NV
- [CoinhubATM](https://coinhubatm.com/contact-us) (💵) Santa Monica, CA & Las Vegas, NV [Appointment required due to COVID measure]
- [Crypto Plug](https://www.cryptopluginglewood.com/) (💵) Inglewood, CA [Trading Space]
- [Yap.cx](https://yap.cx/) (💵) Montreal, Quebec, Canada
- [Coin Nerds](https://coinnerds.ca/) (💵) Mississauga, Ontario, Canada
- [The Coin Shack](https://thecoinshack.ca/) (💵) Toronto, Ontario, Canada

**Asia-Pacific:**

- [Bitcoin Dealers](https://bitcoindealers.com.au/buy-bitcoins.html) (💵) Melbourne, Adelaide, & Sydney, Australia

<hr />

Vouchers (retail)
-----------------

- [Azte.co](https://azte.co/#find_a_vendor) ([⚡](https://lightningnetworkstores.com/wallets), 💵) U.S., Canada, Mexico, Guatemala, Jamaica, UK, Norway, Belgium, Spain, Malaysia, Namibia, Botswana, South Africa, Nigeria, & Uganda (and more coming)
- [FastBitcoins](https://fastbitcoins.com/#locations) ([⚡](https://lightningnetworkstores.com/wallets), 💵) Canada, UK, Australia, Greece, Estonia, Latvia, Kenya, Uganda, Ghana, Zambia, and Namibia
- [LibertyX](https://libertyx.com/) (💵) U.S. [Locations with a cashier accept cash. Locations with a Kiosk accept debit card.]
- [Rise Wallet](https://www.risewallet.com/locations) (💵) Canada
- [Flexepin](https://www.flexepin.com/sales_outlet_finder) (Debit card or 💵) Canada ([CoinCurve](https://coincurve.com/), [MyBTC.ca](https://mybtc.ca/buy-bitcoin-with-flexepin-canada), [Canadian Bitcoins](https://www.canadianbitcoins.com/), [LocalCoinATM](https://localcoinatm.com/flexepin-v3/#flexepin-form), [FastBitcoins](https://fastbitcoins.com/voucher) & [Crypto Voucher](https://cryptovoucher.io/redeem-now)), United Kingdom ([FastBitcoins](https://fastbitcoins.com/voucher)), Australia ([Coinloft](https://www.coinloft.com.au/buy/flexepin), [FastBitcoins](https://fastbitcoins.com/voucher) & [Crypto Voucher](https://cryptovoucher.io/redeem-now)), Greece ([Crypto Voucher](https://cryptovoucher.io/redeem-now)), Cyprus ([Crypto Voucher](https://cryptovoucher.io/redeem-now)), Estonia ([FastBitcoins](https://fastbitcoins.com/voucher)), Latvia ([FastBitcoins](https://fastbitcoins.com/voucher)), Romania ([Crypto Voucher](https://cryptovoucher.io/redeem-now)), Kenya ([FastBitcoins](https://fastbitcoins.com/voucher)), Uganda, ([FastBitcoins](https://fastbitcoins.com/voucher)), Ghana ([FastBitcoins](https://fastbitcoins.com/voucher)), Zambia ([FastBitcoins](https://fastbitcoins.com/voucher)), & Namibia ([FastBitcoins](https://fastbitcoins.com/voucher))
- [Neosurf](https://www.neosurf.com/en_GB/application/findcard) (💵) Canada ([BitIt](https://bitit.io/))
- [BlueShyft](https://coinloft-locations.blueshyft.com.au/) (💵) Newsagents across Australia ([Coinloft](https://coinloft.com.au/buy/blueonline))
- [AusPost](https://www.coindesk.com/australia-post-now-lets-customers-buy-bitcoin-at-over-3500-outlets) (💵) Australia
- [Yellow Card](https://www.yellowcard.io/locations) (💵) Nigeria
- [Tikebit](https://tikebit.com/map#marker=null&panel=false&lat=40.19146303804063&lng=-4.696655273437501&zoom=7) (💵) Portugal & Spain
- [BitIns](https://www.bitins.net/#map-module) (💵) Slovenia
- [BitNovo](https://www.bitnovo.com/bitcoin-selling-point-en) (💵) Spain, France, Italy, & Portugal

Gift Cards
----------

- Visa, Mastercard, American Express and Discover U.S. (Redeem through [CardCoins](https://www.cardcoins.co/)) (Non-reloadable prepaid gift cards only)
- [Crypto Voucher](https://cryptovoucher.io/#giftCard)
- Other Gift cards are bought at retail locations and used to buy bitcoin using [P2P Trading Exchanges](https://medium.com/@cointastical/p2p-otc-exchanges-e-g-localbitcoins-bisq-hodlhodl-etc-20f293a2c72e)

Vending
-------

- Bitcoin ATMs: [Coin ATM Radar](https://coinatmradar.com/) (💵) & [coinATMmap](https://coinatmmap.com/) (💵) (Some bitcoin ATMs are ""two-way"", meaning they also dispense cash)
- [Coinstar](https://www.coinstar.com/bitcoin) (💵) (with [2,100+ locations in the U.S.](https://coinme.com/kiosks))
- [SBB/Swiss Railway ticketing](https://www.sbb.ch/en/station-services/services/further-services/ticket-machine-services/bitcoin.html) [Switzerland]

Hardware wallet sales locations
-------------------------------

Note: It's always more secure to buy direct from the manufacturer

- [BitInsight](http://bitinsighthk.com/what.html) Hong Kong (💵)

Over-the-counter (OTC)
----------------------

Some traders may have a fixed location, but are not considered to be ""physical stores"":

- [Person-to-person (P2P) Trading Platforms](https://cointastical.github.io/P2P-Trading-Exchanges)
- Satoshi Square Trading Events / In-Person Trading (e.g., [Satoshi Square L.A.](https://spelunk.in/2021/09/21/september-satoshi-square/), and find a person to trade with at many [Bitcoin conferences](https://www.coindesk.com/events/) and [Bitcoin meetups](https://www.google.com/maps/d/viewer?mid=1rbqiHELgkGta0QLG4TB0toHEdJdOfCRK&ll=41.52428047956433%2C-52.979125950000025&z=3))
- OTC Trading Desks (e.g., Cumberland, SigOne Capital, and [several dozen others](https://medium.com/@cointastical/bitcoin-crypto-otc-trading-desks-7f77276c6dc))

Buy Online
----------------------

This document was specifically created for methods of buying, in-person, however there are numerous methods for buying bitcoin online.  The following is a subset:

- [Dollar Cost Averaging - Exchanges](https://cointastical.medium.com/dollar-cost-averaging-the-answer-to-the-question-is-now-a-good-time-to-buy-bitcoin-a84e518f50f0)
- [Exchanges with support for Lightning Network](https://cointastical.github.io/Exchanges-With-LN)
- [Person-to-Person Trading Platforms](https://cointastical.github.io/P2P-Trading-Exchanges)

[**NO KYC**] Online Exchanges:

- One [Azte.co](https://azte.co/vendors.html) vendor offers the [@BitcoinVoucherBot](https://t.me/BitcoinVoucherBot) on Telegram, for purchase using SEPA/EUR bank transfer (and [@BitcoinVoucherGroup](https://t.me/BitcoinVoucherGroup) for support).
- [lnp2pBot on Telegram](https://t.me/lnp2pBot) buy/sell P2P through LN. Mostly LATAM but also in EU/US.
- [Ramp.Network](https://buy.ramp.network) <-- Paying with bank transfer (GBP, or EUR/SEPA), or Revolut
- [Relai](https://relai.ch) <-- [Bitcoin Only](https://bitcoin-only.com/get-bitcoin), EU, and CH
- [no-KYC only](https://bitcoinqna.github.io/noKYConly) Guide
- [KYCNot.me](https://kycnot.me) Directory

<hr />

Many of the above entries were discovered through online media posts, [like this one](https://www.wsj.com/articles/walk-in-cryptocurrency-exchanges-emerge-amid-bitcoin-boom-11633107697), and social media posts, [like this one](https://twitter.com/parisforpres/status/1174324943850524672), and [this one](https://twitter.com/FxUndisputed/status/1359338797687840772).

Additions, corrections, and other feedback welcome and can be submitted as an [issue or pull request on GitHub](https://github.com/cointastical/Physical-Locations-Bitcoin), or via [e-mail](mailto://cointastical@gmail.com).

[Note: There is also a [corresponding post on Medium](https://cointastical.medium.com/physical-stores-where-you-can-buy-or-sell-bitcoin-9a28686fb625) with this information as well.]
",8,8,3,0,retail,"[bitcoin-exchanges, bitcoinexchanges, physical-locations, retail, retail-locations, shops, stores]",44-45
brotherzhafif,SMERB,,https://github.com/brotherzhafif/SMERB,https://api.github.com/repos/SMERB/brotherzhafif,"Website For SMERB - Retail Business Manager, A Multiplatform Point of Sales Application Using Flutter","# SMERB - Retail Business Manager

## 📝 Description
- Point Of Sales Application For Retail Business
- Easy to Use for Business Beginner or Business Expert
- Multiplatfrom and Flexible for Every Business Scale and Types

## ⏳ SMERB Architecture And Features 
### Build In Business Cashier App
- Barcode Scanner
- Easy To Navigate And Use
- Print Payment Notes Using Bluetooth
### Advanced And Simple Data Presentation
- Search Item Based on Names or Even Price
- Automatically Change And Adjust Data Everytimes Something Is Changed
- Item Sorting In Every Form Such as Item Types, Sales Dates or Popularity
### Business Data Management
- Auto Backup Database to Avoid Database Errors 
- Multiplatform DataSharing (Export Import And Merge Database)
- Local Filebased Database With Excel and Sqlite (No Connection)
- When Restocking Items, There Will be a Warning if The Price is Too Low
### Business Transaction History, Analytics And Sales Graph 
- Show All Transaction In The Business and The Detail
- Showing The Most Popular and Profitable item or The Otherwise
- Showing How Much Items Has Been Sale, The Profit, Comparisons and Etc
- Business Sales Graph with Variety of Graph Model and Can Exported as Picture
- Date Range, Types And Item Names Detailed Analytics And Graph Data Can Be Looked 
### Other Small Features
- User Can Add Their Own Language (Default Is English And Indonesia)
- Showing App Versions, Changelog and Checking for Updates
- Modifiable Theme Color of The Application 
- The Payment Notes Data Can Be Modified

## 🖥️ Supported Platform
- Windows 7 or Higher
- Android
- Linux
- macOS 
- iOS
",7,7,1,0,retail,"[business, flutter, multiplatform, retail]",44-45
SAP-archive,s4hana-retail-store-plugin-zebra-barcode,SAP-archive,https://github.com/SAP-archive/s4hana-retail-store-plugin-zebra-barcode,https://api.github.com/repos/s4hana-retail-store-plugin-zebra-barcode/SAP-archive,SAP S/4HANA retail store plug-in for barcode scanner integration.,"<!--
SPDX-FileCopyrightText: 2020 SAP SE or an SAP affiliate company and s4hana-retail-store-plugin-zebra-barcode contributors

SPDX-License-Identifier: Apache-2.0
-->

# SAP S/4HANA Retail Store Plug-in for Zebra Barcode Scanner Integration

[![REUSE status](https://api.reuse.software/badge/github.com/SAP-samples/s4hana-retail-store-plugin-zebra-barcode)](https://api.reuse.software/info/github.com/SAP-samples/s4hana-retail-store-plugin-zebra-barcode)

This repository contains a sample implementation of a Cordova Plugin to enable the on-device laser scanner as an input device for the [In-Store Merchandise and Inventory Management Fiori Apps](https://help.sap.com/viewer/9905622a5c1f49ba84e9076fc83a9c2c/latest/en-US/4018b657ace85b3be10000000a4450e5.html).

## Description

This sample code can be used to integrate the barcode scanner of a Zebra TC75x as an input device for the **In-Store Merchandise and Inventory Management Fiori Apps** when used as a plugin for the **Custom SAP Fiori Client**.

### Barcode Scanner

Store associates use barcode scanners to scan the global trade item numbers (GTIN) of products that are tagged with barcodes. This enables store associates to manually identify products in a store, for example, to order the scanned products.

## Requirements

- At least Android 7.x running on your Zebra device
- A Zebra TC75x mobile device
- An SAP S/4 HANA with the **In-Store Merchandise and Inventory Management Fiori Apps** up and running

### Third-Party Dependencies

In addition to the sample code provided here, the **Zebra Enterprise Mobility Development Kit (EMDK)** is required and will be automatically downloaded during the build process of your Custom SAP Fiori Client. Visit the [Product Information from Zebra](https://www.zebra.com/us/en/products/software/mobile-computers/mobile-app-utilities/emdk-for-android.html) (Link to external website) for further information about it.

**Please read and comply with the [Zebra EMDK End User License Agreement](https://techdocs.zebra.com/emdk-for-android/EULA/) before using this plugin.**

## Download and Installation

Please follow the official documentation on the [SAP Help Portal](https://help.sap.com/viewer/e2ed9b4f3edb4391a7a89b1af84d9606/latest/en-US/fc001ea645814b6d986669da2879ab58.html) for information on how to build a Custom SAP Fiori Client. Before starting your build by executing `cordova build <your platform>`, you can add the plugin to your Custom SAP Fiori Client:

```cordova plugin add git+https://github.com/SAP-samples/s4hana-retail-store-plugin-zebra-barcode.git```

After that, please proceed with the build process.

## Limitations

*This sample code was tested on a Zebra TC75x running Android 7 and may or may not support other versions of Zebra hardware or Android.*

## How to obtain support

If you have any issues with this sample, please open a report using [GitHub issues](https://github.com/SAP-samples/s4hana-retail-store-plugin-zebra-barcode/issues). Please note that this project is provided ""as-is"" without any official support either explicit or implied. We will try to answer your questions but there are no guarantees regarding the response time, future features or bugfixes.

## License 

Copyright (c) 2019 SAP SE or an SAP affiliate company. All rights reserved. This project is licensed under the Apache Software License, version 2.0 except as noted otherwise in the [LICENSE](LICENSES/Apache-2.0.txt) file.
",6,6,4,3,retail,"[android, cordova, mobile, retail, sample, sample-code, sap-s4hana, zebra-device]",44-45
pbiecek,xai_stories_2,,https://github.com/pbiecek/xai_stories_2,https://api.github.com/repos/xai_stories_2/pbiecek,XAI Stories 2.0. eXplainable Artificial Intelligence for Retail Analytics - case studies,"# XAI Stories 2.0. eXplainable Artificial Intelligence for Retail Analytics

ebook: https://pbiecek.github.io/xai_stories_2/

In 2020, as part of the Interpretable Machine Learning course, students created XAI Stories, an ebook that collects the experiences of the subjects covered in the form of a series of chapters on different applications of XAI techniques.

This was a great idea. Each team developed an interesting solution and then described it in a clear and interesting way. Some of these results were later presented at relevant industry conferences.

This year we are continuing this experiment but focusing on applications in one sector - retail analytics. In cooperation with students from the universities of Warsaw and Lodz, as well as partners from McKinsey and Shumee, this ebook has been created - presenting various ideas and applications on how to use predictive modelling in retail, but also how to enrich these solutions with XAI.

I hope that the presented solutions will trigger development of new interesting solutions implementing explainable machine learning in the retail industry.

## How this book came about

This book is the result of a student projects for [Interpretable Machine Learning](https://github.com/pbiecek/InterpretableMachineLearning2021) course at University of Warsaw and University of Łódź. Each team has prepared one case study for selected XAI technique.

This project is inspired by a fantastic book [Limitations of Interpretable Machine Learning Methods](https://compstat-lmu.github.io/iml_methods_limitations/) done at the Department of Statistics, LMU Munich.
We used the LIML project as the cornerstone for this repository.

## How to build the book

Step 1: Clone or download the repository https://github.com/pbiecek/xai_stories_2.

Step 2: Install dependencies

```
devtools::install_dev_deps()
```

Step 3: Render the book (R commands)

```{r}
bookdown::render_book('./', 'bookdown::gitbook')
```

",6,6,3,0,retail,"[analytics, iml, retail, xai]",44-45
marcotav,machine-learning-regression-models,,https://github.com/marcotav/machine-learning-regression-models,https://api.github.com/repos/machine-learning-regression-models/marcotav,This repository contains only projects using regression analysis techniques. Examples include a comprehensive analysis of retail store expansion strategies using Lasso and Ridge regressions.,"## Machine Learning Regression Models
![Image title](https://img.shields.io/badge/sklearn-0.19.1-orange.svg) ![Image title](https://img.shields.io/badge/seaborn-v0.8.1-yellow.svg) ![Image title](https://img.shields.io/badge/pandas-0.22.0-red.svg) ![Image title](https://img.shields.io/badge/matplotlib-v2.1.2-orange.svg)

<br/>
<p align=""center"">
  <img src='https://github.com/marcotav/machine-learning-regression-models/blob/master/retail/images/liquor.jpeg' width=""200"">
</p>
<br>

<p align=""center"">
  <a href=""#nb""> Notebooks and descriptions </a>  •
  <a href=""#ci""> Contact Information </a> 
</p>

<a id = 'nb'></a>
### Notebooks and descriptions
| Notebook | Brief Description |
|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [retail-store-expansion-analysis-with-lasso-and-ridge-regressions](http://nbviewer.jupyter.org/github/marcotav/deep-learning/blob/master/painters-identification/notebooks/capstone-models-final-model-building.ipynb) | Based on a dataset containing the spirits purchase information of Iowa Class E liquor licensees by product and date of purchase this project provides recommendations on where to open new stores in the state of Iowa. To devise an expansion strategy, I first needed to understand the data and for that I conducted a thorough exploratory data analysis (EDA). With the data in hand I built multivariate regression models of total sales by county, using both Lasso and Ridge regularization, and based on these models, I made recommendations about new locations.|
| [conjoint-analysis](http://nbviewer.jupyter.org/github/marcotav/deep-learning/blob/master/bitcoin/notebooks/deep-learning-LSTM-bitcoins.ipynb) | Conjoint analysis is a technique that allows researchers to predict consumers' choice share. The analysis can be programmed using standard question types, such as the MaxDiff variation of the Matrix Table question. Instead of directly asking the survey respondents which attributes they find most relevant, conjoint analysis asks respondents to evaluate potential product profiles which include multiple product features 

There are several ways to show to respondents the product profiles. In Choice-Based Conjoint (CBC) respondents are shown multiple product conceptsn and asked which option they would choose. By varying the features shown to the respondents and observing their responses to the product profiles, one can statistically deduce the most desired product features and which attributes have the most impact on choice. The end result is a set of preference scores or *part-worth utilities* for each level of each attribute. In this notebook I show how to use Python to calculate the utilities. The notebook is heavily based on [this course](https://www.lynda.com/R-tutorials/Data-Science-Marketing/533306-2.html) and [this book](https://www.amazon.com/Marketing-Data-Science-Techniques-Predictive/dp/0133886557).|

<a id = 'ci'></a>
## Contact Information

Feel free to contact me:

* Email: [marcotav65@gmail.com](mailto:marcotav65@gmail.com)
* GitHub: [marcotav](https://github.com/marcotav)
* LinkedIn: [marco-tavora](https://www.linkedin.com/in/marco-tavora)
* Website: [marcotavora.me](http://www.marcotavora.me)




",6,6,1,0,retail,"[glm, lasso-regression, machine-learning, regression-analysis, regression-models, retail, ridge-regression]",44-45
aws-samples,computer-vision-retail-workshop,aws-samples,https://github.com/aws-samples/computer-vision-retail-workshop,https://api.github.com/repos/computer-vision-retail-workshop/aws-samples,Computer vision for retail inventory workshop,"## Computer vision for retail inventory workshop

This repository accompanies the [Computer vision in retail inventory](https://catalog.workshops.aws/cv-retail) workshop. The workshop takes you through the steps required to build a basic computer vision model which can detect a set of 10 common supermarket products. Then you will deploy your model and integrate it with a web application which counts the number of products on a shelf. Finally, you can challenge yourself to build a better model, or expand the model to solve additional use cases. 

Please follow the setup instructions in the workshop documentation before using this repository, which contains the Jupyter notebooks required to train and deploy the models.

## Security

See [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.

## License Summary

The dataset is made available under the Creative Commons Attribution-ShareAlike 4.0 International License. See the LICENSE file.

The sample code within this documentation is made available under the MIT-0 license. See the LICENSE-SAMPLECODE file.
",5,5,2,0,retail,"[computervision, retail, sagemaker, workshop]",44-45
imshibaji,StoreKeeper,,https://github.com/imshibaji/StoreKeeper,https://api.github.com/repos/StoreKeeper/imshibaji,"Retail Shop Management with Purchase and Sales, Stock Management, Barcode Generate and Reading, Printing/ Download Features Enabled. Also Here Adding GST Features Available and Bill Printing Options Are available. Powered By Medust Technology Pvt. Ltd. ","# StoreKeeper
Shoping Script

## Setup Procidure
First Required PHP Environment Globally
https://www.php.net/downloads.php

Setup MySql Server
https://dev.mysql.com/downloads/

Composer Tools
https://getcomposer.org/download/

After Installation. Start MySql Server
Then Write commands
```
composer install
php artisan migrate --seed
```

After database migration then login with
```
username: admin
password: passoword
```

After Login, Open Browser URL Link with 
http://localhost:8000/setings/create


That It. Software will be run.
",5,5,2,4,retail,"[bill, billing, billing-application, bills, gst-calculator, gstin, retail, retail-shop, shop, shopping, shopstyle, shopware]",44-45
navassherif98,POS_Using_Excel,,https://github.com/navassherif98/POS_Using_Excel,https://api.github.com/repos/POS_Using_Excel/navassherif98,This repository contains a POS(Point-Of-Sale) System for a Retail shop using Excel,"# POS_Using_Excel

### POS(Point-Of-Sale) System for a Retail shop using Excel with Macros


![ezgif com-gif-maker(1)](https://user-images.githubusercontent.com/55757415/115178920-db314800-a0ef-11eb-87bf-b0c77dbbc6e2.gif)



#### You can create Folders in this format for saving the datas :


```
.
├───2020
│   ├───JULY
│   │   └───Invoice
│   ├───JUNE
│   │   └───Invoice
|   └───.....
├───4u Invoice
│   ├───6 JUNE_20
│   ├───7 JULY_20
|   └───.....
├───4u MonthlyReport
├───4u Report
│   ├───06 JUNE_20
│   ├───07 JULY_20
|   └───.....
└───assets
```
",5,5,1,0,retail,"[application, billing-application, billing-invoicing, chart, excel, final, final-project, final-year-project, financial-analysis, graph, invoice, macros, point-of-sale, pos, project, retail, retail-systems, vba, vba-excel, vba-macros]",44-45
loydcose,stylewise,,https://github.com/loydcose/stylewise,https://api.github.com/repos/stylewise/loydcose,A fullstack e-commerce web application for selling apparel built with server-side rendering (SSR).,"# Stylewise

Welcome to Stylewise, a fullstack e-commerce web application for selling apparel built with server-side rendering (SSR).

## Homepage preview

![Homepage Preview](https://i.ibb.co/kqBJSHq/homepage.png)

## Technologies Used

- NextJS
- TailwindCSS
- MongoDB

## Getting Started

These instructions will get you a copy of the project up and running on your local machine for development and testing purposes.

### Installing

```bash
git clone https://github.com/loydcose/stylewise.git
cd stylewise
npm install
npm run dev
```

The application will now be running on http://localhost:3000.

## Features

- Browse and purchase a wide selection of apparel
- Add items to your shopping cart and complete checkout
- Responsive design for optimal viewing on desktop and mobile devices

## Contributing

We welcome contributions to Stylewise! If you have an idea for an improvement or a fix, please open an issue or create a pull request.

## License

This project is licensed under the MIT License - see the LICENSE file for details.
",5,5,1,0,retail,"[apparel, ecommerce, nextjs, products, retail, store, tailwindcss]",44-45
VadimGrigorev,project-free-Russian-retail,,https://github.com/VadimGrigorev/project-free-Russian-retail,https://api.github.com/repos/project-free-Russian-retail/VadimGrigorev,"Бесплатный проект Розница на SunFlurry (ЕГАИС, ФФД, ККТ (Атол, Штрих и др.), остатки, анализы, прибыльность, взаиморасчеты, бух. учет и т.д.)",,5,5,2,1,retail,"[egais, free, retail]",44-45
MagePsycho,Magento2-Store-Restriction-Pro,,https://github.com/MagePsycho/Magento2-Store-Restriction-Pro,https://api.github.com/repos/Magento2-Store-Restriction-Pro/MagePsycho,"Magento 2 Store Restriction Pro (Restrict Access, Require Customer Approval, Disable Registration, etc.)","## Overview:
Magento 2 Store Restriction Pro extension provides the complete restriction functionality for your store in many ways (disabling registration, requiring customer approval, restricting access to special customers while allowing guess access to certain pages)

![Magento 2 Store Restriction Backend Demo](http://g.recordit.co/Xr3dYtofnH.gif)

## Features:
### 1. General

* Upgrade Proof Module(purely event-observer based, no any preferences/rewrites).
* Compatible with Magento CE 2.X & EE 2.X.
* Option to enable/disable the functionality as per store.
* Compatible with Magento 2 Custom Redirect Pro Extension.
* Essential Module for B2B/B2C, private sale, member only, invite only stores.

### 2. Customer Registration
* Provides option to disable the customer registration (with an optional custom message).
* Includes Magento 2 Customer Group Selector / Switcher extension for FREE, which allows customer group selection/automatic group switching from the registration page.
![Magento 2 Store Restriction Customer Registration](https://www.magepsycho.com/media/catalog/product/3/-/3-m2-backend-registration-settings-disabled.png)

### 3. Customer Activation/Approval
This feature gives the store owner the ability to approve or reject the new customer account.  
Since it requires the pre-verification of every new customer, it becomes one of the essentials modules for B2B & B2C.  

Some of the key features:
* Option to enable/disable the customer approval feature
* Configurable customer groups that require admin approval
* Option to auto-approve new customer registration
* Easier approval management for the admin
* Notify admin on new customer registration (with configurable email template)
* Notify customer on his/her account approval or rejection (with configurable email template)
* Configurable message (html tags supported) & redirection for the non-approved customer

![Magento 2 Store Restriction - Require Customer Approval/Activation - General Setting](https://www.magepsycho.com/media/catalog/product/m/2/m2-srp-registration-settings-customer-approval-enabled-general.png)

![Magento 2 Store Restriction - Require Customer Approval/Activation - Notification Setting](https://www.magepsycho.com/media/catalog/product/m/2/m2-srp-registration-settings-customer-approval-enabled-notification.png)

![Magento 2 Store Restriction - Require Customer Approval/Activation - Admin Management](https://www.magepsycho.com/media/catalog/product/m/2/m2-srp-customer-approval-actions-customer-listing.png)

### 4. Store Restriction

Provides Store Restriction of two types
1. Non-Restricted
    * The store is accessible for all the users.
1. Restricted (Only configured pages accessible)
    * The store is restricted, requires login to be accessible.
    * Provides option to select which customer group(s) can access the restricted store.
    * Provides option to redirect the not allowed user to the custom page(Login, CMS or any custom page) with configured message (html tags supported).
    * Provides option to allow certain CMS, Category, Product & custom module pages for guest users.
1. Accessible (Only configured pages/sections restricted) - *coming soon*
    
![Restriction Type: Restricted (Only configured pages accessible)](https://www.magepsycho.com/media/catalog/product/6/-/6-m2-backend-restriction-settings-restricted-accessible-general-2.png)

## Installation
1. Download the extension .zip file and extract the files.
1. Copy the extension files from src/ folder to the {magento2-root-dir}/
1. Run the following series of command from SSH console of your server:
`php bin/magento module:enable MagePsycho_StoreRestrictionPro MagePsycho_GroupSwitcherPro --clear-static-content`
`php bin/magento setup:upgrade`
1. Flush the store cache
`php bin/magento cache:flush`
1. Go to Admin > Stores > Configuration > MagePsycho > Store Restriction Pro > Configure your settings here...

## Live Demo:
* [View Frontend Demo](http://m2-store-restriction-pro.mage-expo.com/customer/account/create/)  
* [View Backend Demo](http://m2-store-restriction-pro.mage-expo.com/admin_m2demo)

## Changelog
**Version v1.0.6 (2020-04-24)**
- Added html tag support in custom messages

**Version v1.0.5 (2020-04-15)**
- Fixed the /customer/account/createPassword bug

**Version v1.0.4 (2020-04-11)**
- Added Added customer activation/approval feature
- Changed Refactored the code
- Added Tested the compatibility with Magento v2.3.4

**Version v1.0.3 (2019-11-12)**
- Refactored the code
- Changed Fixed the system configuration tab issue
- Added Tested the compatibility with Magento v2.3.3

**Version 1.0.2 (2019-05-23)**
- Fixed redirection issue from homepage
- Fixed registration disabled case
- Compatibility tested with Magento v2.3.x

**Version 1.0.0 (2017-06-12)**
    
- Initial Release.

### People also search:

- magento 2 restriction
- magento 2 restriction module
- magento 2 restriction extension
- magento 2 restriction extension free
- magento 2 customer restriction
- magento 2 customer group restriction
- magento 2 restrict by customer group
- magento 2 cms restriction
- magento 2 page restriction
- magento 2 product restriction
- magento 2 category restriction
- magento 2 restrict category access
- magento 2 website restriction
- magento 2 cart restriction
- magento 2 shipping restriction
- magento 2 shipment restriction
- magento 2 payment restriction
- magento 2 restrict shipping method
- magento 2 restrict payment method
- magento 2 restrict cash on delivery
- magento 2 restrict zip codes
- magento 2 require customer approval
- magento 2 customer activation
- magento 2 approve customer
- magento 2 b2c extension
- magento 2 b2b extension
- magento 2 b2b extension free
- magento 2 wholesale extension
- magento 2 call for price
- magento 2 hide price
- magento 2 hide product price
- magento 2 hide product price extension
- magento 2 hide product price extension free
- magento 2 hide add to cart
- magento 2 hide add to cart button
- magento 2 disable add to cart
- magento 2 disable add to cart button
- magento 2 customer group select
- magento 2 customer group switch
- magento 2 restricted access
- magento 2 login only store
- magento 2 force login


### Other FREE Magento 2 Extensions on GitHub

- [Custom Shipping Module for Magento 2](https://github.com/MagePsycho/magento2-custom-shipping)
- [Magento 2 Easy Template Path Hints](https://github.com/MagePsycho/magento2-easy-template-path-hints)
- [Magento 2 Storefront Links Manager](https://github.com/MagePsycho/magento2-storefront-links-manager)
- [Magento 2 Custom Customer Address Attribute](https://github.com/MagePsycho/magento2-custom-customer-address-attribute)
- [Magento 2 Starter Theme](https://github.com/MagePsycho/magento2-starter-theme)
- [Magento 2 Bash Script Installer](https://github.com/MagePsycho/magento2-installer-bash-script)
- [Bash Script: Backup Magento2 Code + Database](https://github.com/MagePsycho/magento2-db-code-backup-bash-script)
",5,5,2,0,retail,"[b2b, b2c, customer-activation, customer-approval, customer-registration, disable-register, disable-registration, ecommerce, magento, magento-extension, magento-module, magento2, magento2-extension, magento2-extension-free, magento2-module, registration, restriction, retail, store, wholesale]",44-45
saurabhkaushik,CustomerChurn,,https://github.com/saurabhkaushik/CustomerChurn,https://api.github.com/repos/CustomerChurn/saurabhkaushik,Customer Chrun Prediction - Retail Store ,"# CustomerChurn

This program builds a model to predict the Customer Churn on retail store. 

* Calcuate Model Accuracy - Support vector machines; Random forest; K-nearest-neighbors
* Plots Confusion Matrix 
",4,4,3,0,retail,"[churn, customer, data-science, retail]",44-45
ManashJKonwar,ML-Retail-Sales,,https://github.com/ManashJKonwar/ML-Retail-Sales,https://api.github.com/repos/ML-Retail-Sales/ManashJKonwar,ML Techniques to predict future sales for products,"# ML-Retail-Sales
> This repository helps us to capture retail sales events and provide us forecasted sales. Classical ML algorithms such as xgboost regression is utilized to predict these demand nos. There are methodologies which plays vital roles in generating these models such as product category level seasonality, cannibalistic price ratios and lags at product category level, trend featrues, etc.

## **Table of Contents**  
* [General Information](#general-information)  
  * [Demand Forecasting](#demand-forecasting)
  * [Why Demand Forecasting using ML?](#why-demand-forecasting-using-ml)  
* [Project Flow](#project-flow)  
* [Technologies Used](#technologies-used)
* [Features](#features)
* [Setup](#setup)
* [Dataset Utilized](#dataset-utilized)
* [Usage](#usage)
* [Project Status](#project-status)
* [Room for Improvement](#room-for-improvement)
* [References](#references)
* [Contact](#contact)
<!-- * [License](#license) -->

## **General Information**
- The aim of this repository is to implement a ML Retail Sales Pipeline which covers vital pointers to capture trends, seasonality, pricing ratios, pricing lags, etc in sales data by incorporating Feature Engineering script, Training Script and Inferencing Script.
- This work will help Data Science professionals gather knowledge on capturing retail based events and provide an idea on to how you can approach any ML based approach for capturing business insights from retail point of sales (POS) data.  
- The code based could be further improved to feature engineer other interaction level features so as to capture excise duty changes, weather information, customer profiling data (geographics, professional details, demographics, etc), product distribution data.
<!-- You don't have to answer all the questions - just the ones relevant to your project. -->

### **Demand Forecasting**  
It is the process of predicting what the demand for certain products will be in the future. This helps manufacturers to decide what they should produce and guides retailers toward what they should stock for days to come.  

It focussess mainly on following processes:  
- Supplier relationship management  
- Customer relatonship management  
- Order fulfillment and logistics  
- Marketing campaigns  
- Manufacturing flow management

### **Why Demand Forecasting using ML?**  
![Demand Forecasting](./assets/demand_forecasting_techniques.jpg)  

There are quantitative and qualitative demand assessment methods. The above-listed traditional sales forecasting methods have been tried and tested for decades. With Artificial Intelligence development, they are now upgraded by modern forecasting methods using Machine Learning (ML).

Machine learning techniques allows for predicting the amount of products/services to be purchased during a defined future period. In this case, a software system can learn from data for improved analysis. Compared to traditional demand forecasting methods, a machine learning approach allows you to:
- Accelerate data processing speed
- Provide a more accurate forecast
- Automate forecast updates based on the recent data
- Analyze more data
- Identify hidden patterns in data
- Create a robust system
- Increase adaptability to changes

## **Project Flow**:  
![Modelling Flow](./assets/ml_project_lifecycle.jpg)
The most unique section of this repository is how the feature engineering and traning scripts have been bifurcated to 2 separate versions -   

a. The first version [feature engineering script](./modelling_pipeline/feature_engg_script_v01.ipynb) and [training script](./modelling_pipeline/training_script_v01.ipynb) are developed in such a way that at the end we have only one model for the entire data and the training data is very extensive in nature.  

b. The second version [feature engineering script](./modelling_pipeline/feature_engg_script_v02.ipynb) and [training script](./modelling_pipeline/training_script_v02.ipynb) are developed in such a way so as to generate separate models for each product category (Accessories, Gaming Consoles, Books, etc). This seems to be a much more practical approach and a real time one.

The EDA remains same for both approaches however pre-processing, feature engineering and training methods are completely different.
1. **EDA:**  
    
    EDA covered certain sections such as:  
    a. Data Sourcing  
    b. Data Cleaning - (Handling Missing Values, Handling Duplicated Values, Handling outliers)  
    c. Univariate Analysis  
    d. Bivariate Analysis  
    e. Multivariate Analysis

2. **Pre-Processing (Approach 1):**  
    
    Following steps are being followed for preprocessing the sales data before feature enginnering part is run:  
    a. Based on unique dates (date_block_num), grid is formed among unique shop ids and item ids.  
    b. Sales are brought from daily level to montly level aggregation where item count is ""summed"" and price of an item is ""averaged"" out.  
    c. Dataframes obtained from step (a) & (b) are merged together and wherever item count per month is 'NA', they are filled up with 0.  
    d. item categories are also added to dataframe from step (c).  

3. **Feature Engineering (Approach 1):**  
    
    9 feature engineering steps are compiled together to generate the train ready dataset and if they are used or not: 
    a. Adding previous item sold for each shop as feature starting from month 1 to month 12. - Groupby (date_block_num) - ✅  
    b. Adding previous sales for each item as feature starting from month 1 to month 12. - Groupby (date_block_num & item_id) - ✅  
    c. Adding previous shop for each item price as feature starting from month 1 to month 12. - Groupby (shop_id & item_id & date_block_num) - ✅  
    d. Adding previous item price as feature starting from month 1 to month 12. - Groupby (item_id & date_block_num) - ✅  
    e. Mean encodings for shop per item pairs / Mean item counts are extracted for shop item pairs wrt week 32 and 33. - Groupby (shop_id & item_id) - ✅  
    f. Mean encodings for item pairs / Mean item counts are extracted wrt week 32 and 33. - Groupby (item_id) - ✅  
    g. Month number from last sale of each shop item pair. - 🔶  
    h. Month number from last sale of each item. - 🔶  
    i. Utilize top 25 features based on item name. - ✅  

4. **Pre-Processing (Approach 2):**  

    Following steps are being followed for preprocessing the sales data before feature engineering part is run:  
    a. Sales are brought from daily level to week level aggregation where item count is ""summed"" and price of an item is ""averaged"" out.  
    b. For dataframes obtained from step (a), wherever item count per month is 'NA', they are filled up with 0.  
    c. Certain categories are removed from the sales data if number of datapoints are less than 10 in number.  
    d. Parent Categories are extracted for each data point.

5. **Feature Engineering (Approach 2):**  

    7 feature engineering steps are compiled together for each parent category to generate the train ready dataset and if they are used or not:  
    a. Adding shop level feature, extracting total no of items sold from each shop for each category. - Groupby (shop_id & item_category_id & week_start_date) - ✅  
    b. Adding shop level feature, extracting mean price of each category sold from each shop. - Groupby (shop_id & item_category_id & week_start_date) - ✅  
    c. Adding price lags, extracting price lags for 1, 4, 12 and 24 weeks for each item. - Groupby (item_id & week_start_date) - ✅  
    d. Adding price lags, extracting price lags for 1, 4, 12 and 24 weeks for each item sold from each shop. - Groupby (shop_id & item_id & week_start_date) - ✅  
    e. Adding seasonality index for each category at monthly level - ✅  
    f. Adding parent category level price ratios at weekly level - ✅  
    g. Adding item category level price ratios at weekly level - ✅

## **Technologies Used**
- XGBoost
- Python 

## **Features**
List the ready features here:
- Exploratory Data Analysis (EDA) Script - Done
- Feature Engineering Script - Done
- Modelling Script - Done
- Inferencing Script - To Be Started

## **Setup**
- git clone https://github.com/ManashJKonwar/ML-Retail-Sales.git (Clone the repository)
- python3 -m venv MLPricingVenv (Create virtual environment from existing python3)
- activate the ""MLPricingVenv"" (Activating the virtual environment)
- pip install -r requirements.txt (Install all required python modules)

## **Dataset Utilized**
- Retail Dataset is obtained from Kaggle and competition name is [Predict Future Sales](https://www.kaggle.com/competitions/competitive-data-science-predict-future-sales/data).
- Please refer to screenshot below for dataset descriptions and data fields within this dataset.  
![Dataset Description / Data Fields](./assets/dataset_description.jpg)

## **Usage**
### For EDA, execute this notebook  
- python preprocessing_pipeline\eda.ipynb
### For Feature engineering, execute this notebook  
- python modelling_pipeline\feature_engg_script_v01.ipynb (For Methodology 1)   
- python modelling_pipeline\feature_engg_script_v02.ipynb (For Methodology 2)
### For Training, execute this notebook  
- python modelling_pipeline\training_script_v01.ipynb (For Methodology 1)   
- python modelling_pipeline\training_script_v02.ipynb (For Methodology 2)

## **Project Status**
Project is: __in progress_ 
<!-- / _complete_ / _no longer being worked on_. If you are no longer working on it, provide reasons why._ -->

## **Room for Improvement**
Room for improvement:
- Generate sku level lag features
- Generate sku level price ratios 
- Build feature selection methodology
- Build model selection methodology 

To do:
- Finish developing inferencing scripts/notebooks for both methodologies

## **References**
[1] https://mobidev.biz/blog/machine-learning-methods-demand-forecasting-retail

## **Contact**
Created by [@ManashJKonwar](https://github.com/ManashJKonwar) - feel free to contact me!

<!-- Optional -->
<!-- ## License -->
<!-- This project is open source and available under the [... License](). -->

<!-- You don't have to include all sections - just the one's relevant to your project -->",4,4,1,0,retail,"[machine-learning, python, retail, xgboost]",44-45
adwansyed,Market-Basket-Analysis-Apriori,,https://github.com/adwansyed/Market-Basket-Analysis-Apriori,https://api.github.com/repos/Market-Basket-Analysis-Apriori/adwansyed,Market basket analysis of retail and movie datasets using brute force and apriori algorithm,"# MarketBasketAnalysis_BruteForce_Apriori
Market basket analysis of retail and movie datasets.

Market basket analysis was performed using two methods:
- Brute force algorithm
- Apriori algorithm (performance enhancement using pruning technique)
",4,4,3,0,retail,"[data-science, market-basket-analysis, market-basket-optimization, movies, python, retail]",44-45
eugenebelieve,marketplace-retail,,https://github.com/eugenebelieve/marketplace-retail,https://api.github.com/repos/marketplace-retail/eugenebelieve,"A Retail Marketplace POV running a frontend application (ReactJS) and Microservices (NodeJS, ExpressJS, JWT). You can quickly import a retail dataset and have a fully functional Marketplace with login, users, products, catalogs, review, orders, payments and much more. ","<img src=""application/public/images/retail/marketplace_retail.png"" alt=""dashboard"" height=""400"">

# Usage

## Retail Marketplace (MongoDB, NodeJS, ExpressJS, ReactJS & JWT)

### Clone Repositorie

Clone this Repositorie to your local machine

```
git clone https://github.com/eugenebelieve/marketplace-retail.git
```

### Add Env Variables

Create or modify the .env file in then root and add the following

```
NODE_ENV = 'development'
PORT = '5000'
MONGO_URI = ""YOUR_MONGODB_URI_HERE""
JWT_SECRET = 'random_secret_key'
PAYPAL_CLIENT_ID = 'YOUR_DEV_PAYPAL_ID_HERE'
```

### Install Dependencies (frontend & backend)

```
npm install
cd application
npm install
```

### Import Dataset

You can use the following commands generate some sample users and products as well as destroy all data, directly in your Database

```
# To Import Retail & User Data, run command from root directory
npm run data:retail

# Destroy data
npm run data:destroy
```

### Run

```
# Run frontend Application (:3000) & Microservices (:5000), from root directory 
npm run dev
```

### Generated Accounts

```
#Sample User Logins created

admin@example.com (Admin)
123456

john@example.com (Customer)
123456

jane@example.com (Customer)
123456
```

## More Previews (Product & Shopping Cart)

<div>
<img src=""application/public/images/retail/product.png"" alt=""dashboard"" height=""230"">
<img src=""application/public/images/retail/kart.png"" alt=""dashboard"" height=""230"">
</div>
",4,4,1,2,retail,"[jwt, marketplace, mongodb, node, react, retail]",44-45
kriolos,kriolos-obiz,kriolos,https://github.com/kriolos/kriolos-obiz,https://api.github.com/repos/kriolos-obiz/kriolos,Kriol Open Source - Open Business,,4,4,1,0,retail,"[enterprise-resource-planning, openbravo, point-of-sale, retail, retail-software, sales-order, unicenta-opos]",44-45
Limmek,Shitlist,,https://github.com/Limmek/Shitlist,https://api.github.com/repos/Shitlist/Limmek,Set a personal note on a player,"[![Check formating](https://github.com/Limmek/Shitlist/actions/workflows/formating.yml/badge.svg)](https://github.com/Limmek/Shitlist/actions/workflows/formating.yml)
[![Package and release](https://github.com/Limmek/Shitlist/actions/workflows/release.yml/badge.svg)](https://github.com/Limmek/Shitlist/actions/workflows/release.yml)
# Shitlist
Are you tired of getting in groups with players you have had a bad experience with before and want a reminder of it.
Or do you and your friends or guild keep a list of good/bad players and keep the names write down on a piece of paper or in text file?

Then this is the addon for you, **#Shitlist** will let you set a note on a player from a list of pre set messages and a optional text.
The player note will then be displayed on the player information tooltip.

##### **How to use?**

_Right click on the targeted player frame you want to set a note on and you will have a option add or remove from Shitlist._

##### **How to edit pre set message (Reason)?**

_To add a pre set message (Reason) go to **Interface Options -&gt; Addons -&gt; Shitlist -&gt; Reasons**._

##### **Can i edit or remove a player if i do not have it as target or if the player is offline?**

_Yes in **Interface Options -&gt; Addons -&gt; Shitlist -&gt; Listed Players** you can add/edit and remove players and their reason/comment._

##### **When this Add-on is useful?** 

_Literally on any part of the game that includes Questing, Raids & Dungeons, World & PvP, getting corpse camped or a player steel a item and leave the party.
This is very useful if you want to blacklist some or just to put a note to be remind of something about that player._

##### **Can i use this addon to blacklist people?**

_Yes, you can._
",4,4,2,1,retail,"[addon, classic, retail, world-of-warcraft, wotlk, wow-addon]",44-45
ca1e,amiitool.net,,https://github.com/ca1e/amiitool.net,https://api.github.com/repos/amiitool.net/ca1e,amiitool C# port,"## amiitool.net

### docs:
- https://wiki.gbatemp.net/wiki/Amiibo
- https://kevinbrewster.github.io/Amiibo-Reverse-Engineering/
- https://www.3dbrew.org/wiki/Amiibo

### ref:
- https://github.com/socram8888/amiitool
- https://github.com/DRKV333/libamiibo
",4,4,1,0,retail,"[libamiibo, nfc, retail]",44-45
neo4j-graph-examples,get-started,neo4j-graph-examples,https://github.com/neo4j-graph-examples/get-started,https://api.github.com/repos/get-started/neo4j-graph-examples,An introduction to graph databases and Neo4j for new users,,4,4,1,1,retail,"[example-data, get-started, graph-database, neo4j-auradb-dev-approved, retail]",44-45
AyushiAsthana18,RFM-Analysis,,https://github.com/AyushiAsthana18/RFM-Analysis,https://api.github.com/repos/RFM-Analysis/AyushiAsthana18,"This repository contains the ""RFM Analysis"" for a Sales Data of a Retailer in SQL. This is part of my Data Science Portfolio Projects","# RFM-Analysis
#### Sales | Marketing | Retail | E-Commerce
RFM Analysis is used to understand and segment customers based on their buying behaviour. RFM stands for recency, frequency, and monetary value, which are three key metrics that provide information about customer engagement, loyalty, and value to a business. These segments enables targeted marketing and personalized strategies for each segment. 
Using RFM Analysis, a business can assess customer's:
* recency (the date they made their last purchase)
* frequency (how often they make purchases)
* monetary value (the amount spent on purchases)

To perform RFM analysis, we need a dataset that includes customer IDs, purchase dates, and transaction amounts. 

Dataset : https://data.world/dataman-udit/us-regional-sales-data


We cab divide Customers into 3 Value segments based on average RFM Score as follows:
* High-Value (Top 20%)
* Mid-Value (Next 30%)
* Low-Value (Next 50%)

  
When segmenting customers based on RFM (Recency, Frequency, Monetary), we  can create customer segments that reflect their purchasing behavior. Here are five segment names we can consider:
1. VIP Customers: This segment represents the high-value customers who have made recent and frequent purchases with a high monetary value. They are the most valuable and loyal customers.
2. Regular Customers: This segment includes customers who make frequent purchases but may not have the highest monetary value. They are reliable and contribute to consistent sales.
3. Dormant Customers: This segment consists of customers who haven't made a purchase in a while, despite having a history of previous purchases. These customers may require targeted marketing efforts to reactivate them.
4. New Customers: This segment represents customers who have recently made their first purchase. They require nurturing and engagement to encourage repeat purchases and long-term loyalty.
5. Churned Customers: This segment comprises customers who were once active but haven't made a purchase in a long time. They require re-engagement strategies to win them back and prevent them from churning.

#### References :
* https://statso.io/rfm-analysis-case-study/
* https://thecleverprogrammer.com/2023/06/12/rfm-analysis-using-python/

",4,4,1,0,retail,"[customer-segmentation, data-science, descriptive-analysis, e-commerce, marketing, retail, rfm-analysis, sql, sql-server, statistics]",44-45
pizofreude,sku-generator,,https://github.com/pizofreude/sku-generator,https://api.github.com/repos/sku-generator/pizofreude,"SKU-Generator is a python script that helps generate unique SKU codes for products. SKU (Stock Keeping Unit) is a unique identifier that is used to track inventory in warehouses, eCommerce stores, and retail stores.","# SKU Generator

SKU-Generator.py is a script generates unique SKU codes based on a CSV file with columns containing product information.

It also includes a function to replace colors with color codes.

SKU (Stock Keeping Unit) is a unique identifier that is used to track inventory in warehouses, retail stores, or even eCommerce stores.

SKU can be used to populate item code in many accounting software such as [Manager.io](https://www.manager.io/).

## Features

* Generate SKUs based on custom rules and templates
* Automatically assign sequential numbers to SKUs to ensure uniqueness
* Support for a wide range of attribute types, including strings, numbers, and dates
* Configurable delimiter characters between attributes in SKUs
* Export generated SKUs to a CSV file for easy integration into other systems

## Requirements

The script was developed using Python3 (3.11) in a virtual environment.

Should you want to use this script, ensure your machine support any version of python3.

Additionally, run this command in your terminal to fulfill the script dependencies:

```bash
pip install -r requirements.txt
```

## Usage

### Fork this repo

Click the ""Fork"" icon in the upper right of the page.

This will create a fork of the project under your user account.

### Cloning it locally

Next, clone your local version down to your local machine:

```bash
git clone https://github.com/<Your-GitHub-Username>/sku-generator.git
```

Alternatively, you can simply download the project as ZIP file and unzip it to use as depicted below:

![Clone or Download project as ZIP file.](./img/Clone.png)

## User Guide

1. Open the CHARTS directory and customize two CSV files namely categories-subcategories.csv and colors.csv according to your use case scenario in any text editor ([Excel spreadsheet](https://support.microsoft.com/en-us/office/import-or-export-text-txt-or-csv-files-5250ac4c-663c-47ce-937b-339e391393ba) or Notepad).
2. Next, fill in your product info in the sku_gen_template.csv accordingly.
3. Run the sku-gen.py script in the terminal. Open your terminal and navigate to the script directory, then enter:

```bash
python sku-gen.py
```

4. Your result will be saved in the same folder of sku-gen.py with the following name:

```bash
sku_gen_template_result.csv
```

For more information on how to use SKU-Generator, please refer to the [documentation](./docs/).

## Contributing

If you would like to contribute to SKU-Generator, please submit a pull request with your changes. Before submitting a pull request, please make sure that your changes are covered by unit tests and follow the [code style guidelines](https://docs.python-guide.org/writing/style/).

## License

SKU-Generator is licensed under the MIT license. See the [LICENSE file](https://github.com/pizofreude/sku-generator/LICENSE) for more information.

By contributing, you agree that your contributions will be licensed under its MIT License.


",4,4,2,0,retail,"[ecommerce, retail, sku-generator]",44-45
yuown,yuventory,yuown,https://github.com/yuown/yuventory,https://api.github.com/repos/yuventory/yuown,Small App to Maintain Inventory in a Jewellery Shop,"# yuventory
Small App to Maintain Inventory in a Jewellery Shop

This Application has feature to Generate and Print Barcode on Stickers and Ability to read them.
",3,3,2,0,retail,"[jewellery-shop, pos, retail]",44-45
predrag-jovicic,Stellar-Clothing-ASP.NET,,https://github.com/predrag-jovicic/Stellar-Clothing-ASP.NET,https://api.github.com/repos/Stellar-Clothing-ASP.NET/predrag-jovicic,"The project is an ASP.NET Core version of an E-commerce website I previously made using PHP. It's not just rewritten to C#, it's quite improved. Languages used: HTML,CSS,JavaScript,jQuery library, C# with ASP.NET Core Framework","# Documentation for Stellar Clothing Website

The project represents an online retailing store for selling clothes. This project doesn't have a real purchasing logic, hence a shopping cart is not entirely implemented. Basically, a user has a variaty of options when surfing through the website. A dropdown menu on the top of the page enables a user to easily navigate through the website. On the first level of the dropdown menu there are categories (e.g MEN,WOMEN). By clicking on an item the dropdown menu shows all submenus which represent subcategories (Accessories, T-Shirts, Skirts...). By clicking on a subitem a user arrives to the page which shows all products that belong to that subcategory. By clicking on a product a user navigates to the page which shows detailed information about the product. On that page a user can rate a product, view product's rates, choose a color or/and a size (if it's available) and an amount and add it to a shopping cart. Finally, when a user decides to buy some products, he proceeds to the shopping cart and finishes shopping.

Front-end technologies used: HTML, CSS, jQuery, JavaScript.

Back-end technologies used: ASP.NET Core, SendGrid API

###### Pages:
1. **Home page** - There's a dynamic slider which consists of images which are fetched from the database. Below slider there are products that have the highest discount. On the bottom of the page (and every other) there is a footer navigation menu which is fetched from the database as well. 

2. **Products page** - A page that contains all products that belong to a previously chosen subcategory from the header menu. On the bottom of the page is a pagination. On the side there is a slider for filtering and a dropdown list for sorting. A user can optionally filter products by moving a slider which represents a price range. There is also a dropdown list which can be used to sort products by a price or a discount.

3. **Single page** - A page which contains all the information about a previously chosen product. A user can rate the product by marking stars. This feature only works for logged users. There is an average rating for a product on the bottom of the page. To add a product to the shopping cart, a user has to choose a quantity before and a color or/and a size (depends on the product). Only logged users can do it.

4. **Checkout page** - A page which contains a list of shopping cart items - products that have been previously added by a user. A user can remove a product from a shopping cart by clicking on a cross on the side or remove all products by clicking on the Delete all button. On the bottom of the page there is a total price. By clicking on the Order button a user finishes his shopping session.

5. **Contact page** - This page represents a place for interacting with the website's administrator. A user (no matter if it's logged in or not) can contact an administrator by filling in the form. Below the form there is an active poll. A user can vote by clicking on one of the options.

6. **Login page** - The place for logging in

7. **Register** - The registration form. After registering, a user must verify his/her email. Sending e-mails with a token for verification purposes is established using SendGrid API.

###### Other pages:
> There are other pages that are auto-generated by model scaffolding. These pages are used for a website administration and aren't entirely completed. Their controllers are stored in AdminPanel folder in Controllers folder.
",3,3,1,0,retail,"[asp-net-core, csharp, css, e-commerce, html, javascript, retail]",44-45
softbankrobotics-labs,pepper-code-scanner,softbankrobotics-labs,https://github.com/softbankrobotics-labs/pepper-code-scanner,https://api.github.com/repos/pepper-code-scanner/softbankrobotics-labs,"A library and sample application to scan barcodes and QR Codes on Pepper QiSDK, using the Google Vision Library","# Pepper Code Scanner Library

This Android Library will help you to scan barcodes and QR codes using the Google Vision library.

### Video Demonstration

This video was filmed at SoftBank Robotics Europe, and shows the basic use cases for this library. 

[Watch video on YouTube](https://youtu.be/qSOL2kl2T4w)

## Getting Started

### Prerequisites

Barcodes and QR codes are needed.

### Running the Sample Application

The project comes complete with two sample projects. You can clone the repository, open it in Android Studio, and run this directly onto a Robot.

The two samples demonstrate two ways of using the library:

* By requesting an intent, that will launch a dedicated activity for scanning: **app-sample-with-intent**
* By integrating a scanning fragment in your activity: **app-sample-with-fragment**

Full implementation details are available to see in those projects.

### Installing

[**Follow these instructions**](https://jitpack.io/#softbankrobotics-labs/pepper-code-scanner)

Make sure to replace 'Tag' by the number of the version of the library you want to use.


## Usage

*This README assumes some standard setup can be done by the user, such as initialising variables or implementing code in the correct functions. Refer to the Sample Project for full usage code.*

You can launch the barcode scanner as an Activity with the following code: 
```
val launchIntent = Intent(this, BarcodeReaderActivity::class.java)
startActivityForResult(launchIntent, BARCODE_READER_ACTIVITY_REQUEST)
```
The Activity will close once a code has been read. You will be able to get the result by overriding the onActivityResult function:
```
override fun onActivityResult(requestCode: Int, resultCode: Int, data: Intent?) {
    super.onActivityResult(requestCode, resultCode, data)

    if (resultCode != Activity.RESULT_OK) {
        Log.e(TAG, ""Scan error"")
        return
    }

    if (requestCode == BARCODE_READER_ACTIVITY_REQUEST && data != null) {
        val barcode: Barcode? = data.getParcelableExtra(BarcodeReaderActivity.KEY_CAPTURED_BARCODE)
        result.text = barcode?.rawValue ?: """"
    }
}
```
Here we display the result in a TextView.

You can also launch the barcode scanner in a Fragment. You'll first have to make your Activity implements BarcodeReaderFragment.BarcodeReaderListener, and then use the following code to add the Fragment:
```
val readerFragment = BarcodeReaderFragment()
readerFragment.setListener(this)
val fragmentTransaction = supportFragmentManager.beginTransaction()
fragmentTransaction.replace(R.id.fm_container, readerFragment)
fragmentTransaction.commitAllowingStateLoss()
```
The result of the scan will be given in the onScanned callback. You'll have to override it:
```
override fun onScanned(barcode: Barcode?) {
    result.text = barcode?.rawValue ?: """"

    // Remove Fragment
    val fragmentTransaction = supportFragmentManager.beginTransaction()
    val fragmentId = supportFragmentManager.findFragmentById(R.id.fm_container)
    if (fragmentId != null) {
        fragmentTransaction.remove(fragmentId)
    }
    fragmentTransaction.commitAllowingStateLoss()
}
```
Here we display the result in a TextView and we remove the Fragment as we don't need it anymore.

By default, a scanner overlay is displayed for UX purposes. If you launch the barcode scanner as an Activity, you can disable the scanner overlay in the Intent used to launch the Activity:
```
val launchIntent = Intent(this, BarcodeReaderActivity::class.java)
launchIntent.putExtra(KEY_SCAN_OVERLAY_VISIBILITY, false)
startActivityForResult(launchIntent, BARCODE_READER_ACTIVITY_REQUEST)
```
With KEY_SCAN_OVERLAY_VISIBILITY = ""key_scan_overlay_visibility""

If you launch the barcode scanner in a Fragment, you can remove the scanner overlay this way:
```
val readerFragment = BarcodeReaderFragment(false)
readerFragment.setListener(this)
val fragmentTransaction = supportFragmentManager.beginTransaction()
fragmentTransaction.replace(R.id.fm_container, readerFragment)
fragmentTransaction.commitAllowingStateLoss()
```


## Known limitations

Because of the resolution and depth of field of the camera, it may be difficult to read small barcodes.


## License

This project is licensed under the BSD 3-Clause ""New"" or ""Revised"" License- see the [LICENSE](LICENSE.md) file for details.",3,3,4,0,retail,"[barcode-scanner, google-vision-api, pepper-qisdk, pepper-robot, qrcode-scanner, retail]",44-45
data-crat,Machine-Learning,,https://github.com/data-crat/Machine-Learning,https://api.github.com/repos/Machine-Learning/data-crat,"This gold mine contains beginner friendly (* maybe), traditional and advanced/sophisticated modelling techniques to solve problems in diverse Industries","# Machine-Learning
# In only data we trust

# 1) Big Mart Sales - 
My take on Analytics Vidhya BigMart’s sale(https://datahack.analyticsvidhya.com/contest/practice-problem-big-mart-sales-iii/) 
prediction hackathon.
We have  to identify the most important variables and to define the best regression model for predicting 
out response variable.
Hence, this analysis will be divided into five stages:

![workflow](https://user-images.githubusercontent.com/45167372/73745321-bf56b600-4778-11ea-9794-760eede39210.png)

The Algorithms aren't covered in an extensive way as it assumes the user to know the maths behind them.
Please visit the Big Mart Sales Sales folder for the self explanatory code and the dataset used

-----------------------------------------------------------------------------------------------------------------------------------------

# 2) Customer Segmentation-
  My take on understanding Unsupervised ML and specifically introduction and implementation of clustering algorithms
  
  # We will dwell in to the unsupervised aspect of machine learning in this section


   
  Unsupervised learning is the training of an artificial intelligence (AI) algorithm using information that is neither classified     nor labeled and allowing the algorithm to act on that information without guidance



  # Pretty interesting right????   
  
  ![unsupervised](https://user-images.githubusercontent.com/45167372/74805633-b08c0980-5309-11ea-88bc-8b9f492db0c3.gif)
  
  
  Please go to the Customer Segementation folder for the code written in jupyter notebook and the dataset used for the same.
  
  
-----------------------------------------------------------------------------------------------------------------------------------------

# 3) Market Mix Modelling-
  My take on market mix modelling concepts and modelling techniques
  
  # We will dwell in to  how four P's vary with each other, explain the model together and how can they solve problems for the organizations

  Marketing Mix Modeling (MMM) is one of the most popular analysis under Marketing Analytics which helps organisations in estimating the   effects of spent on different advertising channels (TV, Radio, Print, Online Ads etc) as well as other factors (price, competition,     weather, inflation, unemployment) on sales. In simple words, it helps companies in optimizing their marketing investments which they     spent in different marketing mediums (both online and offline).
  
![1_TEPTZKBF1wCq56aLo1IAmw](https://user-images.githubusercontent.com/45167372/84584477-929b6500-ae22-11ea-8262-ccea40a1a5fa.png)

  Please visit the Market-Mix-Modelling folder for the self explanatory code and the dataset used.
-----------------------------------------------------------------------------------------------------------------------------------------  
 # 4) Human Activity Recognition (Unsupervised) -
   A classic dataset to work on the unsupervised techniques
    
   With the advancement in IOT, sensors analytics have played a huge role in day-to-day activities. Sudden spike in the availability and usage of smart watches is a great    example for this. Application of sensors analytics living beings to track their actions and vital signs has huge set of problem pool, namely – Calorie tracker, early warning for life threatening diseases, protection of endangered species, improve sports person’s abilities etc. The current problem can be considered as the base for all the above mentions applications of sensor analytics on living things. Certain experiments were carried out with a group of 30 volunteers within an age bracket of 19-48 years. Each person performed six activities (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) wearing a smartphone (Samsung Galaxy S II) on the waist. Using its embedded accelerometer and gyroscope, 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz were captured.
    
   We will touch base on few concepts of k- means and feature transformation 
  
   Please visit the Human Activity Recognition folder for the self explanatory code and the dataset used.
  
  
  
 # Many Many more to be added to the list. May the data be with you.

",3,3,1,0,retail,"[banking, ecommerce, human-resources, insurance, jupyter-notebook, machine-learning, python, python3, retail, social-network-analysis, telecom]",44-45
meet244,BillDesk,,https://github.com/meet244/BillDesk,https://api.github.com/repos/BillDesk/meet244,"Billing software for seamless transaction management, including bill creation, search, product editing, and invoice printing.","# BillDesk 🧾💻

BillDesk is a simple billing software implemented in C. It allows users to create bills, search for existing bills, edit product details, and print invoices for Fresh Mart, a retail store located in Mumbai, Maharashtra,India providing high-quality groceries, fresh produce, and household essentials to its customers.

## Demo Video 🎥

https://github.com/meet244/BillDesk/assets/83262693/5bbcdfb0-8a75-492a-8288-60ab62b199d4

## Features ✨

- Creating bills: Users can create bills by selecting items from a product list and entering the quantity. The software automatically calculates the total price and generates a bill with GST breakup.
- Searching bills: Users can search for bills by entering the bill number. The software displays the bill details if found.
- Editing product details: Users can edit the name, price, and stock of existing products in the product list.
- Printing invoices: Users can print invoices with a unique bill number, store details, item details, and payment information.

## Getting Started 🚀

To run the BillDesk software, follow these steps:

1. Clone the repository: `git clone <repository_url>`
2. Compile the code: `gcc billdesk.c -o billdesk`
3. Run the program: `./billdesk`

## Usage 📝

1. When the program starts, a menu will be displayed with the following options:
   - 0️⃣: Print bill
   - 1️⃣: Search bill
   - 2️⃣: Edit items
   - 3️⃣: Exit

2. Select an option by entering the corresponding number.

3. If you choose to print a bill, you will be prompted to enter the item codes and quantities. Once done, the program will generate a bill and save it in the ""Cmini"" folder.

4. If you choose to search a bill, enter the bill number to retrieve and display the bill details.

5. If you choose to edit items, you can modify the name, price, and stock of existing products in the product list.

6. Exiting the program will terminate the software.

## Requirements 🛠️

- C compiler
- Windows operating system

## Authors 👥

- Meet Patel
- Harshil Damania
",3,3,1,0,retail,"[billing, finance, inventory-management, invoicing, productivity, retail, sales, small-business, transaction]",44-45
DllDroid,Clone_i1D3,,https://github.com/DllDroid/Clone_i1D3,https://api.github.com/repos/Clone_i1D3/DllDroid,"This tool can read the data out of the iD3 and write it to disk, it can also take data on disk and write it back into the i1D3","Notes:

This work is shared AS IS.  You use it at your own risk.  It is possible, though unlikely that you could damage your probe if used.
This work was inspired in part by the driver code from Argyll color management system

full credit is given to Argyll’s author for any code similarities.
https://www.argyllcms.com/

Additional information was obtained by the use of the protocol analysis tool Wireshark
https://www.wireshark.org/

Further information was obtained by extracting the firmware from i1d3 using PICKIT2 tools from Microchip and analysing it with Ghidra
https://ghidra-sre.org/

As a note, no Windows dlls were analysed in order to create i1d3util.  It was just not required!




The i1d3 has both internal and external eeproms.  The internal eeprom contains the serial number, the external eeprom contains the unique
device sensor calibration data and the “signature” that determines which flavour of i1d3 it is locked to (oem, retail, colormunki,C6 etc.)

The command i1d3util -? will give a help screen

The i1d3util tool has a number of command line options, both in lowercase and uppercase.  Lowercase are read commands, uppercase are write commands.
The i1d3util tool can read the data out of the id3 and write it to disk, it can also take data on disk and write it back into the i1d3.
By doing this, you can backup and restore your probe. The i1d3util tool can access both internal and external eeprom data.  It can also specifically access/change the serial number and signature data.

For example, if you have a retail probe and the signature data from an oem probe, you can load the oem signature into the retail probe.
The probe will now operate as if it were a factory oem probe.

The –f option enables you to overwrite (without warning!!) a file on disk.
The –w option enables ACTUAL writing to the i1d3 eeproms!
The –v reads the firmware revision from the i1d3 hardware

Example command to load a oem probe signiture file into ANY i1d3 probe:

i1d3util -w -S oem1D3signature.bin

It is STRONGLY RECOMMENDED that you save you probes current internal eeprom data, external eeprom data, signature data and serial number before you start changing anything

Be VERY careful to write the correct data file to the correct section of the probe!!

It is possible to corrupt the internal eeprom.  If this happens, the i1d3 reports back a different USB Vendor ID.
The i1d3util will try and detect this and correct the problem.

Between each WRITE to the i1d3, it is important that you unplug and plug back in the probe to reset the Windows device driver.

Once a write operation has been performed, the i1d3 sometimes starts flashing its white LEDS.  This is normal, and is part of it visual feedback system.
Most application will either turn this off or allow you to turn it off/on

Have fun!
",3,3,1,0,retail,"[c6, i1display, oem, retail, xrite]",44-45
trujamal,assess-swift,,https://github.com/trujamal/assess-swift,https://api.github.com/repos/assess-swift/trujamal,"The app was designed around creating an AR iOS Application that implemented Zillow API, Google Cloud Platform, and also using Mongo DB for our database. This app allows users to receive basic housing information right at your fingertips.","# assess-swift
[![Swift 4.0](https://img.shields.io/badge/Swift-4.0-green.svg?style=flat)](https://developer.apple.com/swift/)
[![GitHub license](https://img.shields.io/badge/license-MIT-lightgrey.svg)](https://raw.githubusercontent.com/Cuberto/flashy-tabbar/master/LICENSE)
[![Logo](https://cl.ly/054090239846/Image%202019-02-26%20at%202.37.11%20AM.png)](https://veveusa.com)

 * The overall goal with the app was to help entice people to learn more about the home buying practices. Also to create freindly habbits for later on in life.

## Installation
We recommend you clone the repo for easiest implementation:

```
git clone https://github.com/trujamal/Athena.git
```

## Usage

Please run the following command once you've cloned the repo in order to run the project files.

```
pod install
```

### Use the whole directory as needed
It is designed to be an easy, and ready to go start off tempalate for your swift projects.

## Requirements

* iOS 8.0+
* Xcode 10

## Documentation

Take a look at the [documentation table of contents](dist/doc/TOC.md).
This documentation is bundled with the project which makes it
available for offline reading and provides a useful starting point for
any documentation you want to write about your project.


## License

The code is available under the [MIT license](LICENSE.txt).



## Demo

<p align=""center"">

[![IMAGE ALT TEXT](http://img.youtube.com/vi/5UNDT1B4KtE/0.jpg)](https://www.youtube.com/watch?v=5UNDT1B4KtE&feature=youtu.be ""App Demo"")

</p>

***Designer Info***

Jamal Rasool | @trujamal
Aalap Patel  | @aalap07


## iOS support

* iOS 12 ✔️
* iOS 11 ✔️
* iOS 10 ✔️
* iOS 09 ✔️
",3,3,2,0,retail,"[hackathon-2018, housing-prices, retail, swift4]",44-45
Fasust,FroYou,,https://github.com/Fasust/FroYou,https://api.github.com/repos/FroYou/Fasust,To ease the consumption & ordering of Frozen Delights,"# FroYou

Im Rahmen des Moduls [Mobile Computing](https://wiki.moxd.io/display/WPFMoCoSS18/WPF+Mobile+Computing+SS2018+Home) der TH Köln von Sebastian Faust, Julian Schoemaker und Dario Giuseppe Lazzara

Für den Themenbereich “Enterprise” haben wir uns mit dem Unternehmen [Süße Ecke](https://www.forum-gummersbach.info/shop/sascha-stange/) im Gummersbacher Forum zusammengetan und eine App entwickelt die den Bestellprozess von Frozen Yoghurts und Softeis erleichtern soll.

📄 Die vollständige [Dokumentation](https://github.com/Fasust/FrozenJoghurtBuilder/blob/master/Dokumentation.pdf) des Arbeitsprozesses, der Designentscheidungen und der Architektur haben wir nach Abgabe des Projektes ebenfalls zum Repository hinzugefügt.

🏆 ForYou hat in 2018 den [Preis](https://www.th-koeln.de/hochschule/innovativ-kreativ-multimedial_61452.php) “Bestes Projekt des Semesters” gewonnen. 

### Features

- Diese App ermöglicht es den Nutzern einen Frozen Yogurt nach Baukasten Prinzip zusammenzustellen.
- Eigene Kreationen mit anderen Nutzern zu teilen.
- Selbst Rezepte von Freunden auszuprobieren. 
- Bestellung schon auf dem Weg zu der ""Süßen Ecke"" mit derzeitig im Laden verfügbaren Zutaten zusammenzustellen. 
- Das Bestellen selbst geht eben so einfach. Man zeigt einfach den Generierten QR Code dem Mitarbeiter an der Kasse und wartet bis die Kreation fertig ist.

Dies ist unser erstes größeres verteiltes System. Viele Sachen sind bei weitem nicht optimal gelöst und wie das bei jedem Projekt ist, würden wir im nachhinein vieles anders lösen.
Der Lehrerfolg war allerding enorm und wir konnten aus diesem Projekt eine Menge für zukünftige Projekte mitnehmen.

## Poster

![FroYou Poster 28.06.2018](Material/Poster/froYou_poster.jpg)

## Präsentation
[Abschluss-Präsentation](https://docs.google.com/presentation/d/1sWaOMK1kf6j3-Kr_HB4lsXuYvhgcxyrqX5WbNZVMhK4/edit?usp=sharing) stand 19.07.18 
",3,3,1,8,retail,"[android, app, firebase, java, pubsub, retail, ui]",44-45
mhertzfeld,NRF_MerchandisingCalendar,,https://github.com/mhertzfeld/NRF_MerchandisingCalendar,https://api.github.com/repos/NRF_MerchandisingCalendar/mhertzfeld,.Net Standard library for working with the National Retail Federation Merchandising Calendar.,"# NRF_MerchandisingCalendar
.Net Standard Library for working with the National Retail Federation 4-5-4 Merchandising Calendar.

This project is based of my fork https://github.com/mhertzfeld/MerchandiseCalendar of https://github.com/Dromaeosaur/MerchandiseCalendar.

Information regarding the NRF Merchandising Calendar can be found at the NRF website below.
  https://nrf.com/resources/4-5-4-calendar
",3,3,1,0,retail,"[4-5-4-calendar, fiscal-calendar, retail, retail-data]",44-45
Astrodynamic,RetailAnalitycs-in-postgresql,,https://github.com/Astrodynamic/RetailAnalitycs-in-postgresql,https://api.github.com/repos/RetailAnalitycs-in-postgresql/Astrodynamic,"Develop a SQL script to create a database with tables, views, roles, and functions. Form personalized offers to increase average check, frequency of visits, and cross-selling.","# RetailAnalitycs in postgresql

This repository contains scripts and functions for creating a database, creating views, managing user roles, and forming personal offers aimed at the growth of the average check, increasing the frequency of visits, and cross-selling.

## Table of Contents

- [RetailAnalitycs in postgresql](#retailanalitycs-in-postgresql)
  - [Table of Contents](#table-of-contents)
  - [Database Creation](#database-creation)
  - [Views Creation](#views-creation)
  - [User Roles](#user-roles)
  - [Forming Personal Offers for Average Check Growth](#forming-personal-offers-for-average-check-growth)
  - [Forming Personal Offers for Increasing Frequency of Visits](#forming-personal-offers-for-increasing-frequency-of-visits)
  - [Forming Personal Offers for Cross-Selling](#forming-personal-offers-for-cross-selling)
  - [Input Data](#input-data)
    - [Personal information Table](#personal-information-table)
    - [Cards Table](#cards-table)
    - [Transactions Table](#transactions-table)
    - [Checks Table](#checks-table)
    - [Product grid Table](#product-grid-table)
    - [Stores Table](#stores-table)
    - [SKU group Table](#sku-group-table)
    - [Date of analysis formation Table](#date-of-analysis-formation-table)
  - [Output data](#output-data)
    - [Customers View](#customers-view)
    - [Purchase history View](#purchase-history-view)
    - [Periods View](#periods-view)
    - [Groups View](#groups-view)
  - [License](#license)

## Database Creation

To create the database and tables described in the Input data, follow the steps below:

1. Execute the `part1.sql` script provided in the repository.
2. This script will create the necessary tables and also include procedures for importing and exporting data for each table from/to CSV and TSV files.
3. Make sure to upload the required CSV and TSV files from the datasets folder to the repository.

## Views Creation

To create the views described in the Output data, follow the steps below:

1. Execute the `part2.sql` script provided in the repository.
2. This script will create the views and include test queries for each view.

## User Roles

To set up user roles and their permissions, follow the steps below:

1. Execute the `part3.sql` script provided in the repository.
2. This script will create the roles and assign the following permissions:
   - Administrator: This role has full permissions to edit and view any information, as well as start and stop the processing.
   - Visitor: This role only has permission to view information of all tables.

## Forming Personal Offers for Average Check Growth

To form personal offers aimed at the growth of the average check, follow the steps below:

1. Execute the `part4.sql` script provided in the repository.
2. This script contains a function that determines offers based on the average check calculation method, first and last dates of the period, number of transactions, coefficient of average check increase, maximum churn index, maximum share of transactions with a discount, and allowable share of margin.
3. The function will output the customer ID, average check target value, offer group, and maximum discount depth for each offer.

## Forming Personal Offers for Increasing Frequency of Visits

To form personal offers aimed at increasing the frequency of visits, follow the steps below:

1. Execute the `part5.sql` script provided in the repository.
2. This script contains a function that determines offers based on the first and last dates of the period, added number of transactions, maximum churn index, maximum share of transactions with a discount, and allowable margin share.
3. The function will output the customer ID, period start date, period end date, target number of transactions, offer group, and maximum discount depth for each offer.

## Forming Personal Offers for Cross-Selling

To form personal offers aimed at cross-selling, follow the steps below:

1. Execute the `part6.sql` script provided in the repository.
2. This script contains a function that determines offers based on the number of groups, maximum churn index, maximum consumption stability index, maximum SKU share, and allowable margin share.
3. The function will output the customer ID, SKU offers, and maximum discount depth for each offer.

---

## Input Data

### Personal information Table

|       **Field**       | **System field name**  |                                   **Format / possible values**                                    | **Description** |
| :-------------------: | :--------------------: | :-----------------------------------------------------------------------------------------------: | :-------------: |
|      Customer ID      |      Customer_ID       |                                                ---                                                |       ---       |
|         Name          |     Customer_Name      | Cyrillic, the first letter is capitalized, the rest are upper case, dashes and spaces are allowed |       ---       |
|        Surname        |    Customer_Surname    | Cyrillic, the first letter is capitalized, the rest are upper case, dashes and spaces are allowed |       ---       |
|    Customer E-mail    | Customer_Primary_Email |                                           E-mail format                                           |       ---       |
| Customer phone number | Customer_Primary_Phone |                                     +7 and 10 Arabic numerals                                     |       ---       |

---

### Cards Table

|  **Field**  | **System field name** | **Format / possible values** |          **Description**           |
| :---------: | :-------------------: | :--------------------------: | :--------------------------------: |
|   Card ID   |   Customer_Card_ID    |             ---              |                ---                 |
| Customer ID |      Customer_ID      |             ---              | One customer can own several cards |

---

### Transactions Table

|    **Field**     | **System field name** | **Format / possible values** |                          **Description**                           |
| :--------------: | :-------------------: | :--------------------------: | :----------------------------------------------------------------: |
|  Transaction ID  |    Transaction_ID     |             ---              |                            Unique value                            |
|     Card ID      |   Customer_Card_ID    |             ---              |                                ---                                 |
| Transaction sum  |   Transaction_Summ    |        Arabic numeral        | Transaction sum in rubles(full purchase price excluding discounts) |
| Transaction date | Transaction_DateTime  |     dd.mm.yyyy hh:mm:ss      |            Date and time when the transaction was made             |
|      Store       | Transaction_Store_ID  |           Store ID           |              The store where the transaction was made              |

---

### Checks Table

|                    **Field**                     | **System field name** | **Format / possible values** |                                                **Description**                                                |
| :----------------------------------------------: | :-------------------: | :--------------------------: | :-----------------------------------------------------------------------------------------------------------: |
|                  Transaction ID                  |    Transaction_ID     |             ---              |                           Transaction ID is specified for all products in the check                           |
|               Product in the check               |        SKU_ID         |             ---              |                                                      ---                                                      |
|          Number of pieces or kilograms           |      SKU_Amount       |        Arabic numeral        |                                     The quantity of the purchased product                                     |
| Total amount for which the product was purchased |       SKU_Summ        |        Arabic numeral        | The purchase amount of the actual volume of this product in rubles (full price without discounts and bonuses) |
|          The paid price of the product           |     SKU_Summ_Paid     |        Arabic numeral        |                      The amount actually paid for the product not including the discount                      |
|                 Discount granted                 |     SKU_Discount      |        Arabic numeral        |                          The size of the discount granted for the product in rubles                           |

---

### Product grid Table

|       **Field**        | **System field name** |         **Format / possible values**          |                                                                                                        **Description**                                                                                                        |
| :--------------------: | :-------------------: | :-------------------------------------------: | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
|       Product ID       |        SKU_ID         |                      ---                      |                                                                                                              ---                                                                                                              |
|      Product name      |       SKU_Name        | Cyrillic, Arabic numerals, special characters |                                                                                                              ---                                                                                                              |
|       SKU group        |       Group_ID        |                      ---                      | The ID of the group of related products to which the product belongs (for example, same type of yogurt of the same manufacturer and volume, but different flavors). One identifier is specified for all products in the group |
| Product purchase price |  SKU_Purchase_Price   |                Arabic numeral                 |                                                                                       The purchase price of the product for this store                                                                                        |
|  Product retail price  |   SKU_Retail_Price    |                Arabic numeral                 |                                                                               The sale price of the product excluding discounts for this store                                                                                |

---

### Stores Table

|       **Field**        | **System field name** | **Format / possible values** |                         **Description**                          |
| :--------------------: | :-------------------: | :--------------------------: | :--------------------------------------------------------------: |
|         Store          | Transaction_Store_ID  |             ---              |                               ---                                |
|       Product ID       |        SKU_ID         |             ---              |                               ---                                |
| Product purchase price |  SKU_Purchase_Price   |        Arabic numeral        |           Purchasing price of products for this store            |
|  Product retail price  |   SKU_Retail_Price    |        Arabic numeral        | The sale price of the product excluding discounts for this store |

---

### SKU group Table

| **Field**  | **System field name** |         **Format / possible values**          | **Description** |
| :--------: | :-------------------: | :-------------------------------------------: | :-------------: |
| SKU group  |       Group_ID        |                      ---                      |       ---       |
| Group name |      Group_Name       | Cyrillic, Arabic numerals, special characters |       ---       |

---

### Date of analysis formation Table

|    **Field**     | **System field name** | **Format / possible values** | **Description** |
| :--------------: | :-------------------: | :--------------------------: | :-------------: |
| Date of analysis |  Analysis_Formation   |     dd.mm.yyyy hh:mm:ss      |       ---       |

---

## Output data

### Customers View

|                   **Field**                   |     **System field name**      | **Format / possible values** |                                   **Description**                                    |
| :-------------------------------------------: | :----------------------------: | :--------------------------: | :----------------------------------------------------------------------------------: |
|                  Customer ID                  |          Customer_ID           |             ---              |                                     Unique value                                     |
|          Value of the average check           |     Customer_Average_Check     |   Arabic numeral, decimal    |             Value of the average check in rubles for the analyzed period             |
|             Average check segment             | Customer_Average_Check_Segment |      High; Middle; Low       |                                 Segment description                                  |
|          Transaction frequency value          |       Customer_Frequency       |   Arabic numeral, decimal    | Value of customer visit frequency in the average number of days between transactions |
|         Transaction frequency segment         |   Customer_Frequency_Segment   | Often; Occasionally; Rarely  |                                 Segment description                                  |
| Number of days since the previous transaction |    Customer_Inactive_Period    |   Arabic numeral, decimal    |              Number of days passed since the previous transaction date               |
|                  Churn rate                   |      Customer_Churn_Rate       |   Arabic numeral, decimal    |                           Value of the customer churn rate                           |
|              Churn rate segment               |     Customer_Churn_Segment     |      High; Middle; Low       |                                 Segment description                                  |
|                Segment number                 |        Customer_Segment        |        Arabic numeral        |               The number of the segment to which the customer belongs                |
|                 Main store ID                 |     Customer_Primary_Store     |             ---              |                                         ---                                          |

---

### Purchase history View

|     **Field**     | **System field name** | **Format / possible values** |                                                                                                        **Description**                                                                                                        |
| :---------------: | :-------------------: | :--------------------------: | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
|    Customer ID    |      Customer_ID      |             ---              |                                                                                                              ---                                                                                                              |
|  Transaction ID   |    Transaction_ID     |             ---              |                                                                                                              ---                                                                                                              |
| Transaction date  | Transaction_DateTime  | dd.mm.yyyyy hh:mm:ss.0000000 |                                                                                            The date when the transaction was made                                                                                             |
|     SKU group     |       Group_ID        |             ---              | The ID of the group of related products to which the product belongs (for example, same type of yogurt of the same manufacturer and volume, but different flavors). One identifier is specified for all products in the group |
|    Prime cost     |      Group_Cost       |   Arabic numeral, decimal    |                                                                                                              ---                                                                                                              |
| Base retail price |      Group_Summ       |   Arabic numeral, decimal    |                                                                                                              ---                                                                                                              |
| Actual cost paid  |    Group_Summ_Paid    |   Arabic numeral, decimal    |                                                                                                              ---                                                                                                              |

---

### Periods View

|               **Field**               |   **System field name**   | **Format / possible values** |                                                                                                        **Description**                                                                                                        |
| :-----------------------------------: | :-----------------------: | :--------------------------: | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
|              Customer ID              |        Customer_ID        |             ---              |                                                                                                              ---                                                                                                              |
|               SKU group               |         Group_ID          |             ---              | The ID of the group of related products to which the product belongs (for example, same type of yogurt of the same manufacturer and volume, but different flavors). One identifier is specified for all products in the group |
|  Date of first purchase of the group  | First_Group_Purchase_Date | yyyy-mm-dd hh:mm:ss.0000000  |                                                                                                              ---                                                                                                              |
|  Date of last purchase of the group   | Last_Group_Purchase_Date  | yyyy-mm-dd hh:mm:ss.0000000  |                                                                                                              ---                                                                                                              |
| Number of transactions with the group |      Group_Purchase       |   Arabic numeral, decimal    |                                                                                                              ---                                                                                                              |
|     Intensity of group purchases      |      Group_Frequency      |   Arabic numeral, decimal    |                                                                                                              ---                                                                                                              |
|        Minimum group discount         |    Group_Min_Discount     |   Arabic numeral, decimal    |                                                                                                              ---                                                                                                              |

---

### Groups View

|               **Field**               | **System field name**  | **Format / possible values** |                                                              **Description**                                                               |
| :-----------------------------------: | :--------------------: | :--------------------------: | :----------------------------------------------------------------------------------------------------------------------------------------: |
|              Customer ID              |      Customer_ID       |             ---              |                                                                    ---                                                                     |
|               Group ID                |        Group_ID        |             ---              |                                                                    ---                                                                     |
|            Affinity index             |  Group_Affinity_Index  |   Arabic numeral, decimal    |                                                   Customer affinity index for this group                                                   |
|              Churn index              |    Group_Churn_Rate    |   Arabic numeral, decimal    |                                                 Customer churn index for a specific group                                                  |
|            Stability index            | Group_Stability_Index  |   Arabic numeral, decimal    |                               Indicator demonstrating the stability of the customer consumption of the group                               |
|      Actual margin for the group      |      Group_Margin      |   Arabic numeral, decimal    |                                   Indicator of the actual margin for the group for a particular customer                                   |
| Share of transactions with a discount |  Group_Discount_Share  |   Arabic numeral, decimal    | Share of purchasing transactions of the group by a customer, within which the discount was applied (excluding the loyalty program bonuses) |
|     Minimum size of the discount      | Group_Minimum_Discount |   Arabic numeral, decimal    |                                            Minimum size of the group discount for the customer                                             |
|           Average discount            | Group_Average_Discount |   Arabic numeral, decimal    |                                            Average size of the group discount for the customer                                             |

---

## License

This project is licensed under the [MIT License](LICENSE). Feel free to use and modify the code according to your needs.
",3,3,2,0,retail,"[bd, csv, data-analysis, data-export, data-input, data-manipulation, data-validation, database-management, functions, git, margin, offers, postgresql, retail, role-permission-management, selling, sql, transaction, tsv, views]",44-45
asim5800,Retail-Sales-Prediction,,https://github.com/asim5800/Retail-Sales-Prediction,https://api.github.com/repos/Retail-Sales-Prediction/asim5800,Sales forecasting is an essential task for the management of a store. Machine learning can help us discover the factors that influence sales in a retail store and estimate the number of sales in the near future. ,"# Retail-Sales-Prediction

<p align=""center"">
  <img width=""460"" height=""300"" src=""https://upload.wikimedia.org/wikipedia/commons/thumb/b/b2/Ro%C3%9Fmann-Markt_in_Berlin.jpg/1024px-Ro%C3%9Fmann-Markt_in_Berlin.jpg"">
</p>

Rossmann operates over 3,000 drug stores in 7 European countries. Currently, Rossmann store managers are tasked with predicting their daily sales for up to six weeks in advance. Store sales are influenced by many factors, including promotions, competition, school and state holidays, seasonality, and locality. With thousands of individual managers predicting sales based on their unique circumstances, the accuracy of results can be quite varied. My work includes various plots and graphs , visualizations , feature engineering , ensemble techniques , different ML algorithms with their respective parameter tuning , analysis and trends . Predictions are of 6 weeks of daily sales for 1,115 stores located across Germany.

In this project, the Kaggle Rossman challenge is being taken on. The goal is to predict the Sales of a given store on a given day. Model performance is evaluated on the root mean absolute percentage error (MAPE).


The dataset consists of two csv files: store.csv and train.csv

Data Files:

train.csv holds info about each store. store.csv holds the sales info per day for each store.

The repo contains main.py that runs the main script from step one until the end.


## 1. Business Problem.
Rossmann operates over 3,000 drug stores in 7 European countries. Currently, Rossmann store managers are tasked with predicting their daily sales for up to six weeks in advance. Store sales are influenced by many factors, including promotions, competition, school and state holidays, seasonality, and locality. With thousands of individual managers predicting sales based on their unique circumstances, the accuracy of results can be quite varied.

## 2. Solution Strategy
My strategy to solve this challenge was:

Step 01: Data Description: Use statistics metrics to identify data distributions.

Step 02: Feature Engineering: Derive new attributes based on the original variables to better describe the phenomenon that will be modeled.

Step 03: Exploratory Data Analysis: Explore the data to find insights and better understand the impact of variables on model learning.

Step 04: Feature Selection: Selection of the most significant attributes for training the model.

Step 05: Machine Learning Modelling: Machine Learning model training.

Step 06: Hyperparameter Fine Tunning: hoose the best values for each of the parameters of the model selected from the previous step.

Step 07: Convert Model Performance to Business Values: Convert the performance of the Machine Learning model into a business result.



## 3.Machine Learning Model Implementation and performance
At this stage models used : *Linear Regression, *Lasso Regression, *Random Forest Regressor

	                                        Training score                Testing score 
			Linear Regression	0.780750		       0.782392
			
			Lasso Regression	0.780731		       0.782369
			
			Random Forest    	0.993811             	       0.956433





## 4. Conclusion

Acheived MAPE of 5.65% and MAE = $376 showing predictions of model is higly accurate for the sales forecast. Generated insights by EDA and feature importance provide valuable tools to decide the amount of budget and inventory for upcoming sales.
",2,2,2,0,retail,"[linear-regression, machine-learning, mape, python, random-forest, regression, retail, sales]",44-45
Sarmentor,InventoryManagement,,https://github.com/Sarmentor/InventoryManagement,https://api.github.com/repos/InventoryManagement/Sarmentor,"Inventory Management Experimental Setup, developed in NETLOGO and with the use of ""Reverse Auction"" between Client agents and Supply Sellers","# InventoryManagement

- 1 - Inventory Management Experimental Setup
- 2 - ...developed in NETLOGO and with...
- 3 - ...use of ""Reverse Auction"" between Client agents and Supply Sellers.

# Abstract

Multi-Agent Systems (MAS) have been applied to several areas or tasks ranging from energy networks controlling to robot soccer teams. MAS are the ideal solution when they provide decision support in situations where human decision and actions are not feasible to operate the system in control and in real-time. Thus, we present a case study that is related to dynamic simulation of an automatic inventory management system. We provide two types of agents, the clients, and the seller agents. Through a system of communication, the agents exchange messages to fulfill their inventory needs. The client agents trade products in quantities according to their needs and rely on seller agents if other clients in the retailer chain cannot provide the needed items. Additionally, it is expected that the trading between a client and the sellers is done through a reverted type of auction. This case study MAS uses BDI and FIPA-ACL in its implementation resulting in a clear simulation of the system. We expect to provide a comparison between two distinct situations. One with only external transactions with providers, and a situation where both internal and external transactions are allowed. 

# Cite Paper Publication

@misc{sarmento2019inventory,
    title={Inventory Management - A Case Study with NetLogo},
    author={Rui Portocarrero Sarmento},
    year={2019},
    eprint={1905.08041},
    archivePrefix={arXiv},
    primaryClass={cs.MA}
}
",2,2,2,0,retail,"[agent-based-modeling, auctions, inventory-management, netlogo, retail, stock]",44-45
freight-trust,libinterchange,freight-trust,https://github.com/freight-trust/libinterchange,https://api.github.com/repos/libinterchange/freight-trust,X12/EDIFACT EDI Interchange Library,,2,2,2,0,retail,"[asc-x12, data-interchange, edi, edifact, healthcare, retail, supply-chain, x12]",44-45
supervisely-ecosystem,copy-image-tags-to-objects,supervisely-ecosystem,https://github.com/supervisely-ecosystem/copy-image-tags-to-objects,https://api.github.com/repos/copy-image-tags-to-objects/supervisely-ecosystem,Copy selected tags from images to objects of selected classes,"<div align=""center"" markdown>
<img src=""https://user-images.githubusercontent.com/106374579/183398752-8e83aaef-607c-4755-baa1-749f6c617d49.png""/>

# Copy Image Tags To Objects

<p align=""center"">
  <a href=""#Overview"">Overview</a> •
  <a href=""#How-To-Run"">How To Run</a>
</p>


[![](https://img.shields.io/badge/supervisely-ecosystem-brightgreen)](https://ecosystem.supervise.ly/apps/copy-image-tags-to-objects)
[![](https://img.shields.io/badge/slack-chat-green.svg?logo=slack)](https://supervise.ly/slack)
![GitHub release (latest SemVer)](https://img.shields.io/github/v/release/supervisely-ecosystem/copy-image-tags-to-objects)
[![views](https://app.supervise.ly/img/badges/views/supervisely-ecosystem/copy-image-tags-to-objects.png)](https://supervise.ly)
[![runs](https://app.supervise.ly/img/badges/runs/supervisely-ecosystem/copy-image-tags-to-objects.png)](https://supervise.ly)

</div>

## Overview

This app copies selected tags to objects of selected classes (by default all classes and tags are selected). New project is created. 

1. Input project info
2. Define the name of result project
3. Choose how to resolve conflicts with existing tags: skip existing tag, replace it or raise error
4. Choose what tags have to be copied
5. Select classes: tags are assigned to all objects of selected classes, objects of other classes will be copied without changes
6. Output section: after you press `Run` button, progress will appear. Then result project will be displayed.
7. App shuts down automatically

<img src=""https://i.imgur.com/jBVHcxj.png""/>

## How To Run

1. Add app to your team if it is not there
2. Run this app from the context menu of project: `Run App` -> `Copy image tags to objects`

",2,2,2,2,retail,"[retail, supervisely, tags]",44-45
MarvinKweyu,marastore,,https://github.com/MarvinKweyu/marastore,https://api.github.com/repos/marastore/MarvinKweyu,A backpaker's store.,"# [The Traveller's Store](https://www.marvinkweyu.net/projects/the_travellers_store)

The Traveller's Shopping experience


![The Traveller](./screens/marastorereview.gif)
- [The Traveller's Store](#the-travellers-store)
  - [Core features](#core-features)
  - [Setup](#setup)
    - [Setting up a Braintree account](#setting-up-a-braintree-account)
    - [Bare metal](#bare-metal)
      - [Base requirements](#base-requirements)
      - [Running message brokers;](#running-message-brokers)
    - [Your credentials](#your-credentials)
    - [Sample credit card details to test with](#sample-credit-card-details-to-test-with)



An article around the build process can be found on [marvinkweyu/themarastore](https://www.marvinkweyu.net/projects/the_travellers_store)
## Core features
:heavy_check_mark: Viewing items in the shop

:heavy_check_mark: Filtering items by category

:heavy_check_mark: Managing your cart

:heavy_check_mark: Email notifcations on order

:heavy_check_mark: Credit card payment

:heavy_check_mark: Generate PDF invoice on sale and send the invoice to the customer

:heavy_check_mark: Recommendation engine for products that go well with others


## Setup
---

### Setting up a Braintree account

This solution uses [Braintree](https://www.braintreepayments.com/) for payment. Create your account on the [developer](https://sandbox.braintreegateway.com) portal and get the sandbox keys.

Once done, copy `maranomadstore/config/.env.example` to `maranomadstore/config/.env.example` making sure to fill your keys where necessary.

As an overview the `base.py` settings file in the config folder.

```python
# PAYMENTS
BRAINTREE_CONF = braintree.Configuration(
    environment=braintree.Environment.Sandbox,
    merchant_id=env(""BRAINTREE_MERCHANT_ID""),
    public_key=env(""BRAINTREE_PUBLIC_KEY""),
    private_key=env(""BRAINTREE_PRIVATE_KEY""),
)

```

### Bare metal
#### Base requirements

Install the following dependencies **before** running the migrations.

- [Postgresql](https://www.postgresql.org/download/)
- [RabbitMQ](https://www.rabbitmq.com/download.html)
- [Redis](https://redis.io/)
- [Weasyprint](https://weasyprint.org/)


Create a database matching what is in your environment file as specified before.

Example:
```
Database name: marastore
Database user: marastore
Database password: marastore
```
Setup a virtual environment, install requirements , run migrations, load sample data and run the server

```bash
python3 -m venv .venv
source .venv/bin/activate

pip3 install -r requirements/local.txt
python3 manage.py migrate
python3 manage.py loaddata marastoredata.json # optional
python3 manage.py runserver
```

#### Running message brokers;
Launch rabbitMQ on *terminal(2)*
```bash
sudo rabbitmq-server
```
On a different *terminal(3)*, launch celery

```bash
celery -A maranomadstore worker -l info
```

To monitor asynchronous tasks i.e task statistics - *terminal(4)*
```bash
celery -A maranomadstore flower
```
Then access the task list queue on *localhost:5555*


### Your credentials

```
Email: hello@marvinkweyu.net
Password: marvin
```
Access the project via: **[127.0.0.1:8000](127.0.0.1:8000)**



### Sample credit card details to test with

*Credit card numbers*
- 4111111111111111
- 4005519200000004
- 4012000033330026

*Sample CVC* - 123

Key in any data in the future as the expiration date

**Example:**
12/2030

---
",2,2,1,0,retail,"[braintree, e-commerce, payments, retail, traveling]",44-45
affilinet,mobile-sdk-advertiser-ios,affilinet,https://github.com/affilinet/mobile-sdk-advertiser-ios,https://api.github.com/repos/mobile-sdk-advertiser-ios/affilinet,Mobile SDK Advertiser iOS Packages,"# affilinet Mobile SDK Developer Wiki
Customer behavior has dramatically changed in recent years. People are busy, are always out and are constantly on their mobile devices. Therefore tracking on mobile devices is a key part of affilinet tracking efforts.

affilinet’s [Mobile SDK for Advertisers](http://developer.affili.net/mobile-sdk-advertiser/documentation/) is a feature-rich mobile SDK which offers you the full functional package to be successful in your mobile efforts.

The affilinet Advertiser Mobile SDK provides the following functionality:
* Easy integration for iOS and Android based Apps
* Full support of app download tracking (lead rates)
* In-app purchasing (order tracking with sale and basket rates)
* Profiling (PageView, ProductView, CategoryView, Add & RemoveFromCart, CartView, Search & Checkout)
including affilinet's device tracking - a cookieless tracking mechanism supporting web-to-app, app-to-web and app-to-app security context switches.

# How to start?
The integration of affilinet's Mobile SDK is straight forward. Just download the Mobile SDK Package for [iOS](https://github.com/affilinet/mobile-sdk-advertiser-ios) or [Android](https://github.com/affilinet/mobile-sdk-advertiser-android).

Afterwards you can find all necessary documentation how to start [[here](http://developer.affili.net/mobile-sdk-advertiser/documentation/)]

# What else do you need to consider?
Integrating the affilinet Mobile SDK is just a first step, mainly done by development. Please consider also to: 
* Provide publishers with a link to your mobile app in the affilinet login area
* Provide publishers with a link to your mobile website in the affilinet login area
* Provide mobile creatives
* Integrate the web tracking into your mobile website 
",2,2,8,1,retail,"[advertisers, advertising, affilinet-mobile-sdk, basket, ios, lead, mobile, mobile-app, order, profiling, retail, sale, sdk, tracking]",44-45
iam-mhaseeb,EDA-on-Retail-Data,,https://github.com/iam-mhaseeb/EDA-on-Retail-Data,https://api.github.com/repos/EDA-on-Retail-Data/iam-mhaseeb,This repository is implementation of Exploratory Data Analysis on Retail data.,"# EDA-on-Retail-Data
This repository is implementation of Exploratory Data Analysis on Retail data.
<br>
## Getting Started

To use this repo just download the repository, open in jupyter notebook. Start creating something awesome! Good Luck!

### Prerequisites

Things reuired<br>
1. Python3
2. Jupyter Notebook
3. Matplotlib
4. Pandas
5. Other dependencies

## Find it on Kaggle

* [Kaggle](https://www.kaggle.com/iammhaseeb/exploratory-data-analysis-on-retail-data) - The notebook on kaggle.
* [Dataset](https://www.kaggle.com/manjeetsingh/retaildataset) - The dataset used in this notebook.

## Contributing

Feel free to submit pull requests to me.


## Authors

* **Muhammad Haseeb** - *Initial work* - [Muhammad Haseeb](https://github.com/iam-mhaseeb)


## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details
",2,2,1,0,retail,"[eda, exploratory-data-analysis, exploratory-data-visualizations, kaggle, kaggle-dataset, python3, retail, retail-data]",44-45
rgurlek,FAIRforecast,,https://github.com/rgurlek/FAIRforecast,https://api.github.com/repos/FAIRforecast/rgurlek,Automatic interpretable sales forecasting for R,"# FAIRforecast
*Automatic interpretable sales forecasting*

FAIR is a forecasting tool developed to support decision making in a retail environment. It provides multi-step-ahead sales forecasts at the category-store level, which are based on an interpretable and transparent model. These aspects make it an objective tool with which different promotional strategies (scenarios) can be compared and insights can be generated. Additionally, the FAIRforecast package provides plotting functions that generate figures showing the relative strength of the interactions between categories and important promotional variables. For more information, see the [vignette](https://rgurlek.github.io/FAIRforecast/) and [paper](http://home.ku.edu.tr/~oali/Automatic%20Interpretable%20Retail%20Forecasting%20with%20Promotional%20Scenarios.pdf).

# Installation
``` r
# Simple installation
devtools::install_github(""https://github.com/rgurlek/FAIRforecast"")
# Install and build the vignette
devtools::install_github(""https://github.com/rgurlek/FAIRforecast"",
                          build_opts = c(""--no-resave-data"", ""--no-manual""), build_vignettes = TRUE)
```
",2,2,1,0,retail,"[forecasting, interpretability, r, retail]",44-45
UcGeorge,iAccount,,https://github.com/UcGeorge/iAccount,https://api.github.com/repos/iAccount/UcGeorge,"Track the number of each currency in your cash register with this easy-to-use mobile app. At the end of the shift, get an accounting summary for quick end-of-day reconciliation. Customizable for a variety of countries and regions. Save time and reduce errors with the iAccount app.","# iAccount

Welcome to the Cash Register Counter mobile app! This app allows cashiers to easily track the number of each currency in their cash register throughout the work day. At the end of the shift, the app provides an accounting summary to make end-of-day reconciliation a breeze. With this app, cashiers can save time and reduce errors in their accounting process.

## Features

- Easy-to-use interface for quickly adding and subtracting the number of each currency in the cash register
- Accounting summary at the end of the work day to facilitate end-of-day reconciliation
- View reports and transaction history

## Releases

| Release | Description |
|---------|-------------|
| [v1.0-beta](https://github.com/UcGeorge/iAccount/releases/tag/v1.0-beta)    | Initial bata release |

## Contributing

We welcome contributions to the iAccount app! If you have an idea for a new feature or have found a bug, please open an issue on the repository. Pull requests are also welcome.

## Getting Started

These instructions will get you a copy of the project up and running on your local machine for development and testing purposes.

### Prerequisites

- [Flutter](https://flutter.dev/docs/get-started/install)

### Installation

1. Clone the repository: `git clone https://github.com/UcGeorge/iAccount.git`

2. Navigate to the project directory: `cd iAccount`

3. Install the dependencies: `flutter pub get`

4. Run the app: `flutter run`

This will download and install the necessary dependencies for the project and launch the app on your default device.

Note that the project is being developed using the Flutter framework, so you will need to have Flutter installed on your machine in order to run the app. You can find instructions for installing Flutter on the Flutter website (https://flutter.dev/docs/get-started/install).

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE.md) file for details.
",2,2,1,3,retail,"[accounting, cash-register, currency-tracking, financial-management, mobile-app, point-of-sale, retail, small-business, transaction-tracking]",44-45
toretefre,kaching,,https://github.com/toretefre/kaching,https://api.github.com/repos/kaching/toretefre,Django online store with some inventory management ,"# kaching
Simple online store with some inventory management
",2,2,2,0,retail,"[django, retail, store]",44-45
pedrocorma,retail-sales-forecasting,,https://github.com/pedrocorma/retail-sales-forecasting,https://api.github.com/repos/retail-sales-forecasting/pedrocorma,Portfolio project: Machine learning automation project for a large American retailer. Reduced warehouse costs and stock-outs by developing a scalable set of recursive machine learning models that predict the demand in the next 8 days at store-product level based on the data contained in the three-year-history company’s SQL database.,"# Retail sales forecasting

![Esta es una imagen](/01_Documentos/00_Imagenes/featured.png)

- [Introduction](#introduction)
- [Objectives](#objectives)
- [Project results](#project-results)
- [Project structure](#project-structure)
- [Instructions](#instructions)

## Introduction <a name=""introduction""></a>
The client is a large American retailer that desires to implement a sales prediction system based on artificial intelligence algorithms.

- [See a detailed technical explanation of the project here.](https://pedrocorma.github.io/project/0forecasting/)

## Objectives <a name=""objectives""></a>
Developing a set of machine learning models on a three-year-history SQL database to predict sales for the next 8 days at the store-product level using massive modelling techniques.

## Project results  <a name=""project-results""></a>
Warehouse costs and stock-outs have been reduced by developing a scalable set of recursive forecasting machine learning models that predict the demand in the next 8 days at store-product level.

## Project structure <a name=""project-structure""></a>
- :file_folder: 01_Documentos
  - Contains basic project files:
    - `retail.yml`: project environment file.
    - `FaseDesarrollo_PlantillaTransformaciones.xlsx`: support file for designing feature transformation processes.
    - `FaseProduccion_PlantillaProcesos.xlsx`: support file for designing final production script.
  - :file_folder: 00_Imagenes: Contains project images.
- :file_folder: 02_Datos
  - :file_folder: 01_Originales
    - `hipermercado.db`: Original SQL database.
  - :file_folder: 02_Validacion
    - `validacion.csv`: Sample extracted from the original dataset at the beginning of the project in order to be used to check the correct performance of the model once it is put into production.
    - `DatosParaProducción.csv`: Support file for the execution of recursive forecasting models.
  - :file_folder: 03_Trabajo
    - This folder contains the datasets resulting from each of the stages of the project (data quality, exploratory data analysis, feature transformation...).
- :file_folder: 03_Notebooks
  - :file_folder: 01_Funciones
    - `FuncionesRetail.ipynb`: Contains all custom functions used in the training and execution of models.
  - :file_folder: 02_Desarrollo
    - `01_Set Up.ipynb`: Notebook used for the initial set up of the project.
    - `02_Calidad de Datos.ipynb`: Notebook detailing and executing all data quality processes.
    - `03_EDA.ipynb`: Notebook used for the execution of the exploratory data analysis and which collects the business insights found.
    - `04_Transformacion de datos.ipynb`: Notebook that details and executes the data transformation processes necessary to prepare the features for input into the models.
    - `05_Preselección de variables.ipynb`: Notebook used to desing the feature selection process.
    - `06_Modelización para Regresion.ipynb`: Notebook for modelling the predictive forecasting models. Model selection, hyperparameterisation, evaluation. Designed functios for individual modelling, massive modelling and recursive massive modelling.
    - `07_Preparacion del codigo de produccion.ipynb`: Notebook used to compile all the quality, transformation as well as the final models, execution and retraining processes, with the aim of creating the final retraining and execution pipes that condense all the aforementioned processes.
  - :file_folder: 03_Sistema
    - This folder contains the files (app script, production script, models, functions ...) used in the models deployment.
- :file_folder: 04_Modelos
  - `lista_modelos_retail.pickle`: contains all developed models.
  - `ohe_retail.pickle`: one hot encoding pipe.
  - `te_retail.pickle`: target encoding pipe.
- :file_folder: 05_Resultados
  - `FuncionesRetail.py`: Python script that contains all custom functions needed when training or executing the models.
  - `codigo_ejecucion.py`: Python script to execute the models and obtain the results.
  - `codigo_reentrenamiento.py`: Python script to retrain the models with new data when necessary.
  - `lista_modelos_retail.pickle`: contains all developed models.
  - `variables_finales.pickle`: List containing the names of the finally selected features.

## Instructions  <a name=""instructions""></a>
The project should be run using exactly the same environment in which it was created.

- Project environment can be replicated using 'retail.yml' file which was created during the set up phase of the project. It can be found in the folder '01_Documentos'.
- Copy 'retail.yml' file to the directory and using the terminal or anaconda prompt execute:
    > conda env create --file retail.yml --name project_name

By other hand, remember to update the `project_path` variable of the notebooks to the path where you have replicated the project.
",2,2,2,0,retail,"[data-science, forecasting, machine-learning, recursive-forecasting, retail, supervised-learning]",44-45
loandangnt,women-clothing,,https://github.com/loandangnt/women-clothing,https://api.github.com/repos/women-clothing/loandangnt,,"# Topic Modeling Customers' Text Reviews on Women Clothings
## *What do customers complain about the product?*
![banner](./visualization/ft1.jpg)

## Overview
This project aims to **extract meaning from customer' text reviews** to identify what issues that customers dislike on a particular product. Retailers can use the insights to prioritize improvement on the most frequently complaining issues. 

The model produces a probability weight map coresponding to buckets of issue **for each negative review text** (a negative review text in this analysis is defined as low rating lower than 3 in a scale of 5). Out of the **k buckets** of issues extracted from the model, each text will be assigned an *issue* that have the **highest probability weight**.

I run 2 most popular used topic modeling algorithm and choose Latent Dirichlet Allocation (LDA) as the best quality model for this analysis. Ultimately, validating unsupervising model is extreme difficult, especially in NLP. Current evaluations of topical quality rely heavily on experts eaminations, i.e. human eyes validation involved. Based on human reading and validation, this model achieved 57% accuracy, and 77% accuracy on 90% percentile of probability weight. This performance is due to the model's inability to 'understand' the ironicallity and different style of languege expression of different customers, as well as such a narrow subject of this dataset, making it challenging to avoid topical overlapping.

### Business Questions
There are many possible exploratory text analysis, supervised and unsupervised model techiniques on this dataset. Some business questions in scope of this analysis are:

1. Based on review and rating, what do customers **like and dislike** about a clothing item?
Solutions: descriptive statistics.

2. Regarding the above preferences, are there any difference between **category** and **department**?
Solutions: Bag of words, Wordcloud visualization.

3. How to **prioritize which issue for improvement** for a clothing item?
Solution: Topic modeling using LDA on text reviews.

4. How to choose which **product** to improve first?
Solution: Rating statistics, LDA output model accuracy (more accurate prediction is prioritized)


### High Level Approach
**1. Data Preprocessing**:
- Clean: remove unused characters/words (punctuations, tag, special characters and digits, stopwords)
- Tokenize: split text sentences into single words.
- POS tag (an intermediate step to include only types of words that are needed, which are NOUN, VERB, ADJ)
- Lemmatize: changes variation of words into its root (e.g. *go, went, gone, going* into *go*).

**2. Data Exploratory Analysis**: answer the first two business questions in the Business Question section above and to give an idea why the topic modeling task could be valuable.

**3. Data Modeling**

**4. Model Evaluation and Selection**

**5. Business Application of Model Output**


### Data Sourcing
This is a Kaggle dataset. Link: https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews:

This dataset includes **23486 rows and 10 feature variables**. Each row corresponds to a customer review, and includes the variables:
 Column Name | Description |
|-|-|
|1. Clothing ID| Integer Categorical variable that refers to the specific piece being reviewed.
|2. Age| Positive Integer variable of the reviewers age.
|3. Title| String variable for the title of the review.
|4. Review Text| String variable for the review body.
|5. Rating| Positive Ordinal Integer variable for the product score granted by the customer from 1 Worst, to 5 Best.
|6. Recommended IND| Binary variable stating where the customer recommends the product where 1 is recommended, 0 is not recommended.
|7. Positive Feedback Count| Positive Integer documenting the number of other customers who found this review positive.
|8. Division Name| Categorical name of the product high level division.
|9. Department Name| Categorical name of the product department name.
|10. Class Name| Categorical name of the product class name.

### Data Exploratory Analysis
https://nbviewer.jupyter.org/github/loandangnt/women-clothing/blob/master/women_clothing_data_exploration.ipynb

#### 1. Based on review and rating, what do customers like and dislike about a clothing item?
![image](./visualization/wc_pos_vs_neg_title.png)

From the visuals, we can see that people generally don't mention what make them like. They only express the **general look** or **how it feels** in general. For example, top words in **positive reviews** are *gorgegous, comfortable, comfy, fit, soft*.

In contrast, **negative reviews** mention directly in the Title what make the buyers disatisfied with their purchase. Top keywords are largely about **fit, small, large, fabric, color, quality, etc.**

In addition, the word **picture, expect** appear in the top 50 common words. This means the gap between **what buyers expected** from the information online and **what they actually experienced** when receiving the item was large enough to make they so disappointed.

Given this insight, retailers are better provide precise information including *sizing information, product details, real images* that matches as much as possible to the reality to avoid disatisfaction. This is a common challenge in the apparel e-commerce industry. THe problem is that customers are ultimately attracted to gorgeous images online. They make purchase and get disappointed, then, leave negative comments and are likely to never come back! This actually the worst for any retailers who want to build a long-term business.

So, let's take a look at the Review Text field to analyze more details.


![image](./visualization/venn_diagram.png)

There are overlaped words between positive reviews and negative reviews. Out of total **5,460** words in cleaned negative reviews, there are **3,861** words overlap with those in cleaned positive reviews (**70.7%**). Let's take a closer look at the overlapping issue.


![image](./visualization/top_words.png)

Both groups care about some common areas, such as how the they **fit, color, size, material, fabric**, and other details of the clothing item including **sleeve, waist, arm, button, chest, and hip**.

Another interesting pattern in the negative reviews is that the word **small, fabric, material** seems to be more common issues in negative texts than in positive ones. These are things that easily let people down when they receive the items.

These insights strenthen my aprroach to dive depper into only the negative reviews and analyze if there are any cluster of issues (groups of issues) that seller can improve their products. Learning and improving from failures has been a good approach to increase customer satisfaction, and compete in the marketplace.

Therefore, my next step of the analysis is the topic modeling task using Latent Class Allocation LDA and Semantic Class Analysis. I then decide which model yields the most quality topics and map the model result back to the original dataset.


#### 2. Are there any difference in rating and customers' preferences between category and department?

![image](./visualization/rating_statistics_by_class_dept.png)

There are interesting differences in rating statistics between clothing classes:

- Group 1: Classes in **Dresses** and **Tops** are the most frequently bought and also received a large number of bad reviews. That was why rating average of these classes/departments are lower than overall rating average (lower than 4.19, except *Fine Gauge*). They are big bubbles on the lower right corner of the above graph.

- Group 2: **Bottoms** products including *Jeans, Pants, Shorts, Skirts*, and **Intimate** products including *Layering and Lounge* have higher average rating, and lower rating standard deviation. This is understandable because they have smaller customer base, thus, smaller range of opinions than the first group.

- Group 3: The third group are small classes that have high rating average and high rating standard deviation. They are **Jackets** and **Intimate** products including *Intimates, Sleep, Legwear*. As they have very small number of reviews, rating values can be influenced by some extreme opinions and vary more than other classes.

Noting the difference, it is worth looking at customer insights into each classes of clothing. Knowing which classes perform worse and which better is just one part of the journey. The goal is to address what make the difference, i.e. where the issues are; which products within lesser performers contribute the most the overall statistics; what are wrong with those products. Again, the topic modeling applied to each review item can help answer the questions.


![image](./visualization/wordcloud_class_first9.png)
![image](./visualization/wordcloud_class_last9.png)

Here, we explore further in the Review Text field with a separate view for each Class Name. The several wordclouds show that the overlaping problem exists in subsets of negative reviews data. Top issues are two of the four issues: **size, fit, color, fabric**. This can pose a challenge for topic modeling accuracy. Noting this challenge, we can compare these wordclouds with model output to partly evaluate its accuracy.

Besides, each Class Name has each unique issues. For example, customers buying **Intimates, Swim** products concern about *cup*, buying **Dresses, Pants, Shorts** are sensitive about *waist*, buying **Outerwear** cares about the *button*, buying **Lounge** could be disatisfied with issues like *wash, soft,thin*, buying **Sweaters, Fine Gauge, Jackets** cares about *sleeve*, buying **Skirts** cares about *hip, waist*, buying **Sleep** products have issues *robe, thin* issues, buying **Trend** products cares about the fit and details such as *bust, waist, cut*, buying **Jeans, Legwear** would be more satisfied if the item is better at the *waist, stretch, wash*, buying **Layering** cares about *arm*. 

### Topic Modeling
#### Final model output
![image](./visualization/topic_percentage.png)

Size is a prominant issue. Therefore, for products being complained small/large in size, the seller should provide more precise sizing information to customers and guarantee that the products are made with appropriate size as listed.


![image](./visualization/topic_by_class_name.png)

Compare LDA model output to WordCloud exploratory analysis to see if the LDA topic modeling makes sense.

#### Model output validation
After going through some sample and validate the reliability of the model prediction by 'eyes'. 27/57 reviews are correctly labeled (57%). For cases with dominant topic percentage (weight) is >= 0.9, 10/13 cases are correctly labeled (77%).

I also find out that there are some cases where the model predicts incorrectly:\
    - Reviewers mention both on some aspects that they are and aren't satisfied with. The model picks keywords that were positively mentioned.\
    - People don't really express their opinion directly. They can use comparison, metaphors to say to indirectly express what they mean. For example, 'the dress looks like a nightgown on me' (i.e. sizing issue - the dress is too big for this customer).\
    - Sentences using negative form with **not** is also a challenge for the model to correctly articulate what the customers really mean.\
    - Some other cases are ones that show disappointment indirectly or in a ironic way. I haven't found any solution for these problem and would accept model inaccuracy caused by these problems.
### Apply LDA Model Result
#### *How to choose which product to adress first?*
- Brainstorming: 
Question: What are top purchase (top 20%) and negatively reviewed (Rating average (mean) < 3). Analyze what issues with these products, and send reccommendation to retailers on how they can improve their product.


I found out that there should be a method to choose which products need to be prioritized for improvement. I thought of:

+ Method 1: Choose top 80% received largest number of reviews (nlargest_count) (dataset 1), then get products with average rating < 3.
This is unreasonable because the rating_mean of dataset 1 is around 4.19 (std ~ 0.3). There really is only one product from dataset 1 that has average rating <3.


+ Method 2: Choose products with average rating < 3, then get top 80% received largest number of reviews (nlargest_count). This is also unfeaseable because the final selected products still have very small number of reviews (largely, from 1 to 2 reviews).

SO, we need another method:

What we want is to select products receiving a certain number of negative reviews. These number of reviews should be large enough to show that retailers needs to care for improvement, but not some personal negative reviews (a very small number of negative reviews in comparison with the total number of reviews/total number of purchases).

On the dataset, a product would receive a average of 19.5 reviews (Rating_count), and have an average rating value of 4.19 (Rating_mean). Here, I'm going to choose products that have Rating_count < 19.5 and Rating_mean < 4.19.
![image](./visualization/choose_product.png)

There are total of 84 products that needed to be on the priority list with topic results. For example:
Product ID 1087 has Rating_count = 129 (it recevied 129 reviews), R_mean = 3.75 (it had Rating average of 3.75), and also received 29 negative reviews (Rating <=2). The topic modeling ouput provide issues information as follow:

|dominant_topic_theme |Clothing_ID
|-|-|
|Fabric/fit/body	|13
|Color/fabric	|8
|Color/material	|8
|Fit/fabric/price	|6
|Shoulder/arm/length/wide/fabric	|5
|Size/small/large	|5
|Wash/dry/fit/stretch/fabric	|2

This provide a summary view about issues related to a product, which helps buyers better/quicker to response to customer feedbacks.

### Next Steps
1. Further improvement for topic model accuracy includes:
- Modifying the vocabulary to include negation forms, acronyms and multi-word phrases
- Deal with topical overlapping
- Removing nonsensical topics
- Conduct parameter search
- Compare with more topic modeling/text summarization techniques.

2. Use model to predict a new negative review text.

",2,2,1,0,retail,"[lda, nlp, retail, topic-modeling]",44-45
BenClementt,OpenPOS,,https://github.com/BenClementt/OpenPOS,https://api.github.com/repos/OpenPOS/BenClementt,OpenPOS - the point of sale system that streamlines your business.,"# OpenPOS
OpenPOS - the point of sale system that streamlines your business.
",2,2,1,0,retail,"[barcode, card-machine, point-of-sale, pos, retail]",44-45
osa-decentralized,forecastonishing,osa-decentralized,https://github.com/osa-decentralized/forecastonishing,https://api.github.com/repos/forecastonishing/osa-decentralized,"An adaptive selector for short-term forecasting of multiple time series. For each time series, it finds the best method from a pool of candidates based on their past performance.","[![Build Status](https://travis-ci.org/osahp/forecastonishing.svg?branch=master)](https://travis-ci.org/osahp/forecastonishing)
[![codecov](https://codecov.io/gh/osahp/forecastonishing/branch/master/graph/badge.svg)](https://codecov.io/gh/osahp/forecastonishing)
[![Maintainability](https://api.codeclimate.com/v1/badges/62ba0c41d25448bdbaac/maintainability)](https://codeclimate.com/github/osahp/forecastonishing/maintainability)

# forecastonishing

## What is it?
This repo contains easy-to-use tools for forecasting. Currently, the list of provided utilities consists of:
* On-the-fly selector, an adaptive selector that can leverage abilities of robust, yet extremely simple methods such as moving average. More details can be found in [a tutorial](https://github.com/osahp/forecastonishing/blob/master/docs/on_the_fly_selector_demo.ipynb);
* Some auxiliary classes and functions that can make forecasting easier.

## How to install the package?
To install the package in a virtual environment named `your_virtual_env`, run this from your terminal:
```
cd path/to/your/destination
git clone https://github.com/osahp/forecastonishing
cd forecastonishing
source activate your_virtual_env
pip install .
```
",2,2,2,0,retail,"[adaptive-learning, adaptive-selection, ewma, on-the-fly, retail]",44-45
Asikpalysik,Market-Basket-Analysis,,https://github.com/Asikpalysik/Market-Basket-Analysis,https://api.github.com/repos/Market-Basket-Analysis/Asikpalysik,Market basket analysis with Apriori algorithm,"# Market-Basket-Analysis
Market basket analysis with Apriori algorithm

The retailer wants to target customers with suggestions on itemset that a customer is most likely to purchase .I was given dataset contains data of a retailer; the transaction data provides data around all the transactions that have happened over a period of time. Retailer will use result to grove in his industry and provide for customer suggestions on itemset, we be able increase customer engagement and improve customer experience and identify customer behavior. I will solve this problem with use Association Rules type of unsupervised learning technique that checks for the dependency of one data item on another data  item.

 
### Introduction

Association Rule is most used when you are planning to build association in different objects in a set. It works when you are planning to find frequent patterns in a transaction database. It can tell you what items do customers frequently buy together and it allows retailer to identify relationships between the items. 

### An Example of Association Rules

Assume there are 100 customers, 10 of them bought Computer Mouth, 9 bought Mat for Mouse and 8 bought both of them.
- bought Computer Mouth => bought  Mat for Mouse
- support = P(Mouth & Mat) = 8/100 = 0.08
- confidence = support/P(Mat for Mouse) = 0.08/0.09 = 0.89
- lift = confidence/P(Computer Mouth) = 0.89/0.10 = 8.9
This just simple example. In practice, a rule needs the support of several hundred transactions, before it can be considered statistically significant, and datasets often contain thousands or millions of transactions.

### Strategy

- Data Import
- Data Understanding and Exploration
- Transformation of the data – so that is ready to be consumed by the association rules algorithm
- Running association rules
- Exploring the rules generated
- Filtering the generated rules
- Visualization of Rule 


### Dataset Description

- File name: Assignment-1_Data
- List name: retaildata
- File format: . xlsx
- Number of Row: 522065
- Number of Attributes: 7

	- BillNo: 6-digit number assigned to each transaction. Nominal.
	- Itemname: Product name. Nominal.
	- Quantity: The quantities of each product per transaction. Numeric.
	- Date: The day and time when each transaction was generated. Numeric.
	- Price: Product price. Numeric.
	- CustomerID: 5-digit number assigned to each customer. Nominal.
	- Country: Name of the country where each customer resides. Nominal.

<img width=""492"" alt=""image"" src=""https://user-images.githubusercontent.com/91852182/145270162-fc53e5a3-4ad1-4d06-b0e0-228aabcf6b70.png"">

### Libraries in R

First, we need to  load required libraries. Shortly I describe all libraries.

- arules - Provides the infrastructure for representing, 
manipulating and analyzing transaction data and patterns (frequent itemsets and association rules).
- arulesViz - Extends package 'arules' with various visualization.
techniques for association rules and item-sets. The package also includes several interactive visualizations for rule exploration.
- tidyverse - The tidyverse is an opinionated collection of  R packages designed for data science.
- readxl - Read Excel Files in R.
- plyr - Tools for Splitting, Applying and Combining Data.
- ggplot2 - A system for 'declaratively' creating graphics, based on ""The Grammar of Graphics"". You provide the data, tell 'ggplot2' how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details.
- knitr - Dynamic Report generation in R.
- magrittr- Provides a mechanism for chaining commands with a new forward-pipe operator, %>%. This operator will forward a value, or the result of an expression, into the next function call/expression. There is flexible support for the type of right-hand side expressions.
- dplyr - A fast, consistent tool for working with data frame like objects, both in memory and out of memory.
- tidyverse - This package is designed to make it easy to install and load multiple 'tidyverse' packages in a single step.

<img width=""481"" alt=""image"" src=""https://user-images.githubusercontent.com/91852182/145270210-49c8e1aa-9753-431b-a8d5-99601bc76cb5.png"">

### Data Pre-processing

Next, we need to upload Assignment-1_Data. xlsx to R to read the dataset.Now we can see our data in R. 

<img width=""255"" alt=""image"" src=""https://user-images.githubusercontent.com/91852182/145270229-514f0983-3bbb-4cd3-be64-980e92656a02.png"">
<img width=""476"" alt=""image"" src=""https://user-images.githubusercontent.com/91852182/145270251-6f6f6472-8817-435c-a995-9bc4bfef10d1.png"">
 
After we will clear our data frame, will remove missing values.

<img width=""285"" alt=""image"" src=""https://user-images.githubusercontent.com/91852182/145270286-05854e1a-2b6c-490e-ab30-9e99e731eacb.png"">

To apply Association Rule mining, we need to convert dataframe into transaction data to make all items that are bought together in one invoice will be in one row. Below lines of code will combine all products from one BillNo and Date and combine all products from that BillNo and Date as one row, with each item, separated by (,)

<img width=""311"" alt=""image"" src=""https://user-images.githubusercontent.com/91852182/145270333-b7848fce-53d9-4486-b2ae-c331f13b4275.png"">

We don’t need BillNo and Date, we will make it as Null.
Next, you have to store this transaction data into .csv 

<img width=""311"" alt=""image"" src=""https://user-images.githubusercontent.com/91852182/145270368-787a1721-8d93-4f69-8369-da3a70082e95.png"">


This how should look transaction data before we will go to next step.

<img width=""311"" alt=""image"" src=""https://user-images.githubusercontent.com/91852182/145270884-ca3d6d47-708f-4ab6-bc1d-0e17b912e1f3.png"">

<img width=""482"" alt=""image"" src=""https://user-images.githubusercontent.com/91852182/145270942-14bfed99-473e-444c-9c14-a215c2957bee.png"">


At this step we already have our transaction dataset, and it shows the matrix of items which bought together. We can’t see here any rules and how often it was purchase together. Now let’s check how many transactions we have and what they are. We will have to have to load this transaction data into an object of the transaction class. This is done by using the R function read.transactions of the arules package. Our format of Data frame is basket. 

<img width=""482"" alt=""image"" src=""https://user-images.githubusercontent.com/91852182/145270981-84b76556-380b-4a32-a2ee-465d192141e8.png"">
 
Let’s have a view our transaction object by summary(transaction)

<img width=""160"" alt=""image"" src=""https://user-images.githubusercontent.com/91852182/145271000-33fe3da8-6517-4d7a-a844-017022047ff6.png"">

We can see 18193 transactions (rows) and 7698 items (columns). 7698 is the product descriptions and 18193 transactions are collections of these items.

<img width=""498"" alt=""image"" src=""https://user-images.githubusercontent.com/91852182/145271025-9acd295d-6cef-44eb-8e5b-8ff12ddd798f.png"">

The summary gives us some useful information:
- Density tells the percentage of non-zero cells in a sparse matrix. In other words, total number of items that are purchased divided by a possible number of items in that matrix. You can calculate how many items were purchased by using density: 18193x7698x0.002291294=337445
- Summary will show us most frequent items.
- Element (itemset/transaction) length distribution: It  will gave us how many transactions are there for 1-itemset, 2-itemset and so on. The first row is telling you a number of items and the second row is telling you the number of transactions.
For example, there is only 1546 transaction for one item, 860  transactions for 2 items, and there are 419 items in one transaction which is the longest.

Let’s check item frequency plot, we will generate an itemFrequencyPlot to create an item Frequency Bar Plot to view the distribution of objects based on itemMatrix (e.g., >transactions or items in >itemsets and >rules) which is our case.

<img width=""267"" alt=""image"" src=""https://user-images.githubusercontent.com/91852182/145271096-4279c764-0d0c-41dc-87fc-0b13b217f566.png"">

<img width=""267"" alt=""image"" src=""https://user-images.githubusercontent.com/91852182/145271132-f3bf1374-06b8-4f71-b810-dbb098f6963c.png"">

<img width=""490"" alt=""image"" src=""https://user-images.githubusercontent.com/91852182/145271187-3f6023db-3820-4083-a6e9-cd74020acc70.png"">

In itemFrequencyPlot(transaction,topN=20,type=""absolute"") first argument - our transaction object to be plotted that is tr. topN is allows us to plot top N highest frequency items. type can be as  type=""absolute"" or type=""relative"". If we will chouse absolute it will plot numeric frequencies of each item independently. If relative it will plot how many times these items have appeared as compared to others. As well I made it in colure for better visualization.

### Generating Rules
 
Next, we will generate rules using the Apriori algorithm. The function apriori() is from package arules. The algorithm employs level-wise search for frequent itemsets. Algorithm will generate frequent itemsets and association rules. We pass supp=0.001 and conf=0.8 to return all the rules that have a support of at least 0.1% and confidence of at least 80%. We sort the rules by decreasing confidence and will check summary of the rules.

<img width=""462"" alt=""image"" src=""https://user-images.githubusercontent.com/91852182/145271236-3a82733a-216f-4db2-9f60-9b43c2016ea4.png"">
 
The apriori will take (transaction) as the transaction object on which mining is to be applied. parameter will allow you to set min_sup and min_confidence. The default values for parameter are minimum support of 0.1, the minimum confidence of 0.8, maximum of 10 items (maxlen).

<img width=""462"" alt=""image"" src=""https://user-images.githubusercontent.com/91852182/145271269-4565e292-212a-47cb-8bc1-5bfe7f1819a8.png"">

Summary of rules give us clear information as:
- Number of rules: 97267
- The distribution of rules by length: a length of 6 items has the most 33296 and length of 2 items has lowest number of rules 111
- The summary of quality measures: ranges of support, confidence, and lift.
- The information on data mining: total data mined, and the minimum parameters we set earlier

Now, 97267 it a lot of rules. We will identify only top 10.

<img width=""273"" alt=""image"" src=""https://user-images.githubusercontent.com/91852182/145271351-6977af6b-2d7a-4a81-a3c1-f160722b2897.png"">

<img width=""478"" alt=""image"" src=""https://user-images.githubusercontent.com/91852182/145271400-b74ec578-5bf5-47fd-a564-407eacfb6b43.png"">


Using the above output, you can make analysis such as:
- 100% of the customers who bought 'ART LIGHTS ' also bought 'FUNK MONKEY'.
- 100% of the customers who bought 'BILLBOARD FONTS DESIGN ' also bought 'WRAP'.
We can limit the size and number of rules generated. we can set parameter in Apriori. If we want stronger rules, we must to increase the value of conf. and for more extended rules give higher value to maxlen. 

### Visualizing Association Rules

We have thousands of rules generated based on data, we will need a couple of ways to present our findings. We will use ItemFrequencyPlot to visualize association rules.

#### Scatter-Plot:

<img width=""354"" alt=""image"" src=""https://user-images.githubusercontent.com/91852182/145271541-b9e4eb67-68c4-499b-b5c8-37f27bc27347.png"">

A straight-forward visualization of association rules is to use a scatter plot using plot() of the arulesViz package. It uses Support and Confidence on the axes. In addition, third measure Liftis used by default to color (grey levels) of the points.

<img width=""228"" alt=""image"" src=""https://user-images.githubusercontent.com/91852182/145271713-e995b95e-381a-4e00-83b9-d197b5e4a7a5.png"">
<img width=""201"" alt=""image"" src=""https://user-images.githubusercontent.com/91852182/145271728-e172716c-a3c7-4485-8c00-b5da2435f8b2.png"">



#### Interactive Scatter-Plot:

We can have a look for each rule (interactively) and view all quality measures (support, confidence and lift). 

<img width=""197"" alt=""image"" src=""https://user-images.githubusercontent.com/91852182/145271752-d5d0406e-a42d-4cb9-9c34-db14e51556e9.png"">

<img width=""223"" alt=""image"" src=""https://user-images.githubusercontent.com/91852182/145271774-70479173-6b0e-45bd-b403-b99af9747e31.png"">

<img width=""224"" alt=""image"" src=""https://user-images.githubusercontent.com/91852182/145271779-fa20c370-7e18-4a1f-b70e-47c03aa42033.png"">

#### Graph - Based Visualization and Group Method:

Graph plots are a great way to visualize rules but tend to become congested as the number of rules increases. So, it is better to visualize a smaller number of rules with graph-based visualizations.	We can see as well group method for top 10 items.

<img width=""215"" alt=""image"" src=""https://user-images.githubusercontent.com/91852182/145271807-c6328bf7-2b46-45cf-acec-2f4b858af85b.png"">
<img width=""217"" alt=""image"" src=""https://user-images.githubusercontent.com/91852182/145271812-d9ce223b-337e-44c6-bb8c-c75c77d4ee64.png"">

   

  
### Conclusion
Based on the results of these calculations can be used as a recommendation for retail owners to arrange the arrangement of product catalogs and take strategic steps to improve product marketing.. By utilizing the association rules which are discovered as a result of the analyses, the retailer can apply effective marketing and sales promotion strategies, he will be able increase customer engagement and improve customer experience and identify customer behavior.

### Attached files 

- Assigment Part1 Retailer.R
- Assignment-1_Data.xlsx
",2,2,1,0,retail,"[apriori-algorithm, market-basket-analysis, online-shop, r, retail]",44-45
pianomister,SMAR,,https://github.com/pianomister/SMAR,https://api.github.com/repos/SMAR/pianomister,A shelf management app for Android smart glasses using augmented reality.,"# SMAR
A shelf management app for Android smart glasses using augmented reality.

### About SMAR
SMAR stands for _Shelf Management Augmented Reality_ and supports workers in retail stores with shelf management – receiving and registering incoming goods, filling up shelves, finding products and their locations, and inventury taking.
The app for SMAR is built on Android and optimized for smart glasses, so that the user still has free hands for work.

SMAR was built on Vuzix M100 smart glasses, can be controlled by voice or touch input on the smart glasses, and utilizes the included barcode scanner to recognize products.

This app should be seen as an MVP or prototype to test the capabilities of smart glasses in practice.

### Backend and database
To use SMAR, it is necessary to run the [SMAR Web Administration](https://github.com/pianomister/SMAR-Web-Administration) backend in addition. It allows to add and edit shelf layouts and products, and manage deliveries and stock.
",1,1,3,0,retail,"[android, android-glasses, augmented-reality, retail, shelf-management, shelf-navigation]",44-45
stmunees,Taneira-Titan-Recommendation,,https://github.com/stmunees/Taneira-Titan-Recommendation,https://api.github.com/repos/Taneira-Titan-Recommendation/stmunees,A Custom CNN Architecture based Recommendation System for Taneira,"# Taneira-Titan-Recommendation
A Custom Convolutional Neural Network Architecture based Recommendation System for [Taneira](https://www.taneira.com), [Titan](https://www.titancompany.in).

*This solution was one among 35 selected projects out of 250 projects showcased during the company's ""Technology Day"".*

## Disclaimer
The dataset (images of sarees, SKU details, and attributes file) are proprietary data of Titan Company Limited, and therefore have been omitted from this project repository. They have also been removed from the results images as discussed below.

## Objective
- To design a deep learning model to recognise patterns in [sarees](https://en.wikipedia.org/wiki/Sari).
- By recognising patterns in sarees the model should be able to deliver similar sarees from database, which can be used in various use cases.

## Approach
- We would require the model to predict each characteristic feature/attributes (eg: zari, pallu, colour etc) individually and use the results collectively.

## Attributes
- A saree has multiple attributes, Origin, Weft, Warp, Colour, Loom, Zari, Pallu amongst others.

## Assumptions
- This POC we will look into two attributes that have been implemented- Zari (Present or Not) and Pallu (Contrast or Running).
- The user is allowed to enter the colour of the saree they desire.

## Results

#### Zari Prediction
<p align=""center"">
  <img src=Results/result_zari.png />
</p>

One can see here in 6th prediction above, where the database entry was made wrong, although the prediction was accurate.

#### Pallu Prediction
<p align=""center"">
  <img src=Results/result_pallu.png />
</p>

#### Taneira Product used for finding similar products.
<p align=""center"">
  <img src=Results/taneira_rec.png />
</p>

#### Non-Taneira Product used for finding similar products.
##### 01
<p align=""center"">
  <img src=Results/non_taneira_rec_1.png />
</p>

##### 02
<p align=""center"">
  <img src=Results/non_taneira_rec_2.png />
</p>

",1,1,1,0,retail,"[cnn, cnn-keras, convolutional-neural-networks, deep-learning, recommendation, recommendation-system, retail]",44-45
comonetizer,RetailEngineOS,comonetizer,https://github.com/comonetizer/RetailEngineOS,https://api.github.com/repos/RetailEngineOS/comonetizer,Universal Service Retail Engine PoC,"# Retail Engine - Retail Services & Monetization

collaboration engine / social monetization protocol - PoC

![Web colab PoC](ai-colab-engine-PoC-animated.gif)

## innovation engine install

- download and install node & npm @ https://nodejs.org/
- clone ai player app @ https://github.com/aibase/aigitops/
- open a free basic GCP | Azure | AWS cloud account
- create a realtime db or spreadsheet for users (eg: Firebase or Google Sheets)
- create a realtime db or spreadsheet for data (eg: Firestore or Google Sheets)
- (coming soon: run your own local or cloud AI colab engine)

```
npm install
```

### Compiles and hot-reloads for development

```
npm run serve
```

### Compiles and minifies for production

```
npm run build
```

### Run your tests

```
npm run test
```

### Customize ai engine

Edit src/ai-config.json
",1,1,2,0,retail,"[ai, engine, os, retail, service, universal, webapp]",44-45
NielsMasdorp,LvBuddy,,https://github.com/NielsMasdorp/LvBuddy,https://api.github.com/repos/LvBuddy/NielsMasdorp,[Not maintained] Android applicatie voor Bol.com verkopers om de LVB voorraad te managen en berichten te krijgen wanneer er bestellingen/koopblok statussen veranderen.,"# LvBuddy [NOT MAINTAINED]

![Alt text](/screenshots/screenshot2.png?raw=true ""Voorraad"")
![Alt text](/screenshots/screenshot1.png?raw=true ""Instellingen"")

## Functionaliteit:

- Zie je huidige LVB voorraad
- Zie per product in je LVB voorraad of jij het koopblok hebt (te zien aan de gouden medaille)
- Stuurt notificaties als je voorraad/koopblok status verandert
- Optie om notificaties naar een Telegram (groeps)chat te sturen
- Je Bol.com credentials worden versleuteld opgeslagen op je telefoon en verlaten je telefoon nooit
- Versleutelde verbinding met Bol.com (SSL en Certificate Transparency check)
- Donker thema
- Kopieer snel een EAN door lang op een product te drukken
- Zie de volledige product naam door op een product te drukken

## Benodigdheden voor het gebruik van deze app (tonen LVB voorraad):

- Android apparaat met min. Android 8.0
- Bol.com Retailer V3 client id
- Bol.com Retailer V3 client secret

Ga naar je winkel instellingen in jouw Bol.com account, kies vervolgens API Instellingen (onder ""Diensten"") en maak je credentials aan.

## Benodigdheden voor het laten zien van je koopblok status

- Winkel naam (moet precies overeenkomen)

## Benodigdheden voor het sturen van notificaties in Telegram

In plaats van systeem notificaties kan LvBuddy ook notificaties in een Telegram chat naar keuze sturen, dit is bijvoorbeeld
handig als je met meerdere mensen het zelfde bol.com verkopers account beheert en wilt dat iedereen updates krijgt. 
Je hebt hiervoor nodig:

- Telegram bot token
- Telegram chat id

Om deze informatie te verkrijgen: open Telegram en start een gesprek met `@BotFather` om een nieuwe Telegram bot te maken.
Zodra deze gemaakt is (geef hem een naam en eventueel een plaatje) krijg je een token, dit is het token van jouw bot.
Disable de privacy settings van je bot door het `/setprivacy` command naar `@BotFather` te sturen.
Hierdoor kan jouw bot berichten lezen in de chats waar jij hem in toevoegt.

Maak een nieuwe groepschat aan op Telegram en voeg je bot toe met de naam die je de bot gegeven hebt.
Ga vervolgens naar de instellingen van de chat (klik op de naam) en maak je bot administrator in de chat.
Stuur zelf een bericht in de chat, je kunt vervolgens achter je chat id komen door in je browser de volgende URL aan te roepen: `https://api.telegram.org/{botToken}/getUpdates`,
vervang hierbij `{botToken}` met je eigen token.
Als het goed is zie je dan je eigen bericht voorbij komen en daar kun je ook het id van de chat vinden:

```
{
    ""update_id"": 000000,
    ""message"": {
        ""message_id"": 00000,
        ""from"": {
            ""id"": 000000,
            ""is_bot"": false,
            ""first_name"": ""Jouw naam"",
            ""username"": ""-----""
        },
        ""chat"": {
            ""id"": {dit id moet je hebben},
            ""title"": ""Groepnaam"",
            ""type"": ""supergroup""
        },
        ""date"": 1595505653,
        ""text"": ""jouw bericht""
    }
}
```

Mocht je je eigen chat nog niet zien, soms duurt het wat langer voordat hij in het lijstje staat, probeer het dan later opnieuw.

Je hebt nu je bot token en chat id, deze kun je in de instellingen van LvBuddy invullen.

# Bekende problemen

- Bij het terughalen van voorraad zal de LvBuddy voorraad notificaties versturen omdat deze minder wordt
- Vanwege limieten op de Bol.com API kun je maximaal 1000 producten in je assortiment hebben (inclusief LVB producten zonder voorraad)
- Hoe meer producten hoe langer het duurt om de informatie in te laden als je de koopblok check aan hebt staan omdat er per product een HTML pagina opgehaald wordt.

Het zou kunnen dat jouw type telefoon/OS het achtergrond werk wat LvBuddy doet stopt om batterij te besparen. 
Bijvoorbeeld telefoons van Xiaomi hebben hier last van, om dit op te lossen kun je informatie vinden op https://dontkillmyapp.com/

# Disclaimer

Het bepalen of je het koopblok hebt is niet officieel ondersteund door Bol.com en er is geen hierdoor geen garantie dat dit blijft werken en/of dat Bol.com het er mee eens is.
Het gebruik van LvBuddy is volledig op eigen risico, ik ben niet verantwoordelijk voor geleden schade aan je bol.com business door het gebruik van LvBuddy.

# Licentie

```
Copyright 2020 Niels Masdorp
 
Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
 
http://www.apache.org/licenses/LICENSE-2.0
 
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
```
",1,1,0,2,retail,"[android-application, retail]",44-45
TheMrityunjayPathak,RetailStoreSalesDashboard,,https://github.com/TheMrityunjayPathak/RetailStoreSalesDashboard,https://api.github.com/repos/RetailStoreSalesDashboard/TheMrityunjayPathak,Retail Store Sales Dashboard🏪,"# Retail Store Sales Dashboard

Hello Everyone,

I made this Retail Store Sales Report in PowerBI with sample Data of a Store which I collected from Kaggle.

## Problem Statement

- The aim of this Power BI Dashboard Project is to design a Retail Store Sales Report that provides a clear and concise overview of the Store's Sales performance, enabling the owner to gain actionable insights.

## Sections in the Report

Report has Multiple Section's from where you can manage the Data, Like :

- Report Data can be sliced by Quater of Year to show Data of Q1, Q2, Q3, Q4.

- Report Data can also be sliced by State to show sales of any particular State.

- It has cards showing Total Amount, Total Profit, Total Quantity of Sale.

- Report has a Bar Chart showing Total Profit by Month.

- I conditionally formatted the Bar Chart to show different color when there is loss in Business.

- Report has another Horizontal Bar Chart which shows Profit by Sub Category of Items.

- Then I have included a Donut Chart to show how much Quantity of Product is order through which Payment Mode.

- I also included a Donut Chart to show how much Quantity of Product is ordered in which Category.

- Then I included a Horizontal Bar Chart to show Top 4 Performing State in sales.

- And Lastly I included a Bar Chart to show Top 4 Customers of our Store.

- I have also included a 🔄 Reset Button at the Top to clear all Slicer's.

- To use it hold CTRL and then click it to reset the Slicer's.

## Getting Started

- Clone the repository to your local machine using the following command :
```
git clone https://github.com/TheMrityunjayPathak/RetailStoreSalesDashboard.git
```

## Link to the Dataset
[Retail Store Sales Dataset](https://github.com/TheMrityunjayPathak/RetailStoreSalesDashboard/tree/main/Dataset)

![Screenshot_20230616_035317](https://github.com/TheMrityunjayPathak/RetailStoreSalesDashboard/assets/123563634/1725e494-528f-4ebc-a76a-87704d820f30)
",1,1,1,0,retail,"[dashboard, powerbi, retail, sales, store]",44-45
mateoricio,forecasting_retail_sales_PA,,https://github.com/mateoricio/forecasting_retail_sales_PA,https://api.github.com/repos/forecasting_retail_sales_PA/mateoricio,Helping our cousin from La Mancha to make stimations in the sales of iberian ham the next months. Applying feature engineer we help our SVM and Random Forest models to make the best predictions we can. ,"# forecasting_retail_sales_PA

# Forecasting Ratail Sales in La Mancha

## Introduction

Our cousind from La Mancha has a grocery store and after see in the inetrnet the possiblities of Machine Learning has asked us if we can apply it into his bussines and see if we can predict the sales of one of his products.

For this task we have received the sales of every category product (_Referencia_) each date (_Fecha_) and how much made was earned (_Ventas_)

## Preprocessing and model training stages:

The procedure was clear, first we chose a product (iberian ham, as well spaniards) and after detect a weekly stationality in the model we applied some feature engineer to the dataset:




* One-hot-enconding to create a boolean parameter for each day of the week, so the model knows which day of the week is.

* shifting the sales of the product one week to give the model information about the last sales, it also allows the model to match certain values of sales with certain days since we have the boolean for the days, as we have.

After this preprocessing of the data we applied two different models:

* Random Forest: Since its simplicity and well performence this was our first choice. In order to increase its speed we decided to put the hyperparameter of the stimators to 80 instead of 100 since we observe practicly the same performance in the half of the time. The results we obtained in _32.8s_

![random_forest_prediction](https://user-images.githubusercontent.com/34031559/203390999-038e07c3-0901-46bd-9a63-c432297c2c50.png)


* Support Vector Machine: In \order to check if we could have a similar performance with a even faster model we applied the support vector regressor. After play with the hyperparameters we put regulation parameter _C_ at 0.75 and we observe a very nice performance in even lower time than the Random Forest model. In this case we obtained our results in _3.5s_

![SVM_prediction](https://user-images.githubusercontent.com/34031559/203391237-47481c69-aec2-4655-ab56-a386ed4088c3.png)
",1,1,1,0,retail,"[random-forest-regressor, retail, sales-prediction, svm-regressor, time-series, time-series-prediction]",44-45
chubbyyb,TRB-POS,,https://github.com/chubbyyb/TRB-POS,https://api.github.com/repos/TRB-POS/chubbyyb,A point of sale system,"# TRB-POS

A point of sale system developed with Winforms using C#. 

#### Current:
* Login function [needs to be refined]
* Discount functions
* Quantity functions
* Reciept function ready to be integrated
* Product information retrieved from database [SQL]
* Transactions are logged to another databse [SQL]
* Inventory management

#### TO DO:
* ~~Inventory management~~
* ~~Add a searchable DB inside the program~~
* ~~Test/adapt program for touchscreens~~
* Optimize and ~~Secure SQL connections~~
* Bug testing and cleanup
* Analytics, can be done in a seperate program
* Add features that a normal POS has
* GUI design





## Images
![Alt Text](images/login.png)
![Alt Text](images/main.png)
![Alt Text](images/quantity.png)


## Attributes
<a href=""https://www.flaticon.com/free-icons/goods"" title=""goods icons"">Goods icons created by Smashicons - Flaticon</a>
<a href=""https://www.flaticon.com/free-icons/euro"" title=""euro icons"">Euro icons created by Freepik - Flaticon</a>
<a href=""https://www.flaticon.com/free-icons/keys"" title=""keys icons"">Keys icons created by nawicon - Flaticon</a>
<a href=""https://www.flaticon.com/free-icons/id-card"" title=""id-card icons"">Id-card icons created by nawicon - Flaticon</a>
",1,1,1,0,retail,"[pointofsale, pos, retail]",44-45
siddharthjn,nsaw,,https://github.com/siddharthjn/nsaw,https://api.github.com/repos/nsaw/siddharthjn,A product catalogue for retail firm,"# Nsaw

#####Nsaw or National Scientific Apparatus Works is a retail firm based in India that provides scientific instruments for research to schools and universities

National Scientific's objective is to promote science at every level of education.

We are building a simple ecommerce website for National Scientific to help them reduce their sales and operations cost. 

To visit the vebsite, please go to http://siddharthjn.github.io/nsaw/index.html
",1,1,3,6,retail,"[catalogue, retail]",44-45
dgluesen,wrangling-sales-workload,,https://github.com/dgluesen/wrangling-sales-workload,https://api.github.com/repos/wrangling-sales-workload/dgluesen,"Raw data of real analytical use cases in a number of industries and companies are frequently provided in an Excel-based form. These files usually cannot be processed directly in machine learning models, but must first be cleaned and preprocessed. In this process, many different types of pitfalls may occur. This makes data preprocessing an essential time factor in the daily work of a data scientist. In this concise project an Excel spreadsheet will be presented which in this form is closely oriented to a real case, but contains only simulated figures for reasons of data and business results protection. However, the form and structure of the file corresponds to a real case and could be encountered by a data scientist in a company in this way.",,1,1,2,0,retail,"[cleansing, human-resources, preprocessing, python, python3, realworld, retail, retail-data, seaborn, wholesale, wrangling]",44-45
wahidulalamriyad,Retail_Order_Management_System,,https://github.com/wahidulalamriyad/Retail_Order_Management_System,https://api.github.com/repos/Retail_Order_Management_System/wahidulalamriyad,"In this project, students must design a Retail Order Management System using Java for small retailers. And there are two different stockholders, admin, and customer. Admin can manage customers, manage products, and manage orders. Based on the system requirements, the customer can only manage orders. The developer learns the fundamentals of object-oriented programmings, such as inheritance, polymorphism, encapsulation, and abstraction. The developer got the chance to differentiate between a variety of data types and its unique features, for example, Vectors and ArrayList, then implemented it to their project as well as developer learn how to allocate resources effectively (memory), or how to design an optimal algorithm (timely efficient).","In this project, students must design a Retail Order Management System using Java for small retailers. And there are two different stockholders, admin, and customer. Admin can manage customers, manage products, and manage orders. Based on the system requirements, the customer can only manage orders. The developer learns the fundamentals of object-oriented programmings, such as inheritance, polymorphism, encapsulation, and abstraction. The developer got the chance to differentiate between a variety of data types and its unique features, for example, Vectors and ArrayList, then implemented it to their project as well as developer learn how to allocate resources effectively (memory), or how to design an optimal algorithm (timely efficient).
",1,1,1,0,retail,"[java, management, retail]",44-45
brunokatekawa,RossmannSales,,https://github.com/brunokatekawa/RossmannSales,https://api.github.com/repos/RossmannSales/brunokatekawa,[Project repo] Predicting total sales for the next 42 days for Rossmann stores.,"# Forecasting sales volume for Rossmann stores

![](img/project_banner.png)

---

# 1.0 The context

In a quarterly report meeting at Rossmann, the directors board identified an increase in competitors stores openings.

In order to prevent the competitors from taking part of the market share, the C-Level board decided to conduct an on-site survey with all customers who were shopping at the stores to understand the following points:

- Why did the customer preferred to buy from Rossmann and not from the competition?
- From 0 to 10, how satisfied was the customer with the Rossmann store's products and service? Why? (CSAT)
- Why were there certain products that the customer preferred to buy from the competition and not from Rossmann?
- What were, in the customers opinions, the three strengths and three weaknesses from Rossmann’s stores? Why?
- From 0 to 10, how much would the customer recommend Rossmann to a friend or family member? Why? (NPS)

After the customer survey, the following insights were identified:

- Customers preferred to buy from the competition because the service was better.
- Customers bought the same product from the competition because it offered better prices.
- Customers preferred to go to the competition’s store because the physical environment were more pleasant and some even had a cafeteria chain inside them.

<br>

# 2.0 The problem
Based on the research insights, initiatives have been outlined such as: 

- Redesign the entire training program for store managers and attendants.
- Review the entire pricing strategy.
- Make partnerships with chains of coffee shops and bakeries.
- Deploy self-service totems to customers who preferred to shop without the assistance of attendants.

However, to carry out all these initiatives, well-structured financial planning with a minimum margin of error must be made so that the company wouldn't waste money during the implementation of the initiatives.

The CFO responsible for this plan had a huge difficulty to carry it out because it was necessary to know how much each store was selling and **how much it could sell in the short term**. In addition, there was no easy, automated, or convenient way to obtain this information.

<br>

# 3.0 The solution
The delivered solution was a Telegram bot. 

The user just needed to send the store number and the bot would answer the predicted **total sales that the store would made by the end of the next six weeks**.

![](img/rossmann_bot.gif)

<br>

## 3.1 What drove the solution
### 3.1.1 Exploratory Data Analysis

#### Descriptive analysis

![](img/descriptive_analysis.png)

Key points:
- The **mean sales** value is around **US$ 5,773.82**. 

- There was a **maximum of 7,388 customers** in a single day.

- In addition, there are **competitor stores really near (20 m)** from Rossmann's stores.

<br>

#### Hypothesis Map
We outlined this map to help us to decide which variables we need in order to validate the hypotheses.

![](img/hypothesis_map.png)

<br>

#### Univariate Analysis

![](img/411_target_variable_distrib.png)

As we can observe, the majority of sales lies around US$ 5,700.

In addition, the distribution is **moderately skewed** (`skewness = 0.641460`) and presents a **positive kurtosis** (`1.778375`) which means that we have some possible outliers in our dataset. Thus, the distribution **does not follow a normal distribution**.

<br>

### Hypothesis validation - Bivariate Analysis

#### Main Hypotheses

**H2. Stores with nearer competitors should have lesser sales.**

![](img/42H2_sales_distance_comp_grid.png)

As we can observe from the bar plot, **stores with nearer competitors have higher sales**. In addition, we can observe from the scatter plot that we have a higher concentration of sales as we decrease the competition distance.

> #### Thus, our hypothesis is **FALSE**.

**Correlations**

As observed in the results, the Pearson's correlation coefficient between `competition_distance` and `sales` is `-0.23` which tells us that is a **weak negative correlation**. Despite the weakness, we may include the `competition_distance` because it has a somewhat influence on the target variable (`sales`).

<br>

**H4. Stores with longer period of time in promotion should have higher sales.**

As there is a lot of data, we divided the dataset in two periods: regular promotion and extended promotion.

![](img/42H4_sales_store_promo_grid.png)

As we can observe in the **Total sales x Weeks in extended promotion**, there's a period in which the extended promotion results in more sales, then after a period of time, the total sales starts to decrease.

From the **Total sales x Weeks in regular promotion**, we can observe that as the offset gets more and more near zero, the sales starts to increase.

Thus, **stores with longer period of time in promotion don't have higher sales.**, because the sales start to decrease as the promotion gets longer. 

> #### Thus, the hypothesis is **FALSE**.

**Correlations**

In addition, from the **Correlation Heatmap** we got a coefficient of `-0.029` which is pretty close to `zero`. Thus, we have a **super weak correlation**, which makes sense because looking at our data, we have long periods of almost constant total sales (see  **Total sales x Weeks in extended promotion**).

<br>

**H6. Stores with higher consecutive promotions should have higher sales.**

![](img/42H6_sales_yw_consecutive_promo.png)

Observing the results it seems that **stores with higher consecutive promotions don't have higher sales**. 

> #### Thus, our hypothesis is **FALSE**.

<br>

**H9. Stores should have higher sales on the second semester of the year.**

![](img/42H9_sales_month_grid.png)

As observed in the previous results, stores **don't have higher sales on the second semester of the year**. In addition, by observing the Pearson correlation coefficient of `-0.75`, we can verify that there is a **strong negative correlation** between `month` and `sales`. 

> #### Thus, our hypothesis is **FALSE**.

<br>

#### Hypotheses summary
We also outlined other hypotheses, but the previous presented ones were the most insightful.

| Hypothesis | Conclusion | Relavance to ML model |
| --------------- | --------------- | --------------- |
| H1 | False | Low |
| H2 | False | Medium |
| H3 | False | Medium |
| H4 | False | Low |
| H5 | (later analysis) | (later analysis) |
| H6 | False | Low |
| H7 | False | Medium |
| H8 | False | High |
| H9 | False | High |
| H10 | True | High |
| H11 | True | High |
| H12 | True | Low |

<br>

### Multivariate Analysis

![](img/432_numerical_att_corr.png)

As observed in the matrix above:

| Variable A | Variable B | Correlation |
| --------------- | --------------- | --------------- |
| `day_of_week` | `open` | Medium |
| `day_of_week` | `sales` | Medium |
| `day_of_week` | `promo` | Weak |
| `day_of_week` | `school_holiday` | Weak |
| `open` | `promo` | Weak |
| `open` | `customer` | Strong |

These correlations guided us on the selection of which variables to include in the model. This could be translated into **more assertiveness on the predictions** which means **better budget planning** and **less wasted money**.

<br>

### 3.1.3 Machine Learning

![](img/72_comparing_models_performance_cv.png)

As observed in the results, the Random Forest Regressor had the least MAE (`837.5 +- 218.9`). However, we done a fine tuning in the **XGBoost Regressor** to experiment if could also be a potential candidate.

#### After tuning the model:
![](img/82_hyperparamenter_results.png)

As we can see, there was a great improvement compared to the previous results.

<br>

### 3.1.4 Business Performance

![](img/09_challenging_stores.png)

As we can observe in the sample above, we have both **best and worst scenarios** for the total sales, so we can have a range in which we can base the budget estimations.

<br>

**MAPE x error**

![](img/911_store_analysis.png)

There are stores that are more difficult to make the predictions (circled in red). Thus, some strategies that may solve this challenge in the next project iteration could be:

 - Taking a closer look on the variables (add or remove).
 - Try other methods and other techniques in order to improve the predictions.

<br>

### 3.1.5 Machine Learning Performance

![](img/93_ml_performance.png)

Observing the results, we can see that:
- By observing the **first and second line plots**, we can see that the predictions or our model is pretty close to the real value for `sales`. On the other hand, the error rate has some variance.

- By observing the **histogram**, the error distribution almost follows a normal distribution. 

- By observing the **scatterplot** for the errors, the points seems well fit in a horizontal tube which means that there's a few variation in the error. If the points formed any other shape (e.g opening/closing cone or an arch), this would mean that the errors follows a trend and we would need to review our model.

<br>

# 4.0 Next Steps
- Experiment with other Machine Learning algorithms to improve business performance by 10%.
- Experiment with selecting other features to see how much the RMSE is impacted.
- Experiment with other hyperparameter fine-tuning strategies to see how much the RMSE is impacted.
- Improve bot messages  and its interaction with users.

---

If you're interested in reading the full detailed story and more tech-oriented, please, visit:
 
https://github.com/brunokatekawa/Curso_DS_Prod
",1,1,1,24,retail,"[predictions, predictive-modeling, retail, sales]",44-45
Vikrantnikumbhe,CRM_TMA,,https://github.com/Vikrantnikumbhe/CRM_TMA,https://api.github.com/repos/CRM_TMA/Vikrantnikumbhe,Development and deployment of next generation customer relationship management tool in product based supply chain.,"# [GRAHAK360](https://share.streamlit.io/vikrantnikumbhe/crm_tma/main/app_crm.py)<img src=""https://user-images.githubusercontent.com/94373839/168340125-b6325433-277d-4ab3-8987-57fe6d4a3790.gif"" alt=""Aplus Framework Pagination Library"" align=""right"" width=""150""></a>

[![Python](https://img.shields.io/badge/Python-3.8-3776AB.svg?style=flat&logo=python&logoColor=FFDB4D)](https://www.python.org)
[![Price](https://img.shields.io/badge/price-FREE-0098f7.svg)](https://github.com/froala/design-blocks/blob/master/LICENSE)
[![Streamlit](https://img.shields.io/badge/Streamlit-app-FF4B4B.svg?style=flat)](https://www.streamlit.io)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/Vikrantnikumbhe/CRM_TMA/blob/main/LICENSE)


Development and deployment of next generation customer relationship management tool in product based supply chain.
Link to the Website: [www.grahak360.com](https://share.streamlit.io/vikrantnikumbhe/crm_tma/main/app_crm.py)
<p align=""center"">
  <img src=""https://user-images.githubusercontent.com/94373839/168304752-9e28d6cb-937c-4e9f-8636-0b89465d449a.png"">
 </p>
In the Era of digital disruption, AI-ML methodologies can be combined with customer databases to build a robust Customer relationship management (CRM) system.This Work summarizes the development of such a next generation customer relationship management tool using machine learning methodologies and deploying the tool as a web interface for real time usage. A CRM system is applicable in almost all the domains, but for this project, the context of Online Retail Industry is considered. The tool developed is based on the data and context of a UK-based and registered non-store business. Data science techniques and machine learning algorithms are used to develop the model. Various key performance indicators like RFM, CLTV, etc., predictive techniques for customer segmentation, sales forecasting, etc. and customer relation parameters are inhibited in the tool. As a result, a web interface tool called GRAHAK360 is deployed for real time analysis and usage by any third party industry person using a suitable deployment platform. The tool created as a part of this project surely has a large scope of extension.


## Activities that can be Performed:

-[To predict expected future transactions to be performed by a customer in a given time period.](https://share.streamlit.io/vikrantnikumbhe/crm_tma/main/app_crm.py)

-[To predict a company's net profit contribution to its overall future relationship with a customer](https://share.streamlit.io/vikrantnikumbhe/crm_tma/main/app_crm.py)

-[To segment customers based on RFM modeling technique.](https://share.streamlit.io/vikrantnikumbhe/crm_tma/main/app_crm.py)

-[To segment customers based on Hybrid (K-means with RFM) modeling technique.](https://share.streamlit.io/vikrantnikumbhe/crm_tma/main/app_crm.py)

-[To create association rules for product recommendations using Market-Basket Analysis.](https://share.streamlit.io/vikrantnikumbhe/crm_tma/main/app_crm.py)

-[To create a classifier that will classify the customers into various categories.](https://share.streamlit.io/vikrantnikumbhe/crm_tma/main/app_crm.py)

-[To analyze and validate different classifiers and to select the right classifier based on their predicting ability and quality of fit.](https://share.streamlit.io/vikrantnikumbhe/crm_tma/main/app_crm.py)

-[To forecast the future sales using the historic sales patterns in the dataset.](https://share.streamlit.io/vikrantnikumbhe/crm_tma/main/app_crm.py)

-[To analyze the forensic aspects of a dataset like churn rate using different visual plots and key metrics.](https://share.streamlit.io/vikrantnikumbhe/crm_tma/main/app_crm.py)

-[To analyze the retention analytics over time by computing monthly and cohort based retention rates.](https://share.streamlit.io/vikrantnikumbhe/crm_tma/main/app_crm.py)

## Implementation
[<img src=""https://github.com/froala/design-blocks/blob/dev/assets/logo-html.png?raw=true"" height=""60"" />](https://github.com/froala/design-blocks)    [<img src=""https://github.com/froala/angular-froala-design-blocks/blob/master/src/assets/logo-angluar.png?raw=true"" height=""60"" />](https://github.com/froala/angular-froala-design-blocks)    [<img src=""https://github.com/froala/react-froala-design-blocks/blob/master/public/logo-react.png?raw=true"" height=""60"" />](https://github.com/froala/react-froala-design-blocks)        [<img src=""https://github.com/froala/vue-froala-design-blocks/blob/master/src/assets/logo-vue.png?raw=true"" height=""60"" />](https://github.com/froala/vue-froala-design-blocks)    [<img src=""https://github.com/froala/design-blocks/blob/dev/assets/logo-psd.png?raw=true"" height=""60"" />](https://github.com/froala/design-blocks/blob/dev/assets/psds/psd-pages.zip?raw=true)    [<img src=""https://github.com/froala/design-blocks/blob/dev/assets/logo-sketch.png?raw=true"" height=""60"" />](https://github.com/froala/design-blocks/blob/dev/assets/sketch/froala-design-blocks.sketch?raw=true)  ![](https://forthebadge.com/images/badges/made-with-python.svg)    [<img target=""_blank"" src=""https://i.imgur.com/jAyHARm.png"" width=170>](https://www.streamlit.io/)


## Browser Support

At the moment, we aim to support all major web browsers. Any issue in the browsers listed below should be reported as a bug:

- Internet Explorer 10+
- Microsoft Edge 14+
- Safari 6+
- Firefox (Current - 1) and Current versions
- Chrome (Current - 1) and Current versions
- Opera (Current - 1) and Current versions
- Safari iOS 7.0+
- Android 6.0+

## Feedback:
All comments, bug reports and enhancement requests are welcome. To do so, please submit a new issue and I will investigate it.
Please fill this form as a part of survry on user experience : https://form.jotform.com/221075032353444					
",1,1,1,0,retail,"[crm, data-mining-algorithms, predictive-analysis, python, retail, streamlit, webapp]",44-45
V-Cyberpunk,TrinityCore-Dragonflight-Databases,,https://github.com/V-Cyberpunk/TrinityCore-Dragonflight-Databases,https://api.github.com/repos/TrinityCore-Dragonflight-Databases/V-Cyberpunk,Databases for Trinitycore Retail Dragonflight,"# TrinityCore-Dragonflight-Databases
Databases for Trinitycore Retail

Currently only RetailCoreDB. Check out RetailCore's Discord: https://discord.gg/qF5xPSR34d

Release namescheme: $dbname_$releasedate_tc-$tc-commit
$tc-commit is the needed commit from TrinityCore for the DB.

dbextract.ps1 - for extract the dumps by yourself from RetailCoreDB Discord<br>
kill_bnet.bat - kills bnet launcher<br>
macros-cache.txt - contains macros (for GM) for adding Blizzard shop items, tcg, blizzcon, collectors edition, etc<br>
mk_cdn.sh - download the NGDP CDN files by yourself
",1,1,1,0,retail,"[dragonflight, emu, emulator, retail, retailcore, retailcoredb, trinitycore, wow]",44-45
jacenko,HypeGirlsApp,,https://github.com/jacenko/HypeGirlsApp,https://api.github.com/repos/HypeGirlsApp/jacenko,:womans_clothes: Objective-C shopping cart mobile app proposal for finals project,"![Hype Girls App screenshots](http://s28.postimg.org/mx8u47wgd/hype.png)

The Hype Girls Store app is my senior project from FIU's iOS Develoment course that involved finding a client and working on an idea for an app that would have to have certain features. The client in question, Nichole, has a following with her brand of clothing/music/events under Hype Girls, Inc and wanted a store app that was simple, yet interesting. Based on the project's needed features, a blog (JSON for previews and Readability for full posts) and a personal profile was later added. The concept included here was presented to the client at the time of completion, a few weeks after the first meeting. Certain features were co-developed with Michael O'Reggio.
",1,1,3,0,retail,"[api, blog, camera, ios, ios-app, login, objective-c, profile, readability, retail, store]",44-45
Shuvo-saha,Retail-Analytics-Showcase,,https://github.com/Shuvo-saha/Retail-Analytics-Showcase,https://api.github.com/repos/Retail-Analytics-Showcase/Shuvo-saha,,"# Deep Dive into Retail Analytics
![Python 3.9](https://img.shields.io/badge/Python-3.9-brightgreen.svg) ![Streamlit](https://img.shields.io/badge/Streamlit-Library-orange.svg)<br>
This **fully-fledged, interactive retail analytics showcase** dives into:
- The most important questions that big data and statistical learning can answer in the retail space
- The algorithms driving modern retail analytics
- How effective these algorithms can be in answering those questions 

You can view the web app [here](https://shuvo-saha-retail-analytics-showcase-streamlit-app-kzkeml.streamlit.app/)

---
## Data Size & Algorithms
- Recommendation System: Book ratings for approximately *5812 books* by *15,797 users*, cosine similarity based *Item-Item Collaborative Filtering*
- Market Basket Analysis: *7500* transactions at a grocery store, *Association Rules Analysis* using *Apriori*
- Churn Prediction: *3333 customers* and *17 features* per customer for a telecom operator, multiple classification models including:
  -  *Gradient Boosting Classifier* (81.56% F1-Score)
  -  *Decision Tree* (71.57% F1-Score)
  -  *Logistic Regression* (25.95% F1-Score)
  -  *Random Forest Classifier* (80.9% F1-Score)
  -  *Stacking Classifier* with Logistic Regression, Decision Tree and Random Forest (85.25% F1-Score)
- Clustering: *8590* consumers and *17 different credit information* on each user of a financial institution, *K-Means Clustering* and *Principal Component Analysis*
- Customer Segmentation: Transactions covering *4380 Customers* and *4207 Products* for an e-commerce platform, *Recency, Frequency and Monetary Analysis*

---
## References
This website wouldn't have been possible without the helpful resources below:
- [Book Recommendations Dataset on Kaggle](https://www.kaggle.com/saurabhbagchi/books-dataset)
- [Churn Analysis Dataset on Kaggle](https://www.kaggle.com/sandipdatta/customer-churn-analysis)
- [Clustering Dataset on Kaggle](https://www.kaggle.com/ankits29/credit-card-customer-clustering-with-explanation)
- [RFM Analysis Dataset on Kaggle](https://www.kaggle.com/roshansharma/online-retail) 

For more information on Machine Learning, Statistics and Retail Analytics, check out these great resources:
- [Statquest for Statistics and ML](https://www.youtube.com/user/joshstarmer)
- [3Blue1Brown for Math](https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw)
- [Sentdex for ML Coding Tutorials](https://www.youtube.com/user/sentdex)
- [Google Cloud Next'19 Keynote on Retail and AI](https://www.youtube.com/watch?v=pKEmQ1VMxsM)
- [DataRobot APAC Data Science's Presentation on Data Science in Retail](https://www.youtube.com/watch?v=PThNpfd3waE) 

This website was built using Python and the following libraries:
- [Streamlit](https://docs.streamlit.io/en/stable/index.html)
- [ELI5](https://eli5.readthedocs.io/en/latest/overview.html)
- [Scikit-Learn](https://scikit-learn.org/)
- [Matplotlib](https://matplotlib.org/)
- [Seaborn](https://seaborn.pydata.org/)
- [Pandas](https://pandas.pydata.org/) 

Previous eployment via [Heroku](https://www.heroku.com/) but now hosted on [Streamlit Share](https://share.streamlit.io/)
",1,1,1,0,retail,"[analytics, churn-prediction, market-basket-analysis, recommender-systems, retail, streamlit]",44-45
ferib,OTRWhisper,,https://github.com/ferib/OTRWhisper,https://api.github.com/repos/OTRWhisper/ferib,Off-The-Record Whispers for Wow (AddOn),,1,1,2,4,retail,"[adddon, classic, classic-era, lua, retail, wow]",44-45
SAP-samples,data-warehouse-cloud-fedml-retail-inventory,SAP-samples,https://github.com/SAP-samples/data-warehouse-cloud-fedml-retail-inventory,https://api.github.com/repos/data-warehouse-cloud-fedml-retail-inventory/SAP-samples,Retail Inventory Prediction and Insight use case with SAP Data Warehouse Cloud and federated machine learning.,"[![REUSE status](https://api.reuse.software/badge/github.com/SAP-samples/data-warehouse-cloud-fedml-retail-inventory)](https://api.reuse.software/info/github.com/SAP-samples/data-warehouse-cloud-fedml-retail-inventory)

# Retail Inventory prediction and comparative retail consumption analysis using FedML

## Description
Explore how to use SAP Data Warehouse Cloud and  FedML hyperscaler libraries to source data , build,train and deploy machine learning models on hyperscaler platforms for predicting inventory allocations , and to do comparative retail consumption analysis and derive insights using SAP Analytics cloud - all without data duplication.

## Solution Architecture
![Solution Architecture](./Solution-Diagram.jpg)

## Challenges
In general cases, for conducting machine learning experiments on hyperscaler platforms, the data often have to be extracted out of source business systems and duplicated in cloud stores of the ML platform. This causes issues such as data inconsistencies and increased TCO resulting from additional data storage costs due to data duplication and expensive data pipelines. Moreover comparitive retail consumption analysis on live data is not currently possible with data from cross cloud sources. 


## Solution
FedML python libraries for hyperscalers helps create an end to end automated solution for doing the ML experiments on hyperscalers without moving or duplicating any data to the hyperscaler. The models are deployed on SAP BTP Kyma. The solution uses SAP DWC's data federation architecture and a unified semantic layer that helps model the queries across distributed data sources without the need do extract any data out from anywhere. SAP Analytics Cloud helps visualize live data to compare retail inventory predictions against the live inventory consumption data coming in from online retail platforms.

## Requirements

* SAP BTP Subaccount with Kyma  subscription enabled.
* SAP Data Warehouse Cloud instance
* SAP Analytics Cloud instance
* MS Azure cloud subscription with permissions to create service principal acccounts.

## Download and Installation
* [Sign up for BTP Trial acount and enable Kyma](https://developers.sap.com/tutorials/hcp-create-trial-account.html)
* [Enable Kyma Runtime](https://developers.sap.com/tutorials/cp-kyma-getting-started.html)
* [Sign up for SAP Data Warehouse cloud trial](https://www.sap.com/products/data-warehouse-cloud/trial.html)
* [Run the Retail_Project.ipynb](https://github.com/SAP-samples/data-warehouse-cloud-fedml-retail-inventory/blob/main/Retail_project.ipynb)

## Limitations
This example contains no known limitations.

## Known Issues
This example contains no known issues.

## How to obtain support
This project is provided ""as-is"" with no expectation for major changes or support.

[Create an issue](https://github.com/SAP-samples//issues) in this repository if you find a bug or have questions about the content.

For additional support, [ask a question in SAP Community](https://answers.sap.com/questions/ask.html).

## License
Copyright (c) 2022 SAP SE or an SAP affiliate company. All rights reserved. This project is licensed under the Apache Software License, version 2.0 except as noted otherwise in the [LICENSE](LICENSES/Apache-2.0.txt) file.
",1,1,9,0,retail,"[dwc, fedml, hyperscalers, retail, sample, sample-code, sap-analytics-cloud, sap-data-warehouse-cloud]",44-45
06Reetu,Report-and-Dashboard-of-PowerBi,,https://github.com/06Reetu/Report-and-Dashboard-of-PowerBi,https://api.github.com/repos/Report-and-Dashboard-of-PowerBi/06Reetu,,"# PowerBi
It is a Retail Performance Report using Power-BI.

Here's a Data-Analysis project of Human Resource using Power-BI. It has buttons to navigate between different page, like it has all pages button to go directly in all the pages from Landing Page and has Home button in each pages to directly move to the landing page from any page.


# These are some Screenshot of the Report.
<img src=""https://github.com/06Reetu/PowerBi/blob/main/retail/s1.png"" alt=""alt text"" height=300 width=""500""/>         
<img src=""https://github.com/06Reetu/PowerBi/blob/main/retail/s2.png"" alt=""alt text"" height=300 width=""500""/>  
<img src=""https://github.com/06Reetu/PowerBi/blob/main/retail/s3.png"" alt=""alt text"" height=300 width=""500""/>         
<img src=""https://github.com/06Reetu/PowerBi/blob/main/retail/s4.png"" alt=""alt text"" height=300 width=""500""/>      
<img src=""https://github.com/06Reetu/PowerBi/blob/main/retail/s5.png"" alt=""alt text"" height=300 width=""500""/>  

Please do ⭐ the repository, if you like this.😊


### Connect with me:


[<img align=""left"" alt=""codeSTACKr | Twitter"" width=""22px"" src=""https://cdn.jsdelivr.net/npm/simple-icons@v3/icons/twitter.svg"" />][twitter]
[<img align=""left"" alt=""codeSTACKr | LinkedIn"" width=""22px"" src=""https://cdn.jsdelivr.net/npm/simple-icons@v3/icons/linkedin.svg"" />][linkedin]
[<img align=""left"" alt=""codeSTACKr | Instagram"" width=""22px"" src=""https://cdn.jsdelivr.net/npm/simple-icons@v3/icons/instagram.svg"" />][instagram]

<br />

<br />
 📧 Email : reetu0572@gmail.com





[twitter]: https://twitter.com/Reetu23403806
[instagram]: https://www.instagram.com/_imreetumehra_/
[linkedin]: https://www.linkedin.com/in/reetu-kumari-304788209/
",1,1,1,0,retail,"[dashboard, powerbi, report, retail]",44-45
datasith,ds-datasets-sku110k,,https://github.com/datasith/ds-datasets-sku110k,https://api.github.com/repos/ds-datasets-sku110k/datasith,Scripts for manipulating the annotation files of the SKU110K dataset,"# 🛍️ SKU110K Dataset

Here you will find the images and annotations (labels) for the [SKU110K dataset](https://www.kaggle.com/datasets/thedatasith/sku110k-annotations). Also included is a notebook containing the process to download the SKU110K dataset from its original source, and convert the annotations to a variety of formats (e.g., YOLOv5).

## Files

This dataset contains the labels comprising the bounding boxes (only one class for all detections), as well as all images for the SKU110K dataset. The included notebook is meant to be run locally, and the code shows how to use the Kaggle CLI for downloading the images to their corresponding location.

In order to use YOLOv5, the included notebook shows how to save the annotations in the correct format:
- One row per object
- Each row is `class` &nbsp; `x_center` &nbsp; `y_center` &nbsp; `width` &nbsp; `height` format
- Box coordinates must be in **normalized xywh** format (from 0 - 1). If your boxes are in pixels:
    - Divide `x_center` and `width` by image width
    - Divive `y_center` and `height` by image height
- Class numbers are zero-indexed (start from 0)

Although the included annotations use my preferred directory structure for organization (as detailed below), they can all be contained in a single file as well.

    .
    ├── convert_yolov5.ipynb
    ├── data_kaggle.yaml
    ├── README.md
    ├── SKU110K_fixed    
        ├── images
            ├── test
            ├── train
            ├── val
        ├── labels
            ├── test
            ├── train
            ├── val

## Acknowledgements

I'd love to acknowledge the original authors: https://github.com/eg4000/SKU110K_CVPR19

---

🐞 If you find any bugs or have any questions regarding these notebooks, please open an issue. I'll address it as soon as I can. 

🐦 Reach out on [Twitter](https://twitter.com/datasith) if you have any questions. 

🔗 Please cite the original authors if you use this version of the dataset for your work.
",1,1,1,0,retail,"[dataset, deep-learning, machine-learning, retail, retail-data]",44-45
rmalongo,Wayfair-Data-Challenge-Babson-Hackathlon,,https://github.com/rmalongo/Wayfair-Data-Challenge-Babson-Hackathlon,https://api.github.com/repos/Wayfair-Data-Challenge-Babson-Hackathlon/rmalongo,Analyse SKU level data and suggest best global strategy for increasing supplier adoption of website product description feature - February 2020,"# Wayfair-Data-Challenge---Babson-Hackathlon
Analyse SKU level data and suggest best global strategy for increasing supplier adoption of website product description feature - February 2020

https://drive.google.com/drive/u/1/folders/1RJ3syCeGZ5y5JGSgsA61NW6id_5U-CaZ

# Problem Statment
""*Our customers use visual media to inform, educate, and inspire purchasing decisions. You recently
launched an A/B test that showed when Rich Media (i.e. video, 3D imagery, and interactive content) is
present on our Product Details Page (PDP), we see upwards of 7% conversion rate lift depending on the
Product Class and content type (see Exhibit A & B for details). The Wayfair Merchandising team is
looking to capitalize on this opportunity by increasing Rich Media content across the catalog. Not every
product has this content as we rely on Suppliers to use our proprietary visual merchandising platform,
called WayMore, to design a PDP layout that differentiates their products (see exhibit C below for an
example*.""

# Scope:
Wayfair provided multiple datasets at the product class, Supplier, and SKU level that will help
inform what Rich Media gaps exist. These dataset are content recommendation, Supplier, and modulecount data
The Merchandising Analytics team (you) has been tasked with building a recommendation on one or
more of the following topics:
* Supplier outreach strategy -- including Suppliers to target, prioritized SKU lists, etc.
* Opportunity size for filling Rich Media gaps
* Improve the customer experience through product content optimization
* Any other recommendations for Merchandising Leadership
Merchandising Leadership is looking to you to create a 5-7 minute executive level presentation that
outlines your findings and recommendations
# Vision:
Design a predictive model:
* y: whether to adopt rich advertisement
* x:  the characteristics of a supplier
* Compute Opportunity size
  *  Projected conversion rate / revenue
  *  Content optimization: best combination of product class + content type + modules amount
# Impact. 
On March 24, the Babson Hackathlon was cancelled due to implications from Covid-19
",1,1,1,0,retail,"[analytics, retail, technology]",44-45
Huang9495,GSPN,,https://github.com/Huang9495/GSPN,https://api.github.com/repos/GSPN/Huang9495,Code release for Geometry Supervised Pose Network for Accurate Retail Shelf Pose Estimation (GSPN) (IEEE TII). ,"# Geometry-Supervised-Pose-Network-for-Accurate-Retail-Shelf-Pose-Estimation
Monocular shelf posture estimation algorithm in the retail field

[[Paper]](https://ieeexplore.ieee.org/document/9112652) [[Blog]](https://www.zhihu.com/people/kris-allen-65/posts)

## Overview
Code release for Geometry Supervised Pose Network for Accurate Retail Shelf Pose Estimation.
This is a GSPN which contains a GSL Module. Using VGG as backbone, I add GSL module into vgg as a monitor to supervise the shelf pose estimatiom, a complete pose of shelf containing three value in 3D space.

This paper is jointly complished by [Yongqiang Mou](https://github.com/AIKnowU) *，[Zhiyi Huang](https://github.com/Huang9495) (JXSTNUAA / AIBC)，Lingfan Lin ， Yishi Guo and [Zhen Yang](https://github.com/yangzhen5771) (JXSTNUAA / AIBC) in IEEE Transactions on Industrial Informatics (IEEE TII, IF=9.112) 2020。

![](file:///Users/wangyabei/Pictures/img/dataset.jpg)
Further information please contact [Yongqiang Mou](yongqiang.mou@gmail.com)

## Requirements
[PyTorch](https://pytorch.org/) (version >= 1.1.0)
[Torchvision](https://pytorch.org/) (version >= 0.3.0)

## Getting Started (Testing)

### Testing

* Model-Site:
   ```
   链接:https://pan.baidu.com/s/1Pmi6bfbDok0VOlKjMOGDLA  
   密码:uhnz
   ```  
* Command:
   ```
   cd ~/project  
   mkdir models
   cp ./checkpoint.pth.tar ~/project/models
   download datasets
   mv ./traindata ~/project/dataset
   cd ~/project/code
   python python eval.py
   ```  
## Experimental Result
Basis | Method | Input | Pitch Model | Yaw Model | Roll Model | Average Error
-- | ---- | ---- | ---- | ---- | ---- | ----
FFPVA[1] | FFPVA[1] | RGB | 16.497◦ | 6.574◦ | 5.087◦ | 9.386◦
PoseNet[2] | BL-RGB | RGB | 8.533◦ | 2.991◦ | 1.345◦ | 4.289◦
PoseNet[2] | BL-LSD | GIM | 8.083◦ | 2.662◦ | 1.125◦ | 3.957◦
PoseNet[2] | BL-Fusion | RGB+GIM | 8.078◦ | 2.609◦ | 1.102◦ | 3.929◦ 
L2SD | L2SD | RGB | 7.958◦ | 1.543◦ | 1.119◦ | 3.540◦
GSPN | GSPN | RGB | 7.456◦ | 0.864◦ | 0.761◦ | 3.027◦

## Reference
[1]. J. Lezama, and G. Randall R. G. V. Gioi, and J. M. Morel, “Finding vanishing points via point alignments in image primal and dual domains,” IEEE Computer Vision and Pattern Recognition, pp. 509–515, Jun. 2014.  
[2].  A. Kendall, M. Grimes, and R. Cipolla, “Posenet: A convolutional network for real-time 6-dof camera relocalization,” IEEE International Conference on Computer Vision, p. 265, Dec. 2015.

## License and Citation
@article{0Geometry,
  title={Geometry Supervised Pose Network for Accurate Retail Shelf Pose Estimation},
  author={Mou, Yongqiang  and  Huang, Zhiyi  and  Lin, Lingfan  and  Guo, Yishi  and  Yang, Zhen},
  journal={IEEE Transactions on Industrial Informatics},
  volume={PP},
  number={99},
  pages={1-1},
  year={2020}
}
",1,1,2,0,retail,"[computer-vision, deep-learning, machine-learning, retail, retail-data]",44-45
bagherig,walmart-forecasting,,https://github.com/bagherig/walmart-forecasting,https://api.github.com/repos/walmart-forecasting/bagherig,The goal of this project is to provide 28-days ahead point forecasts for 30490 various items sold by Walmart.,"**Multi-Step Sales Forecasting of Walmart Products**

The goal of this project is to provide **28-days** ahead point forecasts for **30490** various items sold by **Walmart**.

**The dataset**
===============

The M5 dataset, involves the
unit sales of various products sold in the USA, organized in the form of
**grouped time series**. More specifically, the dataset involves the
unit sales of **3,049 products**, classified in **3 product categories**
(Hobbies, Foods, and Household) and **7 product departments**, in which
the above-mentioned categories are disaggregated.  The products are sold
across **ten stores**, located in **three States** (CA, TX, and WI).

The historical data range from **2011-01-29** to **2016-06-19**, which consists of the following **three (3) files**:

**File 1: ""*calendar.csv*""**

Contains information about the dates the products are sold.

-   *date*: The date in a ""y-m-d"" format.

-   *wm\_yr\_wk*: The id of the week the date belongs to.

-   *weekday*: The type of the day (Saturday, Sunday, ..., Friday).

-   *wday*: The id of the weekday, starting from Saturday.

-   *month*: The month of the date.

-   *year*: The year of the date.

-   *event\_name\_1*: If the date includes an event, the name of this
    event.

-   *event\_type\_1*: If the date includes an event, the type of this
    event.

-   *event\_name\_2*: If the date includes a second event, the name of
    this event.

-   *event\_type\_2*: If the date includes a second event, the type of
    this event.

-   *snap\_CA*, *snap\_TX*, and *snap\_WI*: A binary variable (0 or 1)
    indicating whether the stores of CA, TX or WI allow SNAP[^3]
    purchases on the examined date. 1 indicates that SNAP purchases are
    allowed.

**File 2: ""*sell\_prices.csv*""**

Contains information about the price of the products sold per store and
date.

-   *store\_id*: The id of the store where the product is sold.

-   *item\_id*: The id of the product.

-   *wm\_yr\_wk*: The id of the week.

-   *sell\_price*: The price of the product for the given week/store.
    The price is provided per week (average across seven days). If not
    available, this means that the product was not sold during the
    examined week. Note that although prices are constant at weekly
    basis, they may change through time (both training and test set).

**File 3: ""*sales\_train.csv*""**

Contains the historical daily unit sales data per product and store.

-   *item\_id*: The id of the product.

-   *dept\_id*: The id of the department the product belongs to.

-   *cat\_id*: The id of the category the product belongs to.

-   *store\_id*: The id of the store where the product is sold.

-   *state\_id*: The State where the store is located.

-   *d\_1, d\_2, ..., d\_i, ... d\_1941*: The number of units sold at
    day *i*, starting from 2011-01-29.
",1,1,1,0,retail,"[forecasting, kaggle-competition, m5-competition, python3, retail, ryerson-university, thesis-project]",44-45
BastinRobin,Data-Science-On-Action,,https://github.com/BastinRobin/Data-Science-On-Action,https://api.github.com/repos/Data-Science-On-Action/BastinRobin,Applications of Data Science Across Different Domains,"# Data Science On Action
Applications of Data Science Across Different Domains**

## [Healthcare](healthcare/chapter.md)
- [Predict Presence Of Heart Disease Using Machine Learning](healthcare/chapter-1.md)
- [Hospital Readmission for Patients with Diabetes](healthcare/chapter-2.md)


## Pharmaceuticals

- Predict outcomes from fever or diverse(eg: animal testing) experiments to reduce experimental R&D costs and time to market.
- Predict risk of individual patient churn and optimal corrective strategy to maintain adherence.
- Identify target patient subgroups that are underserved (eg: not diagnosed) and recommend mitigation strategy.

## Manufacturing

- Predict failures and proactive maintenance of production and moving equipment
- Demand prediction of raw materials for future
  

## Retail

- Optimize in-store product assortment to maximize sales
- Personalize product recommendations and advertising to target individual consumers.


## Finance

- Personalize product offerings to target individual consumers based on multi-modal data (mobile, social media, locations, etc)
- Identify fraudulent activity using customer transactions and other relevant data.

  

## Agriculture

- Customize growing techniques specific to individual plot characteristics and relevant real-time data.
- Optimize price in real-time based on the future market, weather and other forecasts.

  

## Media

- Personalize advertising and recommendations to target individual consumer based on multi modal (mobile, social media, location..etc)
- Discover new trends in consumption patterns (eg: viral content)

  

## Telecom

- Predict lifetime value and risk of churn for individual customers.
- Personalize strategy to target individual citizens based on multi-modal data (social, mobile, location..etc)

  

  

## Transportation / Logistic

- Optimize pricing and scheduling based on real-time demand updates (eg: airlines, less than truckload shipping, mobility service)
- Optimize routing in realtime (eg: airline, logistics, last mile routing for complex events)

## Energy

- Replicate human-made decisions in control room environments to reduce cost and human error.
- Optimize energy scheduling/dispatching of power plants based on energy pricing, weathers, and other real-time data

  

## Public/ Social

- Optimize public resource allocation for urban development to improve quality of life (eg: reduce traffic, minimize pollution)
- Personalize public services to target individual citizens based on multi-modal data (social, mobile, location..etc)



## Authors
- Bastin Robins .J
- Dr. Vandana Bhagat
",1,1,2,0,retail,"[agriculture, ai, data-science, energy, finance, healthcare, machine-learning, manufacturing, media, pharma, public, retail]",44-45
tspannhw,minifi-estimote,,https://github.com/tspannhw/minifi-estimote,https://api.github.com/repos/minifi-estimote/tspannhw,Reading estimote and ibeacon beacons with BLE scanner python minifi nifi,"# minifi-estimote
Reading estimote and ibeacon beacons with BLE scanner python minifi nifi
",1,1,1,0,retail,"[apache-nifi, beacons, estimote, ibeacon, nifi, python, retail]",44-45
jamesdinardo,Retail-Forecasting,,https://github.com/jamesdinardo/Retail-Forecasting,https://api.github.com/repos/Retail-Forecasting/jamesdinardo,This project uses historical sales data for 45 retail stores and predicts the weekly sales for each store department for the following year.,"# Retail-Forecasting
This project uses historical sales data for 45 retail stores and predicts the weekly sales for each store department for the following year. The data comes from [Kaggle](https://www.kaggle.com/manjeetsingh/retaildataset) and contains information about the store type, store size, temperature, price of fuel, store department, consumer price index each week, whether a holiday occurred that week, and the sales that week.

## Table of Contents

1. Introduction

2. Overview of the data

3. Exploratory Data Analysis

4. Modeling

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; a. K-Nearest Neighbors

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; b. Linear Models

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; d. Decision Tree Regressor

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; e. Random Forest Regressor

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; f. Boosted Trees

5. Conclusion

## 1. Introduction

One of the key challenges for retail companies is to predict how well a store or department will perform in the future based on past performance. Given past information about a particular store department--its size, previous sales, the temperature and unemployment rate that week etc--can we predict what its sales will be for a given week next year? This project attempts to predict the 2012 weekly sales for each department of a set of 45 retail stores, based on data from 2010-2011 (see section 2 for the full list of variables). It makes use of several data-science libraries in Python--pandas for data cleaning and analysis, matplotlib for data visualization, and scikit-learn for machine learning. The goal is to implement several different models (including linear, tree-based, and ensemble methods) to come up with a best model that can predict sales within one standard deviation of the mean.

The code is written in Python inside of Jupyter notebooks (ipynb files). There are 7 notebooks in total: one for data preparation, one for data exploration, and one for each of the 5 types of models that will be trained, tested, and evaluated. This readme contains only minimal code and visualizations needed to express the main insights: the full code can be found in the notebooks, which can be downloaded should one wish to experiment on their own with the data. Additionally, I have created an easy to use [application](https://retail-predictions.herokuapp.com), written in Flask, where the user can input information, such as the store, department, and week, and immediately receive a prediction for weekly sales. 

## 2. Overview of the Data

The data is contained in three csv files: stores, sales, and features. After merging this data and eliminating rows with negative values
we are left with a single dataframe containing 418660 rows and 16 columns. Each row represents a week of sales
for a particular store department. Each column is a variable that pertains to some aspect
of that week of sales. Our task is to use the first 15 variables (called ""features"") to predict the 
variable ""Weekly_Sales"" (called the ""target""). A description of each variable is as follows:

Store - The store number, ranging from 1 to 45

Date - The week for which sales were calculcated

Temperature - Average temperature (Fahrenheit) in the region in which the store is located

Fuel_Price - Cost of fuel (dollars per gallon) in the region in which the store is located

MarkDown1-5 - Promotional markdowns (discounts) that are only tracked from Nov. 2011 onward

CPI - Consumer Price Index, which measures the overall costs of goods and services bought by
a typical customer. As CPI rises, the the average customer has to spend more to maintain the same
standard of living. It is calculated as follows:

(price of a basket of goods in current year / price of the basket in the base year) x 100

Unemployment - Unemployment Rate

IsHoliday - True if the week contains a holiday

Dept - The department number

Type - The type of store (A, B, or C). No further information on type is provided, but it appears to be correlated to the size of the store (see section 3).

Size - The size of the store

Weekly_Sales - The sales for a given department within a given store that week. This is the target variable that we are trying to predict.

## 3. Exploratory Data Analysis

Our dataset has 45 unique stores, 81 unique departments, 3 unique types, and 3323 unique store department combinations. The following table displays general summary statistics for the continuous variables:

~~~
df.describe().T
~~~

![Plot1](https://github.com/jamesdinardo/Retail-Forecasting/blob/master/img/describe.png)

We see that the average store deparment does $16027.10 in sales per week, with a standard deviation of $22726.51. We can plot the average weekly sales as a function of date, using a line plot:

~~~
average_sales_per_week_per_department = df.groupby('Date')['Weekly_Sales'].mean()

fig, ax = plt.subplots(figsize=(15, 5))
_ = ax.set_ylabel('Weekly Sales')
_ = ax.set_title('Average Weekly Sales Per Store Department')
_ = average_sales_per_week_per_department.plot()
~~~

![Plot2](https://github.com/jamesdinardo/Retail-Forecasting/blob/master/img/sales_per_week.png)

Sales appear steady for most of the year up until the holidays, where there is a noticable increase in sales for both 2010 and 2011. The following plot shows the same data, only with each year separated vertically. Note that the final year of data, 2012, only has sales data up until 2012-12-10, which is why the line flattens out toward the end of the year:

~~~
df_indexed = df.set_index('Date')

fig, ax = plt.subplots(nrows=3, ncols=1, figsize=(15, 8))
_ = ax[0].plot(df_indexed['2010'].groupby('Date')['Weekly_Sales'].mean())
_ = ax[1].plot(df_indexed['2011'].groupby('Date')['Weekly_Sales'].mean())
_ = ax[2].plot(df_indexed['2012'].groupby('Date')['Weekly_Sales'].mean())

_ = ax[0].set_yticks([10000, 15000, 20000, 25000])
_ = ax[1].set_yticks([10000, 15000, 20000, 25000])
_ = ax[2].set_yticks([10000, 15000, 20000, 25000])

_ = ax[0].set_title(""Average Weekly Sales Per Store Department, Per Year"")
_ = ax[1].set_ylabel(""Sales"")
_ = ax[2].set_xlabel(""Date"")
~~~

![Plot3](https://github.com/jamesdinardo/Retail-Forecasting/blob/master/img/subplots_per_year.png)

Stores and departments vary considerably. Moreover, they are not equally represented in the dataset.

The average store does $1046624.03 in sales per week, but there is a large difference between the stores with the highest, lowest, and median sales:

~~~
min_max_median_store = df[df['Store'].isin([5, 20, 45])].groupby(['Date', 'Store'])['Weekly_Sales'].sum().dropna().reset_index()

fig, ax = plt.subplots(figsize=(15, 5))
_ = sns.lineplot(x='Date', y='Weekly_Sales', hue='Store', data=min_max_median_store)
_ = plt.xticks(rotation='90')
_ = plt.legend(['Minimum: Store 5', 'Maximum: Store 20', 'Median: Store 45'])
~~~

![Plot4](https://github.com/jamesdinardo/Retail-Forecasting/blob/master/img/min_median_max_stores.png)

The trends observed earlier appear to be consistent among stores with different sales volumes, as each line follows the same general pattern.

There is also a disparity between sales volume for different departments:

~~~
df.groupby('Dept').agg({'Weekly_Sales':'mean'}).sort_values(by='Weekly_Sales', ascending=False)
~~~

![Plot5](https://github.com/jamesdinardo/Retail-Forecasting/blob/master/img/average_weekly_sales_by_dept.png)

In addition, some of the departments only appear for a few weeks, suggesting that those departments might have been phased out or merged with other departments. Since we are predicting store department sales, each of these variables is likely to be important when building models. 

Let's also look at the linear correleation between each of the continuous variables in the dataset. An easy way to view linear correlation is to construct a heatmap:

~~~
fix, ax = plt.subplots(figsize=(7, 5))
_ = sns.heatmap(df.corr(), square=True, cmap='coolwarm', ax=ax)
~~~

![Plot6](https://github.com/jamesdinardo/Retail-Forecasting/blob/master/img/heatmap.png)

Values closer to 0 indicate weak or no correlation, positive values indicate positive correlation, and negative values indicate negative correlation. The only variable with much correlation to the target (Weekly_Sales) is Size, which makes sense since larger stores tend to sell more. But it is important to note that heatmaps only show linear one-to-one correlation, so it is possible that some variables are correlated to the target in tandem with eachother or in non-linear ways.

Another useful type of plot is the countplot, which counts the number of instances of each unique categorical variable. The following code creates three categorical variables by binning the continuous variable, Size, into three categories: small, medium, and large. Then it maps the Type variable to color and counts how many instances there are for each:

~~~
df['Size_Category'] = pd.cut(df['Size'], bins=[0, 100000, 200000, np.inf], labels=['Small', 'Medium', 'Large'])

_ = sns.countplot(x='Size_Category', hue='Type', data=df)
_ = plt.xlabel(""Size Category"")
_ = plt.ylabel(""Count"")
~~~

![Plot7](https://github.com/jamesdinardo/Retail-Forecasting/blob/master/img/size_category_count.png)

What this plot shows is that Size and Type are correlated. All large stores (>200000) are of Type A, and all stores of Type C are small (<100000). 

## 4. Modeling

Before training any machine learning models, we have to ""preprocess"" our data, which means getting it into a format that the models can understand and perform well on. Each modeling notebook begins with the following preprocessing steps:

- Create three categorical features from Date--Week, Month, and Year--and then drop Date

- The scikit-learn API cannot directly work with columns of type ""object."" So we create dummy variables in Pandas, using the following code:

~~~
df_dummies = pd.get_dummies(df, drop_first=True)
~~~

- Split the data into train and test sets, where X_train and X_test contain the features and y_train and y_test contain the target:

~~~
X_train = df_dummies.loc[(df['Year']==2010) | (df['Year']==2011), :].drop('Weekly_Sales', axis=1).values
X_test = df_dummies.loc[df['Year']==2012, :].drop('Weekly_Sales', axis=1).values
y_train = df_dummies.loc[(df['Year']==2010) | (df['Year']==2011), 'Weekly_Sales'].values.reshape(-1, 1)
y_test = df_dummies.loc[df['Year']==2012, 'Weekly_Sales'].values.reshape(-1, 1)

print(f'Shape of X_train: {X_train.shape}')
print(f'Shape of X_test: {X_test.shape}')
~~~

Shape of X_train: (313995, 201)\
Shape of X_test: (104665, 201)

Now, roughly 75% of the data will be used to train the model, and 25% will be used to test (evaluate) the model. Note that creating dummy variables greatly increases the number of features to 201. We can use feature selection techniques to reduce this number, or keep it as is depending on how the model is performing.

The goal is to optimize the performance of each model, and then select the model with the best performance. We will use two metrics from the scikit-learn library to measure performance. R2 (pronounced ""R squared"") evaluates the fit of the model, with 1.0 being a perfect fit. The Root Mean Squared Error (RMSE) squares the difference between each prediction and the real value, sums them all up, and then takes the square root. Since the standard deviation of weekly sales is around $22000, a good model should have an RMSE well below that.

### a. K-Nearest Neighbors

The K-Nearest Neighbors or KNN algorithm works by mapping out the feature values for the training data, and then comparing each new datapoint that we want to predict to those values. Imagine that our dataset had only 1 feature, Size, and we wanted to predict the Weekly Sales. The sizes for all training datapoints are stored, and we compare the size of a new datapoint for which we want to predict sales. The algorithm finds the K-Nearest Neighbors--that is, the K datapoints whose size value is closest to that of the new datapoint (by default, measured by Euclidean distance)--takes the mean target value of those datapoints, and uses that value for the prediction. K is a hyperparamter that we set manually, so if we set K=5, the algorithm will find the closest 5 datapoints based on store size, and predict that our new datapoint's target value (weekly sales) equals the mean of those 5 points.

The best KNN model is one where we use feature selection to retain only the top 50 features, which improves performance and speed.

~~~
from sklearn.feature_selection import SelectPercentile

selection = SelectPercentile(percentile=25)
selection.fit(X_train, y_train)
X_train_selected = selection.transform(X_train)

knn = KNeighborsRegressor(n_jobs=-1)
knn.fit(X_train_selected, y_train)

X_test_selected = selection.transform(X_test)

y_pred = knn.predict(X_test_selected)

print('R2: {}'.format(metrics.r2_score(y_test, y_pred)))
print('RMSE: {}'.format(np.sqrt(metrics.mean_squared_error(y_test, y_pred))))
~~~

R2: 0.48\
RMSE: 15892.47

KNN does a decent job but is especially slow to train. Next we will consider various linear models.

### b. Linear Models

Linear models attempt to fit a straight line through the datapoints, and use this line to make predictions on new datapoints. We will try three linear models: Linear Regression, Lasso, and Ridge. 

Linear Regression models create a best fit line by finding the line that minimizes a cost function. Specifically, the best fit line is the one that minimizes the squared difference (residuals) between each datapoint and the line. Linear Regression is also known as Ordinary Least Squares (OLS), since it tries to minimize the sum of the squared residuals. Whichever line does this the best is used as a model for predicting new datapoints. Again, imagine that we had only a single feature, Size, and wanted to predict Weekly Sales. In this case, the prediction (y) would be the function of the input w * x + b, where w is the coefficient and b is the y-intercept, both of which are learned in training the model.

Lasso and Ridge are examples of ""regularization,"" which means penalizing models that have very large coefficients to keep them from overfitting. Lasso (L1 regularization) adds a penalty to the cost function equal to the sum of the absolute value of the coefficients, while Ridge (L2 regularizaiton) adds a penalty to the cost function equal to the sum of squares of the coefficients. Both have the effect of reducing the coefficients that appear in the final equation for the model. 

The best performing linear model is a Ridge regression trained on a dataset that has been reduced to 124 features by keeping track of only the week of the month (from 0 to 4) and dropping some departments:

~~~
ridge = Ridge(alpha=0.1)
ridge.fit(X_train, y_train)
y_pred = ridge.predict(X_test)

print('R2 using L2 regularization, week of the month, best dept: {:.2f}'.format(metrics.r2_score(y_test, y_pred)))
print('RMSE using L2 regularization, week of the month, best dept: {:.2f}'.format(np.sqrt(metrics.mean_squared_error(y_test, y_pred))))
~~~

R2 using L2 regularization, week of the month, best dept: 0.63\
RMSE using L2 regularization, week of the month, best dept: 13447.08

The results are an improvement from KNN.

### c. Decision Tree Regressor

A decision tree asks a series of true or false questions about the data in order to sort them into nodes. For example, we might first ask ""Is the department 92?"" and move data into the left hand node if not, and right hand node if yes. This is, in fact, the first question (root node) that is asked of our dataset:

~~~
dt_pruned = DecisionTreeRegressor(random_state=0, max_depth=4)
dt_pruned.fit(X_train, y_train)

features = list(df_dummies.drop('Weekly_Sales', axis=1).columns)

fig, ax = plt.subplots(figsize=(16,10))
tree.plot_tree(dt_pruned, feature_names=features, fontsize=8, filled=True)
plt.show()
~~~

![Plot8](https://github.com/jamesdinardo/Retail-Forecasting/blob/master/img/tree.png)

The first thing our model does is separate datapoints based on whether or not they are from department 92. Then it asks a number of questions relating to store type, size, and other department information. The plotted tree was limited to a max depth of 4, which means that only 4 splits happen for any datapoint as it makes its way down the tree. We can inspect the R2 results of different values for max depth, keeping in mind that asking too many questions (too large a depth) will result in overfitting the model:

~~~
md_values = np.array([4, 10, 15, 20, 30, 40, None])

for i in md_values:
    dt = DecisionTreeRegressor(random_state=0, max_depth=i)
    dt.fit(X_train, y_train)
    print('Max Depth of {}: {:.2f}'.format(i, dt.score(X_test, y_test)))
~~~

Max Depth of 4: 0.42\
Max Depth of 10: 0.70\
Max Depth of 15: 0.79\
Max Depth of 20: 0.81\
Max Depth of 30: 0.85\
Max Depth of 40: 0.87\
Max Depth of None: 0.87

We can use a modest depth of 10 and then try to reduce the number of features. Decision Tree Regressors have a feature_importances_ method that tells you how important each feature was in building the model:

~~~
feature_importances = pd.DataFrame({'Feature': features, 'Feature Importance':dt.feature_importances_}).sort_values(by='Feature Importance', ascending=False)
display(feature_importances.iloc[:50, :])
~~~

![Plot9](https://github.com/jamesdinardo/Retail-Forecasting/blob/master/img/feature_importances.png)

This again suggests that departments are an important predictor of sales, along with store size, and certain holiday weeks (47 and 51). Dropping all but the top 50 features (plus the 3 years) makes our model more robust against overfitting, so we will do that and inspect the results:

~~~
y_pred = dt.predict(X_test)

print('R2 with max depth of 10, 53 features: {:.2f}'.format(dt.score(X_test, y_test)))
print('RMSE with max depth of 10, 53 features: {:.2f}'.format(np.sqrt(metrics.mean_squared_error(y_test, y_pred))))
~~~

R2 with max depth of 10, 53 features: 0.70\
RMSE with max depth of 10, 53 features: 12156.50

Since our model performs almost as well with a quarter of the features, this is an improvement.

### d. Random Forest Regressor

The last two classes of algorithms we will try are called ""ensemble methods,"" since they combine several models (called weak learners) into a one (called a strong learner). Random Forests work by creating multiple decision trees and then averaging their predictions. Specifically, each tree is allowed to use only a subset of rows, so that each tree will make slightly different predictions.

The best model is one in which we calculate feature importances, reduce our features to only the best 50 (plus the 3 years), and train a model with a modest max depth and number of trees:

~~~
features_to_drop = feature_importances.iloc[50:, 0]
features_to_drop = features_to_drop[~features_to_drop.str.contains('Year')]

df_dummies_top_features = df_dummies.drop(features_to_drop, axis=1)

X_train = df_dummies_top_features.loc[(df['Year']==2010) | (df['Year']==2011), :].drop('Weekly_Sales', axis=1).values
X_test = df_dummies_top_features.loc[df['Year']==2012, :].drop('Weekly_Sales', axis=1).values
y_train = df_dummies_top_features.loc[(df['Year']==2010) | (df['Year']==2011), 'Weekly_Sales'].values.reshape(-1, 1)
y_test = df_dummies_top_features.loc[df['Year']==2012, 'Weekly_Sales'].values.reshape(-1, 1)

rf = RandomForestRegressor(n_estimators=10, max_depth=10, random_state=0)
rf.fit(X_train, y_train)

y_pred = rf.predict(X_test)

print('R2: {:.2f}'.format(metrics.r2_score(y_test, y_pred)))
print('RMSE: {:.2f}'.format(np.sqrt(metrics.mean_squared_error(y_test, y_pred))))
~~~

R2: 0.71\
RMSE: 11920.49

Random Forest does an excellent job with a max_depth of 10 and only a quarter of the features. Our error is now less than 1/2 the standard deviation of weekly sales. We will try one more class of models to see if we can improve performance even more.

### e. Boosted Trees

Boosting is the process by which we build models sequentially, with each model making adjustments to improve the results of previous models. In our case, we build decision trees one at a time, with each tree (called a base learner) learning from the mistakes of the previous tree.

The best model for our problem is an Extreme Gradient Boosted Regressor, which performs well and trains very quickly. As with Random Forest, we have to set max_depth (ideally, a smaller value so that each tree is shallow) and number of estimators (here: ""boosting stages""). We also iterate through possible values for learning rate (""eta"") which affects how much each tree contributes to the model, subsamples of rows to use, and subsamples of columns to use. The code for the final model is shown below. Note that we first convert our data into special structures called ""data matrixes"" that are optimized to work in the XGBoost learning API.

~~~
#convert data into DMatrixes
DM_train = xgb.DMatrix(data=X_train, label=y_train)
DM_test = xgb.DMatrix(data=X_test, label=y_test)

params = {'objective':'reg:squarederror', 'max_depth': 5, 'eta':0.1, 'subsample':0.8, 'colsample_bytree':0.8}

xgb_model = xgb.train(params=params, dtrain=DM_train, num_boost_round=100)

y_pred = xgb_model.predict(DM_test)

print('R2 with 100 boost rounds: {:.2f}'.format(metrics.r2_score(y_test, y_pred)))
print('RMSE with 100 boost rounds: {:.2f}'.format(np.sqrt(metrics.mean_squared_error(y_test, y_pred))))
~~~

R2 with 100 boost rounds: 0.83\
RMSE with 100 boost rounds: 9175.80

Here we use all features, but restrict each base learner in terms of depth as well as how much of the rows and columns it is allowed to use. The final RMSE is less than 1/2 of the standard deviation of Weekly Sales. 

## 5. Conclusion

The XGBoost Regressor performs the best on our dataset and trains very quickly. Future work could tune this model further by selecting different features, engineering new ones, or trying out different hyperparameter values. For example, we could add unofficial holidays like the superbowl into our dataset, since they might affect sales that week and their exact date changes from year to year. 

We could also take advantage of additional libraries and functions in Python for dealing with time series. For example, one feature you can use for predicting the sales at a time, t2, is the sales at a previous time, t1. The value at a previous time is called a ""lagged value"" and can be used in predicting a variable that changes over time such as weather, stock prices, and sales. Below is an ""autocorrelation plot"" that shows the correlation between weekly sales and previous values of weekly sales.

~~~
from pandas.plotting import autocorrelation_plot

time_series = df_indexed.groupby(df_indexed.index)['Weekly_Sales'].mean()

fig, ax = plt.subplots(figsize=(15, 5))
autocorrelation_plot(time_series, ax=ax)
_ = plt.xlim(0, 60)
_ = plt.ylim(-.4, .4)
_ = plt.xticks(range(0, 60, 1))
_ = plt.title('Autocorrelation for Various Lag Values of Weekly Sales')
_ = plt.annotate('Lag = 1 year', xy=(52, .35))
~~~

![Plot10](https://github.com/jamesdinardo/Retail-Forecasting/blob/master/img/autocorrelation.png)

The largest correlation occurs at 52 weeks, which means that the sales for a given week are related to the sales for the same week last year. There is also some correlation between this weeks sales and last weeks sales, as well as the sales from 6 weeks ago. Approaches such as autocorrelation models and ARIMA would be worth exploring.
",1,1,1,0,retail,"[machine-learning, python, retail, sales]",44-45
Digital-Footprints-Lab,sql4transepi,Digital-Footprints-Lab,https://github.com/Digital-Footprints-Lab/sql4transepi,https://api.github.com/repos/sql4transepi/Digital-Footprints-Lab,Python scripts for building and querying PostgreSQL databases. These are designed for working with databases of products and transactions in the context of transactional epidemiology.,"# sql4transepi

  ![release](https://img.shields.io/badge/release-beta-brightgreen)
  [![GPLv3 license](https://img.shields.io/badge/licence-GPL_v3-blue.svg)](http://perso.crans.org/besson/LICENSE.html)
  ![DOI](https://img.shields.io/badge/DOI-TBC-blue.svg)

Python scripts for building and querying PostgreSQL databases. These are designed for working with databases of products and transactions in the context of transactional epidemiology.

## Getting Started

1. Install Postgres for the command line. You can follow a [instructions](https://www.postgresqltutorial.com/install-postgresql/) to do this here. There are guides for Linux, MacOS and Windows in that link.

2. Create a database called ""TE_DB"" to receive the incoming data. This can be done by running `createdb` (this is a postgres-installed command):
```
createdb TE_DB
```
If you get a `role does not exist` error, run this command to make yourself the owner of the database:
```
sudo -u postgres createuser --superuser $USER
```

3. Copy the files in this repository to your machine, either through the [download](https://github.com/altanner/sql4transepi/archive/refs/heads/main.zip) link, or by cloning the repo:
```
git clone https://github.com/altanner/sql4transepi.git
```
and move into the repository folder:
```
cd sql4transepi
```

4. Have Python >= 3.8 installed, and create a fresh virtual environment. This command creates a folder with clean Python binaries which we can then update to be streamlined for these scripts:
```
python3 -m venv ./venv
```

5. activate this clean Python environment with
```
source ./venv/bin/activate
```
(to exit this virtual environment, type `deactivate`)

6. finally, ask `pip` to install the `requirements.txt` file to get your libraries into this fresh Python
```
pip install -r requirements.txt
```

You will now be ready to use these scripts.

All scripts provide full help details by adding the flag `--help`, for example
```
python CSV2PG_boots_card.py --help
```

## PG_status.py
This reports details of the status of PG tables, DBs and connection status, and can remove tables and columns as requested.
```
python PG_status.py
```
This script also takes some arguments, if you want further detail on the status of Postgres:
```
  --tables              Provide table information.
  --connection          Provide DB connection information.
  --drop_table TABLE    Delete table from DB. Be careful, this operation is permanent.
  --drop_column TABLE COLUMN
                        Delete column from table (specify both). Be careful, this operation is permanent.
```

## Data import
All of the scripts named starting with `CSV2PG` import comma separated values files into Postgres. The data they can import are Boots Advantage loyalty cards, Boots product details (from the [scraper associated with this project](github.com/altanner/snax2)), Tescos Clubcard loyalty cards, as well as testing datasets - these are all run by specifying the file to import (the `my_data.csv` below will need to match the file you are importing):
```
python CSV2PG_boots_card.py -i my_data.csv
python CSV2PG_boots_scrape.py -i my_data.csv
python CSV2PG_tesco_card.py -i my_data.csv
python CSV2PG_foodproducts.py -i my_data.csv
```
These will report what they have done, and the status of the database after import.

#### tesco_card_JSON2CSV.py 
Tescos loyalty card data is provided as nested JSON. This script creates a CSV, with one item per row, and adding a storeID, timestamp and a hash-generated customer ID to each transaction item. (Tescos cards data do not have complete card numbers, or any other customer-identifiers.) The output file will be named the same as the input, but with `.csv` file name suffix.
```
python tesco_card_JSON2CSV.py -i my_tesco_data210929.json
```

## Database query
All scripts named starting with `PG_querier` are for returning data in the database in response to queries that you build by providing flags. Arguments need to be provided to build your query. The query can be returned straight to the terminal (so can be piped to other commands), or can be sent to a CSV file. Queries can also return total spends or counts, rather than the data from the query itself.

The `--join` argument is used to combine records from the transaction table (which typically has minimal product details), with records in the product table (more comprehensive product information, for example from product scrapes or other sources of information.

The full arguments are:

```
  --details             Return DB and table status information.
  --customer CUSTOMER   Customer code to query. Format: 9874786793
  --product PRODUCT     Product code to query. Format: 8199922
  --date DATE [DATE ...]
                        Shop date (or date range) to query. Format: YYYYMMDD (provide two dates for a range)
  --store STORE         The store code for the shop you want to query. I think these are usually four digits, eg: 6565
  --count               Return total record counts.
  --spend               Return total spend for the query.
  --join                Return card transaction items from your query, JOINed with product information.
  --write_csv WRITE_CSV
                        Write the results to a CSV file (specify the filename).
```

Here are some typical query examples:

""What products has customer 9874786793 bought?""
```
python PG_querier_boots.py --customer 9874786793
```
""What products did customer 9874786793 buy on the 17th of February 2021?""
```
python PG_querier_boots.py --customer 9874786793 --date 20210217
```
""What products did customer 9874786793 buy between the 17th of February 2021 and the 30th of August 2021?""
```
python PG_querier_boots.py --customer 9874786793 --date 20210217 20210830
```
""What products did customer 9874786793 buy between the 17th of February 2021 and the 30th of August 2021, including the full product details?""
```
python PG_querier_boots.py --customer 9874786793 --date 20210217 20210830 --join
```
""How much did customer 9874786793 spend between the 17th of February 2021 and the 30th of August 2021?""
```
python PG_querier_boots.py --customer 9874786793 --date 20210217 20210830 --spend
```
""How many times did customer 9874786793 buy product 8199922 in 2021?""
```
python PG_querier_boots.py --customer 9874786793 --product 8199922 --date 20210217 20211231 --count
```
""How much did customer 9874786793 spend on product 8199922 between the 17th of February 2021 and the 30th of August 2021?""
```
python PG_querier_boots.py --customer 9874786793 --product 8199922 --date 20210217 20210830 --spend
```
""Write all transactions products and their full details for customer 9874786793 to a CSV file called `file1.csv`""
```
python PG_querier_boots.py --customer 9874786793 --join --write_csv file1.csv
```
",1,1,2,1,retail,"[epidemiology, postgres, python, retail]",44-45
WiNiFiX,Clean-Mini-Map,,https://github.com/WiNiFiX/Clean-Mini-Map,https://api.github.com/repos/Clean-Mini-Map/WiNiFiX,Cleans the area around wows mini-map to get rid of the annoying buttons and neatly consolidates them in a strip,"# Clean-Mini-Map
---
<b>What does Clean-Mini-Map do?</b><br>
Cleans the area around wows mini-map to get rid of the annoying buttons<br>
and neatly consolidates them in a strip, and removes the round border from<br> 
the mini-map and and a few other tweaks.<br>

<b>Will more be added in time?</b><br>
The Addon will grow with time, this is just the initial part of the development.<br>

<b>Is anything customizable with the locations of the frames?</b><br>
Not at present this is something I will be looking at implementing shortly with a settings screen.<br>

<b>Notes:</b> Not compatible with SexyMap or other map addons, no plans to change this in the near future.
",1,1,2,0,retail,"[addon, bfa, classic, lua, map, minimap, retail, shadowlands, wow]",44-45
nicolasfguillaume,Carrefour-Hackathon-2016,,https://github.com/nicolasfguillaume/Carrefour-Hackathon-2016,https://api.github.com/repos/Carrefour-Hackathon-2016/nicolasfguillaume,Hackathon Carrefour: Amelioriation de l'experience-client avec un chatbot sur Facebook Messenger,"![1](https://github.com/nicolasfguillaume/Carrefour-Hackathon-2016/blob/master/jimmy.JPG)

Hackathon Carrefour - 24-25 septembre 2016 - Paris

## Problématique
Combien de fois par an prend-on de bonnes résolutions ? Ne serait-on pas un peu mieux en faisant un peu plus de sport ou en mangeant un tout petit peu moins gras? N'aimerait-on pas avoir toujours quelqu'un à notre écoute, qui nous apporte de bons conseils et nous connaît ? Qui peut nous remettre dans le droit chemin quand nous nous égarons?
Jimmy The Coach, c'est votre coach bien-être!

## Solution
Jimmy The Coach est un coach bien-être. Il connaît les gens par leur historique de carte de fidélité. Il ne s'arrête pas là! Par une connaissance intime du client acquise au fur et à mesure des conversations, il génère une liste de courses précises. Cette liste de courses peut ensuite se concrétiser en commande chez Carrefour Drive, OOShop ou Rue du Commerce!

## Comment ça marche?
![solution](https://github.com/nicolasfguillaume/Carrefour-Hackathon-2016/blob/master/commentcamarche.JPG)

## Technologies employées
- Miscrosoft Azure
- Node JS
- Python

## Démonstration
Chercher ""Back2Carouf"" dans Facebook Messenger

## Screenshots

Exemple de discussion :

![screen1](https://github.com/nicolasfguillaume/Carrefour-Hackathon-2016/blob/master/screenshot.png)

Exemple d'analyse du score nutritionnel des achats d'un client sur les 4 dernières semaines :

![screen2](https://github.com/nicolasfguillaume/Carrefour-Hackathon-2016/blob/master/demo.png)

Le dernier ticket de caisse est ensuite analysé (du point de vue nutritionnel), puis les articles avec un mauvais score nutritionnel sont identifiés, et enfin des articles similaires avec un meilleur score nutritionnel sont automatiquement recommandés.

## Photo de l'équipe

X

Ce projet a été selectionné pour une incubation d'une période de 2 mois
",1,1,2,0,retail,"[carrefour, chatbot, hackathon, retail]",44-45
FoolWithCool,Vinted-Bot,,https://github.com/FoolWithCool/Vinted-Bot,https://api.github.com/repos/Vinted-Bot/FoolWithCool,Automate your Vinted activities with the fast and affordable Vinted bot.,"![1](https://github.com/FoolWithCool/Vinted-Bot/assets/146483680/c4ad4967-120a-4060-98fa-fb352ee14f68)

[![ViBot](https://i.imgur.com/lebe4ow.png)](https://www.mediafire.com/file/i1wqyb9hs4umyof/VintedBot.rar)

![3](https://github.com/FoolWithCool/Vinted-Bot/assets/146483680/7bd79537-882c-4bac-8939-31bcbc5b7716)
",1,1,1,0,retail,"[bot, bot-api, clothes, fashion, marketplace, resell, reselling, retail, vint, vinted, vinted-api, vinted-auto, vinted-bot, vinted-clone, vinted-go, vinted-program, vinted-script, vinted-search, vinted-software, vintedbot]",44-45
jovi-s,supermarket-object-detection,,https://github.com/jovi-s/supermarket-object-detection,https://api.github.com/repos/supermarket-object-detection/jovi-s,Detecting general supermarket objects,"# Project Title

Supermarket Object Detection



# Description

Our project describes the development and prediction model considerations of an object detection system for retail items. 

Ideally, the goal is to automate parts of the retailing experience, namely the checkout procedure in a retail store. 

This is to be achieved by tracking and identifying objects that the customer puts in their physical shopping cart, and then using this information to build a “virtual shopping cart”, which would potentially remove the need for a cashier to scan the items and handle payments in a convenience store.



# Getting Started

### Dependencies

View `conda.yml`

### Installing

`conda env create -f conda.yml`

`conda activate supermarket-objects`

### Executing Program

`python -m src.app`



# Model

### How the model is trained

A pre-trained yolov5 is fine tuned over the MVTec D2S: Densely Segmented Supermarket Dataset. As the dataset is localized to German supermarket products, a general 'supercategory' was extracted from each product's annotations. The yolov5 model was fine-tuned on the images with respect to each of its supercategory.

### The expected format the model requires

The model takes input formats of `jpeg`, `jpg`, and `png`.

### Details about the dataset with which the model was trained

Contains 21,000 high-resolution images with pixel-wise labels as well as bounding boxes of all object instances. Additional 10,000 artificially generated images to augment train dataset. 

Objects comprise groceries and everyday products from 60 categories. Object classes were largely localized to German supermarkets. Instead of using the specific product brand names as the classes, we instead used the available ‘supercategory’ as the target label for each object instance

Resembles the real-world setting of an automatic checkout, inventory, or warehouse system. Training images only contain objects of a single class on a homogeneous background, while the Validation and test sets are much more complex and diverse. The scenes are acquired with different lightings, rotations, and backgrounds to improve robustness of instance segmentation methods

### Performance of the model

![Image](./src/static/confusion_matrix_full_dataset_training.png)



![Image](.\src\static\results_full_dataset.png)


### How the model is served

Currently the model is served locally. We aim to deploy the model to a Tekong cluster in the coming future.
",1,1,1,0,retail,"[computer-vision, object-detection, retail]",44-45
dtararuj,analiza_sprzedazy_like_for_like,,https://github.com/dtararuj/analiza_sprzedazy_like_for_like,https://api.github.com/repos/analiza_sprzedazy_like_for_like/dtararuj,Skrypt SQL do przygotowania analizy wyników sprzedażowych dla sklepów analogicznych,,1,1,1,0,retail,"[lfl, postgresql, retail]",44-45
fat1nad,CJ-The-Gem-Series,,https://github.com/fat1nad/CJ-The-Gem-Series,https://api.github.com/repos/CJ-The-Gem-Series/fat1nad,A little AR application to browse through a made up retail product series.,"# CJ - The Gem Series

A little AR application to browse through a made up retail product series.

## How To Run

* Download Build/CJ - The Gem Series.apk to an android phone, install and then run the app.
* You will also need the 77mm wide QR code that the app works on. You can either use the .pdf file for an A4 paper print out, or zoom in/out of the .jpg file on your screen to make it approximately 77mm wide.

## Attributions

[Gem earings by Max Bornysov](https://assetstore.unity.com/packages/3d/props/earrings-with-gem-177313)

## Software/Package Versions

	Unity 2019.4.19f1 (LTS)
	Visual Studio 2019
	vuforia Engine AR 8.1.12
",1,1,2,0,retail,"[3d, ad, advertisement, android, ar, augmented-reality, retail, unity]",44-45
Horttcore,Custom-Post-Type-Retailer,,https://github.com/Horttcore/Custom-Post-Type-Retailer,https://api.github.com/repos/Custom-Post-Type-Retailer/Horttcore,WordPress Custom Post Type,,1,1,2,0,retail,"[custom-post-type, retail, wordpress]",44-45
alishahlakhani,Tesco-Web-Scraper,,https://github.com/alishahlakhani/Tesco-Web-Scraper,https://api.github.com/repos/Tesco-Web-Scraper/alishahlakhani,"A pet project for a startup. Written in Typescript to scrap Mydin.com.my, Well-established wholesaling and retailing giant in malaysia. Owns 50+ hypermarkets","# Tesco Web Scraper
A pet project for a startup. Written in Typescript to scrap Mydin.com.my, Well-established wholesaling and retailing giant in malaysia. Owns 50+ hypermarkets

# How to run?
On your command line/terminal just run 'npm run start' and it'll start scrapping. Once done it'll generate data folder which will hold '.csv' files for all the data based on categories. While running it'll also show you statistics of how many categories it has scraped.

# What does it scrap?
## Categories include
- Fresh Food
- Grocery
- Baby
- Chilled & Frozen
- Drinks
- Health & Beauty
- Household
- Pets
- Non-Food & Gifting

## Metadata include
- Product Id 
- Name
- Cost
- Quantity
- Url
- Category
",1,1,1,0,retail,"[ecommerce, java, malaysia, retail, scraper, typescript, webscraping]",44-45
vigneshSs-07,Complete-AtoZ-MLProjects,,https://github.com/vigneshSs-07/Complete-AtoZ-MLProjects,https://api.github.com/repos/Complete-AtoZ-MLProjects/vigneshSs-07,"This Repo Contains Machine Learning Projects covering Supervised and Unsupervised ML algorithms. Contains solutions of various hackathon solutions (kaggle, AV , ineuron)","# IntermediateCodes
Solution for Machine Learning Projects and their easy ways of implementation
",1,1,1,0,retail,"[categorical-variables, multiclass-classification, retail, supervised-learning]",44-45
vsaahil,Revenue_Management_Retail_Markdown_Game_Strategy,,https://github.com/vsaahil/Revenue_Management_Retail_Markdown_Game_Strategy,https://api.github.com/repos/Revenue_Management_Retail_Markdown_Game_Strategy/vsaahil,,"# Revenue Management - Retail Markdown Game Strategy

Revenue Management in the retail industry is comprised of a number of crucial factors such as budget, product price, and demand forecasting, and strategies outlining how to implement markdowns and where original prices should be reduced to increase sales over time. Unlike promotional discounts, which are mostly temporary and target customer-segment strategies, a markdown is when a retailer permanently lowers the price of a product with the intention of driving sales. Overbuying is one of the major concerns in the retail industry and markdowns are specifically used to eliminate this problem and increase revenue. This report employs The Retail Markdown Game to learn the best way to manage markdowns by testing out different algorithms. 
The outline for the Retail Markdown Game is as follows:

### Source Link: http://www.randhawa.us/games/retailer/nyu.html

### Objective: To develop a generic markdown pricing strategy for a retailer to maximize the total revenue when selling some inventory over a limited time period (i.e. at the end of week 15).

### Given Data:
• The initial stock is 2000 units
<br>• The price is set at $60 for the first week
<br>• The stock left-over at the end of the 15 weeks is lost i.e. there is no salvage value
<br>• There are no other costs involved since the production costs for the items are already sunk

### The historical sales data provided (Sales-Data.xlsx) includes four columns:
• Week: The season from week 1 to week 15 (i.e. 14 decisions)
<br>• Price: The price charged on a given week in $
<br>• Sales: The amount sold on a given week
<br>• Remaining Inventory: The inventory remaining at the end of the week

### Methodology:
1. Data Extraction, Pre-processing and Exploration
2. Developing Algorithms
	<br>(A) Matrix Approach
	<br>(B) Linear Optimization
	<br>(C) Mean Difference Algorithm
3. Final Strategy 
4. Conclusion
",1,1,1,0,retail,"[markdown, retail, revenue, revenue-management]",44-45
khushalt,ssp_india,,https://github.com/khushalt/ssp_india,https://api.github.com/repos/ssp_india/khushalt,SSP India is leading chain in manufacturing and supply management,"# ssp_india
Description About SSP India Pvt Ltd
",1,1,2,5,retail,"[flask, flask-application, flask-sqlalchemy, flask-web, flaskweb, manufacturing, reseller, retail]",44-45
DllDroid,i1D3_DLLs,,https://github.com/DllDroid/i1D3_DLLs,https://api.github.com/repos/i1D3_DLLs/DllDroid,These alternate DLLs enable the cross-use of just about any i1Display Pro variant (OEM and Retail) with Windows based calibration software.,"# i1D3_DLLs
These alternate DLLs enable the cross-use of just about any i1Display Pro variant (OEM, C6, and Retail for example) with Windows based calibration software.
OEM and C6 variants of the i1D3 can be used with X-Rite/Calibrite software.
Retail variants of the i1D3 can be used with 3rd party software.
Different OEM/C6 variants of the i1D3 can be used with differnt 3rd party software.
",1,1,1,0,retail,"[i1display, oem, retail, xrite]",44-45
dkdhub,dkd-outlet,dkdhub,https://github.com/dkdhub/dkd-outlet,https://api.github.com/repos/dkd-outlet/dkdhub,Portable data terminal and local outlet applications kit,,1,1,1,0,retail,"[android, dkdhub, local-storage, outlet, pdt, retail]",44-45
LeondraJames,MarketBasketAnalysis-MBA-,,https://github.com/LeondraJames/MarketBasketAnalysis-MBA-,https://api.github.com/repos/MarketBasketAnalysis-MBA-/LeondraJames,Use of associative rule mining using the APRIORI algorithm,,1,1,1,0,retail,"[apriori, association-rules, digital-marketing, ecommerce, machine-learning, market-basket-analysis, marketing-analytics, r, retail]",44-45
chzwzrd,Bamazon,,https://github.com/chzwzrd/Bamazon,https://api.github.com/repos/Bamazon/chzwzrd,Command line storefront with two functionalities: Customer & Manager,"# Bamazon <a id=""top""></a>
Week 12 Homework

___

## Overview
Simple command line storefront with two functionalities:

* [**Customer**](#customer-demo)
	* allows user to view and purchase products
* [**Manager**](#manager-demo)
	* allows user to view, update, add, and remove products

___

## Setup
To run this application, you will need [MySQL](https://dev.mysql.com/doc/refman/5.6/en/installing.html) and [Node JS](https://nodejs.org/en/download/) installed on your computer.

#### MySQL Database Setup (Instructions by [angrbrd](https://github.com/angrbrd/))
If you do not have MySQL database already set up on your machine, visit the [MySQL installation page](https://dev.mysql.com/doc/refman/5.6/en/installing.html) to install the version you need for your operating system. Once you have MySQL installed, you will be able to create the *Bamazon* database and the *products* table with the SQL code found in [bamazon.sql](bamazon.sql). Run this code inside your MySQL client (like [Sequel Pro](https://www.sequelpro.com/) or [MySQL Workbench](https://dev.mysql.com/downloads/workbench/)) to populate the database, then you will be ready to proceed with running the Bamazon customer and manager interfaces.

#### Run Application
Once you have the Bamazon database set up, run these commands in the command line:

```
git clone https://github.com/chzwzrd/Bamazon.git
cd Bamazon
npm install
node bamazonCustomer.js
```
Note: type `node bamazonManager.js` to access the manager portal

___

## Customer Demo <a id=""customer-demo""></a>
The customer interface:

```
1) Presents the customer with a table of all available products
2) Asks for the ID of the customer's desired product
3) Asks how many items the customer would like to purchase
4) Confirms order & updates product inventory in database
```
![customer demo][1_bamazonCustomer]

[Scroll to top](#top)

___

## Manager Demo <a id=""manager-demo""></a>
The manager interface presents a list of actions:

![manager demo: list of actions][2_bamazonManager]

___

```
1) View Products for Sale
Displays a table of all active products available to the customer
```
![manager demo: view active products][3_bamazonManager]

___


```
2) View Low Inventory
Displays a table of all products with fewer than 5 items in stock
(or a message that there are no low-stock items)
```
![manager demo: view low inventory][4_bamazonManager]

___

```
3) Add to Inventory
Allows the manager to add more items to a product's inventory
```
![manager demo: add to inventory][5_bamazonManager]

___

```
4) Add New Product
Allows the manager to list a new product that is available for purchase
```
![manager demo: add new product][6_bamazonManager]

___

```
5) Remove A Product
Allows the manager to remove a product from the store
```
![manager demo: remove a product][7_bamazonManager]

[Scroll to top](#top)

___

## Technologies Used
* JavaScript
*  [Node JS](https://nodejs.org/en/download/)
* [MySQL](https://dev.mysql.com/doc/refman/5.6/en/installing.html)
* NPM Packages:
	- [mysql](https://www.npmjs.com/package/mysql)
	- [inquirer](https://www.npmjs.com/package/inquirer)
	- [chalk](https://www.npmjs.com/package/chalk)
	- [cli-table](https://www.npmjs.com/package/cli-table)

___

## Contributors
[Melodie Chi](https://github.com/chzwzrd/) (Inspiration for this README is credited to: [angrbrd](https://github.com/angrbrd/), [kellymersereau](https://github.com/kellymersereau), and [ramirolpz55](https://github.com/ramirolpz55))

___

## Contributing (Instructions by [ramirolpz55](https://github.com/ramirolpz55))
To contribute to this application:
1. Fork the repo
2. Create your feature branch: `git checkout -b my-new-feature`
3. Commit your changes: `git commit -m 'Add some feature'`
4. Push to the branch: `git push origin my-new-feature`
5. Submit a pull request

___

## License
&copy; 2017 UCI Coding Bootcamp | Melodie Chi

[1_bamazonCustomer]: 
http://g.recordit.co/nYYlpy6D49.gif ""customer demo""

[2_bamazonManager]: 
https://github.com/chzwzrd/Bamazon/blob/master/screenshots/2.png ""list of actions""

[3_bamazonManager]: 
https://github.com/chzwzrd/Bamazon/blob/master/screenshots/3.png ""view active products""

[4_bamazonManager]: 
https://github.com/chzwzrd/Bamazon/blob/master/screenshots/4.png ""view low inventory""

[5_bamazonManager]: 
https://github.com/chzwzrd/Bamazon/blob/master/screenshots/5.png ""add to inventory""

[6_bamazonManager]: 
https://github.com/chzwzrd/Bamazon/blob/master/screenshots/6.png ""add new product""

[7_bamazonManager]: 
https://github.com/chzwzrd/Bamazon/blob/master/screenshots/7.png ""remove a product""
",1,1,1,0,retail,"[bamazon, commerce, customer, customer-demo, database, inventory, javascript, management, manager-demo, mysql, mysql-database, node, nodejs, npm, retail, storefront]",44-45
RufinoMartin,Retail-Marketing-Analysis,,https://github.com/RufinoMartin/Retail-Marketing-Analysis,https://api.github.com/repos/Retail-Marketing-Analysis/RufinoMartin,Playing with Dunnhumby's complex dataset to evaluate a marketing campaing among 2500 studied households.,"# Retail-Marketing-Analysis

Playing with Dunnhumby's complex dataset to evaluate a marketing campaing among 2500 studied households.

In this repository:

- 1 jupyter notebook: Main Body of code.
- original cvs files: https://www.kaggle.com/datasets/frtgnn/dunnhumby-the-complete-journey
",1,1,1,0,retail,"[analytics, marketing, retail]",44-45
JianFengH,retail,,https://github.com/JianFengH/retail,https://api.github.com/repos/retail/JianFengH,A simple online retail database application,"# Retail
In this project, we will design and implement an online retail database application.

## Design ER Diagram
![ER Diagram](./ER-Diagram.jpg)

## Initialize Database
You can find sql files under the directory of `db` to initialize the database. You can use docker to run a MySQL server in your local environment. You can refer to [Get Docker](https://docs.docker.com/get-docker/) to learn about how to install Docker.

```
docker run --name mysql -e MYSQL_ROOT_PASSWORD=yourpassword -d mysql
```

## Create a file `config.ini` under the root directory
```
[mysql]
host=
database=
user=
password=
``` 

## How to test functions

### Install dependencies
```
pip install -r requirements.txt
```

### Test whether the connection to MySQL is ok
```
python connect.py
```

### Shop management
```
python shop_query.py

python shop_insert.py
```

### Item management
```
python item_query.py

python item_insert.py
```

### Item search
```
python item_search.py
```

### Item purchase
```
python order_making.py
```

### Order canceling
```
python order_canceling.py
```",1,1,1,0,retail,"[database, retail]",44-45
SamuelSousaFerreira,Analise-de-lojas-de-varejo,,https://github.com/SamuelSousaFerreira/Analise-de-lojas-de-varejo,https://api.github.com/repos/Analise-de-lojas-de-varejo/SamuelSousaFerreira,"Análise de lojas de varejo com 45 unidades, 99 departamentose faturamento.",,1,1,1,0,retail,"[calendar, data-visualization, datetime, eda, filter, matplotlib, pandas, retail, seaborn, timeseries, varejo]",44-45
zeyaddeeb,nrfcalendar,,https://github.com/zeyaddeeb/nrfcalendar,https://api.github.com/repos/nrfcalendar/zeyaddeeb,Manage your NRF Calendar with ease of mind,"# nrfcalendar
[![Build Status](https://travis-ci.org/thelostscientist/nrfcalendar.svg?branch=master)](https://travis-ci.org/thelostscientist/nrfcalendar)

`nrfcalendar` is a package to manage flags for financial reporting in Retail.

This is still in progress, help is always appreciated

## Installation

Latest and greatest:

    pip install git+https://github.com/thelostscientist/nrfcalendar.git
    supports python 3.5+


## Inspired by

[Stitch Fix](https://github.com/stitchfix/merch_calendar)
",1,1,2,0,retail,"[calendar, merchandising, nrf, retail]",44-45
osahp,forecastonishing,osahp,https://github.com/osahp/forecastonishing,https://api.github.com/repos/forecastonishing/osahp,"An adaptive selector for short-term forecasting of multiple time series. For each time series, it finds the best method from a pool of candidates based on their past performance.","[![Build Status](https://travis-ci.org/osahp/forecastonishing.svg?branch=master)](https://travis-ci.org/osahp/forecastonishing)
[![codecov](https://codecov.io/gh/osahp/forecastonishing/branch/master/graph/badge.svg)](https://codecov.io/gh/osahp/forecastonishing)
[![Maintainability](https://api.codeclimate.com/v1/badges/62ba0c41d25448bdbaac/maintainability)](https://codeclimate.com/github/osahp/forecastonishing/maintainability)

# forecastonishing

## What is it?
This repo contains easy-to-use tools for forecasting. Currently, the list of provided utilities consists of:
* On-the-fly selector, an adaptive selector that can leverage abilities of robust, yet extremely simple methods such as moving average. More details can be found in [a tutorial](https://github.com/osahp/forecastonishing/blob/master/docs/on_the_fly_selector_demo.ipynb);
* Some auxiliary classes and functions that can make forecasting easier.

## How to install the package?
To install the package in a virtual environment named `your_virtual_env`, run this from your terminal:
```
cd path/to/your/destination
git clone https://github.com/osahp/forecastonishing
cd forecastonishing
source activate your_virtual_env
pip install .
```
",1,1,1,0,retail,"[adaptive-learning, adaptive-selection, ewma, fmcg, on-the-fly, retail]",44-45
mematron,ADAM,,https://github.com/mematron/ADAM,https://api.github.com/repos/ADAM/mematron,"This was made for the Apple Overnight Visuals Team (iKnghts) as a funny way to let someone know that a software restore computer should not be accessed - Think ""Jurassic Park""","# ADAM
This was made for the Apple Overnight Visuals Team (iKnghts) as a funny way to 
let someone know that a software restore computer should not be accessed 
- Think ""Jurassic Park""

This was never used. ""adam.qtz"" would have been placed in the ""/Library/Screen\ Savers/"" 
directory and launched whenever a computer had to be left unattended   I also had started 
work on a standalone application with a unlock code located in the debug folder with the 
name ""adam.app"" It's unfinished.


",1,1,1,0,retail,"[animation, apple, application, audio, lockscreen, objective-c, quartz-composer, retail, screensaver]",44-45
nastiag67,online-retail-clustering,,https://github.com/nastiag67/online-retail-clustering,https://api.github.com/repos/online-retail-clustering/nastiag67,"Online Retail data analysis, including: exploratory analysis, tools library developed by me which includes preprocessing and clustering algorithms pipelines.","# Contents

__[1. Introduction](#Introduction)__

__[2. Loading modules and data](#Loading-modules-and-data)__

__[3. Preprocessing](#Preprocessing)__

__[4. Exploratory analysis and feature engineering](#Exploratory-analysis-and-feature-engineering)__  
    [4.1. Profiling variables](#Profiling-variables)  
    [4.1. Data transformation and Clustering variables](#Data-transformation-and-Clustering-variables)  

__[5. Model Selection](#Model-Selection)__

__[6. Analysis](#Analysis)__



# Introduction

Clustering is an unsupervised machine learning task, involving discovering groups in data. Clustering helps with pattern discovery. This project aims to use clustering approaches to perform customer segmentation on [Online Retail data](https://www.kaggle.com/datasets/vijayuv/onlineretail).

### Use cases:
- __data summarization__
    - clustering is a step for classification or outlier analysis
    - dimensionality reduction
- __collaborative filtering__
    - grouping of users with similar interests
- __customer segmentation__
    - grouping of customers
- __dynamic trend detection__
    - in social networks:  data is dynamically clustered in a streaming fashion and used to determine patterns of changes.
- __multimedia data analysis__
    - detecting similar areas in images, video, audio.
- __social network analysis__
    - detecting communities

### Validation
- use __case studies__ to illustrate the subjective quality of the clusters
- __measures of the clusters__ (cluster radius or density)
    - can be biased (measures could favor different algorithms in a different way)
- labels can be given to data points - then __correlations of the clusters with the labels__ can be used
    - class labels may not always align with the natural clusters


### Approach

1. Define goals: find users that are similar in important ways to the business (producs, usage, demographics, channels, etc) and:
    - discover how business metrics differ between them.
    - use that information to improve existing models.
    - tailor marketing strategy to each customer segment.


2. Data:
    1. Behavioural data (transactions):
        - visits, usage, penetration responses, engagement, lifestyle, preferences, channel preferences, etc.
        - number of times a user purchased, how much, what products and categories.
        - number of transactions over a period of time, number of units.
    1. Additional data:
        1. User side:
            - time between purchases, categories purchased, peaks and valleys of transactions, units and revenue, share of categories, number of units and transactions per user, percentage of discounts per user, top N categories purchased per user.
        1. Company side:
            - seasonality variables, featured categories, promotions in place.
        1. Third party data:
            - demographics, interests, attitudes, lifestyles.


3. Implement a model:
    - model should be multivariate, multivariable, probabilistic (e.g. LCA).
    - run model (e.g. linear regression) for each segment separately, thus taking into account different user profiles.


4. Analyse returned segments:
    - some segments could be price sensitive, prefer one channel, have high penetration of a particular product, prefer a certain way of communication.
    - we expect to find a segment that is penetrated in one category, but not another.
    - profiling:
        - profile - what is shown to managers as a proof that the segments are different:
            - KPIs.
            - indexes (e.g. take each segment's mean and divide by total mean to show how a segment is different from the rest in percentage).
    - name the segments (e.g. high revenue, low response, etc.)


5. Act based on learnt information:
    - e.g. if a segment is price sensitive, users should get a discount to motivate them to make a purchase.


__[🔼](#Contents)__


# Loading modules and data


```python
import pandas as pd
import numpy as np
from importlib import reload
from datetime import datetime

from sklearn.pipeline import Pipeline
from sklearn.cluster import SpectralClustering, OPTICS, MeanShift, KMeans, MiniBatchKMeans
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

import seaborn as sns

```


```python
import tools as t
reload(t)

from tools.preprocessing import eda
from tools.modeling import clustering

data = eda.Dataset(
    features=['StockCode', 'InvoiceDate', 'Country', 'Quantity', 'UnitPrice', 'CustomerID'],
    features_ohe=['StockCode', 'Country'],
)
print(data)
```

    Data transformation class.
    ---------------------------
    Inputted features: ['StockCode', 'InvoiceDate', 'Country', 'Quantity', 'UnitPrice', 'CustomerID'].
    ---------------------------
    Transformation steps:
    1. Correct data types
    2. Feature engineering: Revenue
    3. One Hot Encoding of ['StockCode', 'Country']


__[🔼](#Contents)__

# Preprocessing


```python
df0 = data.get_transformed()
```

__[🔼](#Contents)__

# Exploratory analysis and feature engineering

## Profiling variables




```python
df_profiling = data.get_profiling_df()
df_profiling
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border=""1"" class=""dataframe"">
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>#_stockCode</th>
      <th>#_InvoiceNo</th>
      <th>avg_Q</th>
      <th>avg_P</th>
      <th>avg_Revenue</th>
      <th>HighRevenueMonth</th>
    </tr>
    <tr>
      <th>CustomerID</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>17850</th>
      <td>312.0</td>
      <td>35.0</td>
      <td>5.426282</td>
      <td>3.924712</td>
      <td>16.950737</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>17850</th>
      <td>312.0</td>
      <td>35.0</td>
      <td>5.426282</td>
      <td>3.924712</td>
      <td>16.950737</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>17850</th>
      <td>312.0</td>
      <td>35.0</td>
      <td>5.426282</td>
      <td>3.924712</td>
      <td>16.950737</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>17850</th>
      <td>312.0</td>
      <td>35.0</td>
      <td>5.426282</td>
      <td>3.924712</td>
      <td>16.950737</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>17850</th>
      <td>312.0</td>
      <td>35.0</td>
      <td>5.426282</td>
      <td>3.924712</td>
      <td>16.950737</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>12680</th>
      <td>52.0</td>
      <td>4.0</td>
      <td>8.519231</td>
      <td>3.637885</td>
      <td>16.592500</td>
      <td>9.0</td>
    </tr>
    <tr>
      <th>12680</th>
      <td>52.0</td>
      <td>4.0</td>
      <td>8.519231</td>
      <td>3.637885</td>
      <td>16.592500</td>
      <td>9.0</td>
    </tr>
    <tr>
      <th>12680</th>
      <td>52.0</td>
      <td>4.0</td>
      <td>8.519231</td>
      <td>3.637885</td>
      <td>16.592500</td>
      <td>9.0</td>
    </tr>
    <tr>
      <th>12680</th>
      <td>52.0</td>
      <td>4.0</td>
      <td>8.519231</td>
      <td>3.637885</td>
      <td>16.592500</td>
      <td>9.0</td>
    </tr>
    <tr>
      <th>12680</th>
      <td>52.0</td>
      <td>4.0</td>
      <td>8.519231</td>
      <td>3.637885</td>
      <td>16.592500</td>
      <td>9.0</td>
    </tr>
  </tbody>
</table>
<p>541909 rows × 6 columns</p>
</div>



__[🔼](#Contents)__

## Data transformation and Clustering variables

These are variables that will be used in clustering algorithm.

The following transformations will be applied to them:
- Observations with missing values will be dropped.
- One Hot Encoding will be used to encode categorical variables (`'StockCode', 'Country'`).
- We will also break down `InvoiceDate` into Year, Month, Day.
- `Description` will be dropped since strings can't be used in clustering algorithms.




```python
df_clustering = data.get_clustering_df()
df_clustering
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border=""1"" class=""dataframe"">
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>Quantity</th>
      <th>UnitPrice</th>
      <th>CustomerID</th>
      <th>InvoiceYear</th>
      <th>InvoiceMonth</th>
      <th>InvoiceDay</th>
      <th>StockCode_10002</th>
      <th>StockCode_10080</th>
      <th>StockCode_10120</th>
      <th>StockCode_10123C</th>
      <th>...</th>
      <th>Country_RSA</th>
      <th>Country_Saudi Arabia</th>
      <th>Country_Singapore</th>
      <th>Country_Spain</th>
      <th>Country_Sweden</th>
      <th>Country_Switzerland</th>
      <th>Country_USA</th>
      <th>Country_United Arab Emirates</th>
      <th>Country_United Kingdom</th>
      <th>Country_Unspecified</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>6</td>
      <td>2.55</td>
      <td>17850</td>
      <td>2010</td>
      <td>1</td>
      <td>12</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>6</td>
      <td>3.39</td>
      <td>17850</td>
      <td>2010</td>
      <td>1</td>
      <td>12</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>8</td>
      <td>2.75</td>
      <td>17850</td>
      <td>2010</td>
      <td>1</td>
      <td>12</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>6</td>
      <td>3.39</td>
      <td>17850</td>
      <td>2010</td>
      <td>1</td>
      <td>12</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>6</td>
      <td>3.39</td>
      <td>17850</td>
      <td>2010</td>
      <td>1</td>
      <td>12</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>541904</th>
      <td>12</td>
      <td>0.85</td>
      <td>12680</td>
      <td>2011</td>
      <td>9</td>
      <td>12</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>541905</th>
      <td>6</td>
      <td>2.10</td>
      <td>12680</td>
      <td>2011</td>
      <td>9</td>
      <td>12</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>541906</th>
      <td>4</td>
      <td>4.15</td>
      <td>12680</td>
      <td>2011</td>
      <td>9</td>
      <td>12</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>541907</th>
      <td>4</td>
      <td>4.15</td>
      <td>12680</td>
      <td>2011</td>
      <td>9</td>
      <td>12</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>541908</th>
      <td>3</td>
      <td>4.95</td>
      <td>12680</td>
      <td>2011</td>
      <td>9</td>
      <td>12</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>541909 rows × 4114 columns</p>
</div>




```python
df_clustering.dropna(inplace=True)
```


```python
# df_clustering = df_clustering.iloc[:10000, :]
df_clustering
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border=""1"" class=""dataframe"">
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>Quantity</th>
      <th>UnitPrice</th>
      <th>CustomerID</th>
      <th>InvoiceYear</th>
      <th>InvoiceMonth</th>
      <th>InvoiceDay</th>
      <th>StockCode_10002</th>
      <th>StockCode_10080</th>
      <th>StockCode_10120</th>
      <th>StockCode_10123C</th>
      <th>...</th>
      <th>Country_RSA</th>
      <th>Country_Saudi Arabia</th>
      <th>Country_Singapore</th>
      <th>Country_Spain</th>
      <th>Country_Sweden</th>
      <th>Country_Switzerland</th>
      <th>Country_USA</th>
      <th>Country_United Arab Emirates</th>
      <th>Country_United Kingdom</th>
      <th>Country_Unspecified</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>6</td>
      <td>2.55</td>
      <td>17850</td>
      <td>2010</td>
      <td>1</td>
      <td>12</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>6</td>
      <td>3.39</td>
      <td>17850</td>
      <td>2010</td>
      <td>1</td>
      <td>12</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>8</td>
      <td>2.75</td>
      <td>17850</td>
      <td>2010</td>
      <td>1</td>
      <td>12</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>6</td>
      <td>3.39</td>
      <td>17850</td>
      <td>2010</td>
      <td>1</td>
      <td>12</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>6</td>
      <td>3.39</td>
      <td>17850</td>
      <td>2010</td>
      <td>1</td>
      <td>12</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>541904</th>
      <td>12</td>
      <td>0.85</td>
      <td>12680</td>
      <td>2011</td>
      <td>9</td>
      <td>12</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>541905</th>
      <td>6</td>
      <td>2.10</td>
      <td>12680</td>
      <td>2011</td>
      <td>9</td>
      <td>12</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>541906</th>
      <td>4</td>
      <td>4.15</td>
      <td>12680</td>
      <td>2011</td>
      <td>9</td>
      <td>12</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>541907</th>
      <td>4</td>
      <td>4.15</td>
      <td>12680</td>
      <td>2011</td>
      <td>9</td>
      <td>12</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>541908</th>
      <td>3</td>
      <td>4.95</td>
      <td>12680</td>
      <td>2011</td>
      <td>9</td>
      <td>12</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>406829 rows × 4114 columns</p>
</div>



__[🔼](#Contents)__

# Model Selection


```python
import tools as t
reload(t)
from tools.modeling import clustering

clustering = clustering.Clustering(df_clustering)
```


```python
df_clustering
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border=""1"" class=""dataframe"">
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>Quantity</th>
      <th>UnitPrice</th>
      <th>CustomerID</th>
      <th>InvoiceYear</th>
      <th>InvoiceMonth</th>
      <th>InvoiceDay</th>
      <th>StockCode_10002</th>
      <th>StockCode_10080</th>
      <th>StockCode_10120</th>
      <th>StockCode_10123C</th>
      <th>...</th>
      <th>Country_RSA</th>
      <th>Country_Saudi Arabia</th>
      <th>Country_Singapore</th>
      <th>Country_Spain</th>
      <th>Country_Sweden</th>
      <th>Country_Switzerland</th>
      <th>Country_USA</th>
      <th>Country_United Arab Emirates</th>
      <th>Country_United Kingdom</th>
      <th>Country_Unspecified</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>6</td>
      <td>2.55</td>
      <td>17850</td>
      <td>2010</td>
      <td>1</td>
      <td>12</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>6</td>
      <td>3.39</td>
      <td>17850</td>
      <td>2010</td>
      <td>1</td>
      <td>12</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>8</td>
      <td>2.75</td>
      <td>17850</td>
      <td>2010</td>
      <td>1</td>
      <td>12</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>6</td>
      <td>3.39</td>
      <td>17850</td>
      <td>2010</td>
      <td>1</td>
      <td>12</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>6</td>
      <td>3.39</td>
      <td>17850</td>
      <td>2010</td>
      <td>1</td>
      <td>12</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>541904</th>
      <td>12</td>
      <td>0.85</td>
      <td>12680</td>
      <td>2011</td>
      <td>9</td>
      <td>12</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>541905</th>
      <td>6</td>
      <td>2.10</td>
      <td>12680</td>
      <td>2011</td>
      <td>9</td>
      <td>12</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>541906</th>
      <td>4</td>
      <td>4.15</td>
      <td>12680</td>
      <td>2011</td>
      <td>9</td>
      <td>12</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>541907</th>
      <td>4</td>
      <td>4.15</td>
      <td>12680</td>
      <td>2011</td>
      <td>9</td>
      <td>12</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>541908</th>
      <td>3</td>
      <td>4.95</td>
      <td>12680</td>
      <td>2011</td>
      <td>9</td>
      <td>12</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>406829 rows × 4114 columns</p>
</div>




```python
name = 'Kmeans'
model = KMeans(n_clusters=3, random_state=42)
steps = [
#     ('scaler', StandardScaler())
]
plot=True

model_kmeans, ypred_kmeans = clustering.check_model(name, model, steps, plot)
```

    KMeans(n_clusters=3, random_state=42)




![png](README_files/README_16_1.png)




```python
df_clustering['clusters'] = ypred_kmeans
df_clustering_res = df_clustering[['CustomerID', 'clusters']].copy()
```

__[🔼](#Contents)__

# Analysis


```python
df_res = pd.merge(df_clustering_res, df_profiling.drop_duplicates(), how='left', left_on='CustomerID', right_index=True)
df_res
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border=""1"" class=""dataframe"">
  <thead>
    <tr style=""text-align: right;"">
      <th></th>
      <th>CustomerID</th>
      <th>clusters</th>
      <th>#_stockCode</th>
      <th>#_InvoiceNo</th>
      <th>avg_Q</th>
      <th>avg_P</th>
      <th>avg_Revenue</th>
      <th>HighRevenueMonth</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17850</td>
      <td>1</td>
      <td>312.0</td>
      <td>35.0</td>
      <td>5.426282</td>
      <td>3.924712</td>
      <td>16.950737</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>17850</td>
      <td>1</td>
      <td>312.0</td>
      <td>35.0</td>
      <td>5.426282</td>
      <td>3.924712</td>
      <td>16.950737</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17850</td>
      <td>1</td>
      <td>312.0</td>
      <td>35.0</td>
      <td>5.426282</td>
      <td>3.924712</td>
      <td>16.950737</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>17850</td>
      <td>1</td>
      <td>312.0</td>
      <td>35.0</td>
      <td>5.426282</td>
      <td>3.924712</td>
      <td>16.950737</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>17850</td>
      <td>1</td>
      <td>312.0</td>
      <td>35.0</td>
      <td>5.426282</td>
      <td>3.924712</td>
      <td>16.950737</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>541904</th>
      <td>12680</td>
      <td>0</td>
      <td>52.0</td>
      <td>4.0</td>
      <td>8.519231</td>
      <td>3.637885</td>
      <td>16.592500</td>
      <td>9.0</td>
    </tr>
    <tr>
      <th>541905</th>
      <td>12680</td>
      <td>0</td>
      <td>52.0</td>
      <td>4.0</td>
      <td>8.519231</td>
      <td>3.637885</td>
      <td>16.592500</td>
      <td>9.0</td>
    </tr>
    <tr>
      <th>541906</th>
      <td>12680</td>
      <td>0</td>
      <td>52.0</td>
      <td>4.0</td>
      <td>8.519231</td>
      <td>3.637885</td>
      <td>16.592500</td>
      <td>9.0</td>
    </tr>
    <tr>
      <th>541907</th>
      <td>12680</td>
      <td>0</td>
      <td>52.0</td>
      <td>4.0</td>
      <td>8.519231</td>
      <td>3.637885</td>
      <td>16.592500</td>
      <td>9.0</td>
    </tr>
    <tr>
      <th>541908</th>
      <td>12680</td>
      <td>0</td>
      <td>52.0</td>
      <td>4.0</td>
      <td>8.519231</td>
      <td>3.637885</td>
      <td>16.592500</td>
      <td>9.0</td>
    </tr>
  </tbody>
</table>
<p>406829 rows × 8 columns</p>
</div>




```python
sns.pairplot(df_res[['clusters', 'avg_Revenue', 'HighRevenueMonth', '#_InvoiceNo', '#_stockCode', 'avg_Q', 'avg_P']],
             hue=""clusters"",
            palette=sns.color_palette(""hls"", 3))

```




    <seaborn.axisgrid.PairGrid at 0x2212f18a490>





![png](README_files/README_20_1.png)


",1,1,1,0,retail,"[clustering, data-science, machine-learning, retail]",44-45
KarynH,9-2-Front-end-project,,https://github.com/KarynH/9-2-Front-end-project,https://api.github.com/repos/9-2-Front-end-project/KarynH,,"# 9-2-Front-end-project
## [Alt Shopping Outlet](https://karynh.github.io/9-2-Front-end-project/) 🛍

*e-commerce*</br></br>
User Stories</br>
The Alt Shopping Outlet provides products from the H&M, Khol's, and Forever21 api. It allows for users to shop for products</br>
that may or may not be shown on other retail sites and in thier stores.</br>

-As the user I am able to click on listed products and view details about the product including size, color and cost.</br>
-As the user I can search a product by name and see instant results.</br>
-As a user I can click on a button that adds an item of my choice to a cart.</br>
-As the user I can browse by category for a product type.</br>
",1,1,1,0,retail,"[ecommerce, retail, shop]",44-45
supervisely-ecosystem,add-properties-to-image-from-csv,supervisely-ecosystem,https://github.com/supervisely-ecosystem/add-properties-to-image-from-csv,https://api.github.com/repos/add-properties-to-image-from-csv/supervisely-ecosystem,Find row in CSV file and attach row data to image (as tags or as metadata),"<div align=""center"" markdown>
<img src=""https://user-images.githubusercontent.com/48245050/182402861-602040b3-a489-4f1c-9225-0558eac0f1bb.png""/>

# Add Properties To Image From CSV

<p align=""center"">
  <a href=""#Overview"">Overview</a> •
  <a href=""#Preparation"">Preparation</a> •
  <a href=""#How-To-Run"">How To Run</a>
</p>


[![](https://img.shields.io/badge/supervisely-ecosystem-brightgreen)](https://ecosystem.supervise.ly/apps/add-properties-to-image-from-csv)
[![](https://img.shields.io/badge/slack-chat-green.svg?logo=slack)](https://supervise.ly/slack)
![GitHub release (latest SemVer)](https://img.shields.io/github/v/release/supervisely-ecosystem/add-properties-to-image-from-csv)
[![views](https://app.supervise.ly/img/badges/views/supervisely-ecosystem/add-properties-to-image-from-csv.png)](https://supervise.ly)
[![runs](https://app.supervise.ly/img/badges/runs/supervisely-ecosystem/add-properties-to-image-from-csv.png)](https://supervise.ly)

</div>

## Overview

Application allows to add additional information to image from external databases. Developers add some identifiers to images during data aquisition quite often. But then additional human-readable information has to be assigned to images. Because ids are useless for labelers. 

Let's consider the example from retail industry. The same intuition can be applied to other industries as well: agriculture, self-driving cars, visul inspection and so on. Imaging that the task is to create reference images for every product in grocery store: label main object on every photo of grocery store shelves. But in many cases it is impossible to say what object is main: could you guess the main product on the picture from poster above just using `PRODUCT-ID:807090338`? 


But if we add information about product like category, item description, size, etc ... from our internal database (CSV file for simplicity), then labelers will be able to find the right main product on the image.

<img src=""https://i.imgur.com/jtfh7mH.png""/>

To add properties to images it is needed to provide `CSV` file, name of image tag and name of csv column that will be used to match correct row from CSV with image. App takes given tag value from image, finds value in defined column and copies other columns of found CSV row to image. 


## Preparation

Prepare CSV file and upload it to team files. Here is the example of CSV file:

```csv
ITEM DESCRIPTION,CATEGORY,COMMERCIAL BRAND,SIZE,UPC CODE
Honey Nut Toasted Oats,cereal,PICS,12.3 oz,12217777
""Oats Cereal, Gluten Free"",cereal,Honey Nut Cheerios,19.5 oz,807090338
""Frosted Flakes, Breakfast"",cereal,Kellogg's,24 Oz,371107436
Frozen Blueberry Pancakes,waffles & pancakes,De Wafelbakkers,29.6 Oz,13399284
Buttermilk Waffles,waffles & pancakes,Great Value,29.6 oz,16382427
Chocolately Chip Waffles Easy Breakfast,waffles & pancakes,Kellogg's,29.6 oz,13399285
```

<img src=""https://i.imgur.com/YtI2Htx.png""/>

Then copy path to the uploaded file:

<img src=""https://i.imgur.com/ZcxrGgR.png""/>

# How To Run

**Step 1:** Add app to your team from Ecosystem if it is not there.

**Step 2:** Run app from the context menu of project

<img src=""https://i.imgur.com/UHkbfRS.png"" width=""500px""/>

**Step 3:** Fill in the fields in modal window and press `Run` button

<img src=""https://i.imgur.com/iaQV5Sw.png"" width=""600px""/>

**Step 4:** Wait until the task is finished, new project will be created, find link in task output

<img src=""https://i.imgur.com/ziEkbmL.png""/>

**Step 5:** All warnings and errors can be found in task log

",1,1,2,0,retail,"[retail, supervisely, tags]",44-45
Novik-data-analyst,Data-Analyst-career-portfolio-projects,,https://github.com/Novik-data-analyst/Data-Analyst-career-portfolio-projects,https://api.github.com/repos/Data-Analyst-career-portfolio-projects/Novik-data-analyst,Practical projects showing my capabilities. ,"# Data Analyst career portfolio projects

Practical experience in various spheres such as finance, marketing, product analysis.
Graduation diploma : [link](https://www.dropbox.com/s/u91j52qh0w1l5ll/Diploma%20number.pdf?dl=0) 

## Project summary

This projects were complited during Data Analyst internship at Practicum. 
Please reach out to me at aleknv@gmail.com if you have and questions or open job opportunities. 

| Project name | Description | Anaysis area | Used skills | 
| :---------------------- | :---------------------- | :---------------------- |:---------------------- |
| [Client reliability research](https://github.com/Aleknv/yandex-praktikum-projects/tree/main/Credit%20Scoring ) | Based on clients payment history, detetmine, whether martial status and number of kids affect on-time loan payoff.  | Banking, finance | *pandas* , *PyMystem3* , *python*, *lemmatization* , *ETL* |
|[Researching apartment listings](https://github.com/Novik-data-analyst/Data-Analyst-career-portfolio-projects/tree/main/Real%20estate%20Market%20and%20Fraud%20analysis) | From the dataset of archived ads for the sale of apartments in St. Petersburg and nearby area, we need to determine the market value of real estate properties. | Real estate, fraud analysis | *Matplotlib* , *Pandas* , *python* , *Data Visualization* , *Exploratory Data Analysis* , *Data Preprocessing* |
|[Determine promising tariff for telecom company](https://github.com/Novik-data-analyst/Data-Analyst-career-portfolio-projects/tree/main/Telecom%20analysis) | Analyze customer data for Megaline's ""Smart"" and ""Ultra"" tariffs from 500 customers to determine the more profitable plan, guiding marketing and pricing strategies. | Telecom | *pandas* , *python*, *ETL* , *Matplotlib* , *NumPy* , *SciPy* , *Descriptive Statistics* , *Statistical Hypothesis Testing* |
",1,1,1,0,retail,"[banking, fitness, real-estate, retail, telecom]",44-45
kaustavkoley,python-project-using-pandas,,https://github.com/kaustavkoley/python-project-using-pandas,https://api.github.com/repos/python-project-using-pandas/kaustavkoley,its just a simple python retail cli application,"# project-pandas-
",1,1,1,0,retail,"[cli, python, retail]",44-45
nnvij,Superstore-Sales-Dashboard---Powerbi,,https://github.com/nnvij/Superstore-Sales-Dashboard---Powerbi,https://api.github.com/repos/Superstore-Sales-Dashboard---Powerbi/nnvij,"For the superstore sales dataset, lets built a sales dashboard in powerbi","# Superstore-Sales-Dashboard---Powerbi
- Power BI dashboards can be a helpfull tool to provide insights into sales performance and trends of an organization.
- The dashboard should include key metrics such as total sales, sales by product, sales by location, sales by customer segment, and sales by product category. 
- It should also provide comparisons of performance to prior years, and allow for drill-down into individual market and products.

## Problem Statement:
- Build a Sales Analysis, Product Analysis and Shipping Analysis Dashboard.

### Tools Used: 
- Microsoft Power BI

## Data:
- Data Source : Superstore dataset from kaggle https://www.kaggle.com/datasets/laibaanwer/superstore-sales-dataset
- The sales is between the period of 2011 to 2014
   ![image](https://user-images.githubusercontent.com/103464406/218605670-7fe2f56e-7cc7-433e-9a09-fd0b655195aa.png)

## Dashboards:

### Sales Analysis Dashboard:
- Sales department can analyze sales and profit through this dashboard. Filter through years and quarterly sales.
- The company has made a Total Sales of 3 million and Total Profit of 400.9K between 2011- 2014.
- 1 million worth of sales and 119.11K profit was made in 2014.
- 46% of profit came from product category technology.
- Year 2014 March and April has seen high sales revenue and December being the holiday Season has the lowest revenue sales.
    ![image](https://user-images.githubusercontent.com/103464406/218605834-500b9b47-8873-4b4a-94ae-ed86486e3194.png)
- Filter for year 2014, category: technology, Order Priority: Medium
    !![image](https://user-images.githubusercontent.com/103464406/218607224-3a1bf219-c16f-44cb-bb8e-4e7b0ee22838.png)


### Product Analysis Dashboard:
- This dashboard dive deep into product categories, quantity ordered, product- subcategory
-  Highest amount of office supplies have been ordered in terms of quantity.
-  Tables have been sold at an average discount of 28% in 2014
   ![image](https://user-images.githubusercontent.com/103464406/218607609-fcf817e6-17a1-4b03-af50-00bbe806ad79.png)
- Filter year: 2014, market: US
    ![image](https://user-images.githubusercontent.com/103464406/218608118-393e40ea-9f77-46f2-9826-82f7be538d1d.png)

### Shipping Analysis Dashboard:
- 53% of the orders have Standard shipping mode and lowest preferred is same day shipping.
- 41% of Orders with critical order priority have been ordered by firstclass ship mode.
- 100 % of the orders with low priority have been ordered through standard class.
- APAC markets have highest shipping cost, followed by EU and US.
   ![image](https://user-images.githubusercontent.com/103464406/218608777-892f74b8-ee20-4c22-8c2f-953638930e54.png)

",1,1,1,0,retail,"[dashboard, powerbi, retail]",44-45
easonlai,market_basket_analysis_retail_sample,,https://github.com/easonlai/market_basket_analysis_retail_sample,https://api.github.com/repos/market_basket_analysis_retail_sample/easonlai,This is a demo repo demonstrating how to perform Market Basket Analysis (MBA) with a Retail (Grocery Store) sample. ,"﻿# Market Basket Analysis (MBA) with Retail (Grocery Store) sample

This is a demo repo demonstrating how to perform Market Basket Analysis (MBA) with a Retail (Grocery Store) sample. I based on the [classic grocery dataset from Kaggle](https://www.kaggle.com/datasets/heeraldedhia/groceries-dataset) to further enhance and develop the sample dataset for advanced MBA in multiple dimensions and aspects.

Content:
* [data/groceries_dataset_sample_v1.xlsx](https://github.com/easonlai/market_basket_analysis_retail_sample/blob/main/data/groceries_dataset_sample_v1.xlsx) <-- Version 1 of the grocery sample dataset with 1,920 records (716 unique Transaction/Invoice IDs, 194 unique Customer IDs).
* [frequently_bought_together_v1.ipynb](https://github.com/easonlai/market_basket_analysis_retail_sample/blob/main/frequently_bought_together_v1.ipynb) <-- Sample of frequently bought together by applying the [Apriori algorithm](https://en.wikipedia.org/wiki/Apriori_algorithm) in [Association Rule Learning](https://en.wikipedia.org/wiki/Association_rule_learning) with version 1 sample data.

Enjoy!


",1,1,1,0,retail,"[grocery, grocery-store, market-basket-analysis, market-basket-optimization, market-basket-recommender, product-recommendation, product-recommender, python, python3, retail]",44-45
darkbluein,user,darkbluein,https://github.com/darkbluein/user,https://api.github.com/repos/user/darkbluein,User app to order from local retail stores.,"# Locality App

<img src=""https://www.ec2server.online/localityorg/logo/screens-user.png"" alt=""Locality app screens""/>

## Overview

Locality is a mobile app that allows users to order groceries or any retail item from their local retail store. The app also has a feature that lets users maintain their khaata (running account) at their trusted retail stores and choose appropriate delivery time according to their necessity.

Our app is designed with your convenience in mind. We’ve made it easy to navigate and order items with just a few taps. You can search for specific items, filter by category, or simply browse through our curated list of popular products. Our user-friendly interface ensures that anyone can use the app without any hassle.

Simply download the app and order everything you need from the comfort of your home. You can browse through thousands of items available in your local store, add them to your cart, and choose the most convenient delivery time according to your schedule.

This app is built majorly using React Native, Graphql, react-native-ui-lib, and Redux.

## Installation

1. Clone the repository

```bash
git clone https://github.com/localityorg/user.git locality-user
```

2. Install dependencies

```bash
cd locality-user
npm install
```

3. Run the app

```bash
npx react-native start
```

This will start the Metro bundler. You can then run the app on your iOS or Android device using the Expo app.

## Features

- Order groceries or any retail item from your local retail store.
- Maintain your khaata (running account) at your trusted retail stores.
- Choose appropriate delivery time according to your necessity.

## Technologies

- React Native
- react-native-ui-lib
- GraphQL
- Redux
- Apollo Framework

## Contributing

Pull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.
",1,1,0,0,retail,"[apollo-client, ecommerce, graphql, graphql-client, react-native, react-native-ui-lib, redux, redux-thunk, retail, typescript]",44-45
EnventureEnterprises,EnventureAndroid-MVP,EnventureEnterprises,https://github.com/EnventureEnterprises/EnventureAndroid-MVP,https://api.github.com/repos/EnventureAndroid-MVP/EnventureEnterprises,,"# Enventure Android App

Welcome to Enventure's open source Android app! ENVision mobile is a simple accounting app for small businesses in emerging markets to use to track their inventory, input sales (including credit), track debts and expenses, and export ledgers and profit-loss statements. Help us innovate on supporting small businesses in emerging markets to keep better records on their businesses and thus access more opportunities as a result! Watch: (https://www.youtube.com/watch?v=o8mG4ownxuo&t=4s)

## License
MIT License

![Enventure for Android](.github/enventurelogo.png)


## Getting Started

_Follow these instructions to build and run the project

1. Clone this repository.
2. Download the appropriate [JDK](http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html)
for your system. We are currently on JDK 8.
3. [Install Android Studio](https://developer.android.com/sdk/index.html).

4. Import the project. Open Android Studio, click `Open an existing Android
   Studio project` and select the project. Gradle will build the project.
5. Run the app. Click `Run > Run 'app'`. After the project builds you'll be
   prompted to build or launch an emulator.


## Contributing

The best way to submit feedback and report bugs is to open a Github issue.
Please be sure to include your operating system, device, version number, and
steps to reproduce reported bugs.

https://www.enventureenterprises.org
",1,1,1,0,retail,"[accounting-tools, africa, android, android-app, business-analytics, emerging-markets, finance-management, finance-tracker, retail, retail-data]",44-45
alishahlakhani,Mydin-ecommerce-store-data-scraper,,https://github.com/alishahlakhani/Mydin-ecommerce-store-data-scraper,https://api.github.com/repos/Mydin-ecommerce-store-data-scraper/alishahlakhani,"A pet project for a startup. Written in Typescript to scrap Mydin.com.my, largest and well-established Malaysian-owned company involved in wholesaling and retailing.","# Mydin-ecommerce-store-data-scraper
A pet project for a startup. Written in Typescript to scrap Mydin.com.my, largest and well-established Malaysian-owned company involved in wholesaling and retailing.

# How to run?
On your command line/terminal just run 'npm run start' and it'll start scrapping. Once done it'll generate data folder which will hold '.csv' files for all the data based on categories. While running it'll also show you statistics of how many categories it has scraped.

# What does it scrap?
## Categories include
- Baby
- Apparel
- Chill & Frozen
- Drinks
- Fresh
- Groceries
- Health & Beauty
- Home & Outdoor
- Household Products
- Muslimin Needs
- Pets
- Stationery
- Super Star

## Metadata include
- Product Id 
- Name
- Cost
- Quantity
- Url
- Category
",1,1,1,0,retail,"[ecommerce, javascript, malaysia, retail, scraper, typescript, webscrapping]",44-45
akshay-madar,amazon-customer-retail-analytics-solution,,https://github.com/akshay-madar/amazon-customer-retail-analytics-solution,https://api.github.com/repos/amazon-customer-retail-analytics-solution/akshay-madar,"Deep dive into Amazon's top 100 ranking methodology, studying over 700 products, coupled with wide-scope ecommerce analytics solution for enhanced customer targeting and promotional outreach","# Amazon - Customer and Retail Analytics Solution for ECommerce

## Motivation:
Amazon sells millions of products through thousands of sellers on its platforms. This portfolio of product varies widely across different product categories and even different times of the year. To boost visibility and popularity of products, Amazon hosts a New Releases ranking of top 100 products of all categories, a list which is updated hourly. To enable sellers insert their products into this list and move them further up the order, it is crucial to gain an understanding of which factors influence their ranks and how. 

### But why Amazon?
Because it is the world's largest online marketplace, with more than 310 active user base, and sells more than 12 millions products across different categories.

<p align=""center"">
  <img width=""560"" height=""400"" src=""https://media.giphy.com/media/3JVHCwSgWWDUVMiTQs/giphy.gif"">
</p>

## Project Flow:
1. [**Objective**](#1-objective)
2. [**Computational Steps**](#2-computational-steps): data collection | data preparation | sentiment of reviews | modeling
3. [**Insights and Value Proposition**](#3-insights-and-value-proposition)
4. [**Detailed overview of steps to follow**](#4-detailed-overview-of-steps-to-follow)

## 1. Objective:
The objective is to build statistical model on publicly available data from Amazon so that sellers and ultimately Amazon can leverage our insights to push their products up the trending ladder.

<p align=""center"">
  <img width=""800"" height=""500"" src=""https://github.com/akshay-madar/amazon-customer-retail-analytics-solution/blob/master/amazon/data_problem.PNG"">
</p>

## 2. Computational Steps:
To examine the impact of various variables on rank of sellers, **Amazon - [""Hot New Releases""](https://www.amazon.com/gp/new-releases/?ref_=nav_cs_newreleases)** is leveraged as the platform to obtain data points for few vital attributes in our solution – rank, prime delivery status, discount offered status, price, top 10 customer reviews, average rating, rating count, number of answered questions, and number of product images. Data is scraped rigourously from Amazon using BeautifulSoup, for seven different categories – **Electronics, Computers and Accessories, Clothing, Pet Supplies, Tools and Home Improvement, Toys and Games, and Grocery** – that catalyze most of the sales for Amazon. 

<p align=""center"">
  <img width=""800"" height=""500"" src=""https://github.com/akshay-madar/amazon-customer-retail-analytics-solution/blob/master/amazon/data_collection.PNG"">
</p>

<p align=""center"">
  <img width=""800"" height=""500"" src=""https://github.com/akshay-madar/amazon-customer-retail-analytics-solution/blob/master/amazon/new_releases.PNG"">
</p>

This is followed by necessary data preparation to account for skewness, and a check on correlation amongst variables.

<p align=""center"">
  <img width=""800"" height=""500"" src=""https://github.com/akshay-madar/amazon-customer-retail-analytics-solution/blob/master/amazon/data_preparation.PNG"">
</p>

<p align=""center"">
  <img width=""760"" height=""500"" src=""https://github.com/akshay-madar/amazon-customer-retail-analytics-solution/blob/master/amazon/correlation.png"">
</p>

To ensure a comprehensive analysis, top 10 customer reviews are collected which captured the overall product sentiment adequately. Thereafter, **Microsoft Azure Text Analytics API** is used to generate averaged sentiment scores for each product.

<p align=""center"">
  <img width=""800"" height=""500"" src=""https://github.com/akshay-madar/amazon-customer-retail-analytics-solution/blob/master/amazon/sentiment.PNG"">
</p>


After requisite data preparation to account for skewness in certain variables, log-log regression model was deployed to measure elasticity of rank with respect to given variables. We also accounted for synergy effect between variables and included interaction terms. The full model encompassing all product categories was run to give significant variables that affected ranking of products. For a deeper analysis into how different product categories behaved, a subsampled regression analysis for each category was also performed to see how significant the variables were for these categories.

## 3. Insigths and Value Proposition:
There are multiple parameters which have considerable effect on the ranks of new releases. Also, the extent of impact of different parameters is different across the categories. Amazon and its sellers can benefit from the model by identifying which areas help in increasing the rank of their newly released products. The probability of higher sales increases massively due to higher visibility, if the rank increases. Also, while it is essential that sellers manage to enter this trending list of new releases, they would be able to reap maximum benefit if they move from the rank bucket of 51-100 on second page to the first page of top 50 product releases. We believe our analysis and accompanying insights would help them achieve this critical objective to stay on the trending list for longer, which would result in improved sales for their products, and edge out competing brands

## 4. Detailed overview of steps to follow:

							Amazon Hot New Releases   


	1.) Run the “product and review links.ipynb” file to get the links of individual categories. Make sure to change the link for each category. The category links will be dynamic and updates 
    	    every hour. After exporting the results to CSV file, the product links in the csv file have to be considered only till ""ProducID"". This needs to be done because, the link gets changed 
    	    dynamically every hour and the hyperlink changes. 		Output file obtained: cumulative links.csv

	2.) Use the product links and review links obtained from the previous step and run the “Reviews and # of images.ipynb” to scrape the top reviews of each of the product. 
            This file also gives number of images displayed for each product.     Output file obtained: cumulative review.csv
 

	3.) Run the “Sentiment Analysis.ipynb” file to get the sentiment score of each review. Azure API is used to get the sentiment score. 	After appending the sentiment scores to the file
            generated in step 2 we obtain cumulative sentiments.csv

	4.) Run the “Product Attributes.ipynb” file to get the attributes of each product. Following attributes can be obtained (price, average rating, number of ratings, number of reviews,
            number of answered questions, prime category, Discount) cumulative sentiments.csv

	5.) Use “Data cleaning.ipynb” to merge all the datasets obtained above. output file obtained: modelfile.csv

	6.) Use “LinearRegression.R” to get the regression results.


Data Dictionary for the csv file : modelfile.csv

	Rank -> Rank of the product (for the specific category)
	Category -> Product Category field
	Product Link -> Link of the product needed to access the data
	Name - > Product Name
	Avg_Rating -> Rating of the product
	Review_link -> Hyperlink 
	rating_count -> Number f ratings (in number) given to the product and its different from average rating
	prime -> prime delivery or not
	Images -> Number of images for each product
	sentiment score -> sentiment score extracted from the top 10 reviews
	Answered Questions - > Number of answered questions present for the product
	Price -> Price of the product
	Discount -> Discount (in percentage) offered per product
	Customer_review_count -> Number of customer reviews for each product

Data Dictionary for the csv file : cumulative reviews.csv

	Rank -> Rank of the product (for the specific category)
	Category -> Product Category field
	review -> review of the product
	sentiment score -> sentiment score extracted from the top 10 reviews

Data Dictionary for the csv file : cumulative attributes.csv

	Rank -> Rank of the product (for the specific category)
	Product Link -> Link of the product needed to access the data
	Answered Questions - > Number of answered questions present for the product
	Price -> Price of the product
	Discount -> Discount (in percentage) offered per product
	Customer_review_count -> Number of customer reviews for each product
	Category -> Product Category field

Data Dictionary for the csv file : cumulative links.csv

	Rank -> Rank of the product (for the specific category)
	Category -> Product Category field
	Product Link -> Link of the product needed to access the data
	Name - > Product Name
	Avg_Rating -> Rating of the product
	Review_link -> Hyperlink 
	rating_count -> Number f ratings (in number) given to the product and its different from average rating
	prime -> prime delivery or not
	Images -> Number of images for each product






",1,1,1,0,retail,"[amazon, beautifulsoup4, churn, clustering, customer, ecommerce, lifetime-value, ranking, regression, retail, rfm-analysis, selenium-webdriver, web-scra, xgboost]",44-45
ambientWave,Odoo-Retail-Store-With-Accounting-Integration,,https://github.com/ambientWave/Odoo-Retail-Store-With-Accounting-Integration,https://api.github.com/repos/Odoo-Retail-Store-With-Accounting-Integration/ambientWave,A custom odoo module that can be used to manage simple retail store operations including basic accounting,"# Odoo-Retail-Store-With-Accounting-Integration

A custom odoo module that can be used to manage simple retail store operations including basic accounting.

## Screenshots

<picture>
 <img alt=""Screenshot1"" src=""https://raw.githubusercontent.com/ambientWave/Odoo-Retail-Store-With-Accounting-Integration/main/Image1.png"">
</picture>

<picture>
 <img alt=""Screenshot2"" src=""https://raw.githubusercontent.com/ambientWave/Odoo-Retail-Store-With-Accounting-Integration/main/Image2.png"">
</picture>

<picture>
 <img alt=""Screenshot3"" src=""https://raw.githubusercontent.com/ambientWave/Odoo-Retail-Store-With-Accounting-Integration/main/Image3.png"">
</picture>

<picture>
 <img alt=""Screenshot4"" src=""https://raw.githubusercontent.com/ambientWave/Odoo-Retail-Store-With-Accounting-Integration/main/Image4.png"">
</picture>

<picture>
 <img alt=""Screenshot5"" src=""https://raw.githubusercontent.com/ambientWave/Odoo-Retail-Store-With-Accounting-Integration/main/Image5.png"">
</picture>
",1,1,1,0,retail,"[backend, database, ecommerce, erp, management, management-system, odoo, postgresql, python, retail]",44-45
pforret,retail.unlockdown.be,,https://github.com/pforret/retail.unlockdown.be,https://api.github.com/repos/retail.unlockdown.be/pforret,Directory of shops under lockdown with online ordering/delivery options,"# unlockdown_horeca
Directory of restaurants under lockdown with takeaway/delivery options
",0,0,3,0,retail,"[belgium, directory, jekyll, lockdown, retail, shops]",44-45
gakas14,Online_Retail,,https://github.com/gakas14/Online_Retail,https://api.github.com/repos/Online_Retail/gakas14,"Perform customer segmentation using RFM analysis. The resulting segments can be ordered from most valuable (highest recency, frequency, and value) to least valuable (lowest recency, frequency, and value). ","# Online_Retail
Problem statement
It is a business critical requirement to understand the value derived from a customer. RFM is a method used for analyzing customer value.
Perform customer segmentation using RFM analysis. The resulting segments can be ordered from most valuable (highest recency, frequency, and value) to least valuable (lowest recency, frequency, and value). Identifying the most valuable RFM segments can capitalize on chance relationships in the data used for this analysis.


Approach:

Following pointers will be helpful to structure your findings.   


1.	Perform a preliminary data inspection and Data cleaning


  a.	Check for missing data and formulate apt strategy to treat them.
  
  b.	Are there any duplicate data records? Remove them if present.
  
  c.	Perform Descriptive analytics on the given data.



2.	Cohort Analysis: A cohort is a group of subjects who share a defining characteristic. We can observe how a cohort behaves across time and compare it to other cohorts. 


  a.	Create month cohorts and analyse active  customers for each cohort.
  
  b.	Also Analyse the retention rate of customers. Comment.



3.	Build a RFM model – Recency Frequency and Monetary based on their behaviour.
Recency is about when was the last order of a customer. It means the number of days since a customer made the last purchase. If it’s a case for a website or an app, this could be interpreted as the last visit day or the last login time.
Frequency is about the number of purchase in a given period. It could be 3 months, 6 months or 1 year. So we can understand this value as for how often or how many a customer used the product of a company. The bigger the value is, the more engaged the customers are. Could we say them as our VIP? Not necessary. Cause we also have to think about how much they actually paid for each purchase, which means monetary value.
Monetary is the total amount of money a customer spent in that given period. Therefore big spenders will be differentiated with other customers such as MVP or VIP.

  a.	Calculate RFM metrics.
  
    i.	Recency as the time in no. of days since last transaction
    
    ii.	Frequency as  count of purchases done 
    
    iii.	Monetary value  as total amount spend 
    
  b.	Build RFM Segments.
  
    i.	Give Recency Frequency and Monetary scores individually by dividing them in to quartiles.
    Note: Rate ""Recency"" for customer who have been active more recently better than the less recent customer, because each company wants its customers to be recent 
    Rate ""Frequency"" and ""Monetary Value"" higher label because we want Customer to spend more money and visit more often.
    
    ii.	Combine three ratings to get a RFM segment (as strings)
    
    iii.	Get the RFM score by adding up the three ratings.
    
  c.	Analyse the RFM Segments by summarizing them and comment on the findings.



4.	Create clusters using k means clustering algorithm.


  a.	Prepare the data for the algorithm.
  
    i.	If the data is Un Symmetrically distributed, manage the skewness with appropriate transformation.
    
    ii.	Standardize / scale the data.
    
  b.	Decide the optimum number of clusters to be formed
  
  c.	Analyse these clusters and comment on the results.
  


5.	Create a dashboard in tableau by choosing appropriate chart types and metrics useful for the business. The dashboard must entail the following: 


  a)	Country-wise analysis to demonstrate Average spend. Use a bar chart show monthly figures.
  
  b)	Bar graph of top 15 products which are mostly ordered by the users to show the number of products sold.
  
  c)	Bar graph to show the count of orders Vs. hours throughout the day. What are the peak hours per your chart?
  
  d)	Plot the distribution of RFM values using histogram and frequency-charts.
  
  e)	Plot error(cost) vs no of clusters selected
  
  f)	 Visualize to compare the RFM values of the clusters using heatmap



",0,0,1,0,retail,"[cohort-analysis, kmeans-clustering, retail, rfm-analysis, tableau]",44-45
Rajil101,The-Sparks-Foundation,,https://github.com/Rajil101/The-Sparks-Foundation,https://api.github.com/repos/The-Sparks-Foundation/Rajil101,The Sparks foundation is nonprofit organisation where i  worked as Data Science and Bussiness Analytics intern . ,"# The-Sparks-Foundation
The Sparks foundation is nonprofit organisation based in singapore, where i worked as Data Science and Business Analytics intern by their Graduate Rotational Internship Program (GRIP). 
We need to perform various task given by the company, you can see some of the task performed by myself in the repository.


Used-

⁕Language : Python

⁕IDE : Jupyter Notebook



TASK 1: In these task we carried out prediction using Supervised ML - 

                          •Find out the percentage of student based on the no. of study hours.
                          •Predicted score if a student studies for 9.25 hours per day.
Data Set of task 1 - http://Bit.ly/w-data

Watch Youtube Video👇

[![ Prediction using Supervised ML](https://img.youtube.com/vi/Rqkc4WnXFgg/0.jpg)](https://www.youtube.com/watch?v=Rqkc4WnXFgg)




TASK 2: In these task we carried out prediction using Unsupervised ML-

                          •Predict optimum number of clusters ans represent it visually.
Data Set of task 2 - https://bit.ly/3kXTdox

Watch Youtube Video👇

[![Prediction using Unsupervised ML ](https://img.youtube.com/vi/-0r5axvilYs/0.jpg)](https://www.youtube.com/watch?v=-0r5axvilYs)




TASK 3: In these task we carried out the exploratory Data Analysis for Retail - 

                          •Find out business problem and profit making analysis.
Data Set of task 3 - https://bit.ly/3i4rbWl

Watch Youtube Video👇

[![Exploratory Data Analysis - Retail ](https://img.youtube.com/vi/dV1-tqsq_0s/0.jpg)](https://www.youtube.com/watch?v=dV1-tqsq_0s)




TASK 4: In these task we carried out the exploratory Data Analysis for Terrorism - 

                          •Find out hot zones of terrorism,security issues & insights deriving EDA.
Data Set of task 4 - https://bit.ly/2TK5Xn5

Watch Youtube Video👇

[![Exploratory Data Analysis - Terrorism ](https://img.youtube.com/vi/wEHQ2DqtWjw/0.jpg)](https://www.youtube.com/watch?v=wEHQ2DqtWjw)




TASK 5: In these task we carried out the exploratory Data Analysis for Sports - 

                          •Find out most successfull teams, player and factors contributing win or loss of a team.
                          •Suggest teams or players a company should endorse for its product.
Data Set of task 5 - https://bit.ly/34SRn3b

Watch Youtube Video👇

[![Exploratory Data Analysis - Sports ](https://img.youtube.com/vi/7_VgctmW7EE/0.jpg)](https://www.youtube.com/watch?v=7_VgctmW7EE)


",0,0,1,0,retail,"[data-science, deep-learning, jupyter-notebook, lstm-neural-networks, python, retail, rnn-keras, sport, supervised-learning, tensorflow, terrorism-analysis, unsupervised-machine-learning]",44-45
SeifReda30,HM-Web-Scraping,,https://github.com/SeifReda30/HM-Web-Scraping,https://api.github.com/repos/HM-Web-Scraping/SeifReda30,Python Script to Scrape All Men Clothes Data from H&M Website through Hidden API,"# H&M-Web-Scraping
![92c582cb-e857-43b7-b109-50587a774021](https://user-images.githubusercontent.com/69864768/189006550-a00e181a-917b-49f1-86f9-5c2058c6db9a.jpg)


## Summary
H&M Group is a family of brands and businesses, making it possible for customers around the world to express themselves through fashion and design, and to choose a more sustainable lifestyle.<br>
This is a Python Script to Scrape All Men Clothes Data from [H&M](https://eg.hm.com/en/shop-men/new-arrivals/clothes/) Website through Hidden API<br>
Extracted Data are : Item Title, Item Price, Available Colors, Available Size, Item Context and Item Url

## Result Table Sample
![Screenshot 2022-09-08 021619](https://user-images.githubusercontent.com/69864768/189006638-5418be33-50dc-4816-a0b2-d54d12024f78.png)

",0,0,1,0,retail,"[api, clothes, dataextraction, python, retail, webscraping]",44-45
shreyanshissingh,ContossoRetailAnalysis,,https://github.com/shreyanshissingh/ContossoRetailAnalysis,https://api.github.com/repos/ContossoRetailAnalysis/shreyanshissingh,Analysis of Contosso Retails using Tableau and Qliksense,"# ContossoRetailAnalysis
Analysis of Contosso Retails using Tableau and Qliksense
",0,0,1,0,retail,"[b2b, c2c, period-over-period, retail, sales-dashboard]",44-45
nataljunior,Classifyingr-in-R,,https://github.com/nataljunior/Classifyingr-in-R,https://api.github.com/repos/Classifyingr-in-R/nataljunior,Classifying in R Identifying Categories for Customer Complaint’s Mediation Automation,"# Classifying-in-R
Classifying in R Identifying Categories for Customer Complaint’s Mediation Automation

Function 1: Develop a text analytic tool that enables the automation of a customer complaints mediation sector in a Brazilian retail company. Using 1,175 complaints from customers.
",0,0,0,0,retail,"[classifier, complain, r, retail, retail-data, svm, svm-classifier, svm-learning, svm-training, text, text-classification, text-mining]",44-45
stekarag,getPrice,,https://github.com/stekarag/getPrice,https://api.github.com/repos/getPrice/stekarag,A python crawler that reads a barcode as an argument and prints the price of the respective product in an online greek supermarket,"# getPrice
A python crawler that reads a barcode as an argument and prints the price of the respective product in an online greek supermarket
",0,0,1,0,retail,"[barcode, crawler, python, retail]",44-45
JazerBarclay,barbercrm,,https://github.com/JazerBarclay/barbercrm,https://api.github.com/repos/barbercrm/JazerBarclay,Basic Barber CRM,"# Barber CRM
A simple CRM (Customer Relationship Manager) designed for a local barber shop to track customers and haircuts",0,0,2,0,retail,"[content-management-system, crm, java, retail]",44-45
oordenesg,retail_real_data,,https://github.com/oordenesg/retail_real_data,https://api.github.com/repos/retail_real_data/oordenesg,"Project with real retail data. This repository shows my proposed solution to estimate the sale using information obtained from IT sensors. Due to confidentiality issues, the entire data analysis and transformation process is not shown.","# retail_real_data

The objective of this project is to predict the sale of a retail chain according to the signals captured by the beacons (IoT sensor) within a store. To do this, different methods such as time series, SVM or LSTM can be used. Although this summary will not analyze statistical issues of the data, it will mention some characteristics of this data set as well as the main results obtained with the SVM model.

- The data set has 14 variables. Some of these variables have similar characteristics and others are not useful for estimating sales.
- There are 6 variables that have null values. One of the attributes has 87% missing values.
- Within the data set there are different IDs for the sensors. The objective is to determine the sale of each one of them and then add these results to obtain the total sales.
- Within the data set there are also duplicate values. 4.6% of the data are repeated.
- Although there is a date attribute for each of the records. This must be separated by year, month and day.

The first idea was to use the SVM model to predict a sales threshold. However, this idea is not entirely good because as the threshold increases, there is less data to predict. Figure 1 illustrates this problem.


<p align=""center"" width=""100%"">
    <img width=""35%"" src=""https://user-images.githubusercontent.com/76072249/113222557-af901000-925d-11eb-9a6e-da0d629dd1c0.png""> 
</p>

For this reason, and given the low amount of data for some sensors, an alternative is to go from a regression problem to a classification problem. This was done by creating sales ranges. The objective of this was to create small groups of data that allow us to apply some oversampling technique on the data. Figure 2 shows the results of the confusion matrix of the SVM model with oversampling and without an optimized model.

<p align=""center"" width=""100%"">
    <img width=""35%"" src=""https://user-images.githubusercontent.com/76072249/113223201-fe8a7500-925e-11eb-9c7a-34e31c652402.png""> 
</p>

The next step was the process of optimization of the model's hyperparameters. In this stage 3 types of kernels were used as well as different values for the parameter C.

<p align=""center"" width=""100%"">
    <img width=""35%"" src=""https://user-images.githubusercontent.com/76072249/113223396-72c51880-925f-11eb-9179-305d1c82f356.png""> 
</p>

The results of the optimization process allowed to improve the accuracy of the model. With this, we managed to achieve an accuracy of 90.6%. 5% higher than that obtained with a non-optimized SVM model. Figure 4 shows the new confusion matrix.


<p align=""center"" width=""100%"">
    <img width=""35%"" src=""https://user-images.githubusercontent.com/76072249/113223551-cb94b100-925f-11eb-8eaf-91c1da62a2f4.png""> 
</p>

This problem can be addressed using different analytical techniques. In the future, new methods will be added to make a comparison between all the models.
",0,0,1,0,retail,"[python3, retail, retail-data, retail-intelligence, support-vector-classifier, support-vector-machines]",44-45
RHEA211,SPARKS-FOUNDATION-TASK-7,,https://github.com/RHEA211/SPARKS-FOUNDATION-TASK-7,https://api.github.com/repos/SPARKS-FOUNDATION-TASK-7/RHEA211,,,0,0,1,0,retail,"[exploratory-data-analysis, retail]",44-45
dtararuj,aplikacja_do_typowania_produktow_pod_zlecenia,,https://github.com/dtararuj/aplikacja_do_typowania_produktow_pod_zlecenia,https://api.github.com/repos/aplikacja_do_typowania_produktow_pod_zlecenia/dtararuj,"Korzystając z aplikacji możemy wytypować indeksy bestsellerów i kitów, które chcemy dołożyć lub zabrać z konkretnego sklepu w ramach zarządzania grupą produktową w przedsiębiorstwie handlowym.","# aplikacja_do_wyznaczania_indeksow

### Korzystając z aplikacji możemy wytypować indeksy bestsellerów i kitów, które chcemy dołożyć lub zabrać z konkretnego sklepu w ramach grup produktowych.

&nbsp;

Narzedzie zostalo przygotowane w jezyku R, przy pomocy R Shiny.
Dane surowe zostały wygenerowane sztucznie.

Aplikacja w wersji demonstracyjnej jest dostepna pod poniższym [adresem](https://tararuj4.shinyapps.io/aplikacja_do_wyznaczania_indeksow/)

![strona główna aplikacji](foto/obraz1.png)


## Źródło danych

Plik jest zasilany następującymi danymi zewnętrznymi:
- ranking indeksow (wgrywany bezpośrednio przez użytkownika) generowany przez ""skrypt do wyznaczania indeksów""
- stany_biezace_sklepy - aktualne zatowarowanie sklepów (w wersji demonstracyjnej dane zaszyte w skrypcie, oryginalnie dostarczane w formie danych surowych),
- stan_sklep - skrócona wersja pliku stany_biezace_sklepu, przetworzona w skrypcie ""skrypt do wyznaczania indeksów"" (w wersji demonstracynej dane zaszyte w skrypcie).

## Schemat działania
---

Aplikacja po wgraniu danych surowych (""ranking indeksów"" w przypadku wersji demonstracyjnej), wybraniu dostępnych ustawień oraz po wytypowaniu ilości w ramach zdefiniowanych grup, generuje nam listę indeksów, które należy zwrócić z danego sklepu lub dotowarować nimi dany salon.


## Korzyści
---

Wykorzystując skrypt udało się zaoszczędzić: 
- 2 roboczogodziny raz na 2 tygodnie, w sytuacji gdy towarujemy nowy sklep i poszukujemy określonej ilosci indeksów pod jego zatowarownie.
- średnio po 40 minut na 1 sklep podczas bieżacej pracy alokatora, który potrzebuje wskazać indeksy, które powinien dotowarować lub zabrać z danego sklepu

Dodatkowo wybór indeksów do wskazanych ruchów towarowych nie jest przypadkowy tylko uzasadniony danymi statystycznymi. 
Oprócz tego udało się również wykluczyć błąd ludzki polegający na wytypowaniu niewłaściwych indeksów, o najnizszym potencjale lub nieświadomym dublowaniu tych samych pozycji. 

## Szczegółowy opis funkcjonalności
---

- Na wstępie wgrywamy plik ze wskazanego miejsca na serwerze

![wgrywanie danych](foto/obraz2.png)

&nbsp;

- Następnie możemy wskazać, który sklep chcemy odtowarować.
Możemy wybrać konkretny salon lub wybierać indeksy z całej sieci.
![a](foto/obraz3.png)

&nbsp;

- Następnie wskazujemy czy jesteśmy zainteresowani indeskami z oferty wyprzedażowej, niewyprzedażowej czy nie ma to dla nas znaczenia.

![b](foto/obraz4.png)

&nbsp;

- Kolejny krok to wybór indeksów, wskazujemy czy chcemy patrzeć i typować bestsellery czy indeksy sprzedające się najsłabiej, tzw. kity.

![b](foto/obraz5.png)

&nbsp;

- W kolejnym kroku wskazujemy jaki sklep chcemy dotowarować oraz poniżej czy chcemy typować indeksy, które już znajdują się na tym sklepie. 

![c](foto/obraz6.png)

&nbsp;

- Głównym elementem pracy na poniżej aplikacji jest wskazanie ile modeli ma wygenerować skrypt z poszczególnej grupy towarowej. Dokonujemy tego poprzez wpisanie ilości w kolumnie ile_modeli

Dodatkowo gdy towarujemy sklep lub chcemy wzmocnić zatowarowanie najlepszym indeksami, możemy poprzez wpisanie ilości w kolumnie ile_bestów przekazać do generowanych na końcu danych surowych informacje, które indeksy mają być potraktowane specjalnie.

![d](foto/obraz7.png)

&nbsp;

- Aby skrypt mógł się uruchomić należy po wgraniu wszystkich potrzebnych danych i ustawień kliknąć opcję ""odśwież"".

Wtedy skrypt wyświetli listę indeksów wraz z dopiskiem czy dany indeks ma mieć dopisek 'tak' w kolumnie czy_best (przy wybraniu opcji INGORUJ w polu Sklepy do odtowarowania) lub wartość i ilość towaru do ściągnięcia ze sklepu z danej grupy towarej (przy wybraniu jakiegoś konkretnego sklepu).

![odswiez](foto/obraz8.png)

![odswiez2](foto/obraz9.png)

&nbsp;

- Gdy rezultat pracy algorytmu jest satysfakcjonujący, u dołu alplikacji klikamy w opcję ""pobierz plik"".


W zależności od tego czy wskazaliśmy sklep do odtowarowania otrzymamy inne dane w pliku, który pobieramy z aplikacji.  

**Rezultatem powyższych działań jest plik csv, w poniższym układzie - dla opcji IGNORUJ:**

| KodProduktu |czy_best | 
|----|---|
|    |   |
|    |   | 
|    |   |

**Jeżeli wskazaliśmy konkretny sklep to dane wygenerują się w poniższym formacie:**

| KodProduktu |Rozmiar | Magazyn| Ilosc | 
|----|---|---------|---------|
|    |   |         |         |
|    |   |         |         |
|    |   |         |         |

----
Wygenerowane dane wykorzystywane są w innych skryptach:
- skrypt do towarowania nowych sklepów,
- aplikacja_algorytm autumatyczne MMki.
",0,0,1,0,retail,"[optymalization, r, retail, shiny-apps]",44-45
shrutibalan4591,Store-Sale-Prediction,,https://github.com/shrutibalan4591/Store-Sale-Prediction,https://api.github.com/repos/Store-Sale-Prediction/shrutibalan4591,"An end-to-end ML project, which aims at developing a regression model for the problem of predicting the sales of a given product, based on its properties like item category, weight, visibility, MRP, type of outlet the product is sold, size of the outlet etc.","## Supermarket Store Sales Prediction

### Overview

This is an end-to-end ML project, which aims at developing a regression model for the problem of predicting the sales of a given product, based on its properties like item category, weight, visibility, MRP, type of outlet the roduct is sold, size of the outlet etc.

Multiple regressor like Linear Regression, RandomForestRegressor, Lasso, XGBoost Regressor were tested and the best one was selected. The performance of the selected model was further improved using hyperparameter tuning.

Deployed in Railway.app.

Link to the application : https://store-sales-prediction.up.railway.app/

*************************************************

### Motivation

Nowadays, shopping malls and Big Marts keep track of individual item sales data in order to forecast future client demand and adjust inventory management. In a data warehouse, these data stores hold a significant amount of consumer information and particular item details. By mining the data store from the data warehouse, more anomalies and common patterns can be discovered.

The goal of this project is to build a solution that should able to predict the sales of the items in different stores of the supermarket chain, according to the provided dataset.

**************************************************

### Dataset Information

This dataset is taken from the kaggle. Link: https://www.kaggle.com/brijbhushannanda1979/bigmart-sales-data

***************************************************

### Installation

The Code is written in Python 3.7. If you don't have Python installed you can find it here. If you are using a lower version of Python you can upgrade using the pip package, ensuring you have the latest version of pip.

*****************************************************

### App Interface

![Capture](https://user-images.githubusercontent.com/77207245/224479168-9a5eb38e-0bb0-4e99-8ff5-96cb50ff22c8.PNG)

*****************************************************

### Directory Tree

![image](https://user-images.githubusercontent.com/77207245/224477824-c52f7e96-4b20-4898-9e2c-275e0fc81c19.png)


*****************************************************

### Technologies Used

![image](https://user-images.githubusercontent.com/77207245/224478400-7288b9a0-ef21-4d6d-bba7-47aae9daf11a.png)
",0,0,2,0,retail,"[data, data-analysis, data-mining, flask, html-css, lasso-regression, linear-regression, machine-learning, ml-deployment, ml-model-development, python, random-forest-regression, regression, retail, sales, xgboost-regression]",44-45
AmyCh4n,Population-vs-Retail,,https://github.com/AmyCh4n/Population-vs-Retail,https://api.github.com/repos/Population-vs-Retail/AmyCh4n,This code wrangles raw data to spatially analyse the relationship between population (number of people) and retail floorspace at borough and LSOA levels.,"# pop_vs_retail

This code wrangles raw data to spatially analyse the relationship between population (number of people) and retail floorspace at borough and LSOA levels.

References for datasets: 
London Datastore. (2014) Statistical GIS Boundary Files for London. Available at: https://data.london.gov.uk/dataset/statistical-gis-boundary-files-london

Gov.uk. (2019) Non-domestic rating: stock of properties including business floorspace, 2019. Available at: https://www.gov.uk/government/statistics/non-domestic-rating-stock-of-properties-including-business-floorspace-2019

London Datastore. (2016) Super Output Area Population (LSOA, MSOA) London. Available at: https://data.london.gov.uk/dataset/super-output-area-population-lsoa-msoa-london

ONS. (2020) Estimates of the population for the UK, England and Wales, Scotland and Northern Ireland. Available at: 
https://www.ons.gov.uk/peoplepopulationandcommunity/populationandmigration/populationestimates/datasets/populationestimatesforukenglandandwalesscotlandandnorthernireland
",0,0,1,0,retail,"[population, retail, rstudio]",44-45
jtc17,day-to-day,,https://github.com/jtc17/day-to-day,https://api.github.com/repos/day-to-day/jtc17,Most used HTML at Prim,"# day-to-day
<!DOCTYPE html>
<html>
<body>

<h1>Heading</h1>
<p>Paragraph.</p>

</body>
</html>


Headings
<span style=""color:#005B47; font-size:14px; font-weight:bold;"">Bold Green Header</span>
<h1>This is heading 1</h1>
<h2>This is heading 2</h2>
<h3>This is heading 3</h3>

Paragraphs
<p>This is a paragraph.</p>
<p>This is another paragraph.</p>

Links
<a href=""https://www.w3schools.com"">This is a link</a>
To other products
Optional Extra Box
<php>multi_addition_box( array(00000,00000,00000) );</php>
You May Also Like
<php> echo build_add_products_row(00000,00000); </php>

Product Variations
This product is also available in ------ and -----<br>
<div align=""center"">
<div  align=""center""; style=""display:inline-block;"">
   <div style=""display:inline-block;"">
    <a href=""/-p-11111.html"">
     <php> echo get_product_image (11111,100,100);</php>
     <p>-----</p>
    </a>
   </div>
   <div style=""display:inline-block;"">
    <a href=""/-p-22222.html"">
     <php> echo get_product_image (22222,100,100);</php>
     <p>-----</p>
    </a>
   </div>
</div>
</div>


Images
<img src=""w3schools.jpg"" alt=""W3Schools.com"" width=""104"" height=""142"">
Central Tagged Image 
<!-- img_1 -->
<div align=""center""><php> echo tep_image('images/DF3006_add.jpg', '', 600, 600); </php></div>
<!-- endimg_1 -->

Double Columning 
<div class=""clearfix"">
<div class=""twocol text_vtop"">
FIRST COL. THING
</div>
<div class=""twocol text_vtop"">
SECOND COL. THING
</div>


Title Attribute - a title attribute means you can have a message appear when the cursor hovers over a given text. For example: 

<!DOCTYPE html>
<html>
<body>

<h2>The title attribute</h2>

<p title=""I'm a tooltip"">
Mouse over this paragraph, to display the title attribute as a tooltip.
</p>
</body>
</html>
",0,0,0,0,retail,"[double-column, html, image, link, product-description, retail]",44-45
SchillingEnterprises,Schilling-Interiors,SchillingEnterprises,https://github.com/SchillingEnterprises/Schilling-Interiors,https://api.github.com/repos/Schilling-Interiors/SchillingEnterprises,,,0,0,2,0,retail,"[retail, wholesale]",44-45
adesgautam,grocery_obj_detection,,https://github.com/adesgautam/grocery_obj_detection,https://api.github.com/repos/grocery_obj_detection/adesgautam,Detecting Cigarette brands from shelves using object detection,"# Grocery Objects Detection

## The data could be downloaded from [this](https://github.com/gulvarol/grocerydataset) repository

## A YOLOv3 model will be trained on the dataset.

## Run grocery_object_detection.ipynb in Google colab.

## The pretrained YOLOv3 model on the dataset can be download from [here](https://drive.google.com/file/d/1-KFW4WjhrpQGxp-UMB1LB38m_YNLdPXw/view?usp=sharing)
",0,0,2,0,retail,"[computer-vision, grocery, keras, object-detection, retail, tensorflow, yolov3]",44-45
Retail-Edge-Platform,Retail-Edge-Platform,Retail-Edge-Platform,https://github.com/Retail-Edge-Platform/Retail-Edge-Platform,https://api.github.com/repos/Retail-Edge-Platform/Retail-Edge-Platform,Retail-Edge-Platform (REP) - is the beginning of an open-source platform targeted at the brick-and-mortar retail space that has been seeing a steady increase in digitization.,"# Retail-Edge-Platform (REP)
REP is the beginning of an open-source platform targeted at the brick-and-mortar retail space that has been seeing a steady increase in digitization.

## Goals
### Initially REP goal was to:
 * Provide a working example of a retail end customer facing immersive and interactive portal to assist in the retail experience
 * Create a pipeline to gather streaming data about the interactive experience
 * Simulate this interactive data stream for analytics development
 * Host a data-science centric analytics development environment. (Jupyter Notebooks)
 * Provide a means to move data from the Edge -> Cloudward

### To affect the operations of the retailers and brands, Retail-Edge-Platrom needs to:
 * Continue the connection to the cloud with examples of AWS and GCP IoT integrations
 * Leverage Jupyter Notebook analytic development in the stream-based cloud framework
 * Provide extensible analytics backed dashboard for retailers
 * Provide API endpoints to these dashboard elements to allow retailers and brands the opportunity to integrate with other systems

### Delighting both retailers, brands, and end customer at the Edge means:
 * Expanding the nature of the Edge platform's presence in the physical space with display, lighting, sound, and sensors options
 * For sensors, leverage Docker containers to deliver high performance on Edge processing that will feedback to customer experience and to the cloudward datastream
 * Providing remote management of the Retail-Edge-Platform forward deployed asset. This includes installation and bring-up assistance, platform and experience calibration, and the positioning of new end-customer facing material on the Edge.
 * Creating a self-service means to create new end-customer facing material that will allow marketers, brands, and even smaller retailers to create compelling material that takes advantage of the lavish area of interactive space to educate/assist potential end customers about their purchase.


This material is much like traditional web material but because of the use of sensors can be much more interactive. Along with that theming to fit the retail or brand environment is much richer because it’s not fighting for page real-estate. 
This project uses Docker to setup data analysis pipeline.

## What is here now
### Simple Example Model
Below is a diagram of initial data analysis pipeline as built.

<img src=""/assets/Overview-As Built Overview.png?raw=true"" alt=""drawing"" width=""700""/>

Example retail end customer user interface experience currently captured in the repo.

<img src=""/assets/Overview-User Interface.png?raw=true"" alt=""drawing"" width=""700""/>

Data Analysis workbench using Jupyter Notebooks. Tied into the local Redis database.

<img src=""/assets/Overview-DataAnalysis.png?raw=true"" alt=""drawing"" width=""700""/>

Session Generator to create fictional traffic to allow experimentation with analytics.

<img src=""/assets/Overview-Session Generator.png?raw=true"" alt=""drawing"" width=""700""/>

There is 

Here is how to build it.

<img src=""/assets/Overview-Build.png?raw=true"" alt=""drawing"" width=""700""/>


## Future

 * It would be easy to expand this with an OpenCV container and provide people and facial detection to start to gather real analytics.
 * Adding a IOT MQTT endpoint to AWS or GCP would be very easy. This would allow further analysis off the Edge device.
 * As to the edge device an Intel NUC would be more than enough. A USB video camera like an Intel Real Sense L515 would work nicely.
 * To keep cost down and make development easier load it with Linux, Docker or K8S. Edge management could be either AWS or other.
 * As to developing content my other project B4Time uses draw.io/diagram.net to allow computer architects to model a system.  This same plugin approach could allow a skilled graphics artist to design the user experience and deploy it without the need to involve a UI developer.
 * Retail experience could be packaged in a stylish and in expensive furniture quality shell allowing the customer to access the content through a touch screen while also accessing the physical product. 
 * The product could be a pay as you go model with sales inspired by the interface paying for the subscription.

<img src=""/assets/sideboards-buffets.jpeg?raw=true"" alt=""sidboard"" width=""200""/>












",0,0,2,0,retail,"[cloud, customers, data-science, docker, edge, full-stack, interactive, iot, opencv, python, retail, sensors, startup]",44-45
booleanhunter-tech-blog,e3-retail,booleanhunter-tech-blog,https://github.com/booleanhunter-tech-blog/e3-retail,https://api.github.com/repos/e3-retail/booleanhunter-tech-blog,An E-commerce retail website demonstrating Elasticsearch,"![](/public/images/index.png)
## E³Retail

> An E-commerce products-list website that demonstrates the application of Elasticsearch in retail & online shopping.

### App Screenshots

#### Home Page

![Screenshot of home page](/public/images/e3-retail_home.png)

#### Products filter page

![Screenshot of products filter page](/public/images/e3-retail_search.png)

### Features

- Responsive on web and mobile.
- Built using [Progressive enhancement](https://en.wikipedia.org/wiki/Progressive_enhancement) philosophy.

#### Search features

- Search for a product by typing in the search input.
- Search within a category.
- Search across multiple categories.
- Filter by brand, pricing and ratings.
- Sort by relevance, ratings or price.
- Paginate through search results.
- Search input autocomplete suggestions (requires JavaScript to be enabled).

### Requirements

- A laptop or a desktop computer.
- Chrome browser.
- Internet connectivity.

### Instructions to install

1. Requires **[AWS OpenSearch](https://aws.amazon.com/opensearch-service/)**
2. Initialize environment variables. If you're running the app locally, create a `.env` file:

```shell
    NODE_ENV=development
    ELASTICSEARCH_HOST=your-elasticsearch-domain
    ELASTICSEARCH_PORT=443
    ELASTICSEARCH_USER=your-elasticsearch-master-user
    ELASTICSEARCH_PASSWORD=your-elasticsearch-master-password
    ELASTICSEARCH_INDEX=index-to-search-on
```

3. Install [Node.js](https://nodejs.org/)
4. Clone or fork this repo
5. Run `npm install` in your project root
6. Run `npm start` to start the app server


### Common commands

#### Creating an index

`npm run create-index your-index-name`

#### Bulk uploading Data

Syntax for uploading data in `.ndjson` format:

```
curl -XPOST -u 'username:password' 'your-elasticsearch-url/_bulk' --data-binary @datasets/flipkart-products-bulk.ndjson -H 'Content-Type: application/json'
```

### Known issues

- Images may not load if they have been removed at the source.",0,0,1,0,retail,"[css3, ecommerce, elasticsearch, html5, javascript, learning-by-doing, nodejs, opensearch, progressive-enhancement, project, retail, tutorial-code, webapp]",44-45
estorgio,StoreHelper,,https://github.com/estorgio/StoreHelper,https://api.github.com/repos/StoreHelper/estorgio,Basic cashier and stock tracking app,"# StoreHelper

StoreHelper is a simple store management app for cashier and stock tracking.

This personal project is still in progress.",0,0,0,0,retail,"[express, inventory, mongodb, nodejs, point-of-sale, retail]",44-45
dgop92,retail-tech-business-api,,https://github.com/dgop92/retail-tech-business-api,https://api.github.com/repos/retail-tech-business-api/dgop92,a retail tech business api for a specific client,"# Retail Tech Business API

This is an API that allows you to control exits and entries while collecting useful data to create insights for your business

## Project setup

### Download the dependencies with pip

```
pip install -r requirements.txt
```

### Before running the project apply the respective migrations

```
python manage.py migrate
```

### Run the API in localhost

```
python manage.py runserver
```

## API reference

The API reference was created using postman. [Go to Docs](https://documenter.getpostman.com/view/12352026/TVsxB6Z3)

## Database ERD

![Database Diagram](docs/database-erd.png ""Database Diagram"")
",0,0,1,0,retail,"[django, django-rest-framework, python, rest-api, retail]",44-45
jessearmenio,TulsaTrap,,https://github.com/jessearmenio/TulsaTrap,https://api.github.com/repos/TulsaTrap/jessearmenio,"A website designed for Tulsa Trap clothing, an apparel startup from Tulsa, OK. Website has been archived as a creative project and is inactive. ",,0,0,1,0,retail,"[animation, branding, clothing, creative, engagement, learning, logo, project, retail]",44-45
leassis91,rossmann_store,,https://github.com/leassis91/rossmann_store,https://api.github.com/repos/rossmann_store/leassis91,Rossmann Drugstores Sales Prediction Repository,"# <p align=""center"">🏪💊 ROSSMANN STORE SALES PREDICTION 💊🏪</p> 
<p align=""center""><img src=""https://github.com/leassis91/rossmann_store/blob/master/img/rossmann-store.jpg?raw=true""></p>
<br>

## 📖 Background

Rossmann operates over 3,000 drug stores in 7 European countries. Currently, Rossmann store managers are tasked with predicting their daily sales for up to six weeks in advance. Store sales are influenced by many factors, including promotions, competition, school and state holidays, seasonality, and locality. With thousands of individual managers predicting sales based on their unique circumstances, the accuracy of results can be quite varied.

<br>


## 📌 Problem Statement

*Forecasting the sales' income of the next 6 weeks.*

Since the recent results's accuracy are quite noisy, our work here is to give an **assertive prediction** of the sales of each store, up to 6 weeks in advance. This task has been assined to the whole team of Data Scientists, who are given a historical database in order to generate the desired forecasting. To catch up all details of the request, the team had a business meeting with company's CFO, who explained the need to establish a budget to carry out a general repair in each store.

<br>

## 💾 Data Understanding

 ### - Code Used:

* Python Version : 3.8
* Packages : Jupyter, Pandas, Numpy, Matplotlib, Seaborn, Scikit-Learn among others (please, check full list [here](https://github.com/leassis91/rossmann_store/blob/master/requirements.txt))
* Frontend API: Telegram Bot
* Backend: Heroku

<br>

 ### - Importing Dataset:

* Kaggle: https://www.kaggle.com/competitions/rossmann-store-sales/data

<br>

 ### - Data Dictionary

| Variable                       | Descriptions                                                      |
| -------------------------------- | ------------------------------------------------------------ |
| Id                               | An id that represents a (store, date) duple within the test set|
| Store                            | A unique id for each store                                   |
| Sales                            | The turnover for any given day                          |
| Customers                        | The number of customers on a given day                       |
| Open                             | An indicator for whether the store was open: 0 = closed, 1 = open |
| Stateholiday                     | Indicates a state holiday. Normally all stores, with few exceptions, are closed on state holidays. Note that all schools are closed on public holidays and weekends. A = public holiday, b = easter holiday, c = christmas, 0 = none |
| Schoolholiday                    | Indicates if the (store, date) was affected by the closure of public schools |
| Storetype                        | Differentiates between 4 different store models: a, b, c, d  |
| Assortment                       | Describes an assortment level: a = basic, b = extra, c = extended |
| Competitiondistance              |Distance in meters to the nearest competitor store           |
| Competitionopensince[month/year] | Gives the approximate year and month of the time the nearest competitor was opened |
| Promo                            | Indicates whether a store is running a promo on that day        |
| Promo2                           | Promo2 is a continuing and consecutive promotion for some stores: 0 = store is not participating, 1 = store is participating |
| Promo2since[year/week]           | Describes the year and calendar week when the store started participating in promo2 |
| Promointerval                    | Describes the consecutive intervals promo2 is started, naming the months the promotion is started anew. E.G. ""Feb,may,aug,nov"" means each round starts in february, may, august, november of any given year for that store |
 
<!--  ### - Data Exploration
 
   1. The train data is contains large number of categorical features that should be treated. For the 1º cicle run, we tried Ordinal Encoding, then proceed with LabelEncoding, and OneHotEncoding.
   2. The numeric columns are not normally distributed, which need to be standardized. 
   3. The target variable is kind of imbalanced (14% for 1-class), it might have to be treated.
   4. The outliers of the numeric column should be treated. -->

<br>

 ### - Business Assumptions

  * Stores with sales equal 0 were discarded.
  * Days when stores were closed were discarded.
  * Stores missing information about ""Competition Distance"" will be set a value of '200000' distance.
  * Analysis will be made one week after last date record.
 
 <br>
 
## 🧾 Evaluation Metric
 
 We used many error metrics during the project. Main model metric for evaluation was the Root Mean Squared Error (RMSE). The RMSE is calculated as  
<p align=""center"">
  <img width=""400"" src=""https://user-images.githubusercontent.com/67332395/175569005-2bd2789e-85bc-44d8-8378-faa3e03f019c.png"" alt=""RMSE"">
</p>

where *y_hat* is the predicted value, *y* is the ground truth value, and *n* is the number of stores in the dataset.

<br>

## 🔬 Solution Approach

The approach used to solve this task was done by applying CRISP-DM¹ methodology, which was divided in the following parts:

1. **Data Description:** understanding of the status of the database and dealing with missing values properly. Basic statistics metrics furnish an overview of the data.  
2. **Feature Engineering:** derivation of new attributes based on the original variables aiming to better describe the phenomenon that will be modeled, and to supply interesting attributes for the Exploratory Data Analysis.
3. **Feature Filtering:** filtering of records and selection of attributes that do not contain information for modeling or that do not match the scope of the business problem.
4. **Exploratory Data Analysis (EDA):** exploration of the data searching for insights and seeking to understand the impact of each variable on the upcoming machine learning modeling.
5. **Data Preparation:** preprocessing stage required prior to the machine learning modeling step.
6. **Feature Selection:** selection of the most significant attributes for training the model.
7. **Machine Learning Modeling:** implementation of a few algorithms appropriate to the task at hand. In this case, models befitting the *regression* assignment - *i.e.*, forecasting a continuous value, namely sales.
8. **Hyperparameter Fine Tuning:** search for the best values for each of the parameters of the best performing model(s) selected from the previous step.
9. **Statistical Error Analysis:** conversion of the performance metrics of the Machine Learning model to a more tangible business result.
10. **Production Deployment:** deployment of the model in a cloud environment (Heroku), using Flask connected to our model in a pickle file.
11. **Telegram Bot:** deployment of Telegram Bot API, here used as our user receiver. Check out at ""Deployment"" section.

<br>

## 🕵🏽‍♂️ Exploratory Data Analysis & Main Insights

### - Numerical Attributes Correlation

![image](https://user-images.githubusercontent.com/67332395/175460617-43a48c17-d3e0-4d05-9c86-b7d7be891572.png)

### - Categorical Attributes Correlation

![image](https://user-images.githubusercontent.com/67332395/175460670-83e08358-dd8f-432a-ad87-51037abe5fad.png)

### - Main Hypothesis Chosen

Here, the criterion used to choose the main hypotheses was in the sense of how shocking and impacting the result would be for the business team's beliefs.

- **Hypothesis 1 (H2 in notebook):** Stores with closer competitors should sell less. 
   ***False:*** Data showed us that, actually, they sell MORE.
   
   Business team's belief revealed us their thoughts on lower sales while drugstores are closer to the competitors. This hypothesis proves the opposite The correlation analysis of ""competition distance"" and ""sales"" shows a small correlation, indicating that sales do not increase when competitors are closer.
   
   ![image](https://user-images.githubusercontent.com/67332395/175464452-7d3ae44c-9e38-435a-9029-e5c95f4c55ba.png)


- **Hypothesis 2 (H4 in notebook):** Stores with longer active offers should sell more.
   ***False:*** Data showed us that, stores that kept products on sale for a long time performed worse than before.
   
   Again, we shocked business team's belief and common sense that. Here are the visualizations.
   
   ![image](https://user-images.githubusercontent.com/67332395/175464762-d1ba1d94-3e8f-4372-a435-f8e5c4603740.png) 
   
   
- **Hypothesis 3:** Stores should sell more after the 10th day of each month.  
   ***True:*** the average performance is better after 10 days of the month.
   
   ![image](https://user-images.githubusercontent.com/67332395/175465319-f058885e-1deb-45f2-b401-d6515bb71727.png)

<br>

## 💻 Machine Learning Modeling & Evaluation

 * Cross Validation
 
  *Performance on 5 K-Fold CV.*

| Model Name | MAE CV   | MAPE CV      | RMSE CV |
|-----------|---------|-----------|---------|
|  Random Forest Regressor  | 837.68 +/- 218.74| 0.12 +/- 0.02  | 1256.08 +/- 320.36 |
|  XGBoost Regressor	  | 1039.91 +/- 167.19 | 0.14 +/- 0.02   | 1478.26 +/- 258.52 |
|  Linear Regression	  | 2081.73 +/- 295.63 | 0.3 +/- 0.02   | 2952.52 +/- 468.37 |
|  Linear Regression - Lasso	  | 2116.38 +/- 341.50 | 0.29 +/- 0.01	   | 3057.75 +/- 504.26 |


Although the Random Forest model has proven to be superior to the others, in some cases this model ends up requiring a lot of space to be published, resulting in an extra cost for the company to keep it running. Therefore, the chosen algorithm was the **XGBoost Regressor** which in sequence passed to the Hyperparameter Fine Tunning step.

<br>

 * Final Model (after Hyperparameter Fine-Tuning)

| Model Name | Mean Absolute Error | Mean Absolute Percentage Error | Root Mean Squared Error |
| ---- | :----: | :----: | :----: |
| XGBoost Regressor | 699.43 | 0.1037 | 1005.6039 |

<br>

## 📉 Business Performance

According to our forecasting model, we achieved an efficiency improvement of 48.37% compared to previous forecasts (Average Model had 1354.8 for MAE and our new model has 699.43).
Translating into business terms, we calculate the sum of worst and best revenue scenarios, and the respective forecasts made.

| Scenario | Values |
|-----------|---------|
| Predictions | R$285,934,117.20 |
| Worst Scenario | R$285,150,484.70 |
| Best Scenario | R$286,717,749.69 |

As we can see, our **best scenario** and **worst scenario** only diverge from **predictions** by 0.27%: certainly more assertive than previous one.

<br>

## 💡 Conclusions

 - XGBoostRegressor model had the best performance when it comes to **Results/Time * Accuracy**, and thus gave us a more assertive prediction, helping our CFO on taking futures decisions about budget and repairing the stores.

<br>

## 👣 Next steps

DS team establish to start another cycle to analyze the problem, seeking different approaches, creating another hypothesis, reconsidering the ones not chosen, and reanalysing stores with behavior that were tough to do the forecast. Some approachs in mind to be made:

 - Collect more data;
 - Change aggregate parameter ""sum"" to ""mean"" of all stores for the assortment hypothesis. 'Extended assortment' will probably perform better than the others;
 - Refine the feature engineering, trying to find another good features;
 - Work with GridSearchCV, as we have more time to tune our model, since its already in production;
 and many more.

<br>

## 🚀 Deployment

Go say 'Hi!' to our bot! Check it out at:

[<img alt=""Telegram"" src=""https://img.shields.io/badge/Telegram-2CA5E0?style=for-the-badge&logo=telegram&logoColor=white""/>](http://t.me/rossmannleassis_bot)

 - Sign up in Telegram;
 - Submit one number at a time and wait for prediction!
 - or just look for 'rossmannleassis_bot' in Telegram's search!

![send](https://github.com/leassis91/rossmann_store/blob/master/img/gif_rossmann.gif)

<br>

## 🔗 References

1. Data Science Process Alliance - [What is CRISP-DM](https://www.datascience-pm.com/crisp-dm-2/)
2. Owen Zhang - [Open Source Tools & Data Science Competitions](https://www.slideshare.net/odsc/owen-zhangopen-sourcetoolsanddscompetitions1)
3. Statstest  - [Cramer's V](https://www.statstest.com/cramers-v-2/)
4. Bulldogjob - [How to write a Good readme](https://bulldogjob.com/news/449-how-to-write-a-good-readme-for-your-github-project)


<br>

If you have any other suggestion or question, feel free to contact me via [LinkedIn](https://linkedin.com/in/leandrodestefani).

<br>

## ✍🏽 Author

- [Leandro Destefani](https://github.com/leassis91)

[![linkedin](https://img.shields.io/badge/linkedin-0A66C2?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/leandrodestefani) [![gmail](https://img.shields.io/badge/Gmail-D14836?style=for-the-badge&logo=gmail&logoColor=white)](mailto:leassis.destefani@gmail.com) [![kaggle](https://img.shields.io/badge/Kaggle-3776AB?style=for-the-badge&logo=Kaggle&logoColor=white)](https://kaggle.com/leandrodestefani)

[![MIT License](https://img.shields.io/apm/l/atomic-design-ui.svg?)](https://github.com/tterb/atomic-design-ui/blob/master/LICENSEs)

***

## 💪 How to contribute

1. Fork the project.
2. Create a new branch with your changes: `git checkout -b my-feature`
3. Save your changes and create a commit message telling you what you did: `git commit -m"" feature: My new feature ""`
4. Submit your changes: `git push origin my-feature`
",0,0,1,0,retail,"[data-science, forecasting-time-series, retail, time-series-analysis]",44-45
gigixoliva,distribuidora-oliva,,https://github.com/gigixoliva/distribuidora-oliva,https://api.github.com/repos/distribuidora-oliva/gigixoliva,proyecto final/desarrollo web,,0,0,1,0,retail,"[commercial, retail]",44-45
BracketNerd101,World_Bank_Landing_Page,,https://github.com/BracketNerd101/World_Bank_Landing_Page,https://api.github.com/repos/World_Bank_Landing_Page/BracketNerd101,World Bank Association,,0,0,1,0,retail,"[bootstrap, bootstrap4, corporate, overlays, retail, retailbanking]",44-45
anacoxta,lojamonstra,,https://github.com/anacoxta/lojamonstra,https://api.github.com/repos/lojamonstra/anacoxta,Site da Loja Monstra (2018),"# lojamonstra
Site da Loja Monstra (2018)
",0,0,1,0,retail,"[animation, comics, front-end-development, retail, website]",44-45
yoogun143,PUMA-Retail-Location-Opening,,https://github.com/yoogun143/PUMA-Retail-Location-Opening,https://api.github.com/repos/PUMA-Retail-Location-Opening/yoogun143,Bokeh web app shows the best location in a predefined state to open a new PUMA physical store.,"# PUMA Retail Store Location Opening

`Bokeh` web app shows the best location in a predefined state to open a new PUMA physical store.

[![](https://img.shields.io/badge/Heroku-Open_Web_App-blue?logo=Heroku)](https://pumacps.herokuapp.com/bokeh_app)

## Description
In today's digital age with same-day ecommerce deliveries, brick & mortal stores are in being contracted. However, for a market player which already has a broad range of ecommerce supply chain like PUMA, extending the physical footmark concurrently with online shopping is negligible. [Puma SE](https://about.puma.com/), branded as Puma, is a German multinational corporation that designs and manufactures athletic and casual footwear, apparel and accessories, which is headquartered in Herzogenaurach, Bavaria, Germany. Puma is the third largest sportswear manufacturer in the world.

This dashboard shows the best in-state location covering the most unreached PUMA ecommerce transactions, which are not in vicinity of a physical store.

The dashboard has 3 tabs:
+ **Map** tab is the map plotting all online transactions and retail location in the local area, along with chosen locations.
+ **All Possible Locations** tab is the prediction of all locations in one state if PUMA tends to place a physical store there. This second tab acts as a guide to select solution number in the first dashboard.
+ **All Online Transactions by ZipCode** tab is information about online transactions in each Zip code used to aggregate data in tab *All Possible Locations*.

## Prerequisites
- Python <= 3.7.9
- Anaconda (optional, is used to install environment, you can use python `venv` instead)

## Screenshots (click on image for video demo)
[![](https://user-images.githubusercontent.com/40580775/113587122-8d7d0000-9658-11eb-9c03-644b3cb168fc.PNG)](https://youtu.be/QlFP5K19WhE)

## Parameters
- `Choose State`: Input state as abbreviation. Default state is MA.
- `Select Radius` (Miles): Coverage of a retail store with Ecommerce transactions. Intuitively, Countryside have higher radius than urban area because of population density and transportation access.
- `Transaction Type`: Checkbox to choose Sale or Return or both.
- `Start & End Date`: Data will be aggregated between Start and End date.
- `Select Solutions`: Solution based on ""#"" column in **All Possible Locations** tab. Default solution is 0 for maximum ecommerce transactions.

## Notes
- The dashboard runs really slow so please changing each parameters once at a time and be patient.
- Choosing different state will not change current view and you have to scroll to chosen state
- If new store chosen is adjacent to the border of 2 state, Ecommerce transactions in nearby state within radius are included and shown in **All Online Transactions by ZipCode** tab. For example, a store in NYC can still attract online customers in Newark, NJ.
- The dashboard can also identify best in-city location by sorting **All Possible Locations** by *city*
- Some locations are not shown on map because coordinates for those ZipCode are missing. However, transactions are included and shown in 2 table tabs.
- Choosing timeframe based on desirable season. For example, examining store location measurement for different year, different quarter


## Installation
1. Clone repository:
```bash
$ git clone https://github.com/yoogun143/PUMA-Retail-Location-Opening.git
```

2. Install dependencies using Anaconda and pip
```bash
$ conda create -n puma-location-app python=3.7.9  #Create new environment
$ conda activate puma-location-app #Activate environment
$ conda install pip #install pip inside the environment
$ pip install -r requirements.txt #Install required dependencies
```

3. Run the app
```bash
$ bokeh serve --show bokeh_app.py
 * Bokeh app running at: http://localhost:5006/bokeh_app
```

The app will run at http://localhost:5006/bokeh_app.

## Roadmap
- [ ] Automatically change view to current state when choose state.
- [ ] Fix bug of transactions in other states are included in chose state.
- [ ] Use other Zipcode API for better geo-tagging.
- [ ] Reduce web app latency, loading function.
- [ ] Improve UI to be more user friendly.
- [ ] Mobile application.


## Credits
Northeastern University - College of Professional Studies.

PUMA North America, Inc.

## License
MIT License

Copyright (c) [2019] [Thanh Hoang]
",0,0,1,1,retail,"[bokeh, heroku, location, puma, python, retail, store]",44-45
john200x,merakiguestwifi,,https://github.com/john200x/merakiguestwifi,https://api.github.com/repos/merakiguestwifi/john200x,Leveraging Meraki's guest WiFi captive Wi-Fi portal to improve customer engagement and provide instant contextual feedback to property mgmt duty manager,"# merakiguestwifi
Leveraging Meraki's guest WiFi captive Wi-Fi portal to improve customer engagement and provide instant contextual feedback to property mgmt duty manager. To improve guest feedback which is often shared in social media after the guest has left the premise with vivid negative experience, telling the world via social media their negative experiecnce with damaged reputation difficult to remediate.  This simple guest survey (splash page) is to collect user feedback upon successful guest Wi-Fi login. We leverage the built-in feature of Meraki Access Point. https://documentation.meraki.com/MR/MR_Splash_Page/Splash_Page_Details_for_Meraki_MR

A google form is created to collect feedback with rating between 1 (very bad) to 5 (very good)

If the feedback score is 1, the google script on the google form will triger an email (via Google SMTP) to duty manager's email address.

A language match of the feedback's text will be used to select the appropriate MV to take a snapshot whereby the image will be included in the email to be sent to the duty manager to give more context. For example, if the feedback says ""broken glass near the exit A"", then script will send API call to Meraki MV camera near exit A to take a screenshot and the image is pasted onto the email to Duty Manager to give a visual on the area for impact to customer flow near Exist A.
",0,0,1,0,retail,"[google-script, meraki-api, meraki-dashboard, meraki-mv-cameras, property-management, public-wifi, retail]",44-45
immohsin,Magento2,,https://github.com/immohsin/Magento2,https://api.github.com/repos/Magento2/immohsin,Slanglabs Retail Assistant Extension lets developer add voice assistant to there Magento 2 store in a truely no-code way.,"# Slang Retail Voice Assistant

![Packagist Version (custom server)](https://img.shields.io/packagist/v/immohsin/magento2?style=plastic)

Slanglabs Retail Assistant Extension lets developer add voice assistant to there magento 2 store in a truely no-code way.

Sign up and create your retail assistant on the [SlangLabs Console](console.slanglabs.in).

The extension follows the following practices:

- A minimum of PHP 5.3 is required.
- composer v2.0.0

### Installation

Run this commands from the magento root diirectory:

```shell
$ composer require immohsin/magento2
$ bin/magento setup:upgrade
```

### Usage

- After Installation, You would you see configuratiion setting on the magento admin store.
  Go to `Store > Configuration > Slang Assistant > Retail Voice Assistant`.
- Update the form with the credentials from slang console and select languages of your preference.
- Save the config and clean up the cache.
- Head over to the magento store and you should see a slang microphone appears on the bottom right of the screen.
",0,0,3,0,retail,"[retail, slanglabs, voice, voice-assistant, voice-commands, voice-control, voice-recognition]",44-45
rht52x,rht52x,,https://github.com/rht52x/rht52x,https://api.github.com/repos/rht52x/rht52x,"Erik Lundberg, Senior Architect  for  Cottee Parker Architects","### About

Erik is an Architect with extensive experience across a diverse range of project types. His ability to work across all phases of a project from design concept right through to contract documentation and advice during construction makes him a well rounded and highly versatile Architect.

Erik delivers high quality designs that reflect an affinity for clean, simple and elegant solutions. His detailed understanding of project requirements allows him to understand complex design issues and tailor a design solution that is innovative and commercially driven.

Erik has spent most of his Architectural career in Queensland. He has over 15 years experience in private residential, retail, multi-residential and commercial projects across a range of sizes and complexities, with a tendency towards major projects for commercial developers and large builders. 

### Experience

#### Cottee Parker Architects
- Senior Architect
- 2019 – Present
- Brisbane, Queensland, Australia
 
#### Powe Architects

- Senior Architect
- Apr 2016 – Nov 2019 (3 years 8 months)
- Brisbane, Queensland, Australia

#### ML Design

- Architect, Project Leader
- Sep 2008 – Mar 2016 (7 years 7 months)
- Brisbane, Queensland, Australia

#### LVO' Architecture

- Architect
- Mar 2007 – Jul 2008 (1 year 5 months)
- Brisbane, Queensland, Australia

#### Rice Daubney

- Architectural Graduate
- Mar 2006 – Mar 2007 (1 year)
- Brisbane, Queensland, Australia

#### Cyclone Studios

- Architectural Graduate
- Apr 2004 – Mar 2006 (2 years)
- Airlie Beach, Queensland, Australia

#### Hassell

- Architecture Student
- Apr 2003 – Jul 2003 (4 months)
- Adelaide, South Australia, Australia

#### GABA

- Private Lesson Instructor
- Nov 2001 – Nov 2002 (1 yr)
- Shibuya, Tokyo, Japan

### Education

#### University of Adelaide
- Bachelor of Architecture (BArch) Architecture
- 2000 – 2003
- Activities and Societies:
    - RAIA (SA) Education Standing Committee: Student Rep. (1999-2001)
    - School Board: Student Rep. (1997-2000)
    - School Advisory Board: Student Rep. (1997-2000)
    - Operations Committee: Student Rep. (1997-2000)
    - Workplace Health & Safety Committee: Student Rep. (1997-2001)
    - Architecture Student's Association: 5th Year Rep. (2001)
    - Architecture Student's Association: 4th Year Rep. (2000)
 
#### University of Adelaide
- Bachelor of Design Studies (BDesSt) Architecture
- 1997 – 1999
- Activities and Societies:
    - RAIA (SA) Education Standing Committee: Student Rep. (1999-2001)
    - School Board: Student Rep. (1997-2000)
    - School Advisory Board: Student Rep. (1997-2000)
    - Operations Committee: Student Rep. (1997-2000)
    - Workplace Health & Safety Committee: Student Rep. (1997-2001)
    - Architecture Student's Association: President (1999)
    - Architecture Student's Association: 2nd Year Rep. (1998)
 
#### St Peter's College
 - South Australian Certificate of Education (SACE) Physics, Chemistry, Biology, Mathematics, English Studies
 - 1993 – 1996
 - Activities and Societies:
     - Captained the 2nd XI soccer team 1996;
     - Played water polo, 1996;
     - Rowed in the 1st VIII, 1995;
     - Rowed in the 2nd VIII, 1994;
     - Rowed in the U15D coxed four 1993;
     - Exploration Society member;
     - Muting Club member;
     - Chapel Choir member;
     - Farr house member (symbols).
 
#### Canberra Grammar School
 - 1985 – 1992
 - Activities and Societies:
     - Special Music Class;
     - String Orchestra;
     - School Choir;
     - Orienteering;
     - Swimming;
     - Soccer;
     - Rowing

### Hobbies

This Github account is for Erik's geekier hobby interests and personal projects. These projects are not Erik's day job. These repositories and projects are totally and completely unrelated to Erik's professional role at Cottee Parker Architects:

- Erik is currently working on a combination of Hesiod, DNSSEC, SSHFP and OpenPGP to create a distributed, signature backed, hardware 2FA name and auth single sign on 'system', without Kerberos or ldap, using existing commodity code and infrastructure as much as possible;
- Erik is currently learning about Kerberos, ldap, DNS, OpenPGP cards and Git;
- Erik is looking to collaborate on a KISS Linux package repository that updates then pulls from local git repositories under /usr/src/ rather than downloading full source each time. Step 2 is minimal i18n and l10n bolt on packages to ootb kiss linux;
- Erik is looking for help with an elegant, simple and secure physical console login with an openpgp card based private key, without a requirement to access home folders in any way until after authentication has occured;
",0,0,1,0,retail,"[architect, architecture, commercial, design, multi-residential, private-residential, retail]",44-45
Arpita-deb,Departmental_store_analysis_in_R,,https://github.com/Arpita-deb/Departmental_store_analysis_in_R,https://api.github.com/repos/Departmental_store_analysis_in_R/Arpita-deb,Data Analysis of a departmental store during  Covid-19 period using R programming Language.,"# What were they buying on March 2020? 
## Application of Data Analysis in Business with R

## Introduction: 

The COVID-19 pandemic has taken a sharp economic toll on several large and small scale industries worldwide. It has changed the business strategies from investing in innumerable products of various types to providing basic necessities of life. In this project a hypothetical dataset of a departmental store is analysed. It contains data about different product types, product categories, companies that manufactured them,their prices etc. Working with this dataset will enable us to find patterns and trends in customer behaviour and also get to know the profit and losses of businesses during the pandemic.

This project is done as a guided project on Coursera. There are two datasets namely, DEPARTMENTAL STORE and RATINGS dataset and they contain details of products from May, 2020, a period marked by covid-19. These datasets have been provided by the instructress.

## Objective:

* To apply data manipulation and data Visualization to find out which products were more profitable.
* To perform descriptive statistics on the data set to analyze trends and patterns.
* To find correlation between different variables and gather insights for strategic decision making.

## Tools used:

* Excel - for initial data cleaning
* RStudio - for Data Analysis and Visualization.

## Data Dictionary:

### MY_DEPARTMENTAL_STORE Dataset:
| Column name | Column Description |
| :--- | :--- |
| UNIQUE_ID | Unique ID for each product |
| PRODUCT_NAME | Name of the product |
| COMPANY | Name of the Company |
| PRODUCT_TYPE | Type of the Product |
| PRODUCT_CATEGORY | Category of the product |
| COST_PRICE | The actual price of the product |
| SELLING_PRICE | The price at which a product is sold |
| QUANTITY_DEMANDED | Total quantity of product sold|


### RATINGS Dataset:
| Column name | Column Description |
| :--- | :--- |
| UNIQUE_ID | Unique ID for each product |
| PRODUCT_NAME | Name of the product |
| COMPANY | Name of the Company |
| PRODUCT_TYPE | Type of the Product |
| PRODUCT_CATEGORY | Category of the product |
| COST_PRICE | The actual price of the product |
| SELLING_PRICE | The price at which a product is sold |
| QUANTITY_DEMANDED | Total quantity of product sold|
| RATINGS | Ratings of each product from 1-5 |

## Data Cleaning:

Before feeding the data into RStudio, a quick cleanup is performed in Excel. The following steps are taken to clean and organise the data:

* Created a backup copy of the original data in a separate workbook.

* Ensured that the data is in a tabular format of rows and columns with: similar data in each column, all columns and rows visible, and no blank rows within the range.

* fixed mispelling and typos with the help of Spellcheck and  the Find and Replace dialog box.
  
   <img width=""800"" align='center' src=""https://github.com/Arpita-deb/Departmental_store_analysis_in_R/assets/139372731/9b2eacae-a2cb-491b-9873-f7b1864d3b67.png"" />

* Removed duplicates using Remove Duplicate.

* Changed the uppercases of the header row using LOWER Function.

    <img width=""800"" align='center' src=""https://github.com/Arpita-deb/Departmental_store_analysis_in_R/assets/139372731/59032464-90c9-45a3-82f0-9110dd81e582.png""/>

* Used TRIM function to remove the leading or trailing spaces from the texts.

* Removed the Company name from Product_name using the combination of RIGHT, LEN and SEARCH function, and replaced original column with the new one.

    <img width=""800"" align='center' src=""https://github.com/Arpita-deb/Departmental_store_analysis_in_R/assets/139372731/3ce269c4-5edc-495c-a0e6-07dc894609aa.png""/>
  
* Changed the datatype of numerical columns using Text-to-column option.

* Renamed the file as store_csv and saved the file as a Comma Seperated Value (CSV) file.

The same sata cleaning steps were performed for RATINGS dataset, and saved it as rating_csv file.

## Analysis:

### Show the summary statistics for net profit.

<img width='800' align='center' src=""https://github.com/Arpita-deb/Departmental_store_analysis_in_R/assets/139372731/ce42b41f-bafa-46ab-b1ca-aec09ed3a4a0""/>
   
The summary statistics gives a quick view of the important statistics such as average, sum, minimum, maximum, median, variance and standard deviation of the metric net-profit.

<img width='800' align='center' src=""https://github.com/Arpita-deb/Departmental_store_analysis_in_R/assets/139372731/150bdf51-2819-4129-8472-3d4058266f5c""/>
</p><br>

### Find out the profit for each product type.

<img width='800' align='center' src=""https://github.com/Arpita-deb/Departmental_store_analysis_in_R/assets/139372731/1d636d35-89a0-4a6b-9236-b838157490a1""/>

In order to find net profit for each product type, first the data is grouped by product type and then the summary statistics are calculated using summarise function. Finally the result is sorted in descending order from highest to lowest profitable product type.

<img width='800' align='center' src=""https://github.com/Arpita-deb/Departmental_store_analysis_in_R/assets/139372731/12eeac61-7518-4440-b0ee-002e8f5950d4""/>

The bar chart visually represents the above list.

![Rplot02](https://github.com/Arpita-deb/Departmental_store_analysis_in_R/assets/139372731/f88735f3-8e34-4c0c-9da8-368eabcb9cda)

It seems Hygienic product type were most profitable, given the pandemic situation of 2020. A further breaking down of it shows us which hygienic product were most profitable.

![hygiene](https://github.com/Arpita-deb/Do_I_read_all_the_books_I_buy/assets/139372731/7e7281d8-bd34-443f-8968-0319862b9c6b)
</p><br>

### Find out the profit for each product category.

<img width='800' align='center' src=""https://github.com/Arpita-deb/Departmental_store_analysis_in_R/assets/139372731/93e8d0cb-c496-4fa0-b376-829f8dc27c8b""/>

In order to find net profit for each product category, first the data is grouped by product category and then the summary statistics are calculated using summarise function. Finally the result is sorted in descending order from highest to lowest profitable product category.

<img width='800' align='center' src=""https://github.com/Arpita-deb/Departmental_store_analysis_in_R/assets/139372731/015d61dc-af7a-4424-a77d-65862d5bcfa6""/>

The bar chart visually represents the above list.

![Rplot03](https://github.com/Arpita-deb/Departmental_store_analysis_in_R/assets/139372731/0109b9c3-7827-46b3-b6c6-0d6e1f77f58e)
</p><br>


### Which products were most demanded?

<img width='800' align='center' src=""https://github.com/Arpita-deb/Departmental_store_analysis_in_R/assets/139372731/9d033db5-42aa-4549-9bca-98dc59ad5c69""/>

To find out the most demanded product, first the data is grouped by product type and then aggregated using sum function. This gives us the total quantity demanded for each product type. Finally the result is sorted in descending order of quantity demanded.

<img width='800' align='center' src=""https://github.com/Arpita-deb/Departmental_store_analysis_in_R/assets/139372731/4dde7b06-4e0b-4df0-8329-525c1323ce09""/>

The bar chart visually represents the above list. 

![pdt Vs total quan](https://github.com/Arpita-deb/Departmental_store_analysis_in_R/assets/139372731/3158fc30-c6a2-4de8-aa00-821164cd7178)
</p><br>

## Show the relation between Cost price and Selling price.

![cp Vs sp](https://github.com/Arpita-deb/Departmental_store_analysis_in_R/assets/139372731/13d1a440-898d-432c-9599-e6ca610a2a5e)

The relation between cost price and selling is linear and with a positive slope. It means that selling price increases with increase in cost price. There are two data points which have values much different from the rest of the datapoints. Let's look into it.

<img width='800' align='center' src=""https://github.com/Arpita-deb/Departmental_store_analysis_in_R/assets/139372731/90f1c4e6-3c98-4ec4-ae13-90afddddfd23"">

The two products were Pro-Active Health Drink which have a much greater selling price than their cost price. Consequently they have the greatest profit.

<img width='800' align='center' src=""https://github.com/Arpita-deb/Departmental_store_analysis_in_R/assets/139372731/9b173a31-6a19-4187-9ff5-c07672ea63da"">
</p><br>
  
## Which company had the highest net profit?

![Rplot06](https://github.com/Arpita-deb/Departmental_store_analysis_in_R/assets/139372731/d7f2baa3-6efd-4a4d-b563-e8a6550dd81c)

Company S had the highest profit, and from the stacks we can see that they sold all types of products. On the other hand company M sold only hygienic products and organic foods.
</p><br>

### Is there any correlation between quantity demanded and profit?

![Rplot](https://github.com/Arpita-deb/Departmental_store_analysis_in_R/assets/139372731/cfa50a9e-ec5d-4a7b-b529-109da5a645a3)

There is a visible relation between quantity demanded and profit. Profit decreases steeply as quantity of the products increases. Only the hygienic and organic products seem to have both high profit and high demand.
</p><br>

### What is the relation between Profit and ratings?

![Rplot01](https://github.com/Arpita-deb/Departmental_store_analysis_in_R/assets/139372731/01701f7c-19a6-47ff-bc84-c795790fce3d)

There's a visible negative correlation between profit and ratings. Ratings seem to increase as profit decreases.
</p><br>

### What is the relation between ratings and quantity demanded, selling price and cost price ?

![Rplot07](https://github.com/Arpita-deb/Departmental_store_analysis_in_R/assets/139372731/40a8c4c9-e7c8-4056-acc5-de8509dfa042)

Ratings have positive correlation with quantity demanded as can be seen from the correlation chart.

![Rplot08](https://github.com/Arpita-deb/Departmental_store_analysis_in_R/assets/139372731/aa2d397e-5fbe-4189-855c-815f629b35a5)

![Rplot09](https://github.com/Arpita-deb/Departmental_store_analysis_in_R/assets/139372731/da00281a-31e7-4e4b-9353-7fdee5182c92)

Ratings have negative correlation with both selling price and cost price as can be seen from the correlation charts.
</p><br>

### Show the correlation matrices.

A correlation matrix is a statistical technique used to evaluate the relationship between two variables in a data set. The matrix is a table in which every cell contains a correlation coefficient, where 1 is considered a strong relationship between variables, 0 a neutral relationship and -1 a not strong relationship. 

![Rplot05](https://github.com/Arpita-deb/Departmental_store_analysis_in_R/assets/139372731/993f76c8-2674-41d1-a579-d8d7df2c25d6)

![Rplot04](https://github.com/Arpita-deb/Departmental_store_analysis_in_R/assets/139372731/65b80a19-adf2-4d37-9e20-13ed493225eb)
</p><br>

As can be clearly seen from the correlation matrices, cost price and selling price are highly correlated. They are also positively correlated with profit and net profit. Net profit is positively but weakly correlated with selling price, cost price, quantity demanded, profit and profit percentage. On the other hand, quantity demanded is negatively correlated with selling price, cost price, profit and profit percentage.  

## Conclusion: 

* The Top 3 most profitable product type for 2020 were:
    * Hygiene
    * Packed Food
    * Beauty products
* The Top 3 most profitable product category for 2020 were:
    * Dry Fruits
    * Sanitizer
    * Body care
* The Top 3 most demanded products were:
    * Foodgrains and spices
    * Packed Food
    * Hygiene
  
* As can be seen from the above lists, **Hygienic products** and **basic necessities (Packed food, foodgrains and spices, body care etc)** were most profitable and most demanded. It is in accord with the pandemic scenario where people were more health conscious and they spent more of their money basic necessities rather than expensive products.

* Keeping in view of public need, some companies like I, J and M turned their business in capitalizing on hygiene products. 

* The most popular hygiene products in the year 2020 were Sanitizer, Handwash, Disinfectant and Mask. Interestingly as their demand increased their selling price decreased drastically.

* From the correlation plots we’ve found out that-
   * Ratings and quantity demanded has a positive correlation i.e. ratings increase as quantity of a product is increased.

   * Ratings and selling price have a negative correlation i.e. ratings decrease as selling price increases.

   * Ratings and cost price have a negative correlation i.e. ratings decrease as selling price increases.

* From the correlation matrices we found out the strength of correlations between various numerical variables. Cost price, selling price and profit are highly correlated. As quantity demanded increases the cost price, selling price and profit decrease, showing negative correlation between these variables. 


From the above conclusion we can say that during the pandemic year, the companies which were focused on providing healthcare and basic necessities of life were most profitable. As these products were in constant demand, they needed to lower their costs. On the other hand, the demand for luxury products were low those days, and as a result they were costlier than any other time.

## List of References:

* [Pipel Operator (%>%)](https://www.analyticssteps.com/blogs/using-pipe-operator-simplify-your-code-r-programming)
* [tidyverse](https://en.wikipedia.org/wiki/Tidyverse)
* [ggplot](https://ggplot2.tidyverse.org/reference/ggplot.html)
* [Correlation](https://www.analyticsvidhya.com/blog/2021/04/intuition-behind-correlation-definition-and-its-types/)
* [Correlation matrix](https://builtin.com/data-science/correlation-matrix)
",0,0,1,0,retail,"[correlation, covid-19, data-cleaning, data-manipulation, data-visualization, excel, exploratory-data-analysis, r-programming-language, retail]",44-45
ajaysaini-sgvu,ICICI-Appathon,,https://github.com/ajaysaini-sgvu/ICICI-Appathon,https://api.github.com/repos/ICICI-Appathon/ajaysaini-sgvu,ICICI Appathon 2017 - POC,"### ICICI-Appathon

Scan, Pay and Go - is a mobile app for customers which is likely to be used at retail stores. It is helping customers at retailers stores to avoid queue during check out. It aims is to make simple operations, shorter queues.

In store, customers are most likely to use their mobile internet to compare prices, look for discount & reviews. 60% use internet/apps on their mobile phone in a retail store.

### Features

A) Simplify check out process to avoid long queues. 

B) Compare prices and look product reviews.

C) Offering best deals to customers, cashback on check out from mobile app.

The majority would be happy for retailers to send them a text/e-mail with promotional offers whilst they’re in-  store using the free Wi-Fi service, which opens up new communication methods for retailers.

### How it works

Customer have to install application on their android devices. Now go to store for shopping, where you have to scan barcode which is placed on each items at retail stores. Once it is successfull then it will show product description, reviews and compare prices. Now simply proceed and add item in your cart and pay with wallets and different kind of payment methods through mobile app then go.

### Technology Stack

Tech Stack & Architecture Design : Java, Android, Google Mobile Vision, Firebase, MVVM, Dagger2, Retrofit

ICICI Bank API : Authentication, Bill Payment

## Licence

    Copyright 2017 Ajay Saini (TGMCians)
    
    Licensed under the Apache License, Version 2.0 (the ""License"");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at
    
        http://www.apache.org/licenses/LICENSE-2.0
    
    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an ""AS IS"" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.
",0,0,1,0,retail,"[banking, hackathon, icici, payment, retail, retailers, store]",44-45
Vieper1,SEM-3-Retail-Stock-Manager-DBMS-project,,https://github.com/Vieper1/SEM-3-Retail-Stock-Manager-DBMS-project,https://api.github.com/repos/SEM-3-Retail-Stock-Manager-DBMS-project/Vieper1,"An .NET Windows application to run a computer hardware store. This project was created as a part of my Undergrad's 2nd Semester Mini-Project for the subject, DBMS (Database Management Systems)","# SEM-3-Retail-Stock-Manager-DBMS-project
An .NET Windows application to run a computer hardware store. This project was created as a part of my Undergrad's 2nd Semester Mini-Project for the subject, DBMS (Database Management Systems)

### The packaged setup for the application is included here
https://github.com/Vieper1/SEM-3-Retail-Stock-Manager-DBMS-project/tree/master/DBMS%20Project%20Setup
<br><br>

## Download the project report for some overview of the WinApp and some cool screenshots
https://github.com/Vieper1/SEM-3-Retail-Stock-Manager-DBMS-project/blob/master/DBMS%20Project%20VN_34.docx
",0,0,1,0,retail,"[application, database, dbms, dotnet, manager, retail, stock, store, windows]",44-45
rajivgiri513,Desktop-POS-APP,,https://github.com/rajivgiri513/Desktop-POS-APP,https://api.github.com/repos/Desktop-POS-APP/rajivgiri513,Java Swing GUI Desktop Application./*Work in Progress*/,"# Desktop POS-APP
<br>
<--
<br>
Desktop pos application created with Java 8, PGAdmin DB, Swing GUI, Jframe (no Servers needed).
<br>
Download the viewMyDesktopApp.jar file to view this application.
<br>
---------------------------------------------------------------# WIP-------------------------------------------------


# Login Panel
![login-panel](https://user-images.githubusercontent.com/28536965/51454791-95b92100-1d14-11e9-877e-ccfcbf07cbcf.JPG)
# Registration Panel
![registration-panel](https://user-images.githubusercontent.com/28536965/51454792-95b92100-1d14-11e9-9576-dcbc1fd81a03.JPG)
# Vegitables Panel
![vegitables-panel](https://user-images.githubusercontent.com/28536965/51454783-95208a80-1d14-11e9-8638-2118d0617be6.JPG)
# Wines Panel
![wines-panel](https://user-images.githubusercontent.com/28536965/51454784-95208a80-1d14-11e9-8bca-47abdf94bdf2.JPG)
# Beer Panel
![beer-panel](https://user-images.githubusercontent.com/28536965/51454785-95208a80-1d14-11e9-9899-1d76dc5843ea.JPG)
# Clothing Panel
![clothing-panel](https://user-images.githubusercontent.com/28536965/51454786-95208a80-1d14-11e9-94e3-8502b52714a4.JPG)
# Fruits Panel
![fruits-panel](https://user-images.githubusercontent.com/28536965/51454787-95208a80-1d14-11e9-824f-9f614fe1c776.JPG)
# Sale Panel
![items-on-sale-panel](https://user-images.githubusercontent.com/28536965/51454788-95208a80-1d14-11e9-9e7c-a8a7389c82bc.JPG)
# Jewellery Panel
![jewellery-panel](https://user-images.githubusercontent.com/28536965/51454789-95208a80-1d14-11e9-9738-430d2064028a.JPG)
# Shoes Panel
![shoes-panel](https://user-images.githubusercontent.com/28536965/51454793-95b92100-1d14-11e9-953b-ceec02124a12.JPG)
# Soft Drinks Panel
![soft-drinks-panel](https://user-images.githubusercontent.com/28536965/51454795-95b92100-1d14-11e9-9a24-029c732856e2.JPG)

      
",0,0,1,0,retail,"[database, desktopapp, java, jframe, pos, retail, runnablejar, swing]",44-45
rymesaint,retas,,https://github.com/rymesaint/retas,https://api.github.com/repos/retas/rymesaint,"ReTas is an application for food retail which is easy to use. And for those who will having multiple branch, the owner can setup the menu separately from other branch and automatically setting up the price using default price on the menu or just multiplied by the branch tax that u defined it.","# Retail Tasik Application (ReTas)
RetAs is an application for food retail which is easy to use. And for those who will having multiple branch, the owner can setup the menu separately from other branch and automatically setting up the price using default price on the menu or just multiplied by the branch tax that u defined it.

# Module Feature
- Login Authentication Using Role (owner, cashier)
- Manage Branch
- Manage Menu
- Manage Branch Menu
- Manage Food Category
- Manage Order ( In Development )
- Manage Report ( In Development )
- Manage Users ( In Development )
- Manage Application ( In Development )
- ReTas Mobile Application ( In Development)

# Modular Feature
Because this application is a modular system, you can set the module that you just only need without having to much trouble. 
This modular system is based on package `nWidart/laravel-modules`. You can disable or enable the module with reading at the [documentation](https://nwidart.com/laravel-modules/v4/introduction).

# How To Use Application
- You'll need to login as owner first default email `rymetutor@gmail.com` and password `123456`
- After logging in, you'll be redirected to dashboard. Then in dashboard go to page Manage Branch to setup your first branch.
- If you have succeeded to create new branch, then you can add your menu for default pricing and default menu for every branch you've created later.
- Then after you add a pricing on the menu or creating the menus, then you'll need to add them to the selected branch.
- At `Manage Branch Menu`, you can use default price or you can use tax price which you set first at `Manage Branch` or you can setup manually.

* Category is an optional for filtering program later, so the cashier can get the customer request menu much faster.
",0,0,1,19,retail,"[laravel, laravel-framework, laravel-modular, retail]",44-45
SamuelSousaFerreira,Breakfast-at-the-Frat-Time-Series-Analysis,,https://github.com/SamuelSousaFerreira/Breakfast-at-the-Frat-Time-Series-Analysis,https://api.github.com/repos/Breakfast-at-the-Frat-Time-Series-Analysis/SamuelSousaFerreira,,,0,0,1,0,retail,"[groupby, matplotlib, retail, seaborn-plots, time-series, varejo]",44-45
easonlai,customer_segmentation_with_rfm_k-means,,https://github.com/easonlai/customer_segmentation_with_rfm_k-means,https://api.github.com/repos/customer_segmentation_with_rfm_k-means/easonlai,This is a code sample repository to leverage the famous Online Retail dataset by UCI Machine Learning Library to perform Customer Segmentation with RFM Modelling and perform clustering by K-Means cluster algorithm. ,"﻿# Customer Segmentation with RFM Modelling & K-Means Clustering

This is a code sample repository to leverage the famous [Online Retail dataset by UCI Machine Learning Library](https://archive.ics.uci.edu/ml/datasets/Online+Retail) to perform Customer Segmentation with RFM Modelling and perform clustering by K-Means cluster algorithm. The [Online Retail dataset](https://archive.ics.uci.edu/ml/datasets/Online+Retail) is a transnational data set that contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail.

* /data/Online_Retail.csv <-- Online Retail dataset.
* customer_segmentation_with_rfm_k-means_R2.ipynb <-- (Latest) RFM Modelling & K-Means Clustering by 5 Clusters.
* customer_segmentation_with_rfm_k-means_R1.ipynb <-- RFM Modelling & K-Means Clustering by 3 Clusters.

Enjoy.
",0,0,1,0,retail,"[customer-segmentation, k-means, k-means-clustering, k-means-implementation-in-python, python, python3, retail, rfm, rfm-analysis]",44-45
sudhirjagtap17,Top-Learnings,,https://github.com/sudhirjagtap17/Top-Learnings,https://api.github.com/repos/Top-Learnings/sudhirjagtap17,Learnings of Data Science,,0,0,1,0,retail,"[activations, coverage-testing, retail, retails]",44-45
ejpallippurath,sales-data-analysis,,https://github.com/ejpallippurath/sales-data-analysis,https://api.github.com/repos/sales-data-analysis/ejpallippurath,"Analyzing and visualizing sales data to gain insights into customer behavior, product performance, and sales trends. We will be using Python and its data analysis libraries to clean, explore, and visualize the data. The main goal of the project is to provide insights into the followed questions.","## Sales Data Analysis and Visualization

### Overview
In this project, we will be analyzing and visualizing sales data to gain insights into customer behavior, product performance, and sales trends. We will be using Python and its data analysis libraries to clean, explore, and visualize the data. The main goal of the project is to provide insights into the following questions:

  1. Which location has highest and lowest sales? Represent the sales on a barchart, also show the market share for each location using a pie chart.
  2. Which locations has more female customers and male customers?
  3. what days of the month make more sales?
  4. Which branch has more Members vs less Members?
  5. Which branch has highest and lowest rating?
  6. which city has more female shopping?
  7. Who spends more, male or female?
  8. Which type of customer spends more, member or non-member?
  9. Which product line sells more?
  10. Which product line is popular among men and women?
  11. Which month makes more sales?
    
### Data
The dataset used in this project is a sales dataset that includes information on products, customers and sales. It contains the following columns:
  * Invoice ID 	
  * Date
  * Time
  * Gender
  * Location
  * City
  * Member
  * Category
  * Price
  * Quantity
  * Total
  * Payment
  * Rating
",0,0,2,0,retail,"[data-analysis, data-science, data-visualization, jupyter-notebook, matplotlib, pandas, python, retail]",44-45
sandeep540,costing-app,,https://github.com/sandeep540/costing-app,https://api.github.com/repos/costing-app/sandeep540,costing app based on static data,"[![Netlify Status](https://api.netlify.com/api/v1/badges/d270f2b8-4811-454b-be20-d821284a7570/deploy-status)](https://app.netlify.com/sites/stoic-brahmagupta-f5645d/deploys)

# Test Server withh static data
`https://stoic-brahmagupta-f5645d.netlify.com/`

# CostingApp

This project was generated with [Angular CLI](https://github.com/angular/angular-cli) version 7.0.3.

## Development server

Run `ng serve` for a dev server. Navigate to `http://localhost:4200/`. The app will automatically reload if you change any of the source files.

## Code scaffolding

Run `ng generate component component-name` to generate a new component. You can also use `ng generate directive|pipe|service|class|guard|interface|enum|module`.

## Build

Run `ng build` to build the project. The build artifacts will be stored in the `dist/` directory. Use the `--prod` flag for a production build.

## Running unit tests

Run `ng test` to execute the unit tests via [Karma](https://karma-runner.github.io).

## Running end-to-end tests

Run `ng e2e` to execute the end-to-end tests via [Protractor](http://www.protractortest.org/).

## Further help

To get more help on the Angular CLI use `ng help` or go check out the [Angular CLI README](https://github.com/angular/angular-cli/blob/master/README.md).
",0,0,2,28,retail,"[angular, flex, retail, rfa]",44-45
smburdick,modular,,https://github.com/smburdick/modular,https://api.github.com/repos/modular/smburdick,An online 3D modeling and printing service,"# ![alt](logo/modular_logo.png)
## Contributors
- Gabriel Pinkard
- Ian White
- Presley Reed III
- Sam Burdick

### About Modular
Modular is an application that allows users to order 3D printed models on demand as well as create/upload and share their own models. Users will be able to log into individual accounts with their own profile information and list of 3D models they have created and/or purchased. Users will also be able to browse the site for models that other users have created/uploaded. Along with the ability to upload models from third parties, users will be able to, in browser, create their own models that are saved and viewable by other users. Likewise users will be able to edit other people’s models and save the new modified models to their profile. Our database will be used not only to store basic user information such as name, userid, age, bio, purchasing history, created models, hashed password, and contact info/social media profiles. The database will also hold all of the model data including price, material, number of purchases, dimensions, category, and creator/editors. 

### Gantt Chart

![alt](proposal/Modular.png)",0,0,4,4,retail,"[3d-models, retail]",44-45
heatherannelynn,AppFactory-B2C,,https://github.com/heatherannelynn/AppFactory-B2C,https://api.github.com/repos/AppFactory-B2C/heatherannelynn,"Basic Consulting, Contractor Template with Carousel, Bootstrap jQuery",,0,0,2,0,retail,"[local, portfolio, retail]",44-45
dosrd,coinbasewrapperapi,,https://github.com/dosrd/coinbasewrapperapi,https://api.github.com/repos/coinbasewrapperapi/dosrd,A wrapper for coinbase api for trading and wallet integration,"# Coinbase Wrapper API
##A wrapper for coinbase api for trading and point of sale integration
   
  ```
 CoinbaseCredentials cc = new CoinbaseCredentials(SECRET, API_KEY);
 CoinbaseCallWrappers ccc = new CoinbaseCallWrappers(cc);
 System.out.println(ccc.createBitcoinAddress(""Test Account""));
",0,0,1,0,retail,"[cryptocurrency, money, retail, wallets]",44-45
Ekas-118,TimCoRetailManager,,https://github.com/Ekas-118/TimCoRetailManager,https://api.github.com/repos/TimCoRetailManager/Ekas-118,A retail management system built using .NET,"# TimCo Retail Manager
A retail management system built using .NET.

Created by following the [tutorial series by IAmTimCorey on YouTube](https://www.youtube.com/playlist?list=PLLWMQd6PeGY0bEMxObA6dtYXuJOGfxSPx).
",0,0,1,0,retail,"[aspnetcore, blazor-webassembly, retail, sqlserver, wpf]",44-45
dinachoir,Churn-Report,,https://github.com/dinachoir/Churn-Report,https://api.github.com/repos/Churn-Report/dinachoir,Calculates the total number of customers that have churned during a selected time period.,"# Churn-Report
Calculates the total number of customers that have churned during a selected time period.
",0,0,1,0,retail,"[churn, retail]",44-45
fractaldatalearning,instacart_recommendations,,https://github.com/fractaldatalearning/instacart_recommendations,https://api.github.com/repos/instacart_recommendations/fractaldatalearning,Katin’s Instacart Recommendation System: Which items will each user reorder when they log in next?,"# Instacart Recommendations 

![shopping cart filling automatically](https://upload.wikimedia.org/wikipedia/commons/1/1c/NutEdVolunteerMaggie.jpg)

My goal in this project is to determine which items instacart should recommend to each user next time they log into their account (based on predictions of what they'll reorder from items purchased in the past). 

Dataset originated here: https://www.kaggle.com/competitions/instacart-market-basket-analysis/overview

My [summative project report is available here](https://github.com/fractaldatalearning/instacart_recommendations/blob/main/reports/katin_instacart_report.pdf), or read a summary below to understand what can be found in which notebook:

1. In the [wrangling notebook](https://github.com/fractaldatalearning/instacart_recommendations/blob/main/notebooks/1-kl-wrangling.ipynb), I load data and merge data about products with that about users and their orders. 

2. 💜 **Exploratory data analysis** begins in [notebook 2](https://github.com/fractaldatalearning/instacart_recommendations/blob/main/notebooks/2-kl-eda-w-data-direct-from-wrangling.ipynb) but continues all the way through notebook 6, as more insights about the data become possible after some features are engineered. Specifically, in notebooks [3](https://github.com/fractaldatalearning/instacart_recommendations/blob/main/notebooks/3-kl-eda-w-single-user.ipynb) and [4](https://github.com/fractaldatalearning/instacart_recommendations/blob/main/notebooks/4-kl-eda-modeling-w-single-user.ipynb), I explore in-depth one single user's purchasing and re-ordering habits in order to deeply understand factors that might support accurate predictions of items they will reorder in the future. 

3. Notebooks [5.1](https://github.com/fractaldatalearning/instacart_recommendations/blob/main/notebooks/5.1-kl-preprocess-select-users-add-rows.ipynb) and [5.2](https://github.com/fractaldatalearning/instacart_recommendations/blob/main/notebooks/5.2-kl-preprocess-get-usable-data.ipynb) are dedicated to the most crucial step of 💜 **adding non-ordered items** to each order by each user. This spans two notebooks only because the size of the dataset was slowing down my work in the first. This is such a crucial step because the raw data came with only information about items a user actually purchased in each order. In order to make predictions about what a user will re-order in their 100th order, for example, we need to know, from their 99th order, which items they reordered AND *did not* reorder from among all items they ever ordered. This required creating new rows such that the size of each order grew cumulatively over time for each user, and this resulted in a dataframe with approxiimately 15 times as many rows as the raw set.  

4. I had the most fun with [notebook 6](https://github.com/fractaldatalearning/instacart_recommendations/blob/main/notebooks/6-kl-preprocess-feature-engineer.ipynb), where I 💜 **engineered several new features** For example, for each item in each order, I added a column to indicate the 💜 **percentage of past orders where that item had been purchased**. Other new features included ""percent of past orders where this product was one of the first 6 items placed in this user's cart"" and product kewords such as ""organic"" or ""fresh."" These improve modeling because ""percent of past orders where this item was purchased,"" for example, is highly correlated with whether an item will be reordered again in the future, but until the feature was engineered, a model would have had no way to pcik up on this valuable variable. See visualizations of the top three most predictive features:

![Reorders by Past Order Rate](https://github.com/fractaldatalearning/instacart_recommendations/blob/main/reports/figures/fig3.png)
![Reorders by Past Cart Placement](https://github.com/fractaldatalearning/instacart_recommendations/blob/main/reports/figures/fig4.png)
![Reorders by Days Since Prior Order](https://github.com/fractaldatalearning/instacart_recommendations/blob/main/reports/figures/fig5.png)

5. In [notebook 7](https://github.com/fractaldatalearning/instacart_recommendations/blob/main/notebooks/7-kl-preprocess-encoding.ipynb), I tried multiple strategies for encoding categorical data and chose a 💜 **Target Encoder** with default hyperparameters in order to encode categorical features such as the user id, product name, and aisle and department in which each product is classified. 

6. Finally, in [notebook 8](https://github.com/fractaldatalearning/instacart_recommendations/blob/main/notebooks/8-kl-modeling.ipynb), I use a random grid search with cross-validation to test multiple classifiers. I selected the 💜 **Random Forest Classifier** because it performed best overall in a combined evaluation using log loss, roc_auc, and f1 metrics. I tuned the hyperparameters and finalized the model. 

# Evaluation:
💜 Log Loss: 2.55, vs. 5.68 with naive model 

💜 ROC_AUC: 0.71 

💜 F1: 0.4 vs. 0.09 with naive model 

# 

<p><small>Project based on the <a target=""_blank"" href=""https://drivendata.github.io/cookiecutter-data-science/"">cookiecutter data science project template</a>. #cookiecutterdatascience</small></p>
",0,0,1,0,retail,"[data-science, machine-learning, prediction-model, python, recommendation-system, retail]",44-45
simisola-ade,Data-Analytics-Project-,,https://github.com/simisola-ade/Data-Analytics-Project-,https://api.github.com/repos/Data-Analytics-Project-/simisola-ade,"This project showcases Excel, SQL and Data Visualization skills to make business strategy recommendations in Retail","# Data-Analytics-Project-
This project showcases Excel, SQL and Data Visualization skills to make business strategy recommendations in Retail
",0,0,1,0,retail,"[analytics, data-visualization, powerbi, retail, sql, visualization]",44-45
jpradas1,Crawling_Engineer_Retail,,https://github.com/jpradas1/Crawling_Engineer_Retail,https://api.github.com/repos/Crawling_Engineer_Retail/jpradas1,This repository extracts data from the clothing website Puma using the python framework Scrapy.,"# Crawling Engineer Challenge

This repository extract data from the clothing website [Puma](https://eu.puma.com/) using the python framework Scrapy. This scraper has one spiders, and extracts the following data, i.e.:
- Product id
- Product title
- Product brand
- Product description.
- Product current price
- Product original price
- Product availability
- A list of all the image URLs
- Product URL
- All available colors for the product
- All available sizes for the product
- Category paths leading to the product (e.g. Women > Footwear > Running)

## Running the Code
To run this scraper properly, follow these steps.
### Virtual Environment
First we need a virtual environment to display this project. We could use conda or the python module venv. We use the last one.
```
python -m venv venv
```
To active this environment
- Linux case:
```
source venv/bin/activate
```
- Windows case:
```
venv\Scripts\activate
```
### Installing Libraries
```
pip install scrapy pymongo
```
### Ride Spiders
Before to run any spider we need a MongoDB server in order to store our data. This server could be local or on cloud. To create a local one, use docker.
```
sudo docker pull mongo
sudo docker run -d -p 27017:27017 --name mongodb mongo
```
Because Puma crawler needs the Mongo server, we run:
```
scrapy crawl -s MONGODB_URI=""mongodb://localhost:27017/"" -s MONGODB_DATABASE=""Products"" puma
```
And to ignore the log output
```
scrapy crawl -s MONGODB_URI=""mongodb://localhost:27017/"" -s MONGODB_DATABASE=""Products"" puma 2>/dev/null
```
## Dataset
The final extrated data is located in the compress file [products](https://github.com/jpradas1/Crawling_Engineer_Challenge/blob/main/products.tar.gz) which contains all data extracted from the website, storing 24544 items.
```
tar -xzvf products.tar.gz
```
",0,0,1,0,retail,"[python, retail, scraping, scrapy, spider, web-scraper]",44-45
akpumacy,GRAIL,,https://github.com/akpumacy/GRAIL,https://api.github.com/repos/GRAIL/akpumacy,GRAIL: Migration of Ethereum based smart contracts to XRPL,"# GRAIL Implementation/ Existing Architecture

GRAIL brings the existing Redimi solution to XRPL.

[Redimi](https://redimi.net) solution comprises of several modules where each module accomplishes a crucial task. Following is the list of these modules:

1. [Ethereum based smart contracts](/1_Solidity%20Smart%20Contracts/): solidity contracts to manage gift card functionalities
2. Mobile Wallet Application: allows customers to manage their gift cards, transactions and a lot more...
3. Platform Provider (PP) API: connects all modules together

For the purpose of demonstration, the gift cards can behave as a fungible tokens since they are partially redeemable and transferable. Therefore, a token can be created as shown in [Tokenomy](/2_XRPL%20Sample%20Scripts/1_Tokenomy/). For transfering of tokens, we have setup multiple wallets including a cold wallet. We have also added token transfer functionality to already available wallet management application as shown in [Wallet Management](/2_XRPL%20Sample%20Scripts/2_Account_Management/). This allows wallets to transfer tokens directly from the app.

## Ethereum <> XRPL Migration

The actual implementation of GRAIL use case on XRPL can be found under [GRAIL Use Case](/3_GRAIL%20Use%20Case/). The scripts (developed in JS) demonstrate the implementation plan of Redimi Gift Cards in form of Tokens. These scripts are integrated in the XRPL version of our PP API, developed using [STRAPI](https://strapi.io/), a headless content management system. The [*REST API documentation*](https://documenter.getpostman.com/view/12104204/2s93eVYZeV) for GRAIL implementation is also available.

Following sections describe further our existing solution and its sub-modules: 

## Mobile Wallet Application

The [mobile application](https://play.google.com/store/apps/details?id=com.redimigmbh.redimi) for customers, also known as Mobile Wallet allows customers to register/login and manage their existing gift cards, coupons and transactions. Furthermore, customers can also store 3rd party gift cards by scanning their QR/Barcodes and store receipts for budgeting. To improve user experience, customers can see their friends and family (already existing on Redimi) as contacts, making it easier to select recipients when transfering gift cards. When redeeming, customers can also choose the amount they wish to redeem from the gift card. A QR code is created with encrypted information regarding the gift card. This information also includes signature from the customer's wallet.

Retailers can simply scan this gift card which will redirect them to a redeem portal website.

<img src=""https://github.com/akpumacy/GRAIL/blob/main/media/login.jpg"" alt=""Login Page"" width=""30%"" height=""30%""> <img src=""https://github.com/akpumacy/GRAIL/blob/main/media/gift-cards.jpg"" alt=""Gift Card Management"" width=""30%"" height=""30%""> <img src=""https://github.com/akpumacy/GRAIL/blob/main/media/redeem-portal.jpg"" alt=""Redeem Portal"" width=""30%"" height=""30%"">

### Purchase Portal

To allow customers to purchase gift cards with easy and comfort, a [purchase portal](https://redimi.net/gift-cards) is setup on the Redimi's website where customers can choose a mall and amount of gift card they wish to purchase. Multiple payment methods are integrated at checkout for the ease of the customer.

## Payment Settlement using Monerium 

After customers redeem gift cards at several retailers, these amounts have to be summed up and transfered to the retailers' bank account. This is made possible by the help of Monerium's e-money solution. Using Monerium's API, the total amount calculated fir each retailer can be transfered from the Mall's Monerium account to the retailers' bank account automatically at regular predefined intervals.
",0,0,3,0,retail,"[android, blockchain, ethereum, retail, shopping, wallet, xrpl]",44-45
rraisanr915,RaisaNurima-Data-Analyst-Portfolio,,https://github.com/rraisanr915/RaisaNurima-Data-Analyst-Portfolio,https://api.github.com/repos/RaisaNurima-Data-Analyst-Portfolio/rraisanr915,"Analyze online retail data using SQL and Power BI. Uncover insights, trends, and recommendations for e-commerce success.","# Online Retail Analysis 

![](online-shopping-concept.jpg)
---
## Introduction
Welcome to my first project portfolio! In this portfolio, I am excited to present my work on a SQL and Power BI project focused on analyzing online retail data for a UK-based non-store online retailer. The dataset comprises transactions recorded between December 1, 2010, and December 9, 2011. The retailer specializes in selling unique gifts suitable for various occasions, with a significant portion of its customer base consisting of wholesalers.

**_Disclaimer:_**
_It is important to note that this report is solely intended to demonstrate the capabilities of SQL and Power BI for data analysis and does not represent any specific company or organization._

## Problem Statement
1. During the period from December 1, 2010, to December 9, 2011, how many transactions occurred, what was the total sales, how many distinct customers made purchases, and what was the average order value?
2. How did sales evolve over the period from December 1, 2010, to December 9, 2011? What are the notable trends and patterns in sales during this time?
3. Which country had the highest number of customers during this period?
4. Which country recorded the highest total sales during this period?
5. What is the top-selling item by quantity during the specified time frame?

## Dataset Source:
The dataset used for this project is in CSV format and can be accessed on Kaggle at the following link [here](https://www.kaggle.com/datasets/ulrikthygepedersen/online-retail-dataset). This is a table containing transaction records for a specific period and serves as the main data source for analysis.

## Methods Employed
For this project, I employed a combination of SQL and Power BI for data analysis:

- SQL was used for data cleaning, transformation, and aggregation.
- Power BI was utilized to create interactive and informative visualizations.

## Data Cleaning and Preprocessing
The dataset underwent comprehensive data cleaning and preprocessing steps, including:

- Addressing NULL values.
- Data type conversions.
- Deleting Irrelevant Rows
- Creat additional columns 
- Dealing with duplicate rows in the dataset.

For detailed information and SQL scripts used for data cleaning and preprocessing, please refer to this [SQL Scripts](Online_Retail.sql) .

## Visualization

![](Online_Retail_Dashboard.jpg)

## Analysis and Insights
### Transaction Summary
- __Number of Transactions__:In the period from December 1, 2010, to December 9, 2011, the dataset recorded a total of 19,959 successful unique transactions, illustrating the extent of customer engagement during this time.
- __Total Sales__:The total sales for the one-year period, spanning December 1, 2010, to December 9, 2011, amounted to $10.9 million, providing a comprehensive overview of annual revenue.
- __Customer__:Excluding cancellations, the analysis revealed 4,339 distinct customers who participated in transactions between December 1, 2010, and December 9, 2011, shedding light on the size and diversity of the customer base.
- __Average Order Value__:The average order value for this year was $533, offering insight into typical customer spending behavior over the entire year.

### Sales Trends
- __Total Sales by Month__:A monthly sales trend analysis revealed fluctuations throughout the year, with November 2011 as the highest-grossing month and February 2011 as the lowest. These trends can inform demand forecasting and inventory management strategies.
- __Customers by Country__:The analysis identified the United Kingdom as the country with the highest number of customers, highlighting a significant domestic customer base.
- __Total Sales by Country__:The United Kingdom emerged as the top contributor to total sales, underlining its significance within the market.
- __Top 5 Selling Products by Quantity__:The analysis pinpointed the best-selling products by quantity, with ""Paper Craft, Small Birdie"" leading the pack, followed by ""Medium Ceramic Top Storage Jar,"" ""World war 2 glider asstd design,"" ""Jumbo bag red retro spot,"" and ""White hanging heart t-light holder."" This knowledge can guide inventory management and marketing decisions.
- __Transaction Status(%)__:Transaction statuses were distributed with 98.27% of transactions being successful and 1.73% as cancellations, providing an overview of transaction outcomes and potential areas for improvement.
- __Transaction Trends by Day and Time__: Use the Heatmap table to analyze transaction data to identify patterns in customer behavior throughout the week and within a 24-hour period. The analysis shows that Thursdays are consistently the busiest days of the week for transactions, and noon is the highest transaction volume on a daily basis; this information highlights the specific day of the week and time of day when most transactions occur.
  
## Recommendations
1. Advertise more during popular shopping months like November.
2. Make sure popular items are available online.
3. Focus on UK customers with special deals.
4. Explore selling to other countries.
5. Have more workers available on Thursdays and around 12 noon.
6. Offer great customer service and loyalty programs.
7. Accept various online payment methods and currencies.
8. Find why people leave their online carts and make the process smoother.
9. Suggest related items to boost online order values.
10. Keep an eye on online sales trends for better decisions.

",0,0,1,0,retail,"[data, data-analysis, e-commerce, powerbi, retail, sql, visualization]",44-45
benamrou,GOLDEYC,,https://github.com/benamrou/GOLDEYC,https://api.github.com/repos/GOLDEYC/benamrou,GOLD EYC tool kit,"## GOLD EYC Tool Box

This repository regroups principal development made for EYC GOLD Solution Deployment.

### Interfaces

Most of the developments are Third-Party ERP interfaces implementations. The development structure is designed and defined, from ERP implementation to others mapping might differs.

### Reporting

Specific projects reporting through the GOLD Query Management solution or SQL extractions.

",0,0,1,0,retail,"[erp, eyc, gold, retail, supply, supply-chain]",44-45
samlaubscher,Boutique-Ado-Ecommerce,,https://github.com/samlaubscher/Boutique-Ado-Ecommerce,https://api.github.com/repos/Boutique-Ado-Ecommerce/samlaubscher,"A full stack Django ecommerce shopping web app, utilizing Django, Python, jQuery, SQL, Jinja2 & Bootstrap technologies, set up to handle payments with Stripe. (CI walkthrough)","<div align=""center"">

<img src=""media/boutique-ado.jpg"" alt=""Main Logo"" width=""400"">

# Boutique Ado Ecommerce Store

### Built using the Django python framework

###### Please note the products on this site are stock images.",0,0,1,4,retail,"[boutique, ecommerce, retail]",44-45
jpoland00,Elite-A,,https://github.com/jpoland00/Elite-A,https://api.github.com/repos/Elite-A/jpoland00,Coding for our API,"## Table of Content

* <a href=""https://asapmktg.com"">Emerging Marketing</a>
* <a href=""https://beefjerky.com"">Beef Jerky</a>
* <a href=""https://fascontent.com"">Float & Sting</a>
* <a href=""https://patridgetires.com"">Mobile Tire Alignment</a>
* <a href=""https://theplatform.shop"">TPS</a>
* <a href=""https://www.24hourlocksmith-texas.com/austin"">United Locksmith</a>
* <a href=""https://www.patridgetires.com/precision-mobile-alignment"">Precision Semitruck Alignments</a>
* <a href=""https://Naturalforcesmassage.com"">Sports Massages</a>
* <a href=""https://houstonvendingmachinerepair"">Houston Vending</a>
* <a href=""https://crt-services.com"">CRT Services</a>
* <a href=""https://crtsupply.com"">CRT Supply</a>
* <a href=""https://fullarmorgunrange.com"">FAGR</a>
* <a href=""https://hathornrepair.com"">HR</a>
* <a href=""https://hathornbuysgold.com"">HB Gold</a>


GCR
https://gcr-socal.com/windows/
https://gcr-socal.com/exterior-painting/
https://gcr-socal.com/windows/
https://gcr-socal.com/commercial-exterior-services/
https://gcr-socal.com/lifetime-paint/
https://gcr-socal.com/foundation-retrofitting/
https://gcr-socal.com/patio-covers/
https://gcr-socal.com/pavers-and-cement/
https://gcr-socal.com/solar-panel/


ASAP Mktg
https://asapmktg.com/services/search-engine-marketing/
https://asapmktg.com/services/audio-video-content-creation/
https://asapmktg.com/services/social-media-marketing/
https://asapmktg.com/services/email-marketing/
https://asapmktg.com/services/web-design/
https://asapmktg.com/services/content-marketing/
https://asapmktg.com/services/ecommerce-international-trade/
https://asapmktg.com/services/graphics-design/
https://asapmktg.com/services/lead-generation/



# Elite-A
Coding for our API project in conjunction with Energy Star the government program
<a href=""https://blog.eliteappliance.com"">Blog Arena</a>




















https://blog.eliteappliance.com/small-dishwashers/
https://blog.eliteappliance.com/texas-appliances
https://blog.eliteappliance.com/jenn-air-jx3-downdraft-cooktop-review/
https://www.eliteappliance.com/by-brand/zep/Zephyr.html
https://blog.eliteappliance.com/built-in-coffee-machine-5-benefits/
http://www.eliteappliance.com/Bertazzoni/Bertazzoni-Appliance.php?x=t
https://blog.eliteappliance.com/the-high-end-french-door-oven-trend/
https://blog.eliteappliance.com/capital-ranges-vs-culinarian-series/
https://blog.eliteappliance.com/comparing-double-wall-ovens-five-brands/
https://blog.eliteappliance.com/the-best-high-end-vintage-appliances
https://blog.eliteappliance.com/cooking-beer-can-chicken-big-green-egg/
http://www.eliteappliance.com/Samsung/RF265BEAES.php
	
https://blog.eliteappliance.com/capital-culinarian-range-review/	
https://blog.eliteappliance.com/sealed-vs-open-burners-range-burner-types-compared/	
https://blog.eliteappliance.com/the-best-high-end-appliances-for-vintage-lovers/	
https://blog.eliteappliance.com/bertazzoni-vs-verona-comparing-36-inch-dual-fuel-gas-and-electric-ranges/	
https://blog.eliteappliance.com/capital-ranges-precision-series-vs-culinarian-series/	
https://blog.eliteappliance.com/most-popular-colors-for-viking-ranges/	
https://blog.eliteappliance.com/u-line-wine-captain-review-best-in-home-wine-storage/	
https://blog.eliteappliance.com/what-is-the-difference-between-a-beverage-center-and-a-mini-fridge/	
https://blog.eliteappliance.com/an-overview-of-vikings-freestanding-tuscany-ranges/	
https://blog.eliteappliance.com/jenn-air-jx3-downdraft-cooktop-review/	
https://blog.eliteappliance.com/comparing-double-wall-ovens-reviewing-five-popular-high-end-brands/	
https://blog.eliteappliance.com/cooking-beer-can-chicken-on-a-big-green-egg/	
https://blog.eliteappliance.com/high-btu-gas-burners-in-pro-style-ranges/	
https://blog.eliteappliance.com/drawer-dishwasher-review/	
https://blog.eliteappliance.com/30-pro-ranges-compared-viking-vgic5304bss-and-american-range-arrob-430/	
https://blog.eliteappliance.com/electrolux-vs-electrolux-icon-appliances-whats-the-difference/	
https://blog.eliteappliance.com/tecnogas-superiore-rn361gpss-review-36-all-gas-next-series-range/	
https://blog.eliteappliance.com/designer-home-surplus/	
https://blog.eliteappliance.com/why-we-love-european-made-appliances/	
https://blog.eliteappliance.com/viking-rdfn536dss-review-d3-series-french-door-refrigerator-with-dual-drawers/	
https://blog.eliteappliance.com/viking-rdsce2305bss-review-d3-series-30-electric-range/	
https://blog.eliteappliance.com/the-best-high-end-appliances-for-vintage-lovers/	
http://www.eliteappliance.com/Samsung/RF265BEAES.php	
https://blog.eliteappliance.com/northland-48ss-ws-review-48-of-customizable-refrigeration-space/	
http://www.eliteappliance.com/KitchenAid/KDRS483VSS.php	
https://www.eliteappliance.com/products/samsung/rf260beaesr.html	
https://www.eliteappliance.com/products/KitchenAid/kta/kdte204gps.html	
https://blog.eliteappliance.com/introducing-bertazzonis-newest-built-in-and-freestanding-bottom-mount-refrigerators/	
https://blog.eliteappliance.com/best-induction-cooktops-for-home-chefs/	
	
		
https://blog.eliteappliance.com/capital-culinarian-range-review/	
https://blog.eliteappliance.com/sealed-vs-open-burners-range-burner-types-compared/	
https://blog.eliteappliance.com/the-best-high-end-appliances-for-vintage-lovers/	
https://blog.eliteappliance.com/bertazzoni-vs-verona-comparing-36-inch-dual-fuel-gas-and-electric-ranges/	
https://blog.eliteappliance.com/capital-ranges-precision-series-vs-culinarian-series/	
https://blog.eliteappliance.com/most-popular-colors-for-viking-ranges/	
https://blog.eliteappliance.com/u-line-wine-captain-review-best-in-home-wine-storage/	
https://blog.eliteappliance.com/what-is-the-difference-between-a-beverage-center-and-a-mini-fridge/	
https://blog.eliteappliance.com/an-overview-of-vikings-freestanding-tuscany-ranges/	
https://blog.eliteappliance.com/jenn-air-jx3-downdraft-cooktop-review/	
https://blog.eliteappliance.com/comparing-double-wall-ovens-reviewing-five-popular-high-end-brands/	
https://blog.eliteappliance.com/cooking-beer-can-chicken-on-a-big-green-egg/	
https://blog.eliteappliance.com/high-btu-gas-burners-in-pro-style-ranges/	
https://blog.eliteappliance.com/drawer-dishwasher-review/	
https://blog.eliteappliance.com/30-pro-ranges-compared-viking-vgic5304bss-and-american-range-arrob-430/	
https://blog.eliteappliance.com/electrolux-vs-electrolux-icon-appliances-whats-the-difference/	
https://blog.eliteappliance.com/tecnogas-superiore-rn361gpss-review-36-all-gas-next-series-range/	
https://blog.eliteappliance.com/designer-home-surplus/	
https://blog.eliteappliance.com/why-we-love-european-made-appliances/	
https://blog.eliteappliance.com/viking-rdfn536dss-review-d3-series-french-door-refrigerator-with-dual-drawers/	
https://blog.eliteappliance.com/viking-rdsce2305bss-review-d3-series-30-electric-range/	
https://blog.eliteappliance.com/the-best-high-end-appliances-for-vintage-lovers/	
http://www.eliteappliance.com/Samsung/RF265BEAES.php	
https://blog.eliteappliance.com/northland-48ss-ws-review-48-of-customizable-refrigeration-space/	
http://www.eliteappliance.com/KitchenAid/KDRS483VSS.php	
https://www.eliteappliance.com/products/samsung/rf260beaesr.html	
https://www.eliteappliance.com/products/KitchenAid/kta/kdte204gps.html	
https://blog.eliteappliance.com/introducing-bertazzonis-newest-built-in-and-freestanding-bottom-mount-refrigerators/	
https://blog.eliteappliance.com/best-induction-cooktops-for-home-chefs/	
URL
https://www.eliteappliance.com/
https://www.eliteappliance.com/by-brand/vik/Viking.html
http://www.eliteappliance.com/Bertazzoni/Bertazzoni-Appliance.php
https://www.eliteappliance.com/landing_all_brands.html
http://www.eliteappliance.com/Northstar/Northstar-Appliance.php?x=t
https://www.eliteappliance.com/locations/ELITE.html
https://www.eliteappliance.com/by-brand/jen/JennAir.html
http://www.eliteappliance.com/Capital/CGRT484BB.php
https://www.eliteappliance.com/by-brand/tecnos/Superiore.html
https://www.eliteappliance.com/by-brand/kta/KitchenAid.html
https://www.eliteappliance.com/by-brand/vhood/Ventahood.html
http://www.eliteappliance.com/Jenn-Air/Jenn-Air-Appliance.php?ref=3620
http://www.eliteappliance.com/Electrolux/Electrolux-Appliance.php
https://www.eliteappliance.com/by-brand/ge/GE-Appliances.html
https://www.eliteappliance.com/locations.html
https://www.eliteappliance.com/by-brand/mie/Miele.html
http://www.eliteappliance.com/Sharp/R121.php
https://www.eliteappliance.com/SHIPPING_AND_DELIVERY.HTML
http://www.eliteappliance.com/Northstar/1955A.php
https://www.eliteappliance.com/products/Big-Green-Egg/bge/eggcelerator.html
https://www.eliteappliance.com/reviews/review_showcase.html
https://www.eliteappliance.com/miscellaneous-in-dallas-tx.html
http://www.eliteappliance.com/RCS/RWD1.php
https://www.eliteappliance.com/by-brand/ber/Bertazzoni.html
https://www.eliteappliance.com/products/Hoshizaki/hzk/dcm500bahos.html
http://www.eliteappliance.com/Whirlpool/WRT541SZD.php
http://www.eliteappliance.com/Frigidaire/FGGF3060SF.php
https://www.eliteappliance.com/by-brand/lnx/Lynx.html
https://www.eliteappliance.com/by-brand/shp/Sharp.html
http://www.eliteappliance.com/Electrolux/RH30WC55GS.php
https://www.eliteappliance.com/by-brand/frig/Frigidaire.html
https://www.eliteappliance.com/products/Miele/mie/eba6708.html
https://www.eliteappliance.com/products/Zephyr/zep/ak7036bs.html
https://www.eliteappliance.com/products/elica/ech623s1.html
http://www.eliteappliance.com/Zephyr/Zephyr-Appliance.php?x=t
http://www.eliteappliance.com/KitchenAid/KBHS109S.php
https://www.eliteappliance.com/products/Miele/mie/hgbb71.html
https://www.eliteappliance.com/products/Fisher-and-Paykel/fpk/479595.html
http://www.eliteappliance.com/Miele/HR1956DFGD.php
https://www.eliteappliance.com/products/GE-Appliances/ge/jk3800shss.html
https://www.eliteappliance.com/by-brand/whirl/Whirlpool.html
https://www.eliteappliance.com/by-brand/fab/Faber.html
https://www.eliteappliance.com/by-brand/dcs/DCS.html
http://www.eliteappliance.com/_CGI/MODEL?KEY=CAPITAL:GSCR486G
http://www.eliteappliance.com/Zephyr/ASLE42ASX.php
https://www.eliteappliance.com/products/Viking/vik/tcb12fcu15.html
""http://www.eliteappliance.com/Samsung/RF265BEAES.php
URL
https://www.eliteappliance.com/
https://www.eliteappliance.com/by-brand/vik/Viking.html
http://www.eliteappliance.com/Bertazzoni/Bertazzoni-Appliance.php
https://www.eliteappliance.com/landing_all_brands.html
http://www.eliteappliance.com/Northstar/Northstar-Appliance.php?x=t
https://www.eliteappliance.com/locations/ELITE.html
https://www.eliteappliance.com/by-brand/jen/JennAir.html
http://www.eliteappliance.com/Capital/CGRT484BB.php
https://www.eliteappliance.com/by-brand/tecnos/Superiore.html
https://www.eliteappliance.com/by-brand/kta/KitchenAid.html
https://www.eliteappliance.com/by-brand/vhood/Ventahood.html
http://www.eliteappliance.com/Jenn-Air/Jenn-Air-Appliance.php?ref=3620
http://www.eliteappliance.com/Electrolux/Electrolux-Appliance.php
https://www.eliteappliance.com/by-brand/ge/GE-Appliances.html
https://www.eliteappliance.com/locations.html
https://www.eliteappliance.com/by-brand/mie/Miele.html
http://www.eliteappliance.com/Sharp/R121.php
https://www.eliteappliance.com/SHIPPING_AND_DELIVERY.HTML
http://www.eliteappliance.com/Northstar/1955A.php
https://www.eliteappliance.com/products/Big-Green-Egg/bge/eggcelerator.html
https://www.eliteappliance.com/reviews/review_showcase.html
https://www.eliteappliance.com/miscellaneous-in-dallas-tx.html
http://www.eliteappliance.com/RCS/RWD1.php
https://www.eliteappliance.com/by-brand/ber/Bertazzoni.html
https://www.eliteappliance.com/products/Hoshizaki/hzk/dcm500bahos.html
http://www.eliteappliance.com/Whirlpool/WRT541SZD.php
http://www.eliteappliance.com/Frigidaire/FGGF3060SF.php
https://www.eliteappliance.com/by-brand/lnx/Lynx.html
https://www.eliteappliance.com/by-brand/shp/Sharp.html
http://www.eliteappliance.com/Electrolux/RH30WC55GS.php
https://www.eliteappliance.com/by-brand/frig/Frigidaire.html
https://www.eliteappliance.com/products/Miele/mie/eba6708.html
https://www.eliteappliance.com/products/Zephyr/zep/ak7036bs.html
https://www.eliteappliance.com/products/elica/ech623s1.html
http://www.eliteappliance.com/Zephyr/Zephyr-Appliance.php?x=t
http://www.eliteappliance.com/KitchenAid/KBHS109S.php
https://www.eliteappliance.com/products/Miele/mie/hgbb71.html
https://www.eliteappliance.com/products/Fisher-and-Paykel/fpk/479595.html
http://www.eliteappliance.com/Miele/HR1956DFGD.php
https://www.eliteappliance.com/products/GE-Appliances/ge/jk3800shss.html
https://www.eliteappliance.com/by-brand/whirl/Whirlpool.html
https://www.eliteappliance.com/by-brand/fab/Faber.html
https://www.eliteappliance.com/by-brand/dcs/DCS.html
http://www.eliteappliance.com/_CGI/MODEL?KEY=CAPITAL:GSCR486G
http://www.eliteappliance.com/Zephyr/ASLE42ASX.php
https://www.eliteappliance.com/products/Viking/vik/tcb12fcu15.html
http://www.eliteappliance.com/Samsung/RF265BEAES.php
https://www.eliteappliance.com/about_us.html
https://www.eliteappliance.com/by-brand/rcs/Renaissance-Cooking-Systems.html
https://www.eliteappliance.com/products/Miele/mie/wtv407.html
https://www.eliteappliance.com/products/samsung/nk36m9600wm.html
https://www.eliteappliance.com/ge-appliances-in-dallas-tx.html
https://www.eliteappliance.com/by-brand/ver/Verona.html
http://www.eliteappliance.com/Viking/VGIC5304B.php
http://www.eliteappliance.com/Capital/GSCR484BG.php
https://www.eliteappliance.com/products/Frigidaire/frig/fggf3036td.html
https://www.eliteappliance.com/products/Miele/mie/km6365.html
https://www.eliteappliance.com/products/monogram/zet1fhss.html
https://www.eliteappliance.com/products/Miele/mie/hfc71.html
https://www.eliteappliance.com/products/Viking/vik/hcb.html
http://www.eliteappliance.com/Sharp/KB-6524PK.php
https://www.eliteappliance.com/products/Miele/mie/classicc1puresuctionpowerlinesban0.html
https://www.eliteappliance.com/products/JennAir/jen/jud24fcers.html
https://www.eliteappliance.com/products/Danby/dan/dbc039a1bdb.html
https://www.eliteappliance.com/products/Miele/mie/kwf1000.html
https://www.eliteappliance.com/by-brand/aga/AGA.html
http://www.eliteappliance.com/Electrolux/RH36PC60GS.php
http://www.eliteappliance.com/Vent-A-Hood/PWVH30242.php
http://www.eliteappliance.com/_CGI/MODEL?KEY=VIK:RVGR33025BWHLP
http://www.eliteappliance.com/Capital/P36SHB.php
http://www.eliteappliance.com/Smeg/PGFU30X.php
https://www.eliteappliance.com/by-brand/bge/Big-Green-Egg.html
https://www.eliteappliance.com/products/Viking/vik/vbcv54838.html
https://www.eliteappliance.com/products/Viking/vik/cfor.html
https://www.eliteappliance.com/products/capital/mcor304.html
https://www.eliteappliance.com/products/Bertazzoni/ber/mast365gasxe.html
http://www.eliteappliance.com/KitchenAid/KUCS03FTPA.php
https://www.eliteappliance.com/products/Miele/mie/km3475g.html
https://www.eliteappliance.com/products/JennAir/jen/jgc3536gs.html
https://www.eliteappliance.com/products/Hoshizaki/hzk/dcm270bah.html
https://www.eliteappliance.com/products/samsung/we302nw.html
https://www.eliteappliance.com/products/Miele/mie/da7000dauraamblack.html
http://www.eliteappliance.com/Verona/Verona-Appliance.php
http://www.eliteappliance.com/Viking/VQGFS5360NSS.php
https://www.eliteappliance.com/products/dcs/wrt24t.html
https://www.eliteappliance.com/products/Ventahood/vhood/bh134sldss.html
https://www.eliteappliance.com/products/Zephyr/zep/ak8400as.html
https://www.eliteappliance.com/products/Renaissance-Cooking-Systems/rcs/rmc28.html
https://www.eliteappliance.com/Northstar/1950QS.php
https://www.eliteappliance.com/products/Sharp/shp/smc1131cb.html
http://www.eliteappliance.com/TecnogasSuperiore/RD361GCNB.php
https://www.eliteappliance.com/products/Asko/ask/t411vdw.html
https://www.eliteappliance.com/products/Miele/mie/km2355g.html
https://www.eliteappliance.com/by-brand/lbr/Liebherr.html
https://www.eliteappliance.com/products/monogram/zv830smss.html
https://www.eliteappliance.com/products/Viking/vik/vcih53608ss.html
https://www.eliteappliance.com/products/Liebherr/lbr/ws1200.html
https://www.eliteappliance.com/products/Faber/fab/clas30ss.html
http://www.eliteappliance.com/Fisher&Paykel/RH361.php
https://www.eliteappliance.com/products/monogram/zwe23eshss.html
""
""https://www.eliteappliance.com/about_us.htmlhttps://www.eliteappliance.com/about_us.html
https://www.eliteappliance.com/by-brand/rcs/Renaissance-Cooking-Systems.html
https://www.eliteappliance.com/products/Miele/mie/wtv407.html
https://www.eliteappliance.com/products/samsung/nk36m9600wm.html
https://www.eliteappliance.com/ge-appliances-in-dallas-tx.html
https://www.eliteappliance.com/by-brand/ver/Verona.html
http://www.eliteappliance.com/Viking/VGIC5304B.php
http://www.eliteappliance.com/Capital/GSCR484BG.php
https://www.eliteappliance.com/products/Frigidaire/frig/fggf3036td.html
https://www.eliteappliance.com/products/Miele/mie/km6365.html
https://www.eliteappliance.com/products/monogram/zet1fhss.html
https://www.eliteappliance.com/products/Miele/mie/hfc71.html
https://www.eliteappliance.com/products/Viking/vik/hcb.html
http://www.eliteappliance.com/Sharp/KB-6524PK.php
https://www.eliteappliance.com/products/Miele/mie/classicc1puresuctionpowerlinesban0.html
https://www.eliteappliance.com/products/JennAir/jen/jud24fcers.html
https://www.eliteappliance.com/products/Danby/dan/dbc039a1bdb.html
https://www.eliteappliance.com/products/Miele/mie/kwf1000.html
https://www.eliteappliance.com/by-brand/aga/AGA.html
http://www.eliteappliance.com/Electrolux/RH36PC60GS.php
http://www.eliteappliance.com/Vent-A-Hood/PWVH30242.php
http://www.eliteappliance.com/_CGI/MODEL?KEY=VIK:RVGR33025BWHLP
http://www.eliteappliance.com/Capital/P36SHB.php
http://www.eliteappliance.com/Smeg/PGFU30X.php
https://www.eliteappliance.com/by-brand/bge/Big-Green-Egg.html
https://www.eliteappliance.com/products/Viking/vik/vbcv54838.html
https://www.eliteappliance.com/products/Viking/vik/cfor.html
https://www.eliteappliance.com/products/capital/mcor304.html
https://www.eliteappliance.com/products/Bertazzoni/ber/mast365gasxe.html
http://www.eliteappliance.com/KitchenAid/KUCS03FTPA.php
https://www.eliteappliance.com/products/Miele/mie/km3475g.html
https://www.eliteappliance.com/products/JennAir/jen/jgc3536gs.html
https://www.eliteappliance.com/products/Hoshizaki/hzk/dcm270bah.html
https://www.eliteappliance.com/products/samsung/we302nw.html
https://www.eliteappliance.com/products/Miele/mie/da7000dauraamblack.html
http://www.eliteappliance.com/Verona/Verona-Appliance.php
http://www.eliteappliance.com/Viking/VQGFS5360NSS.php
https://www.eliteappliance.com/products/dcs/wrt24t.html
https://www.eliteappliance.com/products/Ventahood/vhood/bh134sldss.html
https://www.eliteappliance.com/products/Zephyr/zep/ak8400as.html
https://www.eliteappliance.com/products/Renaissance-Cooking-Systems/rcs/rmc28.html
https://www.eliteappliance.com/Northstar/1950QS.php
https://www.eliteappliance.com/products/Sharp/shp/smc1131cb.html
http://www.eliteappliance.com/TecnogasSuperiore/RD361GCNB.php
https://www.eliteappliance.com/products/Asko/ask/t411vdw.html
https://www.eliteappliance.com/products/Miele/mie/km2355g.html
https://www.eliteappliance.com/by-brand/lbr/Liebherr.html
https://www.eliteappliance.com/products/monogram/zv830smss.html
https://www.eliteappliance.com/products/Viking/vik/vcih53608ss.html
https://www.eliteappliance.com/products/Liebherr/lbr/ws1200.html
https://www.eliteappliance.com/products/Faber/fab/clas30ss.html
http://www.eliteappliance.com/Fisher&Paykel/RH361.php
https://www.eliteappliance.com/products/monogram/zwe23eshss.html
""
""https://www.eliteappliance.com/by-brand/rcs/Renaissance-Cooking-Systems.html
http://www.eliteappliance.com/TecnogasSuperiore/RD361GCNB.php
https://www.eliteappliance.com/products/Asko/ask/t411vdw.html
https://www.eliteappliance.com/products/Miele/mie/km2355g.html
https://www.eliteappliance.com/by-brand/lbr/Liebherr.html
https://www.eliteappliance.com/products/monogram/zv830smss.html
https://www.eliteappliance.com/products/Viking/vik/vcih53608ss.html
https://www.eliteappliance.com/products/Liebherr/lbr/ws1200.html
https://www.eliteappliance.com/products/Faber/fab/clas30ss.html
http://www.eliteappliance.com/Fisher&Paykel/RH361.php
https://www.eliteappliance.com/products/monogram/zwe23eshss.html
""
""https://www.eliteappliance.com/products/monogram/zwe23eshss.html
URL
https://www.eliteappliance.com/
https://www.eliteappliance.com/by-brand/vik/Viking.html
http://www.eliteappliance.com/Bertazzoni/Bertazzoni-Appliance.php
https://www.eliteappliance.com/landing_all_brands.html
http://www.eliteappliance.com/Northstar/Northstar-Appliance.php?x=t
https://www.eliteappliance.com/locations/ELITE.html
https://www.eliteappliance.com/by-brand/jen/JennAir.html
http://www.eliteappliance.com/Capital/CGRT484BB.php
https://www.eliteappliance.com/by-brand/tecnos/Superiore.html
https://www.eliteappliance.com/by-brand/kta/KitchenAid.html
https://www.eliteappliance.com/by-brand/vhood/Ventahood.html
http://www.eliteappliance.com/Jenn-Air/Jenn-Air-Appliance.php?ref=3620
http://www.eliteappliance.com/Electrolux/Electrolux-Appliance.php
https://www.eliteappliance.com/by-brand/ge/GE-Appliances.html
https://www.eliteappliance.com/locations.html
https://www.eliteappliance.com/by-brand/mie/Miele.html
http://www.eliteappliance.com/Sharp/R121.php
https://www.eliteappliance.com/SHIPPING_AND_DELIVERY.HTML
http://www.eliteappliance.com/Northstar/1955A.php
https://www.eliteappliance.com/products/Big-Green-Egg/bge/eggcelerator.html
https://www.eliteappliance.com/reviews/review_showcase.html
https://www.eliteappliance.com/miscellaneous-in-dallas-tx.html
http://www.eliteappliance.com/RCS/RWD1.php
https://www.eliteappliance.com/by-brand/ber/Bertazzoni.html
https://www.eliteappliance.com/products/Hoshizaki/hzk/dcm500bahos.html
http://www.eliteappliance.com/Whirlpool/WRT541SZD.php
http://www.eliteappliance.com/Frigidaire/FGGF3060SF.php
https://www.eliteappliance.com/by-brand/lnx/Lynx.html
https://www.eliteappliance.com/by-brand/shp/Sharp.html
http://www.eliteappliance.com/Electrolux/RH30WC55GS.php
https://www.eliteappliance.com/by-brand/frig/Frigidaire.html
https://www.eliteappliance.com/products/Miele/mie/eba6708.html
https://www.eliteappliance.com/products/Zephyr/zep/ak7036bs.html
https://www.eliteappliance.com/products/elica/ech623s1.html
http://www.eliteappliance.com/Zephyr/Zephyr-Appliance.php?x=t
http://www.eliteappliance.com/KitchenAid/KBHS109S.php
https://www.eliteappliance.com/products/Miele/mie/hgbb71.html
https://www.eliteappliance.com/products/Fisher-and-Paykel/fpk/479595.html
http://www.eliteappliance.com/Miele/HR1956DFGD.php
https://www.eliteappliance.com/products/GE-Appliances/ge/jk3800shss.html
https://www.eliteappliance.com/by-brand/whirl/Whirlpool.html
https://www.eliteappliance.com/by-brand/fab/Faber.html
https://www.eliteappliance.com/by-brand/dcs/DCS.html
http://www.eliteappliance.com/_CGI/MODEL?KEY=CAPITAL:GSCR486G
http://www.eliteappliance.com/Zephyr/ASLE42ASX.php
https://www.eliteappliance.com/products/Viking/vik/tcb12fcu15.html
""http://www.eliteappliance.com/Samsung/RF265BEAES.php
URL
https://www.eliteappliance.com/
https://www.eliteappliance.com/by-brand/vik/Viking.html
http://www.eliteappliance.com/Bertazzoni/Bertazzoni-Appliance.php
https://www.eliteappliance.com/landing_all_brands.html
http://www.eliteappliance.com/Northstar/Northstar-Appliance.php?x=t
https://www.eliteappliance.com/locations/ELITE.html
https://www.eliteappliance.com/by-brand/jen/JennAir.html
http://www.eliteappliance.com/Capital/CGRT484BB.php
https://www.eliteappliance.com/by-brand/tecnos/Superiore.html
https://www.eliteappliance.com/by-brand/kta/KitchenAid.html
https://www.eliteappliance.com/by-brand/vhood/Ventahood.html
http://www.eliteappliance.com/Jenn-Air/Jenn-Air-Appliance.php?ref=3620
http://www.eliteappliance.com/Electrolux/Electrolux-Appliance.php
https://www.eliteappliance.com/by-brand/ge/GE-Appliances.html
https://www.eliteappliance.com/locations.html
https://www.eliteappliance.com/by-brand/mie/Miele.html
http://www.eliteappliance.com/Sharp/R121.php
https://www.eliteappliance.com/SHIPPING_AND_DELIVERY.HTML
http://www.eliteappliance.com/Northstar/1955A.php
https://www.eliteappliance.com/products/Big-Green-Egg/bge/eggcelerator.html
https://www.eliteappliance.com/reviews/review_showcase.html
https://www.eliteappliance.com/miscellaneous-in-dallas-tx.html
http://www.eliteappliance.com/RCS/RWD1.php
https://www.eliteappliance.com/by-brand/ber/Bertazzoni.html
https://www.eliteappliance.com/products/Hoshizaki/hzk/dcm500bahos.html
http://www.eliteappliance.com/Whirlpool/WRT541SZD.php
http://www.eliteappliance.com/Frigidaire/FGGF3060SF.php
https://www.eliteappliance.com/by-brand/lnx/Lynx.html
https://www.eliteappliance.com/by-brand/shp/Sharp.html
http://www.eliteappliance.com/Electrolux/RH30WC55GS.php
https://www.eliteappliance.com/by-brand/frig/Frigidaire.html
https://www.eliteappliance.com/products/Miele/mie/eba6708.html
https://www.eliteappliance.com/products/Zephyr/zep/ak7036bs.html
https://www.eliteappliance.com/products/elica/ech623s1.html
http://www.eliteappliance.com/Zephyr/Zephyr-Appliance.php?x=t
http://www.eliteappliance.com/KitchenAid/KBHS109S.php
https://www.eliteappliance.com/products/Miele/mie/hgbb71.html
https://www.eliteappliance.com/products/Fisher-and-Paykel/fpk/479595.html
http://www.eliteappliance.com/Miele/HR1956DFGD.php
https://www.eliteappliance.com/products/GE-Appliances/ge/jk3800shss.html
https://www.eliteappliance.com/by-brand/whirl/Whirlpool.html
https://www.eliteappliance.com/by-brand/fab/Faber.html
https://www.eliteappliance.com/by-brand/dcs/DCS.html
http://www.eliteappliance.com/_CGI/MODEL?KEY=CAPITAL:GSCR486G
http://www.eliteappliance.com/Zephyr/ASLE42ASX.php
https://www.eliteappliance.com/products/Viking/vik/tcb12fcu15.html
http://www.eliteappliance.com/Samsung/RF265BEAES.php
https://www.eliteappliance.com/about_us.html
https://www.eliteappliance.com/by-brand/rcs/Renaissance-Cooking-Systems.html
https://www.eliteappliance.com/products/Miele/mie/wtv407.html
https://www.eliteappliance.com/products/samsung/nk36m9600wm.html
https://www.eliteappliance.com/ge-appliances-in-dallas-tx.html
https://www.eliteappliance.com/by-brand/ver/Verona.html
http://www.eliteappliance.com/Viking/VGIC5304B.php
http://www.eliteappliance.com/Capital/GSCR484BG.php
https://www.eliteappliance.com/products/Frigidaire/frig/fggf3036td.html
https://www.eliteappliance.com/products/Miele/mie/km6365.html
https://www.eliteappliance.com/products/monogram/zet1fhss.html
https://www.eliteappliance.com/products/Miele/mie/hfc71.html
https://www.eliteappliance.com/products/Viking/vik/hcb.html
http://www.eliteappliance.com/Sharp/KB-6524PK.php
https://www.eliteappliance.com/products/Miele/mie/classicc1puresuctionpowerlinesban0.html
https://www.eliteappliance.com/products/JennAir/jen/jud24fcers.html
https://www.eliteappliance.com/products/Danby/dan/dbc039a1bdb.html
https://www.eliteappliance.com/products/Miele/mie/kwf1000.html
https://www.eliteappliance.com/by-brand/aga/AGA.html
http://www.eliteappliance.com/Electrolux/RH36PC60GS.php
http://www.eliteappliance.com/Vent-A-Hood/PWVH30242.php
http://www.eliteappliance.com/_CGI/MODEL?KEY=VIK:RVGR33025BWHLP
http://www.eliteappliance.com/Capital/P36SHB.php
http://www.eliteappliance.com/Smeg/PGFU30X.php
https://www.eliteappliance.com/by-brand/bge/Big-Green-Egg.html
https://www.eliteappliance.com/products/Viking/vik/vbcv54838.html
https://www.eliteappliance.com/products/Viking/vik/cfor.html
https://www.eliteappliance.com/products/capital/mcor304.html
https://www.eliteappliance.com/products/Bertazzoni/ber/mast365gasxe.html
http://www.eliteappliance.com/KitchenAid/KUCS03FTPA.php
https://www.eliteappliance.com/products/Miele/mie/km3475g.html
https://www.eliteappliance.com/products/JennAir/jen/jgc3536gs.html
https://www.eliteappliance.com/products/Hoshizaki/hzk/dcm270bah.html
https://www.eliteappliance.com/products/samsung/we302nw.html
https://www.eliteappliance.com/products/Miele/mie/da7000dauraamblack.html
http://www.eliteappliance.com/Verona/Verona-Appliance.php
http://www.eliteappliance.com/Viking/VQGFS5360NSS.php
https://www.eliteappliance.com/products/dcs/wrt24t.html
https://www.eliteappliance.com/products/Ventahood/vhood/bh134sldss.html
https://www.eliteappliance.com/products/Zephyr/zep/ak8400as.html
https://www.eliteappliance.com/products/Renaissance-Cooking-Systems/rcs/rmc28.html
https://www.eliteappliance.com/Northstar/1950QS.php
https://www.eliteappliance.com/products/Sharp/shp/smc1131cb.html
http://www.eliteappliance.com/TecnogasSuperiore/RD361GCNB.php
https://www.eliteappliance.com/products/Asko/ask/t411vdw.html
https://www.eliteappliance.com/products/Miele/mie/km2355g.html
https://www.eliteappliance.com/by-brand/lbr/Liebherr.html
https://www.eliteappliance.com/products/monogram/zv830smss.html
https://www.eliteappliance.com/products/Viking/vik/vcih53608ss.html
https://www.eliteappliance.com/products/Liebherr/lbr/ws1200.html
https://www.eliteappliance.com/products/Faber/fab/clas30ss.html
http://www.eliteappliance.com/Fisher&Paykel/RH361.php
https://www.eliteappliance.com/products/monogram/zwe23eshss.html
""
""https://www.eliteappliance.com/about_us.htmlhttps://www.eliteappliance.com/about_us.html
https://www.eliteappliance.com/by-brand/rcs/Renaissance-Cooking-Systems.html
https://www.eliteappliance.com/products/Miele/mie/wtv407.html
https://www.eliteappliance.com/products/samsung/nk36m9600wm.html
https://www.eliteappliance.com/ge-appliances-in-dallas-tx.html
https://www.eliteappliance.com/by-brand/ver/Verona.html
http://www.eliteappliance.com/Viking/VGIC5304B.php
http://www.eliteappliance.com/Capital/GSCR484BG.php
https://www.eliteappliance.com/products/Frigidaire/frig/fggf3036td.html
https://www.eliteappliance.com/products/Miele/mie/km6365.html
https://www.eliteappliance.com/products/monogram/zet1fhss.html
https://www.eliteappliance.com/products/Miele/mie/hfc71.html
https://www.eliteappliance.com/products/Viking/vik/hcb.html
http://www.eliteappliance.com/Sharp/KB-6524PK.php
https://www.eliteappliance.com/products/Miele/mie/classicc1puresuctionpowerlinesban0.html
https://www.eliteappliance.com/products/JennAir/jen/jud24fcers.html
https://www.eliteappliance.com/products/Danby/dan/dbc039a1bdb.html
https://www.eliteappliance.com/products/Miele/mie/kwf1000.html
https://www.eliteappliance.com/by-brand/aga/AGA.html
http://www.eliteappliance.com/Electrolux/RH36PC60GS.php
http://www.eliteappliance.com/Vent-A-Hood/PWVH30242.php
http://www.eliteappliance.com/_CGI/MODEL?KEY=VIK:RVGR33025BWHLP
http://www.eliteappliance.com/Capital/P36SHB.php
http://www.eliteappliance.com/Smeg/PGFU30X.php
https://www.eliteappliance.com/by-brand/bge/Big-Green-Egg.html
https://www.eliteappliance.com/products/Viking/vik/vbcv54838.html
https://www.eliteappliance.com/products/Viking/vik/cfor.html
https://www.eliteappliance.com/products/capital/mcor304.html
https://www.eliteappliance.com/products/Bertazzoni/ber/mast365gasxe.html
http://www.eliteappliance.com/KitchenAid/KUCS03FTPA.php
https://www.eliteappliance.com/products/Miele/mie/km3475g.html
https://www.eliteappliance.com/products/JennAir/jen/jgc3536gs.html
https://www.eliteappliance.com/products/Hoshizaki/hzk/dcm270bah.html
https://www.eliteappliance.com/products/samsung/we302nw.html
https://www.eliteappliance.com/products/Miele/mie/da7000dauraamblack.html
http://www.eliteappliance.com/Verona/Verona-Appliance.php
http://www.eliteappliance.com/Viking/VQGFS5360NSS.php
https://www.eliteappliance.com/products/dcs/wrt24t.html
https://www.eliteappliance.com/products/Ventahood/vhood/bh134sldss.html
https://www.eliteappliance.com/products/Zephyr/zep/ak8400as.html
https://www.eliteappliance.com/products/Renaissance-Cooking-Systems/rcs/rmc28.html
https://www.eliteappliance.com/Northstar/1950QS.php
https://www.eliteappliance.com/products/Sharp/shp/smc1131cb.html
http://www.eliteappliance.com/TecnogasSuperiore/RD361GCNB.php
https://www.eliteappliance.com/products/Asko/ask/t411vdw.html
https://www.eliteappliance.com/products/Miele/mie/km2355g.html
https://www.eliteappliance.com/by-brand/lbr/Liebherr.html
https://www.eliteappliance.com/products/monogram/zv830smss.html
https://www.eliteappliance.com/products/Viking/vik/vcih53608ss.html
https://www.eliteappliance.com/products/Liebherr/lbr/ws1200.html
https://www.eliteappliance.com/products/Faber/fab/clas30ss.html
http://www.eliteappliance.com/Fisher&Paykel/RH361.php
https://www.eliteappliance.com/products/monogram/zwe23eshss.html
""
""https://www.eliteappliance.com/by-brand/rcs/Renaissance-Cooking-Systems.html
http://www.eliteappliance.com/TecnogasSuperiore/RD361GCNB.php
https://www.eliteappliance.com/products/Asko/ask/t411vdw.html
https://www.eliteappliance.com/products/Miele/mie/km2355g.html
https://www.eliteappliance.com/by-brand/lbr/Liebherr.html
https://www.eliteappliance.com/products/monogram/zv830smss.html
https://www.eliteappliance.com/products/Viking/vik/vcih53608ss.html
https://www.eliteappliance.com/products/Liebherr/lbr/ws1200.html
https://www.eliteappliance.com/products/Faber/fab/clas30ss.html
http://www.eliteappliance.com/Fisher&Paykel/RH361.php
https://www.eliteappliance.com/products/monogram/zwe23eshss.html
""
""https://www.eliteappliance.com/products/monogram/zwe23eshss.html

URL
https://www.eliteappliance.com/
https://www.eliteappliance.com/by-brand/vik/Viking.html
http://www.eliteappliance.com/Bertazzoni/Bertazzoni-Appliance.php
https://www.eliteappliance.com/landing_all_brands.html
http://www.eliteappliance.com/Northstar/Northstar-Appliance.php?x=t
https://www.eliteappliance.com/locations/ELITE.html
https://www.eliteappliance.com/by-brand/jen/JennAir.html
http://www.eliteappliance.com/Capital/CGRT484BB.php
https://www.eliteappliance.com/by-brand/tecnos/Superiore.html
https://www.eliteappliance.com/by-brand/kta/KitchenAid.html
https://www.eliteappliance.com/by-brand/vhood/Ventahood.html
http://www.eliteappliance.com/Jenn-Air/Jenn-Air-Appliance.php?ref=3620
http://www.eliteappliance.com/Electrolux/Electrolux-Appliance.php
https://www.eliteappliance.com/by-brand/ge/GE-Appliances.html
https://www.eliteappliance.com/locations.html
https://www.eliteappliance.com/by-brand/mie/Miele.html
http://www.eliteappliance.com/Sharp/R121.php
https://www.eliteappliance.com/SHIPPING_AND_DELIVERY.HTML
http://www.eliteappliance.com/Northstar/1955A.php
https://www.eliteappliance.com/products/Big-Green-Egg/bge/eggcelerator.html
https://www.eliteappliance.com/reviews/review_showcase.html
https://www.eliteappliance.com/miscellaneous-in-dallas-tx.html
http://www.eliteappliance.com/RCS/RWD1.php
https://www.eliteappliance.com/by-brand/ber/Bertazzoni.html
https://www.eliteappliance.com/products/Hoshizaki/hzk/dcm500bahos.html
http://www.eliteappliance.com/Whirlpool/WRT541SZD.php
http://www.eliteappliance.com/Frigidaire/FGGF3060SF.php
https://www.eliteappliance.com/by-brand/lnx/Lynx.html
https://www.eliteappliance.com/by-brand/shp/Sharp.html
http://www.eliteappliance.com/Electrolux/RH30WC55GS.php
https://www.eliteappliance.com/by-brand/frig/Frigidaire.html
https://www.eliteappliance.com/products/Miele/mie/eba6708.html
https://www.eliteappliance.com/products/Zephyr/zep/ak7036bs.html
https://www.eliteappliance.com/products/elica/ech623s1.html
http://www.eliteappliance.com/Zephyr/Zephyr-Appliance.php?x=t
http://www.eliteappliance.com/KitchenAid/KBHS109S.php
https://www.eliteappliance.com/products/Miele/mie/hgbb71.html
https://www.eliteappliance.com/products/Fisher-and-Paykel/fpk/479595.html
http://www.eliteappliance.com/Miele/HR1956DFGD.php
https://www.eliteappliance.com/products/GE-Appliances/ge/jk3800shss.html
https://www.eliteappliance.com/by-brand/whirl/Whirlpool.html
https://www.eliteappliance.com/by-brand/fab/Faber.html
https://www.eliteappliance.com/by-brand/dcs/DCS.html
http://www.eliteappliance.com/_CGI/MODEL?KEY=CAPITAL:GSCR486G
http://www.eliteappliance.com/Zephyr/ASLE42ASX.php
https://www.eliteappliance.com/products/Viking/vik/tcb12fcu15.html
""http://www.eliteappliance.com/Samsung/RF265BEAES.php
URL
https://www.eliteappliance.com/
https://www.eliteappliance.com/by-brand/vik/Viking.html
http://www.eliteappliance.com/Bertazzoni/Bertazzoni-Appliance.php
https://www.eliteappliance.com/landing_all_brands.html
http://www.eliteappliance.com/Northstar/Northstar-Appliance.php?x=t
https://www.eliteappliance.com/locations/ELITE.html
https://www.eliteappliance.com/by-brand/jen/JennAir.html
http://www.eliteappliance.com/Capital/CGRT484BB.php
https://www.eliteappliance.com/by-brand/tecnos/Superiore.html
https://www.eliteappliance.com/by-brand/kta/KitchenAid.html
https://www.eliteappliance.com/by-brand/vhood/Ventahood.html
http://www.eliteappliance.com/Jenn-Air/Jenn-Air-Appliance.php?ref=3620
http://www.eliteappliance.com/Electrolux/Electrolux-Appliance.php
https://www.eliteappliance.com/by-brand/ge/GE-Appliances.html
https://www.eliteappliance.com/locations.html
https://www.eliteappliance.com/by-brand/mie/Miele.html
http://www.eliteappliance.com/Sharp/R121.php
https://www.eliteappliance.com/SHIPPING_AND_DELIVERY.HTML
http://www.eliteappliance.com/Northstar/1955A.php
https://www.eliteappliance.com/products/Big-Green-Egg/bge/eggcelerator.html
https://www.eliteappliance.com/reviews/review_showcase.html
https://www.eliteappliance.com/miscellaneous-in-dallas-tx.html
http://www.eliteappliance.com/RCS/RWD1.php
https://www.eliteappliance.com/by-brand/ber/Bertazzoni.html
https://www.eliteappliance.com/products/Hoshizaki/hzk/dcm500bahos.html
http://www.eliteappliance.com/Whirlpool/WRT541SZD.php
http://www.eliteappliance.com/Frigidaire/FGGF3060SF.php
https://www.eliteappliance.com/by-brand/lnx/Lynx.html
https://www.eliteappliance.com/by-brand/shp/Sharp.html
http://www.eliteappliance.com/Electrolux/RH30WC55GS.php
https://www.eliteappliance.com/by-brand/frig/Frigidaire.html
https://www.eliteappliance.com/products/Miele/mie/eba6708.html
https://www.eliteappliance.com/products/Zephyr/zep/ak7036bs.html
https://www.eliteappliance.com/products/elica/ech623s1.html
http://www.eliteappliance.com/Zephyr/Zephyr-Appliance.php?x=t
http://www.eliteappliance.com/KitchenAid/KBHS109S.php
https://www.eliteappliance.com/products/Miele/mie/hgbb71.html
https://www.eliteappliance.com/products/Fisher-and-Paykel/fpk/479595.html
http://www.eliteappliance.com/Miele/HR1956DFGD.php
https://www.eliteappliance.com/products/GE-Appliances/ge/jk3800shss.html
https://www.eliteappliance.com/by-brand/whirl/Whirlpool.html
https://www.eliteappliance.com/by-brand/fab/Faber.html
https://www.eliteappliance.com/by-brand/dcs/DCS.html
http://www.eliteappliance.com/_CGI/MODEL?KEY=CAPITAL:GSCR486G
http://www.eliteappliance.com/Zephyr/ASLE42ASX.php
https://www.eliteappliance.com/products/Viking/vik/tcb12fcu15.html
http://www.eliteappliance.com/Samsung/RF265BEAES.php
https://www.eliteappliance.com/about_us.html
https://www.eliteappliance.com/by-brand/rcs/Renaissance-Cooking-Systems.html
https://www.eliteappliance.com/products/Miele/mie/wtv407.html
https://www.eliteappliance.com/products/samsung/nk36m9600wm.html
https://www.eliteappliance.com/ge-appliances-in-dallas-tx.html
https://www.eliteappliance.com/by-brand/ver/Verona.html
http://www.eliteappliance.com/Viking/VGIC5304B.php
http://www.eliteappliance.com/Capital/GSCR484BG.php
https://www.eliteappliance.com/products/Frigidaire/frig/fggf3036td.html
https://www.eliteappliance.com/products/Miele/mie/km6365.html
https://www.eliteappliance.com/products/monogram/zet1fhss.html
https://www.eliteappliance.com/products/Miele/mie/hfc71.html
https://www.eliteappliance.com/products/Viking/vik/hcb.html
http://www.eliteappliance.com/Sharp/KB-6524PK.php
https://www.eliteappliance.com/products/Miele/mie/classicc1puresuctionpowerlinesban0.html
https://www.eliteappliance.com/products/JennAir/jen/jud24fcers.html
https://www.eliteappliance.com/products/Danby/dan/dbc039a1bdb.html
https://www.eliteappliance.com/products/Miele/mie/kwf1000.html
https://www.eliteappliance.com/by-brand/aga/AGA.html
http://www.eliteappliance.com/Electrolux/RH36PC60GS.php
http://www.eliteappliance.com/Vent-A-Hood/PWVH30242.php
http://www.eliteappliance.com/_CGI/MODEL?KEY=VIK:RVGR33025BWHLP
http://www.eliteappliance.com/Capital/P36SHB.php
http://www.eliteappliance.com/Smeg/PGFU30X.php
https://www.eliteappliance.com/by-brand/bge/Big-Green-Egg.html
https://www.eliteappliance.com/products/Viking/vik/vbcv54838.html
https://www.eliteappliance.com/products/Viking/vik/cfor.html
https://www.eliteappliance.com/products/capital/mcor304.html
https://www.eliteappliance.com/products/Bertazzoni/ber/mast365gasxe.html
http://www.eliteappliance.com/KitchenAid/KUCS03FTPA.php
https://www.eliteappliance.com/products/Miele/mie/km3475g.html
https://www.eliteappliance.com/products/JennAir/jen/jgc3536gs.html
https://www.eliteappliance.com/products/Hoshizaki/hzk/dcm270bah.html
https://www.eliteappliance.com/products/samsung/we302nw.html
https://www.eliteappliance.com/products/Miele/mie/da7000dauraamblack.html
http://www.eliteappliance.com/Verona/Verona-Appliance.php
http://www.eliteappliance.com/Viking/VQGFS5360NSS.php
https://www.eliteappliance.com/products/dcs/wrt24t.html
https://www.eliteappliance.com/products/Ventahood/vhood/bh134sldss.html
https://www.eliteappliance.com/products/Zephyr/zep/ak8400as.html
https://www.eliteappliance.com/products/Renaissance-Cooking-Systems/rcs/rmc28.html
https://www.eliteappliance.com/Northstar/1950QS.php
https://www.eliteappliance.com/products/Sharp/shp/smc1131cb.html
http://www.eliteappliance.com/TecnogasSuperiore/RD361GCNB.php
https://www.eliteappliance.com/products/Asko/ask/t411vdw.html
https://www.eliteappliance.com/products/Miele/mie/km2355g.html
https://www.eliteappliance.com/by-brand/lbr/Liebherr.html
https://www.eliteappliance.com/products/monogram/zv830smss.html
https://www.eliteappliance.com/products/Viking/vik/vcih53608ss.html
https://www.eliteappliance.com/products/Liebherr/lbr/ws1200.html
https://www.eliteappliance.com/products/Faber/fab/clas30ss.html
http://www.eliteappliance.com/Fisher&Paykel/RH361.php
https://www.eliteappliance.com/products/monogram/zwe23eshss.html
""
""https://www.eliteappliance.com/about_us.htmlhttps://www.eliteappliance.com/about_us.html
https://www.eliteappliance.com/by-brand/rcs/Renaissance-Cooking-Systems.html
https://www.eliteappliance.com/products/Miele/mie/wtv407.html
https://www.eliteappliance.com/products/samsung/nk36m9600wm.html
https://www.eliteappliance.com/ge-appliances-in-dallas-tx.html
https://www.eliteappliance.com/by-brand/ver/Verona.html
http://www.eliteappliance.com/Viking/VGIC5304B.php
http://www.eliteappliance.com/Capital/GSCR484BG.php
https://www.eliteappliance.com/products/Frigidaire/frig/fggf3036td.html
https://www.eliteappliance.com/products/Miele/mie/km6365.html
https://www.eliteappliance.com/products/monogram/zet1fhss.html
https://www.eliteappliance.com/products/Miele/mie/hfc71.html
https://www.eliteappliance.com/products/Viking/vik/hcb.html
http://www.eliteappliance.com/Sharp/KB-6524PK.php
https://www.eliteappliance.com/products/Miele/mie/classicc1puresuctionpowerlinesban0.html
https://www.eliteappliance.com/products/JennAir/jen/jud24fcers.html
https://www.eliteappliance.com/products/Danby/dan/dbc039a1bdb.html
https://www.eliteappliance.com/products/Miele/mie/kwf1000.html
https://www.eliteappliance.com/by-brand/aga/AGA.html
http://www.eliteappliance.com/Electrolux/RH36PC60GS.php
http://www.eliteappliance.com/Vent-A-Hood/PWVH30242.php
http://www.eliteappliance.com/_CGI/MODEL?KEY=VIK:RVGR33025BWHLP
http://www.eliteappliance.com/Capital/P36SHB.php
http://www.eliteappliance.com/Smeg/PGFU30X.php
https://www.eliteappliance.com/by-brand/bge/Big-Green-Egg.html
https://www.eliteappliance.com/products/Viking/vik/vbcv54838.html
https://www.eliteappliance.com/products/Viking/vik/cfor.html
https://www.eliteappliance.com/products/capital/mcor304.html
https://www.eliteappliance.com/products/Bertazzoni/ber/mast365gasxe.html
http://www.eliteappliance.com/KitchenAid/KUCS03FTPA.php
https://www.eliteappliance.com/products/Miele/mie/km3475g.html
https://www.eliteappliance.com/products/JennAir/jen/jgc3536gs.html
https://www.eliteappliance.com/products/Hoshizaki/hzk/dcm270bah.html
https://www.eliteappliance.com/products/samsung/we302nw.html
https://www.eliteappliance.com/products/Miele/mie/da7000dauraamblack.html
http://www.eliteappliance.com/Verona/Verona-Appliance.php
http://www.eliteappliance.com/Viking/VQGFS5360NSS.php
https://www.eliteappliance.com/products/dcs/wrt24t.html
https://www.eliteappliance.com/products/Ventahood/vhood/bh134sldss.html
https://www.eliteappliance.com/products/Zephyr/zep/ak8400as.html
https://www.eliteappliance.com/products/Renaissance-Cooking-Systems/rcs/rmc28.html
https://www.eliteappliance.com/Northstar/1950QS.php
https://www.eliteappliance.com/products/Sharp/shp/smc1131cb.html
http://www.eliteappliance.com/TecnogasSuperiore/RD361GCNB.php
https://www.eliteappliance.com/products/Asko/ask/t411vdw.html
https://www.eliteappliance.com/products/Miele/mie/km2355g.html
https://www.eliteappliance.com/by-brand/lbr/Liebherr.html
https://www.eliteappliance.com/products/monogram/zv830smss.html
https://www.eliteappliance.com/products/Viking/vik/vcih53608ss.html
https://www.eliteappliance.com/products/Liebherr/lbr/ws1200.html
https://www.eliteappliance.com/products/Faber/fab/clas30ss.html
http://www.eliteappliance.com/Fisher&Paykel/RH361.php
https://www.eliteappliance.com/products/monogram/zwe23eshss.html
""
""https://www.eliteappliance.com/by-brand/rcs/Renaissance-Cooking-Systems.html
http://www.eliteappliance.com/TecnogasSuperiore/RD361GCNB.php
https://www.eliteappliance.com/products/Asko/ask/t411vdw.html
https://www.eliteappliance.com/products/Miele/mie/km2355g.html
https://www.eliteappliance.com/by-brand/lbr/Liebherr.html
https://www.eliteappliance.com/products/monogram/zv830smss.html
https://www.eliteappliance.com/products/Viking/vik/vcih53608ss.html
https://www.eliteappliance.com/products/Liebherr/lbr/ws1200.html
https://www.eliteappliance.com/products/Faber/fab/clas30ss.html
http://www.eliteappliance.com/Fisher&Paykel/RH361.php
https://www.eliteappliance.com/products/monogram/zwe23eshss.html
""
""https://www.eliteappliance.com/products/monogram/zwe23eshss.html
""URL
https://www.eliteappliance.com/
https://www.eliteappliance.com/by-brand/vik/Viking.html
http://www.eliteappliance.com/Bertazzoni/Bertazzoni-Appliance.php
https://www.eliteappliance.com/landing_all_brands.html
http://www.eliteappliance.com/Northstar/Northstar-Appliance.php?x=t
https://www.eliteappliance.com/locations/ELITE.html
https://www.eliteappliance.com/by-brand/jen/JennAir.html
http://www.eliteappliance.com/Capital/CGRT484BB.php
https://www.eliteappliance.com/by-brand/tecnos/Superiore.html
https://www.eliteappliance.com/by-brand/kta/KitchenAid.html
https://www.eliteappliance.com/by-brand/vhood/Ventahood.html
http://www.eliteappliance.com/Jenn-Air/Jenn-Air-Appliance.php?ref=3620
http://www.eliteappliance.com/Electrolux/Electrolux-Appliance.php
https://www.eliteappliance.com/by-brand/ge/GE-Appliances.html
https://www.eliteappliance.com/locations.html
https://www.eliteappliance.com/by-brand/mie/Miele.html
http://www.eliteappliance.com/Sharp/R121.php
https://www.eliteappliance.com/SHIPPING_AND_DELIVERY.HTML
http://www.eliteappliance.com/Northstar/1955A.php
https://www.eliteappliance.com/products/Big-Green-Egg/bge/eggcelerator.html
https://www.eliteappliance.com/reviews/review_showcase.html
https://www.eliteappliance.com/miscellaneous-in-dallas-tx.html
http://www.eliteappliance.com/RCS/RWD1.php
https://www.eliteappliance.com/by-brand/ber/Bertazzoni.html
https://www.eliteappliance.com/products/Hoshizaki/hzk/dcm500bahos.html
http://www.eliteappliance.com/Whirlpool/WRT541SZD.php
http://www.eliteappliance.com/Frigidaire/FGGF3060SF.php
https://www.eliteappliance.com/by-brand/lnx/Lynx.html
https://www.eliteappliance.com/by-brand/shp/Sharp.html
http://www.eliteappliance.com/Electrolux/RH30WC55GS.php
https://www.eliteappliance.com/by-brand/frig/Frigidaire.html
https://www.eliteappliance.com/products/Miele/mie/eba6708.html
https://www.eliteappliance.com/products/Zephyr/zep/ak7036bs.html
https://www.eliteappliance.com/products/elica/ech623s1.html
http://www.eliteappliance.com/Zephyr/Zephyr-Appliance.php?x=t
http://www.eliteappliance.com/KitchenAid/KBHS109S.php
https://www.eliteappliance.com/products/Miele/mie/hgbb71.html
https://www.eliteappliance.com/products/Fisher-and-Paykel/fpk/479595.html
http://www.eliteappliance.com/Miele/HR1956DFGD.php
https://www.eliteappliance.com/products/GE-Appliances/ge/jk3800shss.html
https://www.eliteappliance.com/by-brand/whirl/Whirlpool.html
https://www.eliteappliance.com/by-brand/fab/Faber.html
https://www.eliteappliance.com/by-brand/dcs/DCS.html
http://www.eliteappliance.com/_CGI/MODEL?KEY=CAPITAL:GSCR486G
http://www.eliteappliance.com/Zephyr/ASLE42ASX.php
https://www.eliteappliance.com/products/Viking/vik/tcb12fcu15.html
""http://www.eliteappliance.com/Samsung/RF265BEAES.php
URL
https://www.eliteappliance.com/
https://www.eliteappliance.com/by-brand/vik/Viking.html
http://www.eliteappliance.com/Bertazzoni/Bertazzoni-Appliance.php
https://www.eliteappliance.com/landing_all_brands.html
http://www.eliteappliance.com/Northstar/Northstar-Appliance.php?x=t
https://www.eliteappliance.com/locations/ELITE.html
https://www.eliteappliance.com/by-brand/jen/JennAir.html
http://www.eliteappliance.com/Capital/CGRT484BB.php
https://www.eliteappliance.com/by-brand/tecnos/Superiore.html
https://www.eliteappliance.com/by-brand/kta/KitchenAid.html
https://www.eliteappliance.com/by-brand/vhood/Ventahood.html
http://www.eliteappliance.com/Jenn-Air/Jenn-Air-Appliance.php?ref=3620
http://www.eliteappliance.com/Electrolux/Electrolux-Appliance.php
https://www.eliteappliance.com/by-brand/ge/GE-Appliances.html
https://www.eliteappliance.com/locations.html
https://www.eliteappliance.com/by-brand/mie/Miele.html
http://www.eliteappliance.com/Sharp/R121.php
https://www.eliteappliance.com/SHIPPING_AND_DELIVERY.HTML
http://www.eliteappliance.com/Northstar/1955A.php
https://www.eliteappliance.com/products/Big-Green-Egg/bge/eggcelerator.html
https://www.eliteappliance.com/reviews/review_showcase.html
https://www.eliteappliance.com/miscellaneous-in-dallas-tx.html
http://www.eliteappliance.com/RCS/RWD1.php
https://www.eliteappliance.com/by-brand/ber/Bertazzoni.html
https://www.eliteappliance.com/products/Hoshizaki/hzk/dcm500bahos.html
http://www.eliteappliance.com/Whirlpool/WRT541SZD.php
http://www.eliteappliance.com/Frigidaire/FGGF3060SF.php
https://www.eliteappliance.com/by-brand/lnx/Lynx.html
https://www.eliteappliance.com/by-brand/shp/Sharp.html
http://www.eliteappliance.com/Electrolux/RH30WC55GS.php
https://www.eliteappliance.com/by-brand/frig/Frigidaire.html
https://www.eliteappliance.com/products/Miele/mie/eba6708.html
https://www.eliteappliance.com/products/Zephyr/zep/ak7036bs.html
https://www.eliteappliance.com/products/elica/ech623s1.html
http://www.eliteappliance.com/Zephyr/Zephyr-Appliance.php?x=t
http://www.eliteappliance.com/KitchenAid/KBHS109S.php
https://www.eliteappliance.com/products/Miele/mie/hgbb71.html
https://www.eliteappliance.com/products/Fisher-and-Paykel/fpk/479595.html
http://www.eliteappliance.com/Miele/HR1956DFGD.php
https://www.eliteappliance.com/products/GE-Appliances/ge/jk3800shss.html
https://www.eliteappliance.com/by-brand/whirl/Whirlpool.html
https://www.eliteappliance.com/by-brand/fab/Faber.html
https://www.eliteappliance.com/by-brand/dcs/DCS.html
http://www.eliteappliance.com/_CGI/MODEL?KEY=CAPITAL:GSCR486G
http://www.eliteappliance.com/Zephyr/ASLE42ASX.php
https://www.eliteappliance.com/products/Viking/vik/tcb12fcu15.html
http://www.eliteappliance.com/Samsung/RF265BEAES.php
https://www.eliteappliance.com/about_us.html
https://www.eliteappliance.com/by-brand/rcs/Renaissance-Cooking-Systems.html
https://www.eliteappliance.com/products/Miele/mie/wtv407.html
https://www.eliteappliance.com/products/samsung/nk36m9600wm.html
https://www.eliteappliance.com/ge-appliances-in-dallas-tx.html
https://www.eliteappliance.com/by-brand/ver/Verona.html
http://www.eliteappliance.com/Viking/VGIC5304B.php
http://www.eliteappliance.com/Capital/GSCR484BG.php
https://www.eliteappliance.com/products/Frigidaire/frig/fggf3036td.html
https://www.eliteappliance.com/products/Miele/mie/km6365.html
https://www.eliteappliance.com/products/monogram/zet1fhss.html
https://www.eliteappliance.com/products/Miele/mie/hfc71.html
https://www.eliteappliance.com/products/Viking/vik/hcb.html
http://www.eliteappliance.com/Sharp/KB-6524PK.php
https://www.eliteappliance.com/products/Miele/mie/classicc1puresuctionpowerlinesban0.html
https://www.eliteappliance.com/products/JennAir/jen/jud24fcers.html
https://www.eliteappliance.com/products/Danby/dan/dbc039a1bdb.html
https://www.eliteappliance.com/products/Miele/mie/kwf1000.html
https://www.eliteappliance.com/by-brand/aga/AGA.html
http://www.eliteappliance.com/Electrolux/RH36PC60GS.php
http://www.eliteappliance.com/Vent-A-Hood/PWVH30242.php
http://www.eliteappliance.com/_CGI/MODEL?KEY=VIK:RVGR33025BWHLP
http://www.eliteappliance.com/Capital/P36SHB.php
http://www.eliteappliance.com/Smeg/PGFU30X.php
https://www.eliteappliance.com/by-brand/bge/Big-Green-Egg.html
https://www.eliteappliance.com/products/Viking/vik/vbcv54838.html
https://www.eliteappliance.com/products/Viking/vik/cfor.html
https://www.eliteappliance.com/products/capital/mcor304.html
https://www.eliteappliance.com/products/Bertazzoni/ber/mast365gasxe.html
http://www.eliteappliance.com/KitchenAid/KUCS03FTPA.php
https://www.eliteappliance.com/products/Miele/mie/km3475g.html
https://www.eliteappliance.com/products/JennAir/jen/jgc3536gs.html
https://www.eliteappliance.com/products/Hoshizaki/hzk/dcm270bah.html
https://www.eliteappliance.com/products/samsung/we302nw.html
https://www.eliteappliance.com/products/Miele/mie/da7000dauraamblack.html
http://www.eliteappliance.com/Verona/Verona-Appliance.php
http://www.eliteappliance.com/Viking/VQGFS5360NSS.php
https://www.eliteappliance.com/products/dcs/wrt24t.html
https://www.eliteappliance.com/products/Ventahood/vhood/bh134sldss.html
https://www.eliteappliance.com/products/Zephyr/zep/ak8400as.html
https://www.eliteappliance.com/products/Renaissance-Cooking-Systems/rcs/rmc28.html
https://www.eliteappliance.com/Northstar/1950QS.php
https://www.eliteappliance.com/products/Sharp/shp/smc1131cb.html
http://www.eliteappliance.com/TecnogasSuperiore/RD361GCNB.php
https://www.eliteappliance.com/products/Asko/ask/t411vdw.html
https://www.eliteappliance.com/products/Miele/mie/km2355g.html
https://www.eliteappliance.com/by-brand/lbr/Liebherr.html
https://www.eliteappliance.com/products/monogram/zv830smss.html
https://www.eliteappliance.com/products/Viking/vik/vcih53608ss.html
https://www.eliteappliance.com/products/Liebherr/lbr/ws1200.html
https://www.eliteappliance.com/products/Faber/fab/clas30ss.html
http://www.eliteappliance.com/Fisher&Paykel/RH361.php
https://www.eliteappliance.com/products/monogram/zwe23eshss.html
""
""https://www.eliteappliance.com/about_us.htmlhttps://www.eliteappliance.com/about_us.html
https://www.eliteappliance.com/by-brand/rcs/Renaissance-Cooking-Systems.html
https://www.eliteappliance.com/products/Miele/mie/wtv407.html
https://www.eliteappliance.com/products/samsung/nk36m9600wm.html
https://www.eliteappliance.com/ge-appliances-in-dallas-tx.html
https://www.eliteappliance.com/by-brand/ver/Verona.html
http://www.eliteappliance.com/Viking/VGIC5304B.php
http://www.eliteappliance.com/Capital/GSCR484BG.php
https://www.eliteappliance.com/products/Frigidaire/frig/fggf3036td.html
https://www.eliteappliance.com/products/Miele/mie/km6365.html
https://www.eliteappliance.com/products/monogram/zet1fhss.html
https://www.eliteappliance.com/products/Miele/mie/hfc71.html
https://www.eliteappliance.com/products/Viking/vik/hcb.html
http://www.eliteappliance.com/Sharp/KB-6524PK.php
https://www.eliteappliance.com/products/Miele/mie/classicc1puresuctionpowerlinesban0.html
https://www.eliteappliance.com/products/JennAir/jen/jud24fcers.html
https://www.eliteappliance.com/products/Danby/dan/dbc039a1bdb.html
https://www.eliteappliance.com/products/Miele/mie/kwf1000.html
https://www.eliteappliance.com/by-brand/aga/AGA.html
http://www.eliteappliance.com/Electrolux/RH36PC60GS.php
http://www.eliteappliance.com/Vent-A-Hood/PWVH30242.php
http://www.eliteappliance.com/_CGI/MODEL?KEY=VIK:RVGR33025BWHLP
http://www.eliteappliance.com/Capital/P36SHB.php
http://www.eliteappliance.com/Smeg/PGFU30X.php
https://www.eliteappliance.com/by-brand/bge/Big-Green-Egg.html
https://www.eliteappliance.com/products/Viking/vik/vbcv54838.html
https://www.eliteappliance.com/products/Viking/vik/cfor.html
https://www.eliteappliance.com/products/capital/mcor304.html
https://www.eliteappliance.com/products/Bertazzoni/ber/mast365gasxe.html
http://www.eliteappliance.com/KitchenAid/KUCS03FTPA.php
https://www.eliteappliance.com/products/Miele/mie/km3475g.html
https://www.eliteappliance.com/products/JennAir/jen/jgc3536gs.html
https://www.eliteappliance.com/products/Hoshizaki/hzk/dcm270bah.html
https://www.eliteappliance.com/products/samsung/we302nw.html
https://www.eliteappliance.com/products/Miele/mie/da7000dauraamblack.html
http://www.eliteappliance.com/Verona/Verona-Appliance.php
http://www.eliteappliance.com/Viking/VQGFS5360NSS.php
https://www.eliteappliance.com/products/dcs/wrt24t.html
https://www.eliteappliance.com/products/Ventahood/vhood/bh134sldss.html
https://www.eliteappliance.com/products/Zephyr/zep/ak8400as.html
https://www.eliteappliance.com/products/Renaissance-Cooking-Systems/rcs/rmc28.html
https://www.eliteappliance.com/Northstar/1950QS.php
https://www.eliteappliance.com/products/Sharp/shp/smc1131cb.html
http://www.eliteappliance.com/TecnogasSuperiore/RD361GCNB.php
https://www.eliteappliance.com/products/Asko/ask/t411vdw.html
https://www.eliteappliance.com/products/Miele/mie/km2355g.html
https://www.eliteappliance.com/by-brand/lbr/Liebherr.html
https://www.eliteappliance.com/products/monogram/zv830smss.html
https://www.eliteappliance.com/products/Viking/vik/vcih53608ss.html
https://www.eliteappliance.com/products/Liebherr/lbr/ws1200.html
https://www.eliteappliance.com/products/Faber/fab/clas30ss.html
http://www.eliteappliance.com/Fisher&Paykel/RH361.php
https://www.eliteappliance.com/products/monogram/zwe23eshss.html
""
""https://www.eliteappliance.com/by-brand/rcs/Renaissance-Cooking-Systems.html
http://www.eliteappliance.com/TecnogasSuperiore/RD361GCNB.php
https://www.eliteappliance.com/products/Asko/ask/t411vdw.html
https://www.eliteappliance.com/products/Miele/mie/km2355g.html
https://www.eliteappliance.com/by-brand/lbr/Liebherr.html
https://www.eliteappliance.com/products/monogram/zv830smss.html
https://www.eliteappliance.com/products/Viking/vik/vcih53608ss.html
https://www.eliteappliance.com/products/Liebherr/lbr/ws1200.html
https://www.eliteappliance.com/products/Faber/fab/clas30ss.html
http://www.eliteappliance.com/Fisher&Paykel/RH361.php
https://www.eliteappliance.com/products/monogram/zwe23eshss.html
""
""https://www.eliteappliance.com/products/monogram/zwe23eshss.html

URL
https://www.eliteappliance.com/
https://www.eliteappliance.com/by-brand/vik/Viking.html
http://www.eliteappliance.com/Bertazzoni/Bertazzoni-Appliance.php
https://www.eliteappliance.com/landing_all_brands.html
http://www.eliteappliance.com/Northstar/Northstar-Appliance.php?x=t
https://www.eliteappliance.com/locations/ELITE.html
https://www.eliteappliance.com/by-brand/jen/JennAir.html
http://www.eliteappliance.com/Capital/CGRT484BB.php
https://www.eliteappliance.com/by-brand/tecnos/Superiore.html
https://www.eliteappliance.com/by-brand/kta/KitchenAid.html
https://www.eliteappliance.com/by-brand/vhood/Ventahood.html
http://www.eliteappliance.com/Jenn-Air/Jenn-Air-Appliance.php?ref=3620
http://www.eliteappliance.com/Electrolux/Electrolux-Appliance.php
https://www.eliteappliance.com/by-brand/ge/GE-Appliances.html
https://www.eliteappliance.com/locations.html
https://www.eliteappliance.com/by-brand/mie/Miele.html
http://www.eliteappliance.com/Sharp/R121.php
https://www.eliteappliance.com/SHIPPING_AND_DELIVERY.HTML
http://www.eliteappliance.com/Northstar/1955A.php
https://www.eliteappliance.com/products/Big-Green-Egg/bge/eggcelerator.html
https://www.eliteappliance.com/reviews/review_showcase.html
https://www.eliteappliance.com/miscellaneous-in-dallas-tx.html
http://www.eliteappliance.com/RCS/RWD1.php
https://www.eliteappliance.com/by-brand/ber/Bertazzoni.html
https://www.eliteappliance.com/products/Hoshizaki/hzk/dcm500bahos.html
http://www.eliteappliance.com/Whirlpool/WRT541SZD.php
http://www.eliteappliance.com/Frigidaire/FGGF3060SF.php
https://www.eliteappliance.com/by-brand/lnx/Lynx.html
https://www.eliteappliance.com/by-brand/shp/Sharp.html
http://www.eliteappliance.com/Electrolux/RH30WC55GS.php
https://www.eliteappliance.com/by-brand/frig/Frigidaire.html
https://www.eliteappliance.com/products/Miele/mie/eba6708.html
https://www.eliteappliance.com/products/Zephyr/zep/ak7036bs.html
https://www.eliteappliance.com/products/elica/ech623s1.html
http://www.eliteappliance.com/Zephyr/Zephyr-Appliance.php?x=t
http://www.eliteappliance.com/KitchenAid/KBHS109S.php
https://www.eliteappliance.com/products/Miele/mie/hgbb71.html
https://www.eliteappliance.com/products/Fisher-and-Paykel/fpk/479595.html
http://www.eliteappliance.com/Miele/HR1956DFGD.php
https://www.eliteappliance.com/products/GE-Appliances/ge/jk3800shss.html
https://www.eliteappliance.com/by-brand/whirl/Whirlpool.html
https://www.eliteappliance.com/by-brand/fab/Faber.html
https://www.eliteappliance.com/by-brand/dcs/DCS.html
http://www.eliteappliance.com/_CGI/MODEL?KEY=CAPITAL:GSCR486G
http://www.eliteappliance.com/Zephyr/ASLE42ASX.php
https://www.eliteappliance.com/products/Viking/vik/tcb12fcu15.html
""http://www.eliteappliance.com/Samsung/RF265BEAES.php
URL
https://www.eliteappliance.com/
https://www.eliteappliance.com/by-brand/vik/Viking.html
http://www.eliteappliance.com/Bertazzoni/Bertazzoni-Appliance.php
https://www.eliteappliance.com/landing_all_brands.html
http://www.eliteappliance.com/Northstar/Northstar-Appliance.php?x=t
https://www.eliteappliance.com/locations/ELITE.html
https://www.eliteappliance.com/by-brand/jen/JennAir.html
http://www.eliteappliance.com/Capital/CGRT484BB.php
https://www.eliteappliance.com/by-brand/tecnos/Superiore.html
https://www.eliteappliance.com/by-brand/kta/KitchenAid.html
https://www.eliteappliance.com/by-brand/vhood/Ventahood.html
http://www.eliteappliance.com/Jenn-Air/Jenn-Air-Appliance.php?ref=3620
http://www.eliteappliance.com/Electrolux/Electrolux-Appliance.php
https://www.eliteappliance.com/by-brand/ge/GE-Appliances.html
https://www.eliteappliance.com/locations.html
https://www.eliteappliance.com/by-brand/mie/Miele.html
http://www.eliteappliance.com/Sharp/R121.php
https://www.eliteappliance.com/SHIPPING_AND_DELIVERY.HTML
http://www.eliteappliance.com/Northstar/1955A.php
https://www.eliteappliance.com/products/Big-Green-Egg/bge/eggcelerator.html
https://www.eliteappliance.com/reviews/review_showcase.html
https://www.eliteappliance.com/miscellaneous-in-dallas-tx.html
http://www.eliteappliance.com/RCS/RWD1.php
https://www.eliteappliance.com/by-brand/ber/Bertazzoni.html
https://www.eliteappliance.com/products/Hoshizaki/hzk/dcm500bahos.html
http://www.eliteappliance.com/Whirlpool/WRT541SZD.php
http://www.eliteappliance.com/Frigidaire/FGGF3060SF.php
https://www.eliteappliance.com/by-brand/lnx/Lynx.html
https://www.eliteappliance.com/by-brand/shp/Sharp.html
http://www.eliteappliance.com/Electrolux/RH30WC55GS.php
https://www.eliteappliance.com/by-brand/frig/Frigidaire.html
https://www.eliteappliance.com/products/Miele/mie/eba6708.html
https://www.eliteappliance.com/products/Zephyr/zep/ak7036bs.html
https://www.eliteappliance.com/products/elica/ech623s1.html
http://www.eliteappliance.com/Zephyr/Zephyr-Appliance.php?x=t
http://www.eliteappliance.com/KitchenAid/KBHS109S.php
https://www.eliteappliance.com/products/Miele/mie/hgbb71.html
https://www.eliteappliance.com/products/Fisher-and-Paykel/fpk/479595.html
http://www.eliteappliance.com/Miele/HR1956DFGD.php
https://www.eliteappliance.com/products/GE-Appliances/ge/jk3800shss.html
https://www.eliteappliance.com/by-brand/whirl/Whirlpool.html
https://www.eliteappliance.com/by-brand/fab/Faber.html
https://www.eliteappliance.com/by-brand/dcs/DCS.html
http://www.eliteappliance.com/_CGI/MODEL?KEY=CAPITAL:GSCR486G
http://www.eliteappliance.com/Zephyr/ASLE42ASX.php
https://www.eliteappliance.com/products/Viking/vik/tcb12fcu15.html
http://www.eliteappliance.com/Samsung/RF265BEAES.php
https://www.eliteappliance.com/about_us.html
https://www.eliteappliance.com/by-brand/rcs/Renaissance-Cooking-Systems.html
https://www.eliteappliance.com/products/Miele/mie/wtv407.html
https://www.eliteappliance.com/products/samsung/nk36m9600wm.html
https://www.eliteappliance.com/ge-appliances-in-dallas-tx.html
https://www.eliteappliance.com/by-brand/ver/Verona.html
http://www.eliteappliance.com/Viking/VGIC5304B.php
http://www.eliteappliance.com/Capital/GSCR484BG.php
https://www.eliteappliance.com/products/Frigidaire/frig/fggf3036td.html
https://www.eliteappliance.com/products/Miele/mie/km6365.html
https://www.eliteappliance.com/products/monogram/zet1fhss.html
https://www.eliteappliance.com/products/Miele/mie/hfc71.html
https://www.eliteappliance.com/products/Viking/vik/hcb.html
http://www.eliteappliance.com/Sharp/KB-6524PK.php
https://www.eliteappliance.com/products/Miele/mie/classicc1puresuctionpowerlinesban0.html
https://www.eliteappliance.com/products/JennAir/jen/jud24fcers.html
https://www.eliteappliance.com/products/Danby/dan/dbc039a1bdb.html
https://www.eliteappliance.com/products/Miele/mie/kwf1000.html
https://www.eliteappliance.com/by-brand/aga/AGA.html
http://www.eliteappliance.com/Electrolux/RH36PC60GS.php
http://www.eliteappliance.com/Vent-A-Hood/PWVH30242.php
http://www.eliteappliance.com/_CGI/MODEL?KEY=VIK:RVGR33025BWHLP
http://www.eliteappliance.com/Capital/P36SHB.php
http://www.eliteappliance.com/Smeg/PGFU30X.php
https://www.eliteappliance.com/by-brand/bge/Big-Green-Egg.html
https://www.eliteappliance.com/products/Viking/vik/vbcv54838.html
https://www.eliteappliance.com/products/Viking/vik/cfor.html
https://www.eliteappliance.com/products/capital/mcor304.html
https://www.eliteappliance.com/products/Bertazzoni/ber/mast365gasxe.html
http://www.eliteappliance.com/KitchenAid/KUCS03FTPA.php
https://www.eliteappliance.com/products/Miele/mie/km3475g.html
https://www.eliteappliance.com/products/JennAir/jen/jgc3536gs.html
https://www.eliteappliance.com/products/Hoshizaki/hzk/dcm270bah.html
https://www.eliteappliance.com/products/samsung/we302nw.html
https://www.eliteappliance.com/products/Miele/mie/da7000dauraamblack.html
http://www.eliteappliance.com/Verona/Verona-Appliance.php
http://www.eliteappliance.com/Viking/VQGFS5360NSS.php
https://www.eliteappliance.com/products/dcs/wrt24t.html
https://www.eliteappliance.com/products/Ventahood/vhood/bh134sldss.html
https://www.eliteappliance.com/products/Zephyr/zep/ak8400as.html
https://www.eliteappliance.com/products/Renaissance-Cooking-Systems/rcs/rmc28.html
https://www.eliteappliance.com/Northstar/1950QS.php
https://www.eliteappliance.com/products/Sharp/shp/smc1131cb.html
http://www.eliteappliance.com/TecnogasSuperiore/RD361GCNB.php
https://www.eliteappliance.com/products/Asko/ask/t411vdw.html
https://www.eliteappliance.com/products/Miele/mie/km2355g.html
https://www.eliteappliance.com/by-brand/lbr/Liebherr.html
https://www.eliteappliance.com/products/monogram/zv830smss.html
https://www.eliteappliance.com/products/Viking/vik/vcih53608ss.html
https://www.eliteappliance.com/products/Liebherr/lbr/ws1200.html
https://www.eliteappliance.com/products/Faber/fab/clas30ss.html
http://www.eliteappliance.com/Fisher&Paykel/RH361.php
https://www.eliteappliance.com/products/monogram/zwe23eshss.html
""
""https://www.eliteappliance.com/about_us.htmlhttps://www.eliteappliance.com/about_us.html
https://www.eliteappliance.com/by-brand/rcs/Renaissance-Cooking-Systems.html
https://www.eliteappliance.com/products/Miele/mie/wtv407.html
https://www.eliteappliance.com/products/samsung/nk36m9600wm.html
https://www.eliteappliance.com/ge-appliances-in-dallas-tx.html
https://www.eliteappliance.com/by-brand/ver/Verona.html
http://www.eliteappliance.com/Viking/VGIC5304B.php
http://www.eliteappliance.com/Capital/GSCR484BG.php
https://www.eliteappliance.com/products/Frigidaire/frig/fggf3036td.html
https://www.eliteappliance.com/products/Miele/mie/km6365.html
https://www.eliteappliance.com/products/monogram/zet1fhss.html
https://www.eliteappliance.com/products/Miele/mie/hfc71.html
https://www.eliteappliance.com/products/Viking/vik/hcb.html
http://www.eliteappliance.com/Sharp/KB-6524PK.php
https://www.eliteappliance.com/products/Miele/mie/classicc1puresuctionpowerlinesban0.html
https://www.eliteappliance.com/products/JennAir/jen/jud24fcers.html
https://www.eliteappliance.com/products/Danby/dan/dbc039a1bdb.html
https://www.eliteappliance.com/products/Miele/mie/kwf1000.html
https://www.eliteappliance.com/by-brand/aga/AGA.html
http://www.eliteappliance.com/Electrolux/RH36PC60GS.php
http://www.eliteappliance.com/Vent-A-Hood/PWVH30242.php
http://www.eliteappliance.com/_CGI/MODEL?KEY=VIK:RVGR33025BWHLP
http://www.eliteappliance.com/Capital/P36SHB.php
http://www.eliteappliance.com/Smeg/PGFU30X.php
https://www.eliteappliance.com/by-brand/bge/Big-Green-Egg.html
https://www.eliteappliance.com/products/Viking/vik/vbcv54838.html
https://www.eliteappliance.com/products/Viking/vik/cfor.html
https://www.eliteappliance.com/products/capital/mcor304.html
https://www.eliteappliance.com/products/Bertazzoni/ber/mast365gasxe.html
http://www.eliteappliance.com/KitchenAid/KUCS03FTPA.php
https://www.eliteappliance.com/products/Miele/mie/km3475g.html
https://www.eliteappliance.com/products/JennAir/jen/jgc3536gs.html
https://www.eliteappliance.com/products/Hoshizaki/hzk/dcm270bah.html
https://www.eliteappliance.com/products/samsung/we302nw.html
https://www.eliteappliance.com/products/Miele/mie/da7000dauraamblack.html
http://www.eliteappliance.com/Verona/Verona-Appliance.php
http://www.eliteappliance.com/Viking/VQGFS5360NSS.php
https://www.eliteappliance.com/products/dcs/wrt24t.html
https://www.eliteappliance.com/products/Ventahood/vhood/bh134sldss.html
https://www.eliteappliance.com/products/Zephyr/zep/ak8400as.html
https://www.eliteappliance.com/products/Renaissance-Cooking-Systems/rcs/rmc28.html
https://www.eliteappliance.com/Northstar/1950QS.php
https://www.eliteappliance.com/products/Sharp/shp/smc1131cb.html
http://www.eliteappliance.com/TecnogasSuperiore/RD361GCNB.php
https://www.eliteappliance.com/products/Asko/ask/t411vdw.html
https://www.eliteappliance.com/products/Miele/mie/km2355g.html
https://www.eliteappliance.com/by-brand/lbr/Liebherr.html
https://www.eliteappliance.com/products/monogram/zv830smss.html
https://www.eliteappliance.com/products/Viking/vik/vcih53608ss.html
https://www.eliteappliance.com/products/Liebherr/lbr/ws1200.html
https://www.eliteappliance.com/products/Faber/fab/clas30ss.html
http://www.eliteappliance.com/Fisher&Paykel/RH361.php
https://www.eliteappliance.com/products/monogram/zwe23eshss.html
""
""https://www.eliteappliance.com/by-brand/rcs/Renaissance-Cooking-Systems.html
http://www.eliteappliance.com/TecnogasSuperiore/RD361GCNB.php
https://www.eliteappliance.com/products/Asko/ask/t411vdw.html
https://www.eliteappliance.com/products/Miele/mie/km2355g.html
https://www.eliteappliance.com/by-brand/lbr/Liebherr.html
https://www.eliteappliance.com/products/monogram/zv830smss.html
https://www.eliteappliance.com/products/Viking/vik/vcih53608ss.html
https://www.eliteappliance.com/products/Liebherr/lbr/ws1200.html
https://www.eliteappliance.com/products/Faber/fab/clas30ss.html
http://www.eliteappliance.com/Fisher&Paykel/RH361.php
https://www.eliteappliance.com/products/monogram/zwe23eshss.html
""
""https://www.eliteappliance.com/products/monogram/zwe23eshss.html
""URL
https://www.eliteappliance.com/
https://www.eliteappliance.com/by-brand/vik/Viking.html
http://www.eliteappliance.com/Bertazzoni/Bertazzoni-Appliance.php
https://www.eliteappliance.com/landing_all_brands.html
http://www.eliteappliance.com/Northstar/Northstar-Appliance.php?x=t
https://www.eliteappliance.com/locations/ELITE.html
https://www.eliteappliance.com/by-brand/jen/JennAir.html
http://www.eliteappliance.com/Capital/CGRT484BB.php
https://www.eliteappliance.com/by-brand/tecnos/Superiore.html
https://www.eliteappliance.com/by-brand/kta/KitchenAid.html
https://www.eliteappliance.com/by-brand/vhood/Ventahood.html
http://www.eliteappliance.com/Jenn-Air/Jenn-Air-Appliance.php?ref=3620
http://www.eliteappliance.com/Electrolux/Electrolux-Appliance.php
https://www.eliteappliance.com/by-brand/ge/GE-Appliances.html
https://www.eliteappliance.com/locations.html
https://www.eliteappliance.com/by-brand/mie/Miele.html
http://www.eliteappliance.com/Sharp/R121.php
https://www.eliteappliance.com/SHIPPING_AND_DELIVERY.HTML
http://www.eliteappliance.com/Northstar/1955A.php
https://www.eliteappliance.com/products/Big-Green-Egg/bge/eggcelerator.html
https://www.eliteappliance.com/reviews/review_showcase.html
https://www.eliteappliance.com/miscellaneous-in-dallas-tx.html
http://www.eliteappliance.com/RCS/RWD1.php
https://www.eliteappliance.com/by-brand/ber/Bertazzoni.html
https://www.eliteappliance.com/products/Hoshizaki/hzk/dcm500bahos.html
http://www.eliteappliance.com/Whirlpool/WRT541SZD.php
http://www.eliteappliance.com/Frigidaire/FGGF3060SF.php
https://www.eliteappliance.com/by-brand/lnx/Lynx.html
https://www.eliteappliance.com/by-brand/shp/Sharp.html
http://www.eliteappliance.com/Electrolux/RH30WC55GS.php
https://www.eliteappliance.com/by-brand/frig/Frigidaire.html
https://www.eliteappliance.com/products/Miele/mie/eba6708.html
https://www.eliteappliance.com/products/Zephyr/zep/ak7036bs.html
https://www.eliteappliance.com/products/elica/ech623s1.html
http://www.eliteappliance.com/Zephyr/Zephyr-Appliance.php?x=t
http://www.eliteappliance.com/KitchenAid/KBHS109S.php
https://www.eliteappliance.com/products/Miele/mie/hgbb71.html
https://www.eliteappliance.com/products/Fisher-and-Paykel/fpk/479595.html
http://www.eliteappliance.com/Miele/HR1956DFGD.php
https://www.eliteappliance.com/products/GE-Appliances/ge/jk3800shss.html
https://www.eliteappliance.com/by-brand/whirl/Whirlpool.html
https://www.eliteappliance.com/by-brand/fab/Faber.html
https://www.eliteappliance.com/by-brand/dcs/DCS.html
http://www.eliteappliance.com/_CGI/MODEL?KEY=CAPITAL:GSCR486G
http://www.eliteappliance.com/Zephyr/ASLE42ASX.php
https://www.eliteappliance.com/products/Viking/vik/tcb12fcu15.html
""http://www.eliteappliance.com/Samsung/RF265BEAES.php
URL
https://www.eliteappliance.com/
https://www.eliteappliance.com/by-brand/vik/Viking.html
http://www.eliteappliance.com/Bertazzoni/Bertazzoni-Appliance.php
https://www.eliteappliance.com/landing_all_brands.html
http://www.eliteappliance.com/Northstar/Northstar-Appliance.php?x=t
https://www.eliteappliance.com/locations/ELITE.html
https://www.eliteappliance.com/by-brand/jen/JennAir.html
http://www.eliteappliance.com/Capital/CGRT484BB.php
https://www.eliteappliance.com/by-brand/tecnos/Superiore.html
https://www.eliteappliance.com/by-brand/kta/KitchenAid.html
https://www.eliteappliance.com/by-brand/vhood/Ventahood.html
http://www.eliteappliance.com/Jenn-Air/Jenn-Air-Appliance.php?ref=3620
http://www.eliteappliance.com/Electrolux/Electrolux-Appliance.php
https://www.eliteappliance.com/by-brand/ge/GE-Appliances.html
https://www.eliteappliance.com/locations.html
https://www.eliteappliance.com/by-brand/mie/Miele.html
http://www.eliteappliance.com/Sharp/R121.php
https://www.eliteappliance.com/SHIPPING_AND_DELIVERY.HTML
http://www.eliteappliance.com/Northstar/1955A.php
https://www.eliteappliance.com/products/Big-Green-Egg/bge/eggcelerator.html
https://www.eliteappliance.com/reviews/review_showcase.html
https://www.eliteappliance.com/miscellaneous-in-dallas-tx.html
http://www.eliteappliance.com/RCS/RWD1.php
https://www.eliteappliance.com/by-brand/ber/Bertazzoni.html
https://www.eliteappliance.com/products/Hoshizaki/hzk/dcm500bahos.html
http://www.eliteappliance.com/Whirlpool/WRT541SZD.php
http://www.eliteappliance.com/Frigidaire/FGGF3060SF.php
https://www.eliteappliance.com/by-brand/lnx/Lynx.html
https://www.eliteappliance.com/by-brand/shp/Sharp.html
http://www.eliteappliance.com/Electrolux/RH30WC55GS.php
https://www.eliteappliance.com/by-brand/frig/Frigidaire.html
https://www.eliteappliance.com/products/Miele/mie/eba6708.html
https://www.eliteappliance.com/products/Zephyr/zep/ak7036bs.html
https://www.eliteappliance.com/products/elica/ech623s1.html
http://www.eliteappliance.com/Zephyr/Zephyr-Appliance.php?x=t
http://www.eliteappliance.com/KitchenAid/KBHS109S.php
https://www.eliteappliance.com/products/Miele/mie/hgbb71.html
https://www.eliteappliance.com/products/Fisher-and-Paykel/fpk/479595.html
http://www.eliteappliance.com/Miele/HR1956DFGD.php
https://www.eliteappliance.com/products/GE-Appliances/ge/jk3800shss.html
https://www.eliteappliance.com/by-brand/whirl/Whirlpool.html
https://www.eliteappliance.com/by-brand/fab/Faber.html
https://www.eliteappliance.com/by-brand/dcs/DCS.html
http://www.eliteappliance.com/_CGI/MODEL?KEY=CAPITAL:GSCR486G
http://www.eliteappliance.com/Zephyr/ASLE42ASX.php
https://www.eliteappliance.com/products/Viking/vik/tcb12fcu15.html
http://www.eliteappliance.com/Samsung/RF265BEAES.php
https://www.eliteappliance.com/about_us.html
https://www.eliteappliance.com/by-brand/rcs/Renaissance-Cooking-Systems.html
https://www.eliteappliance.com/products/Miele/mie/wtv407.html
https://www.eliteappliance.com/products/samsung/nk36m9600wm.html
https://www.eliteappliance.com/ge-appliances-in-dallas-tx.html
https://www.eliteappliance.com/by-brand/ver/Verona.html
http://www.eliteappliance.com/Viking/VGIC5304B.php
http://www.eliteappliance.com/Capital/GSCR484BG.php
https://www.eliteappliance.com/products/Frigidaire/frig/fggf3036td.html
https://www.eliteappliance.com/products/Miele/mie/km6365.html
https://www.eliteappliance.com/products/monogram/zet1fhss.html
https://www.eliteappliance.com/products/Miele/mie/hfc71.html
https://www.eliteappliance.com/products/Viking/vik/hcb.html
http://www.eliteappliance.com/Sharp/KB-6524PK.php
https://www.eliteappliance.com/products/Miele/mie/classicc1puresuctionpowerlinesban0.html
https://www.eliteappliance.com/products/JennAir/jen/jud24fcers.html
https://www.eliteappliance.com/products/Danby/dan/dbc039a1bdb.html
https://www.eliteappliance.com/products/Miele/mie/kwf1000.html
https://www.eliteappliance.com/by-brand/aga/AGA.html
http://www.eliteappliance.com/Electrolux/RH36PC60GS.php
http://www.eliteappliance.com/Vent-A-Hood/PWVH30242.php
http://www.eliteappliance.com/_CGI/MODEL?KEY=VIK:RVGR33025BWHLP
http://www.eliteappliance.com/Capital/P36SHB.php
http://www.eliteappliance.com/Smeg/PGFU30X.php
https://www.eliteappliance.com/by-brand/bge/Big-Green-Egg.html
https://www.eliteappliance.com/products/Viking/vik/vbcv54838.html
https://www.eliteappliance.com/products/Viking/vik/cfor.html
https://www.eliteappliance.com/products/capital/mcor304.html
https://www.eliteappliance.com/products/Bertazzoni/ber/mast365gasxe.html
http://www.eliteappliance.com/KitchenAid/KUCS03FTPA.php
https://www.eliteappliance.com/products/Miele/mie/km3475g.html
https://www.eliteappliance.com/products/JennAir/jen/jgc3536gs.html
https://www.eliteappliance.com/products/Hoshizaki/hzk/dcm270bah.html
https://www.eliteappliance.com/products/samsung/we302nw.html
https://www.eliteappliance.com/products/Miele/mie/da7000dauraamblack.html
http://www.eliteappliance.com/Verona/Verona-Appliance.php
http://www.eliteappliance.com/Viking/VQGFS5360NSS.php
https://www.eliteappliance.com/products/dcs/wrt24t.html
https://www.eliteappliance.com/products/Ventahood/vhood/bh134sldss.html
https://www.eliteappliance.com/products/Zephyr/zep/ak8400as.html
https://www.eliteappliance.com/products/Renaissance-Cooking-Systems/rcs/rmc28.html
https://www.eliteappliance.com/Northstar/1950QS.php
https://www.eliteappliance.com/products/Sharp/shp/smc1131cb.html
http://www.eliteappliance.com/TecnogasSuperiore/RD361GCNB.php
https://www.eliteappliance.com/products/Asko/ask/t411vdw.html
https://www.eliteappliance.com/products/Miele/mie/km2355g.html
https://www.eliteappliance.com/by-brand/lbr/Liebherr.html
https://www.eliteappliance.com/products/monogram/zv830smss.html
https://www.eliteappliance.com/products/Viking/vik/vcih53608ss.html
https://www.eliteappliance.com/products/Liebherr/lbr/ws1200.html
https://www.eliteappliance.com/products/Faber/fab/clas30ss.html
http://www.eliteappliance.com/Fisher&Paykel/RH361.php
https://www.eliteappliance.com/products/monogram/zwe23eshss.html
""
""https://www.eliteappliance.com/about_us.htmlhttps://www.eliteappliance.com/about_us.html
https://www.eliteappliance.com/by-brand/rcs/Renaissance-Cooking-Systems.html
https://www.eliteappliance.com/products/Miele/mie/wtv407.html
https://www.eliteappliance.com/products/samsung/nk36m9600wm.html
https://www.eliteappliance.com/ge-appliances-in-dallas-tx.html
https://www.eliteappliance.com/by-brand/ver/Verona.html
http://www.eliteappliance.com/Viking/VGIC5304B.php
http://www.eliteappliance.com/Capital/GSCR484BG.php
https://www.eliteappliance.com/products/Frigidaire/frig/fggf3036td.html
https://www.eliteappliance.com/products/Miele/mie/km6365.html
https://www.eliteappliance.com/products/monogram/zet1fhss.html
https://www.eliteappliance.com/products/Miele/mie/hfc71.html
https://www.eliteappliance.com/products/Viking/vik/hcb.html
http://www.eliteappliance.com/Sharp/KB-6524PK.php
https://www.eliteappliance.com/products/Miele/mie/classicc1puresuctionpowerlinesban0.html
https://www.eliteappliance.com/products/JennAir/jen/jud24fcers.html
https://www.eliteappliance.com/products/Danby/dan/dbc039a1bdb.html
https://www.eliteappliance.com/products/Miele/mie/kwf1000.html
https://www.eliteappliance.com/by-brand/aga/AGA.html
http://www.eliteappliance.com/Electrolux/RH36PC60GS.php
http://www.eliteappliance.com/Vent-A-Hood/PWVH30242.php
http://www.eliteappliance.com/_CGI/MODEL?KEY=VIK:RVGR33025BWHLP
http://www.eliteappliance.com/Capital/P36SHB.php
http://www.eliteappliance.com/Smeg/PGFU30X.php
https://www.eliteappliance.com/by-brand/bge/Big-Green-Egg.html
https://www.eliteappliance.com/products/Viking/vik/vbcv54838.html
https://www.eliteappliance.com/products/Viking/vik/cfor.html
https://www.eliteappliance.com/products/capital/mcor304.html
https://www.eliteappliance.com/products/Bertazzoni/ber/mast365gasxe.html
http://www.eliteappliance.com/KitchenAid/KUCS03FTPA.php
https://www.eliteappliance.com/products/Miele/mie/km3475g.html
https://www.eliteappliance.com/products/JennAir/jen/jgc3536gs.html
https://www.eliteappliance.com/products/Hoshizaki/hzk/dcm270bah.html
https://www.eliteappliance.com/products/samsung/we302nw.html
https://www.eliteappliance.com/products/Miele/mie/da7000dauraamblack.html
http://www.eliteappliance.com/Verona/Verona-Appliance.php
http://www.eliteappliance.com/Viking/VQGFS5360NSS.php
https://www.eliteappliance.com/products/dcs/wrt24t.html
https://www.eliteappliance.com/products/Ventahood/vhood/bh134sldss.html
https://www.eliteappliance.com/products/Zephyr/zep/ak8400as.html
https://www.eliteappliance.com/products/Renaissance-Cooking-Systems/rcs/rmc28.html
https://www.eliteappliance.com/Northstar/1950QS.php
https://www.eliteappliance.com/products/Sharp/shp/smc1131cb.html
http://www.eliteappliance.com/TecnogasSuperiore/RD361GCNB.php
https://www.eliteappliance.com/products/Asko/ask/t411vdw.html
https://www.eliteappliance.com/products/Miele/mie/km2355g.html
https://www.eliteappliance.com/by-brand/lbr/Liebherr.html
https://www.eliteappliance.com/products/monogram/zv830smss.html
https://www.eliteappliance.com/products/Viking/vik/vcih53608ss.html
https://www.eliteappliance.com/products/Liebherr/lbr/ws1200.html
https://www.eliteappliance.com/products/Faber/fab/clas30ss.html
http://www.eliteappliance.com/Fisher&Paykel/RH361.php
https://www.eliteappliance.com/products/monogram/zwe23eshss.html
""
""https://www.eliteappliance.com/by-brand/rcs/Renaissance-Cooking-Systems.html
http://www.eliteappliance.com/TecnogasSuperiore/RD361GCNB.php
https://www.eliteappliance.com/products/Asko/ask/t411vdw.html
https://www.eliteappliance.com/products/Miele/mie/km2355g.html
https://www.eliteappliance.com/by-brand/lbr/Liebherr.html
https://www.eliteappliance.com/products/monogram/zv830smss.html
https://www.eliteappliance.com/products/Viking/vik/vcih53608ss.html
https://www.eliteappliance.com/products/Liebherr/lbr/ws1200.html
https://www.eliteappliance.com/products/Faber/fab/clas30ss.html
http://www.eliteappliance.com/Fisher&Paykel/RH361.php
https://www.eliteappliance.com/products/monogram/zwe23eshss.html
""
""https://www.eliteappliance.com/products/monogram/zwe23eshss.html
""


	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	

",0,0,1,0,retail,"[appliances, ecommerce, retail]",44-45
b0bcarlson,CARS-ABD-Converter,,https://github.com/b0bcarlson/CARS-ABD-Converter,https://api.github.com/repos/CARS-ABD-Converter/b0bcarlson,Convert CARS orders to ABD orders,"# CARS-ABD-Converter
Convert CARS orders to ABD orders

Django drop in; place the contents of views.py in your views file, point the urls and views to the appropriate places.

`CSV_DIR` is the path to save the outputted csv file.

`CSV_PATH` is the web path for the csv files.
",0,0,1,0,retail,"[retail, retail-product-management]",44-45
dtararuj,analiza_koszykowa-market_basket_analysis,,https://github.com/dtararuj/analiza_koszykowa-market_basket_analysis,https://api.github.com/repos/analiza_koszykowa-market_basket_analysis/dtararuj,market basket analysis implemented in R based on data from retail shops.,"# analiza_koszykowa

Aplikacja to narzędzie służące do przeprowadzenia analizy koszykowej w sieci sklepow detalicznych za dowolny okres.

Aplikacja dostepna jest w ponizszej lokalizacji [link](https://tararuj4.shinyapps.io/Analiza_Koszykowa/?_ga=2.80249746.4648736.1640269735-1531846937.1640269735).

Oprócz aplikacji w folderze znajduje się skrypt służący do przetworzenia specyficznych danych z pewnego przedsiebiorstwa do formatu potrzebnego do uruchomienia skryptu
&nbsp;

W celu uruchomienia algorytmu należy:
- załadować dane paragonowe w poniższej formie:  
(W zbiorze powinny znaleźć sie jedynie paragony z conajmniej dwoma produktami),

| Data       | NrParagonu | Nazwa Produktu |
|------------|------------|----------------|
| 2021-01-02 | XXX_11     | SPODNIE        |
| 2021-01-02 | XXX_11     | SPODNIE        |
| 2021-01-02 | XXX_12     | BIELIZNA       |

- kliknąć w przycisk ""Oblicz"",
- wybrać interesujący nas zakres dat,
- ustawić parametry tworzenia reguł, tj. support (minimalny poziom wsparcia reguły) oraz confidence (prawdopodobieństwo dobrania danego produktu jak drugą szt.),
- ustawić ile reguł wyświetlać,
- ustawić w jaki sposób sortować reguły (czy po poziomie confidence, support, czy lift),
- kliknąć w przycisk ""Wyswietl"" (Każdorazowo po dokonaniu zmian w ustawieniach).

    Możemy również obserwować reguły wyłącznie dla zadeklarowanego produktu lub dla wszystkich (""IGNORUJ"")

&nbsp;

Opis zakładek:

- Reguły: w tym oknie po poprawnym wprowadzeniu danych pojawią nam odfiltrowane reguły w formie tabeli lub wykresu,
- Wykres czestosci: prezentuje najczęściej występujące w paragonach produkty w ujęciu względnym (procentowym),
- Podsumowanie zbioru: krótka charakterystyka danych zaczytanych do modelu,
- Objasnienia: Słowniczek pojęć i terminów wykorzystanych w aplikacji.

&nbsp;

W celu sprawdzenia narzędzia można skorzystać z załączonych danych surowych w pliku: ""przykladowe_dane.csv"", są to losowo przygotowane dane dla potrzeb prezentacji.

Poniżej przykładowe (wygenerowane w sposób losowy) dane jakie uzyskujemy:

a) wykres częstości danego asortymentu we wszystkich paragonach spełniających kryterium analizy (tj. gdzie sa min. 2 transakcje)  

![nazwa](obrazki/item3.JPG) 

b) zestawienie kilku przykladowych reguł

![nazwa](obrazki/item1.jpg) 

c) wykres prezentujący głowne reguly, które wyklarowały się podczas analizy. 
![nazwa](obrazki/item2.jpg) 

---
Dzieki takiej wiedzy moze podejmowac świadome decyzje o:
- kszatłcie promocji (jaki produkt ze soba łaczyc),
- sposobie ekspozycji produktów na sklepach,
- szkoleniu sprzedawców, w celu maksymalizacji koszyka zakupowego.
",0,0,1,0,retail,"[basket-analysis, r, retail, shiny-apps]",44-45
b0bcarlson,batch,,https://github.com/b0bcarlson/batch,https://api.github.com/repos/batch/b0bcarlson,Combines batches and sorts by order code,"# Batchify
#### This version of Batchify is no longer being supported in favor of the version integrated into Bobworx-Suite

Doing multiple batches at once can be frustrating: flipping through several sheets, losing track of what should be next (you forgot that whole section of back to base, didn't you?), and the batch details having a different order than the signs.

This repo aims to help with all these problems. Upload any number of CSVs, and all items will be ordered by your store mapping, sorted by order code, and combined into one list. 
",0,0,1,2,retail,"[retail, retail-product-management]",44-45
alessiamonfardino22,WholeFoods,,https://github.com/alessiamonfardino22/WholeFoods,https://api.github.com/repos/WholeFoods/alessiamonfardino22,SQL analysis to determine whether the amount of badges increases the price of products,"# SQL/DataBase : WholeFoods
<br>

Companies can use AI to collect insightful data and navigation habits directly from consumers and use it for marketing purposes.
Adding badges not only allows businesses to improve users’ interface but also enhances the understanding of customers’ preferences to drive sales and increase profits (Seranmadevi R. &
Felisiya M., 2019). Whole Foods is taking advantage of the use of badges to engage clients and guide them through their shopping experience.
This analysis aims at answering the quesion: Does the amount of badges increase price?
To acquire this insight, I created and analyzed a DataBase on SQL with all the products sold on a specific date on the company's e-commerce. 
",0,0,1,0,retail,"[badges, database, price-tracker, retail, sql, wholefoods, wholesale]",44-45
haleyscomet1P,Retail-Microlearning,,https://github.com/haleyscomet1P/Retail-Microlearning,https://api.github.com/repos/Retail-Microlearning/haleyscomet1P,A microlearning example crafted for a major retailer.,"# Retail Microlearning
 A microlearning example crafted for a major retailer. <br>
 Click on the GitHub Pages link to the right of this Readme, or click [here](https://haleyscomet1p.github.io/Retail-Microlearning/#/) to view the sample!
",0,0,1,0,retail,"[elearning, microlearning, portfolio, retail, training]",44-45
amitrambaran,retail-store-dbms,,https://github.com/amitrambaran/retail-store-dbms,https://api.github.com/repos/retail-store-dbms/amitrambaran,A retail store database management system written in SQL and in BCNF.,"# Retail Store DBMS
A retail store database management system written in SQL and in BCNF. Various queries included.
",0,0,2,0,retail,"[dbms, pos, retail, sql]",44-45
chichacha,retail_calendar_445,,https://github.com/chichacha/retail_calendar_445,https://api.github.com/repos/retail_calendar_445/chichacha,🗓️Retail 4-4-5 Calendar 👚👗👓,"# retail_calendar_445

📅 4-4-5 calendar....

I look up 4-4-5 calendar little too often!!
So I just wanted to create reference calendar using ggplot. 

### 4-4-5? Calendar? 

A year has 52 weeks, so 52/4 = 13 weeks each for quarter. 

**4** weeks as first month of the quarter
next **4** weeks as 2nd month of the quarter
next **5** weeks as 3rd month of the quarter, and so on...


Here's little calendar for 2018 in 4-4-5 calendar format.

![](Images/2018_445_Cakebdar.png?raw=true)

![](Images/2018_445_Cakebdar2.png?raw=true)

![](Images/2018_445_Cakebdar3.png?raw=true)


",0,0,2,0,retail,"[calendar, ggplot2, retail]",44-45
convenience-org,age-vocab,convenience-org,https://github.com/convenience-org/age-vocab,https://api.github.com/repos/age-vocab/convenience-org,A vocabulary for asserting age-related information,"# Age Vocabulary

The Age vocabulary is used in privacy-protecting age-related transactions, such
as the acquisition of age-gated products like medicine, alcohol, and lottery.

## Specification

The latest vocabulary can be viewed by going to the following location:

https://w3id.org/age

",0,0,9,0,retail,"[age, credentials, retail, verifiable, vocabulary]",44-45
c0der4t,printkicker,,https://github.com/c0der4t/printkicker,https://api.github.com/repos/printkicker/c0der4t,Interface that calls the kick command on a printer with the given kick string. Use with command line calls to integrate into existing software or use as a standalone testing utility,"
# Print Kicker

[Download Package](https://github.com/c0der4t/printkicker/releases/tag/1.2)

![GitHub](https://img.shields.io/github/license/c0der4t/printkicker?style=for-the-badge)

Interface that calls the kick command on a printer with the given kick string.

Use with command line calls to integrate into existing software or use as a standalone testing utility


## Features

- Supports command line calls
- Auto RDP printer correction, i.e Full RDP Support
- IQRetail Integration provided
- Easy to use
- Small Package Size

  
## Usage/Examples

### Command Line

The command line call accepts 3 arguments:

- Full Printer Name of Target Printer
- Full Kick String in Format : 000-000-000-000-000
- [optional] Debug Mode

If arg #3 is set to 'debug' additional pop ups will appear while the interface processes requests to assist with testing and setup


#### Structure
```batch
powershell start-process ''exePath'' -ArgumentList {""""""printername""""""; ""kickstring""; ""debug""} -WindowStyle Hidden
```

#### Full Example
```batch
powershell start-process ''C:\Users\Public\Documents\IQPrinterKicker\IQPrinterKicker.exe'' -ArgumentList {""""""EPSON Slip (redirected 257)""""""; ""027-112-048-055-121""; ""debug""} -WindowStyle Hidden
```

#### Full Example (no debug)
```batch
powershell start-process ''C:\Users\Public\Documents\IQPrinterKicker\IQPrinterKicker.exe'' -ArgumentList {""""""EPSON Slip (redirected 257)""""""; ""027-112-048-055-121""; """"} -WindowStyle Hidden
```


### Integration into IQRetail

```pascal

{Copy and paste the following code into any IQ report.
Once added to a report, you can call the kick function in the report using : IQPrintKickPlugin;
Remember to change the following values:
IQPrinterKick_InstallPath = The location of the IQPrinterKick.exe
KickString = The Kickstring for your printer
DebugMode = If set to debug additional messages will be presented informing you of what the system is doing}

procedure IQPrintKickPlugin;
const
IQPrinterKick_InstallPath = 'C:\Users\Public\Documents\IQPrinterKicker\IQPrinterKicker.exe';
KickString = '027-112-048-055-121';
DebugMode = '';                                          
var
qryTerminalSetup : TfrxDBI4Query;
sTargetPrinter,sTempBatFileName : string;
begin
   if <Original> then
   begin

   qryTerminalSetup := TfrxDBI4Query.Create(nil);
   qryTerminalSetup.SQL.Clear;
   with qryTerminalSetup.SQL do
   begin
   Add('SELECT *');
   Add('FROM ""' + <ProgramPath> + 'Companys\TERMINALSETUP' + '""');
   ADD('WHERE TerminalNo = ' + inttostr(<Till>)  + ' and Company = ' + IQQuoteString(<CurrentCompany>));
   end;
   qryTerminalSetup.ExecSQL;
   sTargetPrinter := qryTerminalSetup.FieldByName('PosPrinterPort').AsString;

   sTempBatFileName := IQGetUniqueTableName('temp') + '.bat';
   qryTerminalSetup.SQL.Clear;
   with qryTerminalSetup.SQL do
   begin
   ADD('@echo off');
   ADD('powershell start-process ''' + IQPrinterKick_InstallPath + ''' -ArgumentList {""""""' + sTargetPrinter + '""""""; ""' + KickString + '""; ""' + DebugMode + '""} -WindowStyle Hidden');
   Add('del ' + sTempBatFileName + '.vbs');
   Add('del ' + sTempBatFileName );
   end;
   qryTerminalSetup.SQL.SaveToFile(sTempBatFileName) ;

   qryTerminalSetup.SQL.Clear;
   with qryTerminalSetup.SQL do
   begin
   ADD('Set WshShell = CreateObject(""WScript.Shell"") ');
   ADD('WshShell.Run chr(34) & ""' + sTempBatFileName + '"" & Chr(34), 0');
   ADD('Set WshShell = Nothing');
   end;
   qryTerminalSetup.SQL.SaveToFile(sTempBatFileName + '.vbs') ;

   IQExecute(sTempBatFileName + '.vbs')

   end; //Is Original
end;

```
  
## Preview

![App Screenshot](https://www.ekronds.co.za/img/PrinterKicker/TestPage.PNG)

  
## Contributing

Contributions are always welcome 🙂

  
## Authors

|  [Montè Ekron (c0der4t)](https://www.github.com/c0der4t)          | [Follow me on Twitter](https://twitter.com/EkronMonte)    
:-------------------------:|:-------------------------:
",0,0,1,1,retail,"[cashdrawer, iq, iqretail, printer, retail, utlilty]",44-45
yhyeh,BLE-Clothing-Smartag,,https://github.com/yhyeh/BLE-Clothing-Smartag,https://api.github.com/repos/BLE-Clothing-Smartag/yhyeh,BLE Smart Tags and Sales in Physical Clothing Retail Markets,"# BLE-Clothing-Smartag
BLE Smart Tags and Sales in Physical Clothing Retail Markets

### Skills
> Arduino IDE, BLE protocol, Feather nRF52840 boards operation, English presentation

### Abstract
- Use BLE dual mode to advertise clothes information for both customer promotion and retailer inventory.
- Simulate the high device density environment to observe the impacts of tx power, number of channels and the position of receivers.
- Implement a hierarchical system with a tag on each clothes.
- Integrate demands of both customers and retailers compared to pure RFID and Beacon.

Please refer to following docs for details:
- [Project Slides](CS498_yhyeh2_ProjectPresentation.pdf)
- [Project Report](CS498_yhyeh2_ProjectReport.pdf)
",0,0,1,0,retail,"[arduino-sketch, bluetooth-low-energy, nrf52840, retail]",44-45
Matcar02,AI-ML-Customer-segmentation,,https://github.com/Matcar02/AI-ML-Customer-segmentation,https://api.github.com/repos/AI-ML-Customer-segmentation/Matcar02,Repository for an academic project held in the AI & ML class.,"# Projectrepo2
 Repository Ai project
<h1> Machine learning Project: Customer Segmentation</h1> 
<h3> Group members: Carucci Matteo, Agudio Tommaso, Natoli Vittorio Alessandro </h3>

<h2>Introduction</h2>
<h3> In the first part of the project, we deal with a customer database where customers' orders in Brazil are registered. There are many informations stored for each order, including the price spent and also some relevant information about the customer and sellers themselves; </h3>
<h3> We then have done some data exploration to check for null values, duplicates, and useless features. </h3>
<h3> Afterward we have done the RFM analysis and calculated the Recency, Frequency, and Monetary values. We then plotted the RFM dataset to have a better look at the data and noticed that customers who purchased more recently spent more and had a higher frequency. To confirm our hypothesis we used the function corr() on the dataset and noticed that there was a positive correlation between frequency and monetary value, thus confirming our initial observation. Afterward, we applied K-means and Hierarchial Clustering on the RFM and plotted the results in a 3D plot, while our third method was Spectral Clustering. Afterward, we did some further data exploration to see if we could consider other aspects of the dataset to potentially confirm or reassess the results obtained in the clustering. Lastly, after finding some interesting features in the dataset, we used two different alternatives; Principal Component Analysis and Autoencoders ANN. When we gather our results, we applied the clustering methods and made our conclusions. </h3>

<h2> Methods Used</h2>
<h3>The main methods we used were KMeans, Hierarchical Cluster Clustering, Spectral Clustering, PCA, and Autoencoders</h3>
<h3>The first step was to calculate the RFM, whereas we have stated before, RFM stands for Recency (How long has been since the last purchase of a customer), Frequency (The frequency of purchases of a customer), and Monetary (How much a customer has spent over its time using the marketplace). To get these values, we simply used a groupby on the original dataframe (because we don't need any categorical features), applied some conditions, obtained the three different dataframes, and concatenated them into a single one. An important aspect was that, after many trials, we decided not to scale data with either MinMax or Linear scaling (Standard) since it would compromise the values of the frequency, with many equal to 0; moreover neither the clusters distance nor the silhouette score improves significantly. After having obtained this new dataframe we applied the following two algorithms.

<h3>KMeans and Hierarchical Cluster Clustering: 

The process for these two algorithms was very similar. We started from the RFM dataframe that we calculated previously and used respectively the Elbow Method, for KMeans, and a Dendrogram, for Hierarchical Clustering, to then analyze the ideal number of clusters to obtain the best results. For both the methods we had a few hyperparameters to tune, when we used KMeans we first used GridSearch to better interpret and tune the parameters which were: the number of clusters, clustering algorithm (lloyd or elkan), and the number of iterations for centroids. We obtained that the parameters to produce better results are likely elkan, five clusters, and three iterations to find the centroids. An important aspect though is that after a few computations, we decided that four clusters are a good compromise. We then created the class that allows us to change the hyperparameters and then compute the resulting graph after applying the KMeans. Instead for HC clustering, after choosing the number of centroids as the result of the Dendrogram, we had two major hyperparameters to tune, the linkage (ward and average work similarly), that is, the metrics to use to determine the distance between clusters, and the affinity, what type of distance measure to use to compute such linkage. For the sake of consistency, we decided to carry out the algorithm with four clusters. For both methods we implemented a 3D plot, using plotly to better visualize the data. Our last step was to compute the silhouette score, this is done to better understand how well the clusters were separated by basically taking the intra-cluster distance into account. We did this for both our methods and obtained that, KMeans had a silhouette score of 0.0.816001, while HC 0.79044, was slightly worse than KMeans.


Spectral Clustering:

We used this third method because we wanted to check if it had better results, especially because it works better when there are a lower number of clusters. The way this method works is by creating an affinity matrix, where each datapoint is compared to others by assessing the ""similarity"", that is, sklearn builds a graph with datapoints as nodes, and uses the number of common neighbors (nearest) to identify specific communities. From the plot obtained, we notice that the cluster separation is fine, but the main issue is that the clustering identified too many high spenders and too few risk customers, which is incongruous with the Exploratory Data Analysis we previously carried out, which shows that only a few people spend a lot, while most spend the same amount.
We also computed the silhouette score for this method and obtained 0.34288 


Principal Component Analysis:

As we have stated before, after computing the clustering on the RFM, we decided to use PCA to detect some important features and preprocess the data and then compute the clustering on these new features. In particular, we analyzed the payment type and the number of installments, the customer state, and lastly the product category. All of these features could have some interesting insights for example, the customer state tells us some important information on the demographics, which is an important aspect considering the huge disparity between the rich and poor in Brazil.

The first step was creating the dataframe with all the features we want to include in our analysis and scale the data, we then plot the cumulative plot of the PC that allows us to understand how much information n components provide together. Afterward, we apply the Elbow Method to decide the number of clusters considering the PCA components and we deduced that we should use four clusters again. The next step is to tune the hyperparameters and visualize the results of PCA, which is a way to detect the most important features in the new dataset. Lastly, we visualized the clusters made by the KMeans after applying the PCA transformation and plot the data in 2D. The results are not that different from those obtained previously. We have a few customers that are more likely to be top spends and others who are mid and low-spenders.


Autoencoder ANN:

This is the last method we used, where the main goal was to reduce the dimensionality of the data using the autoencoder, a sort of Artificial Neural Network. This method works as the following:
    -The autoencoder learns a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore insignificant data (“noise”).

    -It has 2 main parts: an encoder that maps the message (data) to a code, and a decoder that reconstructs the message (processed data) from the code, that is the decoder extracts the most relevant patterns and information we want to retrieve.

![Autoencoder](Images/download.png)


The most important step was to apply the scaling on the data, we then applied the autoencoder, got the output data (obtained after doing the epochs and encoding), and use the KMeans algorithm. After plotting the data, we observe that we obtain different results from our previous analysis, this is because the Autoencoder ANN clusters the customers using the RFM and other characteristics obtained by the ANN.
</h3>

<h2>Experimental Design</h2>

<h3>The main way we tested the obtained results is through the silhouette score and the method describe() to deduce the information of each cluster. 

The silhouette was used to measure and better understand how well the clusters were separated, by basically taking the intra-cluster distance into account. We then tuned the hyperparameters to see how the silhouette score changed, thus deciding which was the optimal combination. To do so, we did a grid search which produced, as a result, the parameters to obtain likely the best score. This was used for the KMeans, while for HC and Spectral we simply used to silhouette score and manually changed the parameters.

The other way to validate the information was through the method described, which we used for the PCA and Autoencoders, but also the other three methods. We did a sort of deep dive into the individual clusters obtained and checked some important insights and derived some information on the cluster.
</h3>

<h2> Results</h2>
<h3>In our analysis we identified 4 main customer segmentations:
•   Low spenders/at-risk 
•   Mid Spenders
•   High spenders
•   Top customers

The way that the various algorithms segmented the data into these four clusters is described as follows:
1.  KMeans has detected the majority of the customers into a single cluster, the low spenders and those who are likely to leave the business, while the high spenders and top customers are a minority.
2.  Hierarchical Cluster had a similar approach to KMeans, but with a key difference. Whilst the low and at-risk customers remain the same, the mid-spenders cluster size has increased, while the high and top spenders are unchanged.
3.  Spectral Clustering has a more balanced partition, we don’t see very small segmentations as we did in KMeans and Hierarchical Cluster. We see a more even distribution of customer segmentation.
4.  For the Principal Component Analysis, we can see that more high spenders have been detected compared to KMeans and HC, but when compared to Spectral, the clusters' size is similar. An interesting difference is the differentiation between those who are at-risk compared to the mid-spenders. 
5.  Autoencoder ANN has provided very similar clusters. Not only it does not take into account the monetary value but also other features such as payment and demographics information do not seem to influence the segments.

Even though the silhouette score of the PCA kmeans is way less than the ones in rfm kmeans and hierarchical clustering, the algorithm has detected better segmentations, while kmeans and hierarchical have identified top customers really well. The spectral clustering instead has done a great job as one can see both in the segmentations' descriptions and also in the silhouette score, which is way less than its 2 main competitors but still decent.

This result can be seen in the following dataframe:
![Dataframe](Images/df.jpeg)


We can also observe the following clustering for the different methods used:
These are the clusters made by the autoencoder:

![Autoencoders Plot](Images/autoencoderplot.png)

These are the clusters made by Hierarchical clustering:

![Hierarchical Cluster plot](Images/hcplot.png)

These are the clusters made by Kmeans clustering:

![KMeans Plot](Images/kmeansplot.png)

These are the clusters made by PCA:

![PCA Plot](Images/pcaplot.png)

And finally, these are the clusters made by the spectral clustering:

![Spectral Clustering Plot](Images/spectralplot.png)




</h3>


<h2>Conclusions</h2>
<h3>After a thorough investigation, there are some takeaways that the Brazilian subsidiary can get:
- Brazilian customers are segmented into 4 categories, low-spenders, at-risk customers, mid-spenders, and finally top and high-spenders customers.

- The subsidiary should focus the email campaign on both low and at-risk customers to retain them, by proposing promotions and discounts to those returning/continuing to buy in the business.

- For mid-spenders, the firm should introduce new and less-bought products. As we have seen there are products that have been purchased way more than others and it can be useful to increase sales in those who are not purchased as much.

- For high and top spenders, the firm could as well promote the usual products they buy, but at the same time propose more general-user products to increase the already large Lifetime value they have.</h3>

<h3>Even though our analysis may be satisfactory, there is still something missing. Ideally, some further information on the customers could have helped us create a better segmentation, for example, the annual income. This is because some data that was in the dataset wasn't significant in our analysis, for example, most if not all of the orders, were done in two states such as Sao Paolo and Rio de Janeiro. The next steps would be to obtain more information regarding the customers to then create a better segmentation.</h3>
",0,0,1,0,retail,"[clustering, machine-learning, retail]",44-45
Matcar02,HackatOw---A-study-into-an-asian-retailer,,https://github.com/Matcar02/HackatOw---A-study-into-an-asian-retailer,https://api.github.com/repos/HackatOw---A-study-into-an-asian-retailer/Matcar02,"Repository for the HackatOW. The Hackaton organized by Oliver Wyman challenged us to assess the effectiveness of the client promotions, an asian retailer.","In this brief project, a colleague and I analyzed and predicted the customers behaviour of an asian healthcare retailer interested in assessing the effectiveness of its promotions and discounts.
The notebook is split into 3 sections:
1) Data cleaning and Prepration (wrangling).
2) Promos and Discounts insights.
3) Time-series Baseline sales forecasting.


The objectives and the tasks are explained below. The analysis is maninly focused on the performance of some products:

**CONTEXT**

• Oliver Wyman have been engaged by an Asian health and beauty retailer to help design a set of promotions in line with their existing promotional offering and long-term sustainability goals

• The retailer has an online and offline (in store) presence in multiple countries across Asia but we have been engaged to focus on the Malaysian business unit and their offline promotions

• They have also asked to the team to focus on a subset of the products within a specific product category 

• To support the Oliver Wyman team, the client has shared multiple datasets for us to understand the current performance of their products and the promotions they offer as they want to move towards a more data-driven approach. Previously individual category managers would rely on industry expertise to set up promotions and campaigns but 
were unable to review the effectiveness of the promotions coherently across the business.
The project was divided into different tasks, listed below:


**TASK**

You are working with the new project team and as the data and analytics consultant on the project, you have been asked by the senior members of the team to assess the data shared by the client, conduct exploratory data analysis and build a simple model to predict product baseline sales.

**More defined tasks**

1) • The local business teams shared the transaction data for their entire 
transaction data history. Unfortunately, some of the business teams use manual steps to create the transaction data, which impacts the overall quality of the data
• Additional details on the datasets provided are available in Appendix A
• Look at the data provided and prepare it so that it can be used for further analysis/modeling
– What data quality issues exist in each dataset? Are values in line with expectations? Were there challenges in joining the datasets?
• The leadership team is also interested in the overall quality of the data and would like an assessment of what things can be improved to make the data more useful as the organization looks to become a more data-driven organization. What recommendations for improving their existing data can you think of?


2) • The local business teams shared the transaction data for their entire 
transaction data history. Unfortunately, some of the business teams use 
manual steps to create the transaction data, which impacts the overall 
quality of the data
• Additional details on the datasets provided are available in Appendix A
• Look at the data provided and prepare it so that it can be used for 
further analysis/modeling
– What data quality issues exist in each dataset? Are values in line with 
expectations? Were there challenges in joining the datasets?
• The leadership team is also interested in the overall quality of the data 
and would like an assessment of what things can be improved to make 
the data more useful as the organization looks to become a more data-
driven organization. What recommendations for improving their 
existing data can you think of?


3) • The client wants to be able to understand the performance of products under 
promotion with a more quantitative measure
• In order to do this, we need to first understand the performance of products 
without promotion e.g., what is the baseline of sales without promotions, and then 
measure how effective promotions are on top of this baseline
• We will need to train a model to predict a baseline of sales without promotional 
activity. During this process, you may need to consider
– If or how to use days where a promotion is active in the training data
– What other factors may influence consumer behavior beyond promotion 
(e.g., weekends, seasons, holidays, natural disasters) and how they can 
be appropriately captured as factors in the model (binary, numerical, 
categorical etc.)
– How to capture long-term trends connected to the product
• A common metric to measure promotion effectiveness for a given product is 
elasticity. This relates the additional uplift in sales gained to the discount given away 
and the client is interested in using it as a consistent promotion performance metric

It is not possible to share the data due to confidential agreements. Hope you enjoy!
",0,0,1,0,retail,"[predictive-analytics, retail, time-series]",44-45
darkbluein,apollo-server,darkbluein,https://github.com/darkbluein/apollo-server,https://api.github.com/repos/apollo-server/darkbluein,High performance apollo-express server for both react native apps,"## Configure server

Setup env variables after creating `.env` file in the root directory. Following env variables are necessary for fully functioning server. More to be added accordingly

```sh
PORT=5000

REDIS_HOST=
REDIS_PORT=6379

NODE_ENV=

MONGODB_CONNECTION_STRING=

TWILIO_ACCOUNT_SID=
TWILIO_AUTH_TOKEN=
TWILIO_MESSAGING_SID=

TOKEN_SECRET=
REFRESH_TOKEN_SECRET=

GOOGLE_MAPS_APIKEY=
```

## Types

### Queries

```javascript
// user
getFeed;
getUser;
twoFactorAuth;
checkAuth;

// store
getStore;
getConfirmation;

// order
getOrder;
getOrders;
getDeliveryTimes; //FIXME: Unnecessary query
```

### Mutations

```javascript
// user
login;
register;
updateAddress;
deleteAddress;
editProfile;
deleteAccount;

// store
addAccount;
editStore;
addToInventory;

// order
createOrder;
alterOrderState;
alterDeliveryState;
```

### Subscriptions

```javascript
// user
userUpdate;

// store
storeUpdate;
inventoryUpdate;
accountUpdate;

// order
orderUpdate;
```
",0,0,0,0,retail,"[apollo-server-express, ecommerce, graphql, graphql-server, mongodb, mongoose, retail]",44-45
cockpit-labs,CockpitCE,cockpit-labs,https://github.com/cockpit-labs/CockpitCE,https://api.github.com/repos/CockpitCE/cockpit-labs,"Cockpit is a mobile solution for checklist used in the retail industry for store audit and management, healthcare for patient medical follow-up and many industries for equipment or process monitoring","# Cockpit Community Edition

Cockpit  is a mobile solution for checklist and audit in the real world.

It helps you to send information to your team, control your processes with checklists and provides dashboard of your operational execution.

Cockpit is used in the retail industry for store audit and management, healthcare for patient medical follow-up and many industries for equipment or process monitoring.

Ready-to-use, cockpit includes : users & groups management, checklist creation, pre-calibrated answer types, scoring and benchmarking as well as many other useful features.

## Features

### Create your checklists

Create/clone/modify questionnaires with dozens of answer types : yes/no, scale, MQC, photos, select, date, numbers...

### Manage users & groups

Import people and organisation or manage your groups, roles & hierarchy as to write checklists, access dashboards and scoring...

### Analyse your feed-backs

Complete dashboard with benchmarks, scoring, filters, progressions and a lot more.

### Photo gallery

Automatically extract all the pics from your questionnaires to display and filter them in the photo gallery.


## Build Cockpit

Build View module:
```shell
cd View
docker run --rm --name yarn -v ""$PWD"":/usr/src/app -w /usr/src/app node:lts-alpine sh -c ""apk --update --no-cache --virtual build-dependencies add python3 make g++ && yarn install""
docker run --rm --name yarn -u $(id -u) -v ""$PWD"":/usr/src/app -w /usr/src/app node:lts-alpine yarn build
```

Build Admin module:
```shell
cd Admin
docker run --rm -v ""$PWD"":/code munenari/sencha-cmd /bin/bash -c ""sencha app upgrade -ext@7.3.0.55; sencha app upgrade /opt/sencha/repo/extract/ext/7.3.0.55; sencha app build; sed -i 's/ src=\""microloader.js\""//g' dist/index.html""
```

Build Studio module:
```shell
cd Studio
docker run --rm -v ""$PWD"":/code munenari/sencha-cmd /bin/bash -c ""sencha app upgrade -ext@7.3.0.55; sencha app upgrade /opt/sencha/repo/extract/ext/7.3.0.55; sencha app build; sed -i 's/ src=\""microloader.js\""//g' dist/index.html""
```


Build Cockpit container:

```shell
docker-compose build
```

## Start Cockpit

Run container:

```shell
docker-compose up -d
```


Create MySQL DB and initialize Cockpit
```shell
docker exec -ti cockpit_app ./createCockpitDB.sh
docker exec -ti cockpit_app bin/console cockpit:core:init --drop
```

At this time, a superuser with the same password as keycloak admin (see [.env file](.env)) can connect to Cockpit [View](http://localhost), [Admin](http://localhost/admin) and [Studio](http://localhost/studio).

",0,0,0,1,retail,"[audit, checklist, checklist-application, checklists, dashboard, healthcare, questionnaire, questionnaire-api, questionnaires, retail, scoring]",44-45
comprocomputers,comprocomputers,,https://github.com/comprocomputers/comprocomputers,https://api.github.com/repos/comprocomputers/comprocomputers,About Compro Computers Pvt. Ltd,"# Introduction
Compro Computers Pvt. Ltd. constitutes a team of highly technical and managerial skilled personnel dedicated towards unparalleled service commitments to its customer and they are well qualified in computer science and engineering, in bachelor electrical engineering, diploma in hardware from IIHT, bachelor in electronics and communications. Besides, we have been providing computing solutions in hardware and soft-ware to a wide spectrum of financial institutions, travel agencies, business houses and non governmental organizations within the country, apart from catering to personal computer needs.

# Background
Information Technology (IT) has brought an unprecedented revolution in the history of human civilization. It is changing the shape of the world. If we lag behind in reaping the benefits, it will be irreparable loss to the nation and people. Compro Computers aims at contributing to facilitate its development in Nepal by providing a wide range of computers products, software and supports. There are quite a few companies like ours which provide a wide range of products and services in the whole country.

<br>

<img src=""https://compro.com.np/assets/uploads/comprocomputers.png"" alt=""Compro Computers Store"" width=""400"">

&copy; _Compro Computers Pvt. Ltd_ 2021.
",0,0,1,0,retail,"[compro, computers, ecommerce, ict, it, nepal, retail, services, solutions, technology]",44-45
priyankaspatil,retail-site-mvp,,https://github.com/priyankaspatil/retail-site-mvp,https://api.github.com/repos/retail-site-mvp/priyankaspatil,This is a React based retail-site mvp.,"1. Run `npm install` to install all project dependencies, then `npm start` to run and launch the app.
2. Run `npm run test` to run tests.",0,0,1,0,retail,"[eccomerce, eccomerce-platform, front-end-development, functional-programming, jest, material-ui, mui, mui-icons, multi-carousel, react, react-hooks, react-hooks-project, react-router, react-router-dom, retail, sass, scss, ssr, webpack]",44-45
supervisely-ecosystem,group-reference-objects-into-batches,supervisely-ecosystem,https://github.com/supervisely-ecosystem/group-reference-objects-into-batches,https://api.github.com/repos/group-reference-objects-into-batches/supervisely-ecosystem,Reference objects are grouped into batches by columns from CSV catalog,"<div align=""center"" markdown>
<img src=""https://user-images.githubusercontent.com/48245050/182596595-3f283416-da7f-4305-b957-9f873208edd0.png""/>

# Group reference objects into batches

<p align=""center"">
  <a href=""#Overview"">Overview</a> •
  <a href=""#How-To-Run"">How To Run</a> •
  <a href=""#How-To-Use"">How To Use</a> •
  <a href=""#Result-JSON-Format"">Result JSON format</a>
</p>


[![](https://img.shields.io/badge/supervisely-ecosystem-brightgreen)](https://ecosystem.supervise.ly/apps/group-reference-objects-into-batches)
[![](https://img.shields.io/badge/slack-chat-green.svg?logo=slack)](https://supervise.ly/slack)
![GitHub release (latest SemVer)](https://img.shields.io/github/v/release/supervisely-ecosystem/group-reference-objects-into-batches)
[![views](https://app.supervise.ly/img/badges/views/supervisely-ecosystem/group-reference-objects-into-batches.png)](https://supervise.ly)
[![runs](https://app.supervise.ly/img/badges/runs/supervisely-ecosystem/group-reference-objects-into-batches.png)](https://supervise.ly)

</div>

# Overview

Labeling for tagging or classification tasks becomes complex when annotation team have to deal with hundreds or thousands of tags or classes. This app groups items from catalog by one or several columns and then splits groups into small batches. This app is a part of complex tagging/classification pipline. For example, you can see all apps from [retail collection](https://ecosystem.supervise.ly/).

Let's consider retail case as example:
- We need to label product shelves: draw bounding boxes around every object and assign correct class from catalog
- The size of catalog: 1350 unique items
- The size of annotation team: 150 labelers
- Images look like this:

<img src=""https://thumbs.dreamstime.com/z/pet-products-shelves-supermarket-pet-products-shelves-supermarket-auchan-romania-145486859.jpg"" width=""400px""/>

Put bounding box around every object - it's a feasible task. But it is hard and time consuming to assign correct product identifier from huge catalog to every bbox. One of the approaches is to split catalog across all labelers: in our case `1350 unique items` / `150 labelers` = `9 items in a batch`. Labeler will work with his batch the following way: go through all bboxes and match them with only 9 items. 

Key **advantages** of this approach: 
- labeler knows his batch very well: it's easy to keep in mind 9 items
- the chance of error is reduced significantly
- if bbox is matched with one of 9 items from batch it takes just few clicks from labeler to assign correct tag

# How To Run

**Step 1:** Add app to your team from Ecosystem if it is not there.

**Step 2:** Run app 
 
 <img src=""https://i.imgur.com/Y5PgfbT.png""/>
 
**Step 3:** What until UI is ready

# How To Use

[![Watch the video](https://i.imgur.com/grDPMed.png)](https://youtu.be/MyrOgn4RpyA)

**Step 1:** Define the path to `CSV` product catalog in `Team Files` and press `Preview catalog` button

Before:
 <img src=""https://i.imgur.com/6ds1Rnl.png""/>

After:
 <img src=""https://i.imgur.com/xTDnKYt.png""/>
 
 **Step 2:** Define the path to directory with `JSON` reference files that created with the app [""Create JSON with reference items""](https://ecosystem.supervise.ly/apps/create-json-with-reference-items), press `Preview files` button, select files that should be used and then press `Validate` button.
 
Before:
 <img src=""https://i.imgur.com/28A6AUg.png""/>

After:
 <img src=""https://i.imgur.com/OUM7FBM.png""/>


**Step 3:** Match reference item with column from CSV catalog, choose `groupBy` columns from catalog (order matters), define batch size and press `Create groups` button. Then preview groups. You can change some grouping parameters and press `Create groups` button again. If you satisfied with results, setup save path and press `Save` button. Resulting groups will be saved to `Team Files` in `JSON` format.


Before:
 <img src=""https://i.imgur.com/EU4cS1g.png""/>

After:
 <img src=""https://i.imgur.com/tWB1Q5G.png""/>
 
**Step 4:** Stop app manually

<img src=""https://i.imgur.com/flXfONq.png""/>


# Result JSON format

```json
[
  {
    ""batch_index"": 0,
    ""items_count"": 3,
    ""group_columns"": {
      ""category"": ""Accessories"",
      ""sub-category"": ""Portable Power Banks"",
      ""brand"": ""Samsung""
    },
    ""key_col_name"": ""upc"",
    ""references"": {
      ""6750711"": [""...""],
      ""7930356"": [""...""],
      ""9994737"": [""...""]
    },
    ""references_catalog_info"": {
      ""6750711"": {
        ""brand"": ""Samsung"",
        ""name"": ""Samsung Universal 3100mAh Portable External Battery Charger - White"",
        ""upc"": 6750711,
        ""weight"": ""5.6 ounces"",
        ""category"": ""Accessories"",
        ""sub-category"": ""Portable Power Banks"",
        ""price"": 17.99,
        ""merchant"": ""Bestbuy.com""
      },
      ""7930356"": {
        ""brand"": ""Samsung"",
        ""name"": ""Samsung Universal 3100mAh Portable External Battery Charger - White"",
        ""upc"": 7930356,
        ""weight"": ""5.6 ounces"",
        ""category"": ""Accessories"",
        ""sub-category"": ""Portable Power Banks"",
        ""price"": 14.84,
        ""merchant"": ""accessorynet""
      },
      ""9994737"": {
        ""brand"": ""Samsung"",
        ""name"": ""Samsung Universal 3100mAh Portable External Battery Charger - White"",
        ""upc"": 9994737,
        ""weight"": ""5.6 ounces"",
        ""category"": ""Accessories"",
        ""sub-category"": ""Portable Power Banks"",
        ""price"": 22.99,
        ""merchant"": ""Bestbuy.com""
      }
    },
    ""catalog_path"": ""/reference_items/1120-water-catalog.csv""
  },
  {
    ""batch_index"": 1,
    ""..."": ""...""
  }
]
```

Result JSON - list of objects, that describe every batch of reference objects:
- `batch_index` - index of the batch
- `items_count` - number of items in batch   
- `group_columns` - the names of columns and corresponding values used to group items (`groupBy` operation)
- `key_col_name` - name of the column in CSV catalog that is used to match reference item with correct row from product catalog
- `references` - dictionary with reference examples for every item (format is the same as in [reference items format](https://github.com/supervisely-ecosystem/create-json-with-reference-items#json-format))
- `references_catalog_info` - information from catalog for every reference item
- `catalog_path` - path to the catalog in Team Files
",0,0,2,0,retail,"[ecosystem, reference-items, retail, supervisely]",44-45
atharvapathak,Sales_Forecasting_Project,,https://github.com/atharvapathak/Sales_Forecasting_Project,https://api.github.com/repos/Sales_Forecasting_Project/atharvapathak,"Forecasted product sales using time series models such as Holt-Winters, SARIMA and causal methods, e.g. Regression. Evaluated performance of models using forecasting metrics such as, MAE, RMSE, MAPE and concluded that Linear Regression model produced the best MAPE in comparison to other models","# Sales_Forecasting_Project

[![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2Fatharvapathak%2Fhit-counter&count_bg=%2379C83D&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false)](https://hits.seeyoufarm.com)
## Sales_Forecasting_Project

[![Generic badge](https://img.shields.io/badge/Datascience-Beginners-Red.svg?style=for-the-badge)](https://github.com/atharvapathak) 
[![Generic badge](https://img.shields.io/badge/LinkedIn-Connect-blue.svg?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/atharva-pathak-126021119/) 
[![Generic badge](https://img.shields.io/badge/Python-Language-blue.svg?style=for-the-badge)](https://github.com/atharvapathak/Sales_Forecasting_Project)

#### The goal of this project is to Predict the Future Sales [#DataScience](https://github.com/atharvapathak/Sales_Forecasting_Project) for the challenging time-series dataset consisting of daily sales data,

[![GitHub repo size](https://img.shields.io/github/repo-size/atharvapathak/Sales_Forecasting_Project.svg?logo=github&style=social)](https://github.com/atharvapathak) [![GitHub code size in bytes](https://img.shields.io/github/languages/code-size/atharvapathak/Sales_Forecasting_Project.svg?logo=git&style=social)](https://github.com/atharvapathak/)[![GitHub top language](https://img.shields.io/github/languages/top/atharvapathak/Sales_Forecasting_Project.svg?logo=python&style=social)](https://github.com/atharvapathak)

#### Few popular hashtags - 
### `#Sales Prediction` `#Time Series` `#Ensembling`
### `#XGBoost` `#Parameter Tuning` `#LightGBM`

### Motivation
In this competition I was working with a challenging time-series dataset consisting of daily sales data, kindly provided by one of the largest Russian software firms - 1C Company. 
To predict total sales for every product and store in the next month. By solving this competition I was able to apply and enhance your data science skills.

This documentation contains general information about my approach and technical information about Kaggle’s Predict Future Sales competition

### Steps involved in this project
### Kaggle Predicting Future Sales- Playground Prediction Competition

### Kaggle Competition: [Predict Future Sales](https://www.kaggle.com/c/competitive-data-science-predict-future-sales)
### Data Description

You are provided with daily historical sales data. The task is to forecast the total amount of products sold in every shop for the test set. Note that the list of shops and products slightly changes every month. Creating a robust model that can handle such situations is part of the challenge.

**File descriptions**
```
- sales_train.csv - the training set. Daily historical data from January 2013 to October 2015.
- test.csv - the test set. You need to forecast the sales for these shops and products for November 2015.
- sample_submission.csv - a sample submission file in the correct format.
- items.csv - supplemental information about the items/products.
- item_categories.csv  - supplemental information about the items categories.
- shops.csv- supplemental information about the shops.
```

**Data fields**
```
- ID - an Id that represents a (Shop, Item) tuple within the test set
- shop_id - unique identifier of a shop
- item_id - unique identifier of a product
- item_category_id - unique identifier of item category
- item_cnt_day - number of products sold. You are predicting a monthly amount of this measure
- item_price - current price of an item
- date - date in format dd/mm/yyyy
- date_block_num - a consecutive month number, used for convenience. January 2013 is 0, February 2013 is 1,..., October 2015 is 33
- item_name - name of item
- shop_name - name of shop
- item_category_name - name of item category
```
## I. Summary
- Main methods I used for this competition that provides the desired Leaderboard score: LightGBM
- Methods I tried to implement but resulted in worse RMSE: XGBoos, Stacking (both simple averaging and metal models such as Linear Regression and shallow random forest)
- The most important features are lag features of previous months, especially the ‘item_cnt_day’ lag features. Some of them, which can be found in my lag dataset, are 
  - **target_lag_1,target_lag_2**: item_cnt_day of each shop – item pair of previous month and previous two months
  - **item_block_target_mean_lag_1, item_block_target_sum_lag_1**: sum and mean of item_cnt_day per item of previous month
Important features are measured from LightGBM model
- Tools I used in this competition are: numpy, pandas, sklearn, XGBoost GPU, LightGBM (running Pytorch)
- All models are tuned on a linux server with Intel i5 processor, 16GB RAM, NVIDIA 1080 GPU. Tuning models took about 8 to 10 hours, and training on the whole dataset took <=5 minutes


## II. Exploratory Data Analysis
More information can be found in [EDA notebook](EDA.ipynb)

Basic data analysis is done, including plotting sum and mean of item_cnt_day for each month to find some patterns, exploring missing values, inspecting test set …

Here are few things interesting I found from doing EDA:
- Number of sold items declines over the year
- There are peaks in November and similar item count zic-zac behaviors in June-July-August. This inspires me to look up Russia national holiday and create a Boolean holiday features. More information can be found in ‘Feature Engineering’ section
- Data has no missing values
- Some interesting information from test set analysis:
  - Not all shop_id in training set are used in test set. Test set excludes following shops (but not vice versa): [0, 1, 8, 9, 11, 13, 17, 20, 23, 27, 29, 30, 32, 33, 40, 43, 51, 54]
  - Not all item in train set are in test set and vice versa
  - In test set, a fixed set of items (5100) are used for each shop_id, and each item only appears one per each shop. This possibly means that items are picked from a generator, which will result in lots of 0 for item count. Therefore, generating all possible shop-item pairs for each month in train set and assigning missing item count with 0 makes sense.


## III. Feature Engineering

### 1. Generate all shop-item pairs and Mean Encoding
Since the competition task is to make a monthly prediction, we need to aggregate the data to monthly level before doing any encodings

Item counts for each shop-item pairs per month (‘target’). I also generated sum and mean of item counts for each shop per month (‘shop_block_target_sum’,’shop_block_target_mean’), each item per month (‘item_block_target_sum’,’item_block_target_mean’, and each item category per month (‘item_cat_block_target_sum’,’item_cat_block_target_mean’)

This process can be found in [this notebook](generate_lag_features.ipynb), under ‘Generating new_sales.csv’. Datasets generated from this steps will be saved under the name ‘new_sales.csv’

### 2. Generate lag features
Lag features are values at prior time steps. I am generating lag features based on ‘item_cnt’ and grouped by ‘shop_id’ and ‘item_id’ .  Time steps are: 1,2,3,5 and 12 months.

All sale record before 2014 are dropped, since there would be no lag features before 2014 as we have a 12-month lag.

These lag features turn out to be the most important features in my dataset, based on gradient boosting’s importance features.

More information can be found in [this notebook](generate_lag_features.ipynb), under ‘Generate lag feature new_sales_lag_after12.pickle’

### 3. Holiday Boolean features
As mentioned above, I look up few Russia national holidays and created few 5 more features: December (to mark December), Newyear_Xmas (for January), Valentine_Menday (February), Women_Day (March), Easter_Labor (April). This might help boosting my score a little since December feature seems to be helpful

After all this steps, you should have a pickle file name in ‘data‘ directory: 'new_sales_lag_after12.pickle'. This is the main file I used for training models


### IV. Cross validations
Since this is time series so I have to pre-define which data can be used for train and test. I have a function called get_cv_idxs in utils.py that will return a list of tuples for cross validation. I decide to use 6 folds, from date_block_num 28 to 33, and luckily this CV score is consistent to leaderboard score.

CV indices can be retrieved from this custom function:

```
cv = get_cv_idxs(dataframe,28,33) 
# dataframe must contain date_block_num features
```

Results from this function can be passed to sklearn GridSearchCV.

### V. Training methods:

### 1. LightGBM
LightGBM is tuned using hyperopt, then manually tune with GridSearchCV to get the optimal result. One interesting thing I found: when tuning the size of the tree, it’s better to tune min_data_in_leaf instead of max_depth. This means to let the tree grows freely until the condition for min_data_in_leaf is met. I believe this will allow deeper logic to develop without overfitting too much. Colsample_bytree and subsample are also used to control overfitting. And I keep the learning rate small (0.03) throughout tuning.

Mean RMSE of 6 folds CV is 0.8088, which is better than any other models I used.

You can find more information in [LGB notebook](lightgbm_tuning.ipynb). From this file I also created out-of-fold features for block 29 to 33, which is used for ensembling later.

Also from this notebook, you can get the leaderboard submission under the file name: ‘coursera_tuned_lightgbm_basic_6folds.csv'

(Note: I do not include some of hyper parameter tuning results from hyperopt since I tuned it at work and I do not have access to that machine now)


### 2. XGBoost
I ran the XGBoost with GPU version, and I follow the same tuning procedures as mentioned in LightGBM. For some reason, I can’t seem to get a consistent result while running XGBoost, even with the same parameters. One example is I get .812 CV score from hyperopt, but I can’t seem to get that result again when getting out-of-fold features (it jumps to .817). This never happens while using LightGBM.

Therefore, I pick 2 models: one with max_depth tuned, and one without max_depth tuned, to get out-of-fold features and hoping they are different enough for ensembling. 

For the record, the first models results .812 CV score (in hyperopt) and .926 LB score, and second models results in .813 CV score (hyperopt) and .927 LB score. Either way, both are worse than LGB model 

``` python 
space = {
    #'n_estimators': hp.quniform('n_estimators', 50, 500, 5),
#     'max_depth': hp.choice('max_depth', np.arange(5, 10, dtype=int)),
    'subsample': hp.quniform('subsample', 0.7, 0.9, 0.05),
    'colsample_bytree': hp.quniform('colsample_bytree', 0.7, 0.9, 0.05),
    'gamma': hp.quniform('gamma', 0, 1, 0.05),
    'max_leaf_nodes': hp.choice('max_leaf_nodes', np.arange(100,140, dtype=int)),
    'min_child_weight': hp.choice('min_child_weight', np.arange(100,140, dtype=int)),
    'learning_rate': 0.03,
    'eval_metric': 'rmse',
    'objective': 'reg:linear' , 
    'seed': 1204,'tree_method':'gpu_hist'
}

```
best_hyperparams = optimize(space,max_evals=200)
print(""The best hyperparameters are: "")
print(best_hyperparams)

You can find more information about this in [XGB notebook](xgb_tuning.ipynb). Prediction for the model with max_depth tuned are named ‘tuned_xgb_basicfeatures_6folds_8126.csv’ and the other one are ‘tuned_xgb_basicfeatures_6folds_8136’


## VI. Ensembling

With LightGBM, XGB model-1 and XGB model-2 out-of-fold features from previous methods, I calculated pairwise differences between them, get the mean of all 3 LGB, XGB1 and XGB2 out-of-fold features, and include the most important features from feature importance: ‘target_lag_1’.

From here I try few ensembling methods
- Simple average and Weighted average 
- SKlearn linear regression and Elasticnet
- Shallow Random Forest, tuned with 5 folds (from 29 to 33)

All of them results in RMSE score that is slightly more than the LightGBM best model, so LightGBM still outperforms them.

``` python
X,y = get_X_y_ensembling(all_oof_df)
params={'alpha': 0.0, 'fit_intercept': False, 'solver': 'sag','random_state':1402}
lr = Ridge(**params)
lr.fit(X,y)
test_pred =  lr.predict(test_df)
pd.Series(test_pred).describe()
get_submission(test_pred,'ensembling_ridge');
```

More information can be found in [Ensembling notebook](ensembling.ipynb)

## VII. Improvement:

Few things that can be improved are:
- Implement neural net WITHOUT categorical embedding
- Generate more feature related to holiday, such as: differences between current month and holiday month.
- Translate item name to English and perform sentiment analysis on item name
- Use only subset of those meta features for ensembling


### Libraries Used

![Ipynb](https://img.shields.io/badge/Python-datetime-blue.svg?style=flat&logo=python&logoColor=white) 
![Ipynb](https://img.shields.io/badge/Python-pandas-blue.svg?style=flat&logo=python&logoColor=white)
![Ipynb](https://img.shields.io/badge/Python-numpy-blue.svg?style=flat&logo=python&logoColor=white) 
![Ipynb](https://img.shields.io/badge/Python-matplotlib-blue.svg?style=flat&logo=python&logoColor=white) 
![Ipynb](https://img.shields.io/badge/Python-seaborn-blue.svg?style=flat&logo=python&logoColor=white)
![Ipynb](https://img.shields.io/badge/Python-scipy-blue.svg?style=flat&logo=python&logoColor=white) 
![Ipynb](https://img.shields.io/badge/Python-sklearn-blue.svg?style=flat&logo=python&logoColor=white) 


### Installation

- Install **datetime** using pip command: `from datetime import datetime`
- Install **pandas** using pip command: `import pandas as pd`
- Install **numpy** using pip command: `import numpy as np`
- Install **matplotlib** using pip command: `import matplotlib`
- Install **matplotlib.pyplot** using pip command: `import matplotlib.pyplot as plt`
- Install **seaborn** using pip command: `import seaborn as sns`
- Install **os** using pip command: `import os`
- Install **scipy** using pip command: `from scipy import sparse`
- Install **scipy.sparse** using pip command: `from scipy.sparse import csr_matrix`
- Install **sklearn.decomposition** using pip command: `from sklearn.decomposition import TruncatedSVD`
- Install **sklearn.metrics.pairwise** using pip command: `from sklearn.metrics.pairwise import cosine_similarity`
- Install **itertools** using pip command: `from itertools import product`


### How to run?

[![Ipynb](https://img.shields.io/badge/Prediction-Sales.Python-lightgrey.svg?logo=python&style=social)](https://github.com/atharvapathak/Sales_Forecasting_Project)


### Project Reports

[![report](https://img.shields.io/static/v1.svg?label=Project&message=Report&logo=microsoft-word&style=social)](https://github.com/atharvapathak/Sales_Forecasting_Project/)

- [Download](https://github.com/atharvapathak/Sales_Forecasting_Project/') for the report.

 
### Related Work

[![Sales Prediction](https://img.shields.io/static/v1.svg?label=Sales&message=Prediction&color=lightgray&logo=python&style=social&colorA=critical)](https://www.linkedin.com/in/atharva-pathak-126021119/) [![GitHub top language](https://img.shields.io/github/languages/top/atharvapathak/Sales_Forecasting_Project.svg?logo=php&style=social)](https://github.com/atharvapathak/)

[Sales Prediction](https://github.com/atharvapathak/Sales_Forecasting_Project) - A Detailed Report on the Analysis


### Contributing

[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?logo=github)](https://github.com/atharvapathak/Sales_Forecasting_Project/pulls) [![GitHub issues](https://img.shields.io/github/issues/atharvapathak/Sales_Forecasting_Project?logo=github)](https://github.com/atharvapathak/Sales_Forecasting_Project/issues) ![GitHub pull requests](https://img.shields.io/github/issues-pr/atharvapathak/Sales_Forecasting_Project?color=blue&logo=github) 
[![GitHub commit activity](https://img.shields.io/github/commit-activity/y/atharvapathak/Sales_Forecasting_Project?logo=github)](https://github.com/atharvapathak/Sales_Forecasting_Project/)

- Clone [this](https://github.com/atharvapathak/Sales_Forecasting_Project/) repository: 

```bash
git clone https://github.com/atharvapathak/Sales_Forecasting_Project.git
```

- Check out any issue from [here](https://github.com/atharvapathak/Sales_Forecasting_Project/issues).

- Make changes and send [Pull Request](https://github.com/atharvapathak/Sales_Forecasting_Project/pull).
 
### Need help?

 [![LinkedIn](https://img.shields.io/static/v1.svg?label=connect&message=@atharvapathak&color=success&logo=linkedin&style=flat&logoColor=white&colorA=blue)](https://www.linkedin.com/in/atharva-pathak-126021119/)

:email: Feel free to contact me @ [atharvapathakb2w@gmail.com](https://mail.google.com/mail/)

[![GMAIL](https://img.shields.io/static/v1.svg?label=send&message=atharvapathakb2w@gmail.com&color=red&logo=gmail&style=social)](https://www.github.com/atharvapathak) [![Twitter Follow](https://img.shields.io/twitter/follow/pathak_atharva?style=social)](https://twitter.com/pathak_atharva)

",0,0,1,0,retail,"[datamining, demand-forecasting, feature-engineering, machine-learning, machinelearning, python, regression-trees, retail, sales, sales-forecasting, seaborn, sklearn, statsmodels, time-series-analysis, time-series-decomposition]",44-45
kristjandee,portfolio,,https://github.com/kristjandee/portfolio,https://api.github.com/repos/portfolio/kristjandee,"Collection of all information concerning planned, in progress, and completing projects","# portfolio
Collection of all information concerning planned, in progress, and completing projects
",0,0,1,2,retail,"[analytics, marketing, retail]",44-45
rodrigobercini,lstm-varejo-brasil,,https://github.com/rodrigobercini/lstm-varejo-brasil,https://api.github.com/repos/lstm-varejo-brasil/rodrigobercini,Utilizando modelo Long Short-Term Memory (LSTM) para previsão da Pesquisa Mensal do Comércio,"## Modelo Long Short-Term Memory (LSTM) para Pesquisa Mensal do Comércio (PMC)

Este repositório aplica uma rede neural recorrente (RNN) através de um modelo LSTM para tentar prever o comportamente da Pesquisa Mensal do Comércio fornecida pelo IBGE

```A recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Derived from feedforward neural networks, RNNs can use their internal state (memory) to process variable length sequences of inputs.```

```A Pesquisa Mensal de Comércio produz indicadores que permitem acompanhar o comportamento conjuntural do comércio varejista no País, investigando a receita bruta de revenda nas empresas formalmente constituídas, com 20 ou mais pessoas ocupadas, e cuja atividade principal é o comércio varejista.```

## Resultados

### Série histórica da PMC

![](1.png)

### Prevendo 24 meses com controle de validação

![](2.png)

### Previsão para futuro sem controle de validação

O modelo conseguiu capturar a sazonalidade da série história na previsão para o futuro. 

![](3.png)
",0,0,1,0,retail,"[data-science, deep-learning, lstm, lstm-neural-networks, machine-learning, recurrent-neural-networks, retail]",44-45
teresaheidt,HeidtMoore,,https://github.com/teresaheidt/HeidtMoore,https://api.github.com/repos/HeidtMoore/teresaheidt,Working on a retail game based on the HeidtMoore book series.,"# Getting Started with Create React App

This project was bootstrapped with [Create React App](https://github.com/facebook/create-react-app).

## Available Scripts

In the project directory, you can run:

### `npm start`

Runs the app in the development mode.\
Open [http://localhost:3000](http://localhost:3000) to view it in the browser.

The page will reload if you make edits.\
You will also see any lint errors in the console.

### `npm test`

Launches the test runner in the interactive watch mode.\
See the section about [running tests](https://facebook.github.io/create-react-app/docs/running-tests) for more information.

### `npm run build`

Builds the app for production to the `build` folder.\
It correctly bundles React in production mode and optimizes the build for the best performance.

The build is minified and the filenames include the hashes.\
Your app is ready to be deployed!

See the section about [deployment](https://facebook.github.io/create-react-app/docs/deployment) for more information.

### `npm run eject`

**Note: this is a one-way operation. Once you `eject`, you can’t go back!**

If you aren’t satisfied with the build tool and configuration choices, you can `eject` at any time. This command will remove the single build dependency from your project.

Instead, it will copy all the configuration files and the transitive dependencies (webpack, Babel, ESLint, etc) right into your project so you have full control over them. All of the commands except `eject` will still work, but they will point to the copied scripts so you can tweak them. At this point you’re on your own.

You don’t have to ever use `eject`. The curated feature set is suitable for small and middle deployments, and you shouldn’t feel obligated to use this feature. However we understand that this tool wouldn’t be useful if you couldn’t customize it when you are ready for it.

## Learn More

You can learn more in the [Create React App documentation](https://facebook.github.io/create-react-app/docs/getting-started).

To learn React, check out the [React documentation](https://reactjs.org/).

### Code Splitting

This section has moved here: [https://facebook.github.io/create-react-app/docs/code-splitting](https://facebook.github.io/create-react-app/docs/code-splitting)

### Analyzing the Bundle Size

This section has moved here: [https://facebook.github.io/create-react-app/docs/analyzing-the-bundle-size](https://facebook.github.io/create-react-app/docs/analyzing-the-bundle-size)

### Making a Progressive Web App

This section has moved here: [https://facebook.github.io/create-react-app/docs/making-a-progressive-web-app](https://facebook.github.io/create-react-app/docs/making-a-progressive-web-app)

### Advanced Configuration

This section has moved here: [https://facebook.github.io/create-react-app/docs/advanced-configuration](https://facebook.github.io/create-react-app/docs/advanced-configuration)

### Deployment

This section has moved here: [https://facebook.github.io/create-react-app/docs/deployment](https://facebook.github.io/create-react-app/docs/deployment)

### `npm run build` fails to minify

This section has moved here: [https://facebook.github.io/create-react-app/docs/troubleshooting#npm-run-build-fails-to-minify](https://facebook.github.io/create-react-app/docs/troubleshooting#npm-run-build-fails-to-minify)
",0,0,1,0,retail,"[books, game, react, retail]",44-45
riversidecoaching,emenu,,https://github.com/riversidecoaching/emenu,https://api.github.com/repos/emenu/riversidecoaching,,"# an accessible digital menu interface for hospitality retailers
https://riversidecoaching.github.io/emenu/
",0,0,1,0,retail,"[food, html, menu, order, retail, standalone]",44-45
kerpa,Friterie,,https://github.com/kerpa/Friterie,https://api.github.com/repos/Friterie/kerpa,Sales analysis of a Foodtruck,"# Friterie
Sales analysis of a foodtruck.
The dataset comes from bills that have been manually tranfered into a CSV

Part 1 is a descriptive analysis of the dataset.
Part2 is the use of different models of Machine Learning on the dataset.
",0,0,1,0,retail,"[data-analysis, data-collection, food, retail]",44-45
mwadieh,walmart-stores-sales-analysis,,https://github.com/mwadieh/walmart-stores-sales-analysis,https://api.github.com/repos/walmart-stores-sales-analysis/mwadieh,Analyze Walmart Sales data for Walmart Stores ,"# walmart-stores-sales-analysis
Analyze Walmart Sales data for Walmart Stores 
",0,0,1,0,retail,"[analysis, data, r, retail, walmart]",44-45
victor-cali,CycleShare,,https://github.com/victor-cali/CycleShare,https://api.github.com/repos/CycleShare/victor-cali,Case Study: Cycle Sharing Scheme — Determining Brand Persona from Python Machine Learning Case Studies by Danish Haroon (2017),"CycleShare
==============================

Case Study: Cycle Sharing Scheme — Determining Brand Persona from Python Machine Learning Case Studies by Danish Haroon

Project Organization
------------

    ├── LICENSE
    ├── Makefile           <- Makefile with commands like `make data` or `make train`
    ├── README.md          <- The top-level README for developers using this project.
    ├── data
    │   ├── external       <- Data from third party sources.
    │   ├── interim        <- Intermediate data that has been transformed.
    │   ├── processed      <- The final, canonical data sets for modeling.
    │   └── raw            <- The original, immutable data dump.
    │
    ├── docs               <- A default Sphinx project; see sphinx-doc.org for details
    │
    ├── models             <- Trained and serialized models, model predictions, or model summaries
    │
    ├── notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
    │                         the creator's initials, and a short `-` delimited description, e.g.
    │                         `1.0-jqp-initial-data-exploration`.
    │
    ├── references         <- Data dictionaries, manuals, and all other explanatory materials.
    │
    ├── reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
    │   └── figures        <- Generated graphics and figures to be used in reporting
    │
    ├── requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
    │                         generated with `pip freeze > requirements.txt`
    │
    ├── setup.py           <- makes project pip installable (pip install -e .) so src can be imported
    ├── src                <- Source code for use in this project.
    │   ├── __init__.py    <- Makes src a Python module
    │   │
    │   ├── data           <- Scripts to download or generate data
    │   │   └── make_dataset.py
    │   │
    │   ├── features       <- Scripts to turn raw data into features for modeling
    │   │   └── build_features.py
    │   │
    │   ├── models         <- Scripts to train models and then use trained models to make
    │   │   │                 predictions
    │   │   ├── predict_model.py
    │   │   └── train_model.py
    │   │
    │   └── visualization  <- Scripts to create exploratory and results oriented visualizations
    │       └── visualize.py
    │
    └── tox.ini            <- tox file with settings for running tox; see tox.readthedocs.io


--------

<p><small>Project based on the <a target=""_blank"" href=""https://drivendata.github.io/cookiecutter-data-science/"">cookiecutter data science project template</a>. #cookiecutterdatascience</small></p>
",0,0,1,0,retail,"[business, machine-learning, probability, retail, statistics]",44-45
PatMyron,data,,https://github.com/PatMyron/data,https://api.github.com/repos/data/PatMyron,,"The World's Billionaires List by Age

![screenshot](forbes-richest-by-age/screenshot.png)

Retailers by Sales per Store

![screenshot](sales-per-store/screenshot.png)

U.S.-Booked Air Volume by Company

![screenshot](corporate-travel/screenshot.png)
",0,0,3,8,retail,"[billionaires, forbes, forbes400, retail, retailers, traveling-salesman, travelling-salesman-problem]",44-45
Priyankadev,djretail,,https://github.com/Priyankadev/djretail,https://api.github.com/repos/djretail/Priyankadev,POC of Retail management project in django,"# djretail
POC of Retail management project in django
",0,0,2,0,retail,"[django, erp, poc, python, retail]",44-45
mlanghinrichs,ion_csvs,,https://github.com/mlanghinrichs/ion_csvs,https://api.github.com/repos/ion_csvs/mlanghinrichs,A utility for adding new filters to cards bought in a specific date range through ION MTG retail.,,0,0,2,0,retail,"[csv, ion, magic-the-gathering, retail]",44-45
mohammad-akhlak-ansari,Retail-Discounting-System,,https://github.com/mohammad-akhlak-ansari/Retail-Discounting-System,https://api.github.com/repos/Retail-Discounting-System/mohammad-akhlak-ansari,,,0,0,1,1,retail,"[discounting, in, java, retail, system]",44-45
pvn-leo,pharmaceutical-retail-MS,,https://github.com/pvn-leo/pharmaceutical-retail-MS,https://api.github.com/repos/pharmaceutical-retail-MS/pvn-leo,Pharmaceutical-Retail Management System using MySQL,"# pharmaceutical-retail-MS

A simple Pharmaceutical-Retail Management System using MySQL

Relation-Schema
---------------

![Schema-Diagram](/Diagrams/Schema.png)


ER-Diagram
----------

![Er-Diagram](/Diagrams/ERD.png)

Execution
-----------

```
sudo mysql
source <path>/SQL/Proj-INIT.sql;
source <path>/SQL/Proj-INSERT.sql;
source <path>/SQL/Proj-QUERY.sql;
```


",0,0,1,0,retail,"[dbms, dbms-project, management-system, mysql, pharmaceuticals, retail]",44-45
boostinwrx,tobacco-hut-centreville-2,,https://github.com/boostinwrx/tobacco-hut-centreville-2,https://api.github.com/repos/tobacco-hut-centreville-2/boostinwrx,,"<!-- AUTO-GENERATED-CONTENT:START (STARTER) -->
<p align=""center"">
  <a href=""https://www.gatsbyjs.org"">
    <img alt=""Gatsby"" src=""https://www.gatsbyjs.org/monogram.svg"" width=""80"" />
  </a>
</p>

<h1 align=""center"">
  London After Midnight

[![Deploy to Netlify](https://www.netlify.com/img/deploy/button.svg)](https://app.netlify.com/start/deploy?repository=https://github.com/vaporwavy/gatsby-london-after-midnight)

</h1>

---

A custom, image-centric theme for Gatsby. Made for publishers and portfolios with plenty of graphics to show off to the world. Completely free and fully responsive, released under the MIT license.

**Based on [London](https://github.com/ImedAdel/gatsby-london) for Gatsby**

**Demo: https://gatsby-lam.cxsmxs.com**

---

### What is different from London?
* Support Tag
* Full Recoloring for Dark themes
* Highlight Syntax with Prism.js
* Support Korean and Japanese
* Parameterize All Colors
* Change the Format of Thumbnailless Postcards

---

_First time with Gatsby? Take a look on the [official and community-created starters](https://www.gatsbyjs.org/docs/gatsby-starters/)._

## 🚀 Quick start

1.  **Create a Gatsby site.**

    Use `npx` and the Gatsby CLI to create a new project

    ```sh
    # create a new Gatsby site using the blog starter
    npx gatsby new my-awesome-portfolio https://github.com/vaporwavy/gatsby-london-after-midnight
    ```

1.  **Start developing.**

    Navigate into your new site’s directory and start it up.

    ```sh
    cd my-awesome-portfolio/
    gatsby develop
    ```

1.  **Open the source code and start editing!**

    Your site is now running at `http://localhost:8000`!

    _Note: You'll also see a second link: _`http://localhost:8000/___graphql`_. This is a tool you can use to experiment with querying your data. Learn more about using this tool in the [Gatsby tutorial](https://www.gatsbyjs.org/tutorial/part-five/#introducing-graphiql)._

    Open the `my-blog-starter` directory in your code editor of choice and edit `src/pages/index.js`. Save your changes and the browser will update in real time!

<!-- AUTO-GENERATED-CONTENT:END -->
",0,0,2,0,retail,"[retail, store]",44-45
dtararuj,aplikacja_algorytm_automatyczne_MMKi,,https://github.com/dtararuj/aplikacja_algorytm_automatyczne_MMKi,https://api.github.com/repos/aplikacja_algorytm_automatyczne_MMKi/dtararuj,Interaktywna aplikacja do automatycznego dokonywania ruchów towarowych między sklepami// An interactive application for automatic goods movements between stores ,"# aplikacja_algorytm_automatyczne_MMKi

### Skrypt jest narzedziem wykorzystywanym w celu wygenerowania listy zlecen dla wskazanego sklepu

Narzedzie zostalo przygotowane w jezyku R, przy pomocy R Shiny.
Dane wrażliwe zostały ocenzurowane.

![strona główna aplikacji](foto/obraz1.png)


## Źródło danych

Plik jest zasilany następującymi danymi zewnętrznymi:
- lista indekso-rozmiarów na których chcemy wykonać ruchy towarowe,
- sprzedaż z ostatniego miesiąca,
- biężące stany sklepowe i magazynowe, 
- aktualny poziom zatowarowania sklepu,
- hierarchia produktów,
- wiekowanie produktów.

## Schemat działania
---

Skrypt tworzy ranking sklepów najbardziej niedotowarowanych w ramach danej kategorii produktowej o najlepszym potencjale sprzedażowym oraz dodatkowo nieposiadającym lub posiadającym w niewielkich ilościach dany produkt.

Na tej podstawie algorytm zawarty w skrypcie wskazuje do jakiego sklepu najlepiej przenieść konkretny produkt (z zejściem do pojedyńcego rozmiaru) ze wskazanej puli produktów.

Jako rezultat działania skryptu otrzymujemy listę, w formacie csv, którą możemy wgrać do systemu produkcyjnego, przekazując tym samym dyspozycji do wskazanych salonów.

## Korzyści
---

Wykorzystując skrypt udało się zaoszczędzić 6 roboczogodzin raz na 2 tygodnie, plus ten skrypt był inicjatorem kolejnych rozwiązań w obszarze zarządzania towarem.  
Dodatkowo wybór odbiorców wskazanych ruchów towarowych nie jest przypadkowa tylko uzasadniona danymi statystycznymi. 
Oprócz tego udało się również wykluczyć błąd ludzki polegający na wytypowaniu zbyt dużej ilości sklepów, co zawsze wiazało się z dodatkowymi kosztami przesyłek (skrypt koncentruje ruchy towarowe i dąży do utworzenia jak najmniejszej liczby przesyłek).

## Szczegółowy opis funkcjonalności
---

Na wstępie wgrywamy plik ze wskazanego miejsca na serwerze

![wgrywanie danych](foto/obraz3.png)

&nbsp;

W tym kroku możemy już wybrać do ilu salonów chcemy kierować nasze paczki.  
Decyzję możemy podjąć również po uzyskaniu wstępnych wyników.

![ilosc sklepo](foto/obraz2.png)

&nbsp;

W kolejnym kroku wskazujemy sklepy, do których na pewno nie chcemy nic wysyłać.  
Powody mogą być różne: tymczasowe zamknięcie, nowe salony, inny koncept.  
Lista dopuszcza multiwybór.  

![wykluczenia](foto/obraz4.png)

&nbsp;

Ważnym elementem jest wskazanie poniżej salonu z którego wykonujemy ruchu.
Ta informacja pojawi się na liście zleceń, ustawiając nam ten sklep jako dawce, a także wykluczy go z listy potencjalnych biorców.

![dawca](foto/obraz5.png)

&nbsp;

Ze względu na wykorzystywania narzędzia przez różnych użytkowników została powołana opcja podawania ścieżki gdzie znajdują się najświeższe dane wsadowe (oprócz listy indeksów do przeniesienia).  
Dla ułatwienia ustawiona jest domyślna ścieżka.

![zródło](foto/obraz6.png)

&nbsp;

Kolejne elementy to różne opcje pracy algorytmu, domyślnie ustawione są te najpopularniejsze.  
Możemy wybrać sposób sortowania sklepów uwzględniając w pierwszej kolejności:
- bieżące zatowarowanie danym indekso-rozmiarem na sklepie,
- sprzedaż per indeks,
- poziom zatowarowania na sklepie,
- bieżące zatowarowanie danym indeksem na sklepie.

![sortowanie](foto/obraz7.png)

&nbsp;

Możemy wybierać jak bardzo szczegółowo chcemy analizować potencjał, czy schodząc do indekso-rozmiaru, czy tylko na poziomie całego indeksu.

![sposob](foto/obraz8.png)

&nbsp;

Ważnym elementem pracy algorytmu jest część kodu, która definiuje jego zachowanie gdy napotka na sytuacje, że dany indeks nie występuje na żadnym innym sklepie.  
Algorytm ma wtedy 2 opcje, zleca wysyłke produktu do sklepu, który ma największe zapotrzebowanie na tego typu asortyment lub uzyskuje najwyższe obroty.

![sposob1](foto/obraz9.png)

&nbsp;

Aby skrypt mógł się uruchomić należy po wgraniu wszystkich potrzebnych danych i ustawień kliknąć opcję ""odśwież"".

![odswiez](foto/obraz10.png)

&nbsp;

## Wyniki
---

Skrypt po odświeżeniu danych, po kilku minutach pracy, generuje nam podsumowanie, prezentujące jak będą wyglądać zlecenia, do jakich salonów i po ile szt.
Na podstawie tych danych możemy podjąć decyzję o ograniczeniu zleceń do X salonów.

![wynik1](foto/obraz11.png)

Wynik po ograniczeniach zaprezentowany jest w zakładce ""Lista ograniczona"".

![wynik2](foto/obraz12.png)

&nbsp;

Gdy rezultat pracy algorytmu jest satysfakcjonujący, u dołu alplikacji klikamy w opcję ""pobierz plik"".

![wynik3](foto/obraz13.png)

&nbsp;

**Rezultatem powyższych działań jest plik csv, w poniższym układzie:**

| LP |Indeks | Rozmiar| Ilosc | Dawca | Biorca|
|----|---|---------|---------|--------|-----------------|
|    |   |         |         |        |                 |
|    |   |         |         |        |                 |
|    |   |         |         |        |                 |
",0,0,1,0,retail,"[automation, r, retail, shiny-apps]",44-45
dinachoir,CRM-Strategy-Development,,https://github.com/dinachoir/CRM-Strategy-Development,https://api.github.com/repos/CRM-Strategy-Development/dinachoir,Propose CRM strategies by segmenting customers into clusters and identifying purchase patterns and give recommendations separately for each group.,"# CRM-Strategy-Development
Propose CRM strategies by segmenting customers into clusters and identifying purchase patterns and give recommendations separately for each group.
",0,0,1,0,retail,"[customer-lifetime-value, ecommerce, market-basket-analysis, retail, segmentation]",44-45
sandraabu,retail-analytics-dashboard,,https://github.com/sandraabu/retail-analytics-dashboard,https://api.github.com/repos/retail-analytics-dashboard/sandraabu,Retail Analytics Dashboard using Plotly dash ,"# Retail analytics dashboard

<img width=""1792"" alt=""Screenshot 2022-07-17 at 11 12 44"" src=""https://user-images.githubusercontent.com/83399849/179393654-6962299b-eba9-47ee-b313-de6fd55f7c46.png"">



## Retail Analytics Dashboard using Plotly dash.

### Heroku deployment 
Please visit:
https://retail-sales-plotly-dash.herokuapp.com/ 
to see deployed dashboard 




### Data 
Dashboard was built using data available on Kaggle:
https://www.kaggle.com/datasets/mohamedharris/supermart-grocery-sales-retail-analytics-dataset

",0,0,1,0,retail,"[dashboard, heroku-deployment, plotly-dash, retail]",44-45
Rose-njeru,Inventory-analysis,,https://github.com/Rose-njeru/Inventory-analysis,https://api.github.com/repos/Inventory-analysis/Rose-njeru,Inventory Analysis,"# Inventory-analysis
## Problem Statement

The survival of a large online retailer is at stake due to inaccurate inventory tracking causing lost sales and unhappy customers.

As a digital marketer, your task is to use SQL to track and analyze the inventory levels of the retailer’s eCommerce store to save the day!

```sql
CREATE TABLE products (
product_id SERIAL PRIMARY KEY,
product_name VARCHAR(50),
product_category VARCHAR(20),
product_price NUMERIC(10,2)
);
```
```sql
INSERT INTO products (product_name, product_category, product_price)
VALUES
('Product A', 'Category 1', 19.99),
('Product B', 'Category 2', 29.99),
('Product C', 'Category 1', 39.99),
('Product D', 'Category 3', 49.99),
('Product E', 'Category 2', 59.99);
```
```sql
CREATE TABLE inventory (
product_id INT,
inventory_date DATE,
inventory_level INT
);
```sql
INSERT INTO inventory (product_id, inventory_date, inventory_level)
VALUE
(1, '2022-01-01', 100),
(2, '2022-01-01', 200),
(3, '2022-01-01', 150),
(4, '2022-01-01', 75),
(5, '2022-01-01', 250),
(1, '2022-01-02', 80),
(2, '2022-01-02', 180),
(3, '2022-01-02', 100),
(4, '2022-01-02', 60),
(5, '2022-01-02', 220),
(1, '2022-01-03', 50),
(2, '2022-01-03', 150),
(3, '2022-01-03', 75),
(4, '2022-01-03', 80),
(5, '2022-01-03', 200);
```

**Question 1**
+ What are the top 5 products with the highest inventory levels on the most recent inventory date?
```sql
SELECT
product_name,
inventory_level
FROM products AS products
INNER JOIN inventory AS inventory
ON products.product_id=inventory.product_id
WHERE inventory_date=(SELECT MAX(inventory_date) FROM inventory)
ORDER BY inventory_level DESC 
LIMIT 5;
```

![image](https://user-images.githubusercontent.com/92436079/220821205-9918f90c-408e-40c5-bd31-691e6bdc553f.png)

**Question 2**
+ What is the total inventory level for each product category on the most recent inventory date?
```sql
SELECT 
product_category,
SUM(inventory_level) AS total_inventory_level
FROM products AS products
JOIN inventory AS inventory
ON products.product_id=inventory.product_id
WHERE inventory_date=(SELECT MAX(inventory_date) FROM inventory)
GROUP BY product_category;
```
![image](https://user-images.githubusercontent.com/92436079/220821274-63752eb1-441b-43c4-826a-c4452d73019c.png)

**Question 3**
+ What is the average inventory level for each product category for the month of January 2022?
``` sql
SELECT
product_category,
ROUND(AVG(inventory_level),2) AS Average_inventory_level
FROM products AS products
JOIN inventory AS inventory
ON products.product_id=inventory.product_id
WHERE inventory_date>='2022-01-01' AND inventory_date <'2022-01-31'
GROUP BY product_category;
``` 

![image](https://user-images.githubusercontent.com/92436079/220821331-680ea54d-5eba-4f6d-9bcf-7abe0cde4b90.png)

**Question 4**
+ Which products had a decrease in inventory level from the previous inventory date to the current inventory date?
```sql
SELECT
p.product_name,
p.product_id,
(a1.inventory_level-a2.inventory_level) AS difference
FROM  inventory AS a1
JOIN inventory AS a2
ON a1.product_id=a2.product_id
AND a1.inventory_date=date_add( a2.inventory_date, INTERVAL 1 DAY)
JOIN products AS p
ON a1.product_id =p.product_id
WHERE a1.inventory_level< a2.inventory_level;
```
![image](https://user-images.githubusercontent.com/92436079/220821395-3df663f5-da7d-4188-b8bc-1dae58234c53.png)

**Question 5**
+ What is the overall trend in inventory levels for each product category over the month of January 2022?
```sql
SELECT 
product_category,
inventory_date,
round(AVG(inventory_level),2) AS average_inventory_level
FROM products AS products
JOIN inventory AS inventory
ON products.product_id=inventory.product_id
WHERE inventory_date>='2022-01-01' AND inventory_date<'2022-01-31'
GROUP BY product_category,inventory_date
ORDER BY product_category,round(AVG(inventory_level),2) DESC;
```

![image](https://user-images.githubusercontent.com/92436079/220822160-acaa6295-744c-4cf5-9dad-9453b5dd09d4.png)
",0,0,1,0,retail,"[customer, e-commerce, mysql, retail]",44-45
cypherjonesgrey,cypherjonesgrey.github.io,,https://github.com/cypherjonesgrey/cypherjonesgrey.github.io,https://api.github.com/repos/cypherjonesgrey.github.io/cypherjonesgrey,"Business website of Meddle Group comprising of other businesses such as events organization, property management and sales/Real estate etc ",,0,0,1,0,retail,"[business, design, estate, eventsourcing, graphic, real, retail, software-development, website]",44-45
sthanhng,wideface.ai,,https://github.com/sthanhng/wideface.ai,https://api.github.com/repos/wideface.ai/sthanhng,,"# wideface.ai
",0,0,4,0,retail,"[artificial-intelligence, deep-learning, face-detection, face-recognition, retail]",44-45
aliffiann,supermarket-sales,,https://github.com/aliffiann/supermarket-sales,https://api.github.com/repos/supermarket-sales/aliffiann,Supermarket Sales Data Analysis using Python & Tableau Start using Python to answer business questions and get insights.,"# Supermarket Sales
## Bussines Overview
Supermarket growth in most various cities is increasing and market competition is intense.
Average supermarket sales growth rate in the past 3 months fell 4.5%

## Bussines Questions
How we can increase sales with customer membership by 10% in 3 months?

##
[Dashboard](https://public.tableau.com/app/profile/aliffian/viz/SupermarketSales_16760935325750/SupermarketSales) |
[Medium article](https://aliffian.medium.com/supermarket-sales-analysis-using-python-tableau-9133fc4e6665) |
[Report file](https://www.linkedin.com/posts/aliffian_supermarket-sales-analysis-activity-7031426860399935488-IEC-?utm_source=share&utm_medium=member_desktop) |
[Dataset](https://www.kaggle.com/datasets/aungpyaeap/supermarket-sales)

**Contact me**
[Linkedin](https://www.linkedin.com/in/aliffian/)
",0,0,1,0,retail,"[data-analysis, jupyter-notebook, python, retail, supermarket]",44-45
sergeymitrichev,retailcrm-api-client,,https://github.com/sergeymitrichev/retailcrm-api-client,https://api.github.com/repos/retailcrm-api-client/sergeymitrichev,Java Client for RetailCRM API,,0,0,2,0,retail,"[api, retail]",44-45
asim5800,Online-Retail-Customer-Segmentation-,,https://github.com/asim5800/Online-Retail-Customer-Segmentation-,https://api.github.com/repos/Online-Retail-Customer-Segmentation-/asim5800,"In this project, task is to identify major customer segments on a transnational data set which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail.","# Online-Retail-Customer-Segmentation-

<p align=""center"">
  <img width=""460"" height=""300"" src=""https://www.bython.com/wp-content/uploads/2021/01/benefits-of-market-segmentation.png"">
</p>


## Problem Statement
In this project, the task is to identify major customer segments on a transnational data set which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail.The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.

## Approach
The RFM model is quite useful model in retail customer segmentation where only the data of customer transaction is available. RFM stands for the three dimensions:

Recency – How recently did the customer purchase? 
Frequency – How often do they purchase? 
Monetary Value – How much do they spend?
A combination of these three attributes can be defined to assign a quantitative value to customers. e.g. A customer who recently bought high value products and transacts regularly is a high value customer

## Segmentation with K-means clustering:
Initially, the data is subject to important stages in an analytics pipeline: exploratory analysis, preprocessing, feature engineering and standardizaton. Then, the unsupervised classification technique, K-means clustering algorithm, is used to determine the ideal segments of customers. Silhouette analysis and related cluster visualizations are leveraged to deduce the optimum value of ""K"" (number of clusters) in the algorithm. The observations from the results are elaborately discussed before reaching the conclusion from the business perspective.

## Conclusion
After forming 4 clusters by k-means and elbow method we can separate our customers as star, light, new, lost. An ideal customer should have low recency , high frequency and high monetary value. These customers can now be targeted according to business need resulting in better customer relationships and profitability. 


",0,0,1,0,retail,"[cluster-analysis, clustering, machine-learning, marketing, retail, segmentation]",44-45
KristiBischoff,Clothing-Business-Analysis-Tableau-Dashboard,,https://github.com/KristiBischoff/Clothing-Business-Analysis-Tableau-Dashboard,https://api.github.com/repos/Clothing-Business-Analysis-Tableau-Dashboard/KristiBischoff,"Repository to preview, describe, and link to Tableau dashboard. ","## Clothing-Business-Analysis-Tableau
This repository links to a fun red color themed Tableau dashboard for a new online business that sells pre-loved clothing. This is a fictional business with mock data created by me. The purpose of the dashboard is to identify key sellers and profits and how to improve marketing for continued success. Geomapping, scatter plots, donut charts, overal graphs for profit target goals, and more are included in this striking and comprehensive dashboard.

## Link to dashboard
[Clothing Business Dashboard by Kristi Bischoff](https://public.tableau.com/app/profile/kristi.bischoff/viz/SmallBusinessAnalysis_16826693039690/Dashboard1)

## Screenshot of dashboard

![ClothingBusiness_dashboard](Clothing_dashboard.png)

",0,0,2,0,retail,"[barchart, business, dashboard, data-visualization, graphs, retail, scatter-plot, tableau, tableau-public]",44-45
DonnC,retail-simulation,,https://github.com/DonnC/retail-simulation,https://api.github.com/repos/retail-simulation/DonnC,a quick and dirty retail and ewallet simulation ,"# retail-simulation
 a quick and dirty simplex retail and ewallet simulation 
",0,0,1,0,retail,"[ewallet, python, retail, simpy, simulation]",44-45
rifset,shopping-missions,,https://github.com/rifset/shopping-missions,https://api.github.com/repos/shopping-missions/rifset,"People do shopping to fulfill their needs. Regardless of the shopping styles, every shopping activity has a specific purpose or mission.","Understanding E-Commerce Consumers Shopping Missions Through Unsupervised Learning
================

![cover.jpg](asset/cover.jpg)

*Illustration asset by [freestocks](https://unsplash.com/@freestocks?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/photos/_3Q3tsJ01nc?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)*

# Table of Content
- [Understanding shoppers behavior](#understanding-shoppers-behavior)
    * [Shopping goals](#shopping-goals)
    * [Shopping missions](#shopping-missions)
- [Offline retail store and online commerce](#offline-retail-store-and-online-commerce)
- [Clustering analysis](#clustering-analysis)
- [Shopping missions identification](#shopping-missions-identification)
  * [Data preparation](#data-preparation)
  * [Segmenting shopping goal](#segmenting-shopping-goal)
  * [Identifying shopping mission](#identifying-shopping-mission)
  * [Results](#results)
- [Closing thoughts](#closing-thoughts)
- [References](#references)

---

Human beings have certain needs. Those needs can be categorized into three distinct groups: primary, secondary, and tertiary needs. To address these needs, we, as humans do shopping, While some individuals adopt a regular and structured approach to shopping, adhering to monthly, weekly, or even daily schedules, others are inclined towards more impulsive buying habits. Regardless of the shopping style one adopts, every shopping activity has a specific purpose or mission.

# Understanding shoppers behavior

Before delving deeper, let’s take a look at shopping goals and missions.

## Shopping goals

Concrete shopping goals usually occur when someone wants to buy specific promotional goods or instantly consumable items. This type of shopping goal implies shoppers' trip behavior into swift decision-making and purposely item selection, making the shopping trips efficient. On the other hand, abstract shopping goals stand as an alternative facet of the shopping spectrum. This objective is characterized by its unpredicted and expansive nature, such as purchasing various miscellaneous items in less frequent shopping trips [1].

Addressing the grey area between concreteness and abstractness, there exist relatively concrete or relatively abstract shopping goals. This category of goals treads the middle ground between the stark specificity of concrete goals and the expansiveness of abstract ones. Such goals typically revolve around the replenishment of perishable commodities. These objectives, while less narrowly defined than concrete goals, showcase a noticeable level of specificity that guides shopping trips.

![Conceptual relationship between shopping goals and missions [1].](asset/shopping-goal-and-mission.jpg)

*Conceptual relationship between shopping goals and missions [1].*

## Shopping missions

Shopping trips characterized by very concrete goals often revolve around the confines of a single product category. In such instances, individuals embark on their shopping journey with a distinct item in mind, driven by an explicit intention to acquire that particular product and only that product. Likewise, abstract shopping trips, characterized by their multifaceted nature, encompass an array of different needs housed within various product categories. These trips make shopping missions hard to identify as there are complex relationships among the product categories involved.

In contrast, relatively concrete shopping trips offer a balanced assortment of product categories showcasing specificity and variety. This equilibrium renders such shopping trips conducive to the identification of shopping missions.

# Offline retail store and online commerce

There are distinct disparate aspects between offline retail stores and e-commerce. Offline retail businesses, encompassing the likes of hypermarkets and convenience stores, exhibit a marked dissimilarity in their product assortments. Hypermarkets stand as expansive retail destinations where an extensive variety of goods is readily available. This contrasts with the relatively narrower product spectrum typically found in convenience stores. This dissimilarity casts a significant impact on the dynamics of shoppers' journeys within these offline retail environments.

![Asset illustration by [Nathália Rosa](https://unsplash.com/@nathaliarosa?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/photos/rWMIbqmOxrY?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) and writer on [Exabytes](https://www.exabytes.co.id/blog/apa-itu-e-commerce-adalah/)](asset/offline-retail-vs-ecommerce.jpg)

*Asset illustration by [Nathália Rosa](https://unsplash.com/@nathaliarosa?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/photos/rWMIbqmOxrY?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) and writer on [Exabytes](https://www.exabytes.co.id/blog/apa-itu-e-commerce-adalah/)*

Studies have demonstrated that hypermarket shoppers tend to exhibit a smaller proportion of low-variety categories baskets compared to their counterparts shopping at convenience stores. Convenience store shoppers, characterized by their penchant for swift purchase-and-go behaviors, display an inclination to purchase a few distinctive products. The convenience store setting, tailored for quick and immediate purchases, fosters this behavior, enabling shoppers to efficiently acquire their desired products with minimal fuss.

Moving on to e-commerce, a parallel can be drawn in terms of the variety of products available, akin to the vast product assortments of hypermarkets. However, the unique value that defines e-commerce is the ease with which products can be procured online. This convenience factor, while encouraging frequent purchases, also implies a distinct pattern in consumers' shopping habits. The accessibility and simplicity of online shopping often steer individuals towards the practice of purchasing fewer items but doing so on a more regular basis. While this hypothesis is rooted in personal observations and experiences, further scholarly inquiry is eagerly recommended.

# Clustering analysis

Cluster analysis, commonly referred to as clustering, is the process of categorizing a collection of objects in a manner that aligns objects within the same group, or cluster, based on their degree of similarity to each other. This principle entails that items sharing greater affinities are grouped together within a cluster, forging connections that are more pronounced compared to connections with objects in other clusters. Clustering is categorized as unsupervised learning. Unlike supervised learning, clustering lacks a predefined reference point, or ""ground truth,"" that would enable the assessment of the accuracy of the clustering outcomes.

![k-medoids.png](asset/k-medoids.png)

*Mean vs medoid illustrated [2].*

In this article, the main analysis uses a specific hard clustering technique known as K-Medoids. This method entails the partitioning of data into clusters, where each cluster is represented by a data point termed a ""medoid."" The K-Medoids algorithm strives to minimize the dissimilarity or distance between data points within a cluster while maximizing the dissimilarity between clusters. To learn more about this kind of clustering, see [2].

# Shopping missions identification

Within this section, I will analyze the shopping baskets of the Flipkart e-commerce platform shoppers. The dataset for this analysis is accessible through [Kaggle](https://www.kaggle.com/datasets/datamonkey121/flipkart-supermart-product-and-transaction-details).

## Data preparation

For simplicity purposes, only June transaction data will be used for the entire analysis. Also, the library `data.table` and `tidyverse` have carried out the entire computation on R Studio.

```r
library(data.table)
library(tidyverse)

setwd(...)

# data importing
sales <- lapply(str_subset(dir(
    file.path(""flipkart-supermarket-dataset"")
  ), ""^fact_sales_jun""), function(filename) {
    fread(file.path(""flipkart-supermarket-dataset"", filename))
  }) %>% rbindlist()

# remove unnecessary columns
sales[, c(""V1"", ""Unnamed: 0"") := NULL]
```

![transaction-data-preview.png](asset/transaction-data-preview.png)

**Product category selection**

In total, there are 205 categories listed on the product database. However, owing to the variance in transactional contributions across these categories over the observed month, a selective analysis is deemed more appropriate. Low-frequency purchase categories (LFC) are considered a refined approach for delineating the segmentation [3], particularly since certain categories, such as fresh vegetables, are frequently purchased by a amass portion of consumers which imbues little information toward the segmentation.

```r
# calculating frequency contribution
purchaseCat <- sales[dimProduct, on = ""product_id"", nomatch = 0][, .(
  sales = sum(total_weighted_landing_price, na.rm = TRUE),
  purchase_frequency = sum(procured_quantity, na.rm = TRUE)),
  by = ""l1_category_id""][order(-purchase_frequency)]
purchaseCat[, pct_pf := cumsum(purchase_frequency)/sum(purchase_frequency)]

# filtering LFC
LFC <- purchaseCat[pct_pf > .8][[""l1_category_id""]]
```

To define LFC, I adopted the Pareto principle, thereby filtering out the 80% of categories that collectively contribute to the overarching transaction frequency. This approach aligns with the methodology shown in [3].

**Transaction (sales) data finalization**

```r
# finalizing sales data
summarySales <- sales[dimProduct, on = ""product_id"", nomatch = 0][
  l1_category_id %in% LFC][, .(
    sales = sum(total_weighted_landing_price, na.rm = TRUE),
    n_item = uniqueN(product_id),
    n_category = uniqueN(l1_category_id)),
    by = ""cart_id""][sales > 0]
```

The remaining 20%, or 155, categories are classified as low-frequency categories (LFC), which also corresponds to 34.3% of overall sales generated. The table below depicts the attributes of each basket (aliased as cart), which include basket value (`sales`), distinct item counts (`n_item`), and distinct category counts (`n_category`).

![cart-attr-preview.jpg](asset/cart-attr-preview.jpg)

## Segmenting shopping goal

To segment shopping goals, a one-dimensional univariate clustering method was employed [4] [5]. The targeted number of clusters was set at 3, aligning with a theoretical perspective that posits shopping goals can be categorized into three distinct groups. Using the distinct category counts as the input for clustering, the computation was performed 50 times, to avoid local minima, then aggregated, with each iteration considering a fixed-size sub-dataset that is equal to 10% of the original dataset.

```r
library(Ckmeans.1d.dp)

# compute clustering
n.sample <- 50L
CKmeans_partition <- lapply(1:n.sample, function(x) {
  summarySales_sampled <- summarySales %>%
    slice_sample(prop = .10)
  categorySegment <- Ckmeans.1d.dp(summarySales_sampled$n_category, k = 3L)
  summarySales_sample_cluster <- summarySales_sampled %>%
    cbind(data.table(cluster = categorySegment$cluster)) %>%
    group_by(cluster) %>%
    summarize(
      N = uniqueN(cart_id),
      GMV = sum(sales),
      min_cat = min(n_category),
      max_cat = max(n_category)
    )
})

# summarizing results
CKmeans_summary <- CKmeans_partition %>%
  rbindlist() %>%
  group_by(cluster) %>%
  summarize(across(c(N, GMV, min_cat, max_cat), mean)) %>%
  mutate(`% GMV` = GMV/sum(GMV), .after = ""GMV"") %>%
  mutate(`% N` = N/sum(N), .after = ""N"") %>%
  select(cluster, `% N`, `% GMV`, min_cat, max_cat) %>%
  arrange(min_cat)

# check results
print(CKmeans_summary)

# applying to sales data
summarySales[, segment := cut(
  n_category, breaks = c(0,1,4,Inf), # based on clustering result, see CKmeans_result
  label = c(""Very concrete"", ""Relatively concrete"", ""Abstract""),
  include.lowest = TRUE)]
summarySales_SG <- summarySales[, .(basket = .N, value = sum(sales)), by = ""segment""]
```

A significant majority, exceeding 60% of the transaction baskets categorized as ""very concrete"", comprises solely single-category baskets. This outcome aligns with the hypotheses mentioned in the preceding section: the convenience of online e-commerce prompts individuals to opt for smaller, more frequent purchases. 

![first-segment-results.jpg](asset/first-segment-results.jpg)

While the ""very concrete"" segment contributes the largest proportion of basket counts, the segment ""relatively concrete"" surpasses others in terms of sales contribution. The 484,992 baskets in “relatively concrete” segment will be utilized to identify shopping missions for the rest of the analysis.

## Identifying shopping mission

Within this section, a second clustering will be performed exclusively on the subset of baskets classified in the ""relatively concrete"" segment. 

```r
# getting the appropriate segment
segmentRC <- summarySales[segment == 'Relatively concrete']
segmentRC_items <- sales[cart_id %in% segmentRC$cart_id, .(
  cart_id, product_id)][dimProduct, on = ""product_id"", nomatch = 0][
    l1_category_id %in% LFC]
segmentRC_item <- unique(segmentRC_items[, .(cart_id, l1_category_id)])[
  order(cart_id, l1_category_id)]

# creating participation matrix
participationMatrix <- dcast(
  data = segmentRC_item[, .(cart_id, l1_category_id, value = 1)],
  formula = cart_id ~ paste0(""cat_"", l1_category_id),
  value.var = ""value"",
  fill = 0
)
```

For the purpose of this clustering, the data undergoes a transformation into a binary flags matrix. This matrix indicates the presence (1) or absence (0) of a specific product category within a given basket, thereby forming what is referred to as a participation matrix. To effectively capture the dissimilarities existing among each individual basket, Jaccard distance is used as internal metrics for K-Medoids clustering.

![participation-matrix-preview.jpg](asset/participation-matrix-preview.jpg)

Due to the substantial volume of the data, the Clustering for Large Applications (CLARA) algorithm was selected for use, as traditional Partition Around Medoids (PAM) clustering might prove to be inefficient in such a scenario. 

```r
library(clustering)
library(factoextra)

# determining optimal cluster (k)
gapStat <- clusGap(
  x = select(participationMatrix, -cart_id),
  FUNcluster = clara, K.max = 10, B = 80,
	samples = 2^10, metric = ""jaccard"", pamLike = TRUE
)
fviz_gap_stat(gapStat)

# clustering model training
fit_clara_SM <- clara(select(participationMatrix, -cart_id), k = 5L,
                      metric = ""jaccard"", pamLike = TRUE)
```

To obtain the optimal cluster, the gap statistics method was chosen. The results indicate that 5 clusters represent the optimal number for this second phase of clustering. This finding implies the establishment of 5 distinct shopping missions as the clustering results.

![plot-gap-stat-SM.png](asset/plot-gap-stat-SM.png)

## Results

![final-results.jpg](asset/final-results.jpg)

Table above depicts the preview of final clustering results. Almost two-thirds of the total baskets have been grouped into a sizable cluster, while the remaining four clusters exhibit a nearly equal distribution amongst themselves.

- Cluster 1, the most extensive cluster in terms of basket count (constituting 61.5% of the total), appears to lack a discernible pattern, which is a somewhat unfortunate outcome given its size.
- Cluster 2 (10.7%), on the other hand, emerges as dominant in categories like Toilet and Floor Cleaners, as well as Dishwashing products, suggesting a mission centered around ""**household needs**.""
- With a composition of 6.7%, cluster 3 represents a ""**personal care**"" mission, evidenced by its high participation in products from categories such as Shampoo, Conditioner, and Deodorants.
- Cluster 4 (11.1%) predominantly features items like Glucose & Marie, Rusks & Wafers, and Healthy & Digestive products, embodying a mission revolving around ""**light snacks**.""
- Lastly, Cluster 5 (10.0%) embodies a ""**breakfast**"" mission, given its domination by products from categories like Dips & Spreads and Cheese.

While the suggested cluster number was guided by a theoretically sound approach (gap statistics),  the actual clustering outcomes have yielded results that are less than satisfactory. This assertion is largely rooted in the presence of a solitary dominant cluster, which, unfortunately, fails to provide any meaningful insights. Despite the fact that the remaining clusters offer valuable and insightful information, the prominence of this single uninformative cluster raises my concerns. Hence, based on this assessment, it is recommended to refine and expand the experimentation process. By doing so, the objective is to enhance the overall quality of the clustering results, to ensure that they are more representative and informative.

# Closing thoughts

For practitioners, understanding the shopping missions of consumers offers lots of advantages, mainly enabling the implementation of more precise, pertinent, and efficient marketing activities. Retailers can tailor their efforts toward creating specific campaigns that cater to the unique characteristics and preferences associated with each shopping mission. Likewise, by strategically positioning their brands within the framework of various shopping missions, goods manufacturers can capitalize on the opportunity to bolster brand relevance and convenience for shoppers. This involves crafting brand narratives and product placements that align seamlessly with the context of each shopping mission, thereby cultivating a stronger resonance with consumers.

For academicians, there is wide room to explore and experiment with this kind of analysis. Refining data processing techniques, exploring alternative methodologies, or even embracing machine learning-based approaches holds significant promise. The objective is to improve the precision of the shopping mission segmentation so that conclusive insights can be drawn. Huge credit is owed to a published paper authored by Sarantopoulos and friends (see [References](#references)), which served as a profound source of inspiration for this article. An alternative avenue for selecting the experimental subject and the clustering methodology was embarked upon to satisfy the inquisitiveness of the author (me).

---

# References

[1] P. Sarantopoulos, A. Theotokis, K. Pramatari, and G. Doukidis, ""Shopping missions: An analytical method for the identification of shopper need states,"" *J. Bus. Res.*, vol. 69, no. 3, pp. 1043–1052, Mar. 2016. Accessed: Aug. 13, 2023. [Online]. Available: [https://doi.org/10.1016/j.jbusres.2015.08.017](https://doi.org/10.1016/j.jbusres.2015.08.017)

[2] S. Mannor *et al.*, ""K-Medoids clustering,"" in *Encyclopedia of Machine Learning*. Boston, MA: Springer US, 2011, pp. 564–565. Accessed: Aug. 20, 2023. [Online]. Available: [https://doi.org/10.1007/978-0-387-30164-8_426](https://doi.org/10.1007/978-0-387-30164-8_426)

[3] B. von Mutius and A. Huchzermeier, ""Customized targeting strategies for category coupons to maximize CLV and minimize cost,"" *J. Retailing*, Feb. 2021. Accessed: Aug. 2, 2023. [Online]. Available: [https://doi.org/10.1016/j.jretai.2021.01.004](https://doi.org/10.1016/j.jretai.2021.01.004)

[4] M. Song and H. Zhong, ""Efficient weighted univariate clustering maps outstanding dysregulated genomic zones in human cancers,"" *Bioinformatics*, vol. 36, no. 20, pp. 5027–5036, Jul. 2020. Accessed: Aug. 20, 2023. [Online]. Available: [https://doi.org/10.1093/bioinformatics/btaa613](https://doi.org/10.1093/bioinformatics/btaa613)

[5] H. Wang and M. Song, ""Ckmeans.1d.dp: Optimal k-means clustering in one dimension by dynamic programming,"" *R J.*, vol. 3, no. 2, p. 29, 2011. Accessed: Aug. 20, 2023. [Online]. Available: [https://doi.org/10.32614/rj-2011-015](https://doi.org/10.32614/rj-2011-015)

[6] DataMonkey. (2022, November). FlipKart Supermart Product and transaction details. Version 1. Retrieved Aug 13, 2023 from [https://www.kaggle.com/datasets/datamonkey121/flipkart-supermart-product-and-transaction-details](https://www.kaggle.com/datasets/datamonkey121/flipkart-supermart-product-and-transaction-details)
",0,0,1,0,retail,"[business, clara, clustering, ecommerce, jaccard-distance, k-medoids, online-shop, pam, r, retail]",44-45
Arpita-deb,Sweet-Symphony-Dessert-Shop-SQL-Analysis,,https://github.com/Arpita-deb/Sweet-Symphony-Dessert-Shop-SQL-Analysis,https://api.github.com/repos/Sweet-Symphony-Dessert-Shop-SQL-Analysis/Arpita-deb,"Database design project: Building a business database from scratch using DDL, DML and DQL.","# Sweet Symphony Dessert Shop Analysis in SQL
## Database Design Project: Building a database from scratch

## Introduction:
Sweet Symphony Desserts Shop is a fictional family owned business known for their variety of desserts and confectionaries. In this project I designed a business database to help the business owner keep track of their products, product categories, employee details, customers and order details. 

## Tool used:
This project is done in Microsoft SQL Server and SQL Server Management Studio(SSMS).

## Project Overview:
In this project a database with 5 tables of a fictional dessert shop has been created using SQL CRUD Statements and different Data Manipulation Languages (DML). **CRUD** stands for **CREATE, READ, UPDATE** and **DELETE** that refers to major operations on databases. Each letter in the acronym can be mapped to SQL operational clauses.

| CRUD statements | SQL Clauses |
| :-- | :-- |
|create | CREATE |
| read | SELECT |
| update | UPDATE |
| delete | DROP |

The project is divided into 3 parts-
1. Creating the Tables using CREATE, ALTER and DROP Clauses
2. Inserting Values and updating them with INSERT INTO, SELECT, UPDATE, ALTER, DELETE FROM Clauses
3. Using Data Query Language (DQL) to answer some basic questions about the data

# Analysis
## Part 1: Creating the Tables

In this part, a Database named **Sweet Symphony Desserts Shop** and 5 tables have been created. The following Data Definition Language (DDL) commands were used-
* *CREATE*    -  to create the database and the tables
* *DROP*      -  to delete/remove the database objects from the SQL database
* *ALTER*     -  to change or modify the existing structure of the database

Here only 2 tables have been shown created.

![Screenshot (452)](https://github.com/Arpita-deb/Sweet-Symphony-Dessert-Shop-SQL-Analysis/assets/139372731/9c5843dd-cac9-4290-acc7-538911d0a235)

![Screenshot (451)](https://github.com/Arpita-deb/Sweet-Symphony-Dessert-Shop-SQL-Analysis/assets/139372731/c61c64d9-4db5-4558-a6d7-e8e32899b7d0)

### **Tables Created**

* Categories table - individual products are grouped into 8 categories. 

   ![cat](https://github.com/Arpita-deb/Sweet-Symphony-Dessert-Shop-SQL-Analysis/assets/139372731/a446bac2-4877-4461-9c86-e57b2a58003f)
  
* Customers table - customers who have ordered products from Sweet Symphony.

   ![cust](https://github.com/Arpita-deb/Sweet-Symphony-Dessert-Shop-SQL-Analysis/assets/139372731/3572de45-a6cf-4844-ac68-c5478226f4f8)
  
* EmployeeDetails table - employees who work for Sweet Symphony.

  ![employee table](https://github.com/Arpita-deb/Sweet-Symphony-Dessert-Shop-SQL-Analysis/assets/139372731/9cfbf668-ecb8-456c-b18d-ce137eff0520)
  
* Products table - The details of products which Sweet Symphony sold in their first 6 months of opening.

   ![pdt](https://github.com/Arpita-deb/Sweet-Symphony-Dessert-Shop-SQL-Analysis/assets/139372731/f2f3411f-dd84-450b-b50f-0ab4d9fc2010)
  
* Orders table - Order details from the first 2 months of opening.

![order](https://github.com/Arpita-deb/Sweet-Symphony-Dessert-Shop-SQL-Analysis/assets/139372731/f4b1147e-4ca0-45c5-b80b-dbef1193f8bb)
<br><br>

![Screenshot (469)](https://github.com/Arpita-deb/Sweet-Symphony-Dessert-Shop-SQL-Analysis/assets/139372731/44329106-1f01-4265-a5d1-91604691ea03)
 
As can be seen from the database design diagram only Orders and Customers tables are joined together with the help of a Foreign key (CustomerID). Each table has individual Primary key and datatypes of the columns defined.

## Part 2: Inserting and updating values in the tables

**Data manipulation language (DML)** is a family of computer languages that permits users to manipulate data in a database by inserting data into database tables, retrieving existing data, deleting data from existing tables and modifying existing data. The following DML commands are used- 
* *SELECT*   - to show the records of the specified table
* *INSERT*   - to insert data in database tables.
* *UPDATE*   - to update or modify the existing data in database tables 
* *DELETE*   - to remove single or multiple existing records from the database tables.

Here values are inserted in the EmployeeDetails table. 
![insert 1](https://github.com/Arpita-deb/Sweet-Symphony-Dessert-Shop-SQL-Analysis/assets/139372731/faa8f849-9670-449d-b696-7a5c06cdae7c)

![insert 2](https://github.com/Arpita-deb/Sweet-Symphony-Dessert-Shop-SQL-Analysis/assets/139372731/89d383da-0484-423e-8810-6057705158c5)

![insert 3](https://github.com/Arpita-deb/Sweet-Symphony-Dessert-Shop-SQL-Analysis/assets/139372731/293b8882-3511-4a49-8871-cc5a328c0f51)

![insert 4](https://github.com/Arpita-deb/Sweet-Symphony-Dessert-Shop-SQL-Analysis/assets/139372731/b27e53de-746a-400c-8f9a-45747a851aa8)
  
## Part 3: Answer some basic questions about the data using SQL

The following clauses and functions were used to obtain various information from the tables.
  
   * *WHERE*         - to filter data of interest
   * *GROUP BY*      - to group rows that have the same values into summary rows
   * *ORDER BY*      - to sort the data in ascending or descending order
   * *CONCAT*        - to  add two or more strings together
   * *DATEDIFF*      - to return the difference between two date values
   * *DATEADD*       - to add a date to another date, then return the new date
   * *JOIN*          - to combine rows from two or more tables, based on a related column between them
   * *BETWEEN*       - to filter values in a range
   * *MAX*           - to obtain the maximum value of a numeric column
   * *MIN*           - to obtain the minimum value of a numeric column
   * *COUNT*         - to return the number of rows that matches a specific criterion
   * *SUM*           - to return the total sum of a numeric column.
   * *AVG*           - to get average value of a numeric column.
   * *LIKE*          - to search for a specified pattern in a column.
   * *IN*            - to specify multiple values in a WHERE clause
<br><br><br>
   
* How many categories of product did they sell?

  ![3C](https://github.com/Arpita-deb/Sweet-Symphony-Dessert-Shop-SQL-Analysis/assets/139372731/179efd18-c5dd-48cb-b09c-53bcb4eb821e)

  ![3A](https://github.com/Arpita-deb/Sweet-Symphony-Dessert-Shop-SQL-Analysis/assets/139372731/6ea96fcb-d042-4921-aaf4-a1d2dfb6fdbb)
  
* Show the total price of products sold by categories.
  
   ![8C](https://github.com/Arpita-deb/Sweet-Symphony-Dessert-Shop-SQL-Analysis/assets/139372731/c9968938-de18-47dc-9d23-7bfcf84c7532)

   ![8A](https://github.com/Arpita-deb/Sweet-Symphony-Dessert-Shop-SQL-Analysis/assets/139372731/2941b66f-8fd7-4b2f-b474-a6203e0c19ab)

* What is the average price of the products sold?

  ![AVG C](https://github.com/Arpita-deb/Sweet-Symphony-Dessert-Shop-SQL-Analysis/assets/139372731/5f65d4c9-d47e-483d-addc-6b64cc709a2b)
  
  ![AVG A](https://github.com/Arpita-deb/Sweet-Symphony-Dessert-Shop-SQL-Analysis/assets/139372731/8891d230-f3fc-4e20-9dbb-014b9b2c2a98)
  
* What are Sweet Symphony's Top 5 profitable products?

  ![6C](https://github.com/Arpita-deb/Sweet-Symphony-Dessert-Shop-SQL-Analysis/assets/139372731/8be4c158-97fc-494f-809f-70fdfc438aff)

  ![6A](https://github.com/Arpita-deb/Sweet-Symphony-Dessert-Shop-SQL-Analysis/assets/139372731/4ed6f7c4-0e14-4073-9afb-56dd9495b58c)
  
* What are Sweet Symphony's 5 least profitable products?

  ![7C](https://github.com/Arpita-deb/Sweet-Symphony-Dessert-Shop-SQL-Analysis/assets/139372731/e7b57692-3927-494e-ae34-a3d38f2b58ab)

  ![7A](https://github.com/Arpita-deb/Sweet-Symphony-Dessert-Shop-SQL-Analysis/assets/139372731/5387ca8d-bc1c-42e9-99dc-75abf848953c)
  
* What is their total revenue?
  
  ![9C](https://github.com/Arpita-deb/Sweet-Symphony-Dessert-Shop-SQL-Analysis/assets/139372731/65060a9b-133b-48a6-96ec-08077bfea8dc)

  ![9A](https://github.com/Arpita-deb/Sweet-Symphony-Dessert-Shop-SQL-Analysis/assets/139372731/5c9728f0-3ba5-4cb7-81e9-c6d112c340b3)

* Which product is most expensive?

  ![5C](https://github.com/Arpita-deb/Sweet-Symphony-Dessert-Shop-SQL-Analysis/assets/139372731/e89f13e5-d8c9-43b7-9a59-a8940c664ea8)

  ![5A](https://github.com/Arpita-deb/Sweet-Symphony-Dessert-Shop-SQL-Analysis/assets/139372731/432e4b68-c0b0-4489-8749-d29444b08cc5)

* Which product is the cheapest?
  
  ![2C](https://github.com/Arpita-deb/Sweet-Symphony-Dessert-Shop-SQL-Analysis/assets/139372731/4cdfbceb-7957-42c0-b623-9de228877dcb)

  ![2A](https://github.com/Arpita-deb/Sweet-Symphony-Dessert-Shop-SQL-Analysis/assets/139372731/614a17a8-1a44-46de-9408-8056b7f8aa08)

* Show the full name of the employees.

  ![4C](https://github.com/Arpita-deb/Sweet-Symphony-Dessert-Shop-SQL-Analysis/assets/139372731/5510917c-5978-4640-85d7-1d4eb0887665)

  ![4A](https://github.com/Arpita-deb/Sweet-Symphony-Dessert-Shop-SQL-Analysis/assets/139372731/feccf292-f5b1-4368-aebf-d1cfe5c820be)

* Show the employee names whose last name consists of 'er'.

  ![13C](https://github.com/Arpita-deb/Sweet-Symphony-Dessert-Shop-SQL-Analysis/assets/139372731/039ffb9f-facc-4881-b92e-a169b5dfba63)

  ![13A](https://github.com/Arpita-deb/Sweet-Symphony-Dessert-Shop-SQL-Analysis/assets/139372731/518f936e-6d26-4e60-a5ae-aa9e61edba27)

* Show the orders from Portland in the month of July.

  ![10C](https://github.com/Arpita-deb/Sweet-Symphony-Dessert-Shop-SQL-Analysis/assets/139372731/42d64ef3-7d43-4d7b-b0b8-1a6581067801)

  ![10A](https://github.com/Arpita-deb/Sweet-Symphony-Dessert-Shop-SQL-Analysis/assets/139372731/ae8e288a-b64c-4aa7-95de-420d86eef2b2)

* The customer from Eugene, Oregon ordered some desserts from Sweet Symphony which they want a week later. Update the shippeddate to a week after and add a required date column.

  ![12C](https://github.com/Arpita-deb/Sweet-Symphony-Dessert-Shop-SQL-Analysis/assets/139372731/12a20d5e-7622-453b-bdc4-3a657853353c)

  ![12A](https://github.com/Arpita-deb/Sweet-Symphony-Dessert-Shop-SQL-Analysis/assets/139372731/5437b5fc-03c5-43ef-b639-a199e7c50602)

* Calculate the difference of days between OrderDate and ShippedDate.

  ![14C](https://github.com/Arpita-deb/Sweet-Symphony-Dessert-Shop-SQL-Analysis/assets/139372731/98ef6028-6364-4dab-8d2f-20a4085d54c0)

  ![14A](https://github.com/Arpita-deb/Sweet-Symphony-Dessert-Shop-SQL-Analysis/assets/139372731/6a8a69bd-7951-40ed-97bd-28ec1ee8e797)

* Show the customer details who ordered products from Sweet Symphony.

   ![11C](https://github.com/Arpita-deb/Sweet-Symphony-Dessert-Shop-SQL-Analysis/assets/139372731/680b2a61-a2a1-4c9e-ae24-ebc5ba06c715)

   ![11A](https://github.com/Arpita-deb/Sweet-Symphony-Dessert-Shop-SQL-Analysis/assets/139372731/e194c827-591e-4b89-9bbd-e6a5834cb364)

## Conclusion:

* A Database named **Sweet Symphony Desserts Shop** is created in Microsoft SQL Server.
* With the help of **CREATE, ALTER, DROP** clauses 5 tables have been created and altered.
* Values have been inserted into the tables using **INSERT INTO, SELECT, UPDATE, ALTER** and **DELETE FROM** Clauses
* Various information has been obtained using **SELECT** statement.
* Use of **WHERE, GROUP BY, ORDER BY** clauses have been shown.
* Different **aggregate functions** have been used to obtain the sum, average, maximum and minimum of numerical data points.

## List of References:

* [Data Definition Language (DDL)](https://en.wikipedia.org/wiki/Data_definition_language)
* [Data Manipulation Language (DML)](https://www.javatpoint.com/dml-commands-in-sql)
* [Data Query Language (DQL)](https://en.wikipedia.org/wiki/Data_query_language)
* [Crud Operations in SQL](https://www.javatpoint.com/crud-operations-in-sql)
* [Primary Key](https://www.w3schools.com/sql/sql_primarykey.asp)
* [Foreign Key](https://www.w3schools.com/sql/sql_foreignkey.asp)
* [CONCAT Function in SQL](https://www.javatpoint.com/concat-function-in-sql)
* [DATEDIFF Function in SQL](https://www.w3schools.com/SQl/func_sqlserver_datediff.asp)
* [DATEADD Function in SQL](https://www.w3schools.com/SQl/func_sqlserver_dateadd.asp)
* [SQL Wildcard Characters](https://www.w3schools.com/SQl/sql_wildcards.asp)
* [Types of SQL Joins](https://www.javatpoint.com/types-of-sql-join)
",0,0,3,0,retail,"[data-definition-language, data-manipulation-language, data-query-language, database-management, retail, sql]",44-45
it3xl,IT-Trade-2009,,https://github.com/it3xl/IT-Trade-2009,https://api.github.com/repos/IT-Trade-2009/it3xl,Some trade app of a solo developer (2009 year),"# IT-Trade-2009

## Some trade app of an solo developer from 2009 year.

WPF .NET",0,0,2,0,retail,"[csharp, dotnetframework, retail, wpf, wpf-application, xaml]",44-45
m-paulus,merch_calendar,,https://github.com/m-paulus/merch_calendar,https://api.github.com/repos/merch_calendar/m-paulus,Python functions and classes related to the National Retail Federation's 4-5-4 merchandise calendar. ,,0,0,0,0,retail,"[merchandising, planning, retail]",44-45
destodasoftware,kately_api,,https://github.com/destodasoftware/kately_api,https://api.github.com/repos/kately_api/destodasoftware,Kately API merupakan interface yang digunakan untuk mengintegrasikan beberapa aplikasi Kately lainnya (Kately Office).,"# Kately API
Kately API merupakan interface yang digunakan untuk mengintegrasikan beberapa aplikasi Kately lainnya (`Kately Office`).

**Tentang Kately**

Sebuah software kelola ritel (khususnya untuk ritel brand pakaian) yang dapat membantu anda mengelola transaksi
penjualan di store. Dengan bantuan Kately, anda bisa mengelola banyak aktivitas ritel yang sebelumnya
biasa Anda kerjakan sendiri.

![Kately - Destoda Software](kately-github.png)

## Prasyarat
- Python 3.6+
- Django 2.2+

## Setup
Anda perlu menginstal beberapa package yang diperlukan:

```
$ pip install -r requirements.txt
```

**Konfigurasi**

Jika Anda ingin mengubah konfigurasi lainnya, Anda bisa melakukannya pada file `config/settings.py`.

**Migrasi**

Jalankan perintah berikut untuk menjalankan migrasi:

```
$ python manage.py migrate
```

**Menjalankan Development Server**

Setelah semuanya selesai, Anda bisa langsung menjalankan development server dengan mengetik perintah:

```
$ python manage.py runserver
```

## Dokumentasi
Ketika development server sudah berjalan, Anda bisa memulai playground API yang tersedia:

```
http://localhost:8000/office/
```

## Lisensi

```
Copyright (c) 2019 Destoda Software

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the ""Software""), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```
",0,0,1,5,retail,"[django, point-of-sale, python-3, rest-framework, retail]",44-45
kamireddig,GetDailyRevenue,,https://github.com/kamireddig/GetDailyRevenue,https://api.github.com/repos/GetDailyRevenue/kamireddig,Scope of this project is to calculate Daily Revenue from retail products,"# GetDailyRevenue
This is a Spark Project to get the Daily revenue of the products based on many parameters which are explained in detail in this project.

### Problem Statement

### UML Diagram

<img src=""https://github.com/kamireddig/GetDailyRevenue/blob/master/Retail_DB_UML.png"" width=""900"">

**Technologies and Conecepts used in this project**
1. Hadoop
   1. HDFS
   2. HIVE
   3. SQOOP
2. Spark
   1. Spark SQL
   2. Spark RDD
   3. Spark Dataframes

**System Configurations**
1. Personal Laptop: MacOS 10.15.4 (Catalina)
2. UNIX System hostname: gw02.itversity.com
   1. Connect using **'ssh gw02.itversity.com'**
   2. Prompts with a password. (Give the password provided to you.)

**Data Locations**
1. Data is present in the server 'gw02.itversity.com' in the HDFS Cluster in path: '/public/retail_db'
   1. Access the data in the path using the command
      > **hadoop fs -ls /public/retail_db**

### Sqoop
<p>Apache Sqoop allows easy import and export of data from structured data stores such as relational databases, enterprise data warehouses, and NoSQL systems. Using Sqoop, you can provision the data from external system on to HDFS, and populate tables in Hive and HBase.
Sqoop integrates with Oozie, allowing you to schedule and automate import and export tasks. Sqoop uses a connector based architecture which supports plugins that provide connectivity to new external systems.</p>

Access mysql in the server
> mysql -u retail_user -h ms.itversity.com -p </br>
> password: itversity

1. **Sqoop Import**   

<p>The below two loads are full loads. To use incremental load, use the below syntax in the sqoop commands.</p>
-> –incremental <mode>  (Modes: append & lastModified)
-> –check-column <column name>
-> –last value <last check column value>

**Sqoop Import into HDFS**

Link to Sqoop Documentation: https://sqoop.apache.org/docs/1.4.3/SqoopUserGuide.html#_literal_sqoop_import_literal
<<Check 7.2.2 Table 3>>

When prompted for password:</br>
>Enter Password: itversity

- sqoop import --connect jdbc:mysql://ms.itversity.com:3306/retail_db --username retail_user -P --table orders --target-dir=/user/pratikgaurav/sqoop_import/retail_db/orders -m 1
- sqoop import --connect jdbc:mysql://ms.itversity.com:3306/retail_db --username retail_user -P --table order_items --target-dir=/user/pratikgaurav/sqoop_import/retail_db/order_items -m 1
- sqoop import --connect jdbc:mysql://ms.itversity.com:3306/retail_db --username retail_user -P --table products --target-dir=/user/pratikgaurav/sqoop_import/retail_db/products -m 1
- sqoop import --connect jdbc:mysql://ms.itversity.com:3306/retail_db --username retail_user -P --table departments --target-dir=/user/pratikgaurav/sqoop_import/retail_db/departments -m 1
- sqoop import --connect jdbc:mysql://ms.itversity.com:3306/retail_db --username retail_user -P --table customers --target-dir=/user/pratikgaurav/sqoop_import/retail_db/customers -m 1
- sqoop import --connect jdbc:mysql://ms.itversity.com:3306/retail_db --username retail_user -P --table categories --target-dir=/user/pratikgaurav/sqoop_import/retail_db/categories -m 1

**Sqoop Import into HIVE**

Link to Sqoop Documentation: https://sqoop.apache.org/docs/1.4.3/SqoopUserGuide.html#_literal_sqoop_import_literal
<<Check 7.2.9 Table 8>>

When prompted or password:</br>
>Enter Password: itversity

- sqoop import --connect jdbc:mysql://ms.itversity.com:3306/retail_db --username retail_user -P --table orders --mysql-delimiters --hive-import --hive-overwrite --create-hive-table --hive-table retail_db.orders_pk -m 1
- sqoop import --connect jdbc:mysql://ms.itversity.com:3306/retail_db --username retail_user -P --table order_items --mysql-delimiters --hive-import --hive-overwrite --create-hive-table --hive-table retail_db.order_items_pk -m 1
- sqoop import --connect jdbc:mysql://ms.itversity.com:3306/retail_db --username retail_user -P --table products --mysql-delimiters --hive-import --hive-overwrite --create-hive-table --hive-table retail_db.products_pk -m 1
- sqoop import --connect jdbc:mysql://ms.itversity.com:3306/retail_db --username retail_user -P --table departments --mysql-delimiters --hive-import --hive-overwrite --create-hive-table --hive-table retail_db.departments_pk -m 1
- sqoop import --connect jdbc:mysql://ms.itversity.com:3306/retail_db --username retail_user -P --table customers --mysql-delimiters --hive-import --hive-overwrite --create-hive-table --hive-table retail_db.customers_pk -m 1
- sqoop import --connect jdbc:mysql://ms.itversity.com:3306/retail_db --username retail_user -P --table categories --mysql-delimiters --hive-import --hive-overwrite --create-hive-table --hive-table retail_db.categories_pk -m 1

2. **Sqoop Export**

3. **Sqoop Merge**

### HIVE
<p>Apache Hive is a data warehouse software project built on top of Apache Hadoop for providing data query and analysis.[2] Hive gives a SQL-like interface to query data stored in various databases and file systems that integrate with Hadoop.</p>
",0,0,2,0,retail,"[data-warehouse, databases, functional-programming, hadoop, hdfs, hive, programming, retail, retail-data, scala, spark, sparksql, sql, sqoop, sqoop-documentation, uml]",44-45
kirolos-esmat,retail-company,,https://github.com/kirolos-esmat/retail-company,https://api.github.com/repos/retail-company/kirolos-esmat,"This program helps the retail company owner to compute many calculations easily such as calculating total sales, Calculating share of each branch and sorting sales data. ",,0,0,1,0,retail,"[c, company, data, retail, sal]",44-45
ukayaj620,Retail,,https://github.com/ukayaj620/Retail,https://api.github.com/repos/Retail/ukayaj620,Simple Retail Application for university assignment,"# Retail
 Simple Retail Application for university assignment
 
 ```
 Copyright 2020 Ferdy Nicolas | Jayaku Briliantio | Jason Alexander
   Licensed under the Apache License, Version 2.0 (the ""License"");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an ""AS IS"" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
```
",0,0,1,0,retail,"[java, java-swing, oop, project, retail]",44-45
cpiccirilli1,GOODSPORTLTD,,https://github.com/cpiccirilli1/GOODSPORTLTD,https://api.github.com/repos/GOODSPORTLTD/cpiccirilli1,A fictional sporting goods store. Group project for Advanced Systems,"# GOODSPORTLTD
A FICTIONAL SPORTING GOODS SITE! 
",0,0,2,0,retail,"[bootstrap, javawebapp, retail, school-project, sports, sql, webapps, website]",44-45
simonchalder,StockManager,,https://github.com/simonchalder/StockManager,https://api.github.com/repos/StockManager/simonchalder,StockManager is a console application written in C# .NET and using a MongoDB remote database. It is meant to simulate till sales and stock management. Console prettiness by Spectre Console.,"# StockManager
Developer: Simon Chalder
Project Start Date: April 2021

StockManager is a simulated retail order processing and stock management system

![1](https://user-images.githubusercontent.com/66743889/115904059-40709a80-a45c-11eb-89c9-bb0208c8a9d8.jpg)

Currently the application features:
 - Create an order and add items to the order whilst browsing by category
 - See the order so far and running total
 - Finalise the order with a simulated payment which then removes the order items from stock
 - The ability to lookup database items according to their category
 - Lookup a particular stock item using its ID number
 - Access a managers menu which in turn allows:
     - See all stock items
     - Lookup items as above
     - Add new items to stock

![2](https://user-images.githubusercontent.com/66743889/115904109-4ebeb680-a45c-11eb-80af-0bfcfcc20487.jpg)
![3](https://user-images.githubusercontent.com/66743889/115904134-58e0b500-a45c-11eb-8396-c4237ee456aa.jpg)
![4](https://user-images.githubusercontent.com/66743889/115904148-5b430f00-a45c-11eb-8b85-c5d14599f063.jpg)
![5](https://user-images.githubusercontent.com/66743889/115904153-5d0cd280-a45c-11eb-9bd3-a945a4610062.jpg)


To-do:
 - Input validation and exception handling
 - Unit tests
 - Improve the order system with ability to choose a quantity when adding items to an order
 - Layout improvements such as tables to make data easier to read
 - Search for item by name - I can currently only do this by using the full description string
 - Speed up database queries
 - View orders for day, total sales etc.
 - Add more items to DB (currently 50)

Find me here on GitHub at github.com/simonchalder on Twitter at twitter.com/ChalderSimon and on Mastodon at cybersi_io@fosstodon.org

Friendly criticism always appreciated!
",0,0,1,0,retail,"[csharp, dotnet-core, inventory-management, mongodb-database, retail, spectre-console]",44-45
gouravdidwania,Customer-Segmentation-in-Online-Retail,,https://github.com/gouravdidwania/Customer-Segmentation-in-Online-Retail,https://api.github.com/repos/Customer-Segmentation-in-Online-Retail/gouravdidwania,"In this project, I am going to try performing customer segmentation and other related analysis on online retail data using some unsupervised learning techniques via python.","<!-- PROJECT LOGO -->
![image](https://user-images.githubusercontent.com/86877457/132400947-436793f0-dc39-4564-8758-ec70e54de0a0.jpg)
<br />
<p align=""center"">

  <h3 align=""center"">Customer Segmentation in Online Retail</h3>

  <p align=""center"">
    In this project, I'll try performing Customer Segmentation in Online Retail dataset using python, focussing on cohort analysis, understanding market base, sales and customer purchase patterns using RFM analysis and clustering.
    <br />
  </p>
</p>




<!-- TABLE OF CONTENTS -->
<details open=""open"">
  <summary>Table of Contents</summary>
  <ol>
    <li>
      <a href=""#about-the-project"">About The Project</a>
    </li>
    <li>
      <a href=""#problem-statement"">Problem Statement</a>
      <ul>
        <li><a href=""#understanding-customer-segmentation"">Understanding Customer Segmentation</a></li>
        <li><a href=""#ways-to-segment-your-customers"">Ways to Segment your Customers</a></li>
      </ul>
    </li>
    <li>
	<a href=""#data-overview"">Data Overview</a>
	<ul>
          <li><a href=""#data-attributes"">Data Attributes</a></li>
          <li><a href=""#data-snapshot"">Data Snapshot</a></li>
        </ul>
    </li>
    <li><a href=""#implementaion"">Implementaion</a>
	<ul>
          <li><a href=""#exploring-the-data"">Exploring the Data</a></li>
          <li><a href=""#cohort-analysis"">Cohort Analysis</a></li>
	  <li><a href=""#rfm-segmentation"">RFM Segmentation</a></li>
	  <li><a href=""#clustering"">Clustering</a></li>
        </ul>
    </li>
    <li><a href=""#final-thoughts"">Final Thoughts</a></li>
  </ol>
</details>



<!-- ABOUT THE PROJECT -->
## About The Project

Customer segmentation has a lot of potential benefits. It helps a company to develop an effective strategy for targeting its customers. This has a direct impact on the entire product development cycle, the budget management practices, and the plan for delivering targeted promotional content to customers. Customer segmentation can also help a company to understand how its customers are alike, what is important to them, and what is not. Often such information can be used to develop personalized relevant content for different customer bases. 

**As per the Pareto Principle, 80% of outcomes result from 20% of all the causes of any given event.**

In business terms, we can say that 20% of customers contribute 80% share of the total revenue of a company. That’s why finding this set of people is important. I will explain the importance of customer segmentation in a detailed manner later in this article itself.


<!-- PROBLEM STATEMENT -->
## Problem Statement

The overall aim of this process is to identify high-value customer base i.e. customers that have the highest growth potential or are the most profitable.

Insights from customer segmentation are used to develop tailor-made marketing campaigns and for designing overall marketing strategy and planning.

### Understanding Customer Segmentation

Customer segmentation is the process of separating customers into groups on the basis of their shared behavior or other attributes.

The type of segmentation criterion followed would create a big difference in the way the business operates and formulates its strategy. This is elucidated below.

  ```sh
  1. Zero segments: This means that the company is treating all of its customers in a similar manner. In other words, there is no differentiated strategy and all of the customer base is being reached out by a single mass marketing campaign.
  
  2. One segment: This means that the company is targeting a particular group or niche of customers in a tightly defined target market.
  
  3. Two or more segments: This means that the company is targeting 2 or more groups within its customer base and is making specific marketing strategies for each segment.
  
  4. Thousands of segments: This means that the company is treating each customer as unique and is making a customized offer for each one of them.
  ```
**Factors for segmentation for a business to consumer marketing company:**

1. **Demographic:** Age, Gender, Education, Ethnicity, Income, Employment, hobbies, etc.
2. **Recency, Frequency, and Monetary:** Time period of the last transaction, the frequency with which the customer transacts, and the total monetary value of trade.
3. **Behavioral:** Previous purchasing behavior, brand preferences, life events, etc.
4. **Psychographic:** Beliefs, personality, lifestyle, personal interest, motivation, priorities, etc.
5. **Geographical:** Country, zip code, climatic conditions, urban/rural areal differentiation, accessibility to markets, etc.

### Ways to Segment your Customers?

To start with customer segmentation, a company needs to have a clear vision and a goal in mind. 

The following steps can be undertaken to find segments in the customer base:

1. Analyze the existing customer pool
2. Develop an understanding of each customer
3. Define segment opportunities
4. Research the segment
5. Experiment with new strategies


<!-- DATA OVERVIEW -->
## Data Overview

Source: [The UCI Machine Learning Repository](https://bit.ly/3BPWgr4)

Data: This is a transnational data set which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail. The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.

Downloaded the xlxs file from Kaggle.

### Data Attributes

**1. InvoiceNo:** Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter ‘c’, it indicates a cancellation.

**2. StockCode:** Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product.

**3. Description:** Product (item) name. Nominal.

**4. Quantity:** The quantities of each product (item) per transaction. Numeric.

**5. InvoiceDate:** Invoice Date and time. Numeric, the day and time when each transaction was generated.

**6. UnitPrice:** Unit price. Numeric, Product price per unit in sterling.

**7. CustomerID:** Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer.

**8. Country:** Country name. Nominal, the name of the country where each customer resides.

### Data Snapshot

![image](https://miro.medium.com/max/2000/1*LiXE_SJlbXXTXoiLFasu7A.png)

<!-- IMPLEMANTATION -->
## Implementaion

**Real-world/Business objectives and constraints**

- Interpretability is important.
- Errors can be costly.
- Segmantation should suit the retail business

### Exploring the Data

Data was processed and cleaned. Here the duplicates were removed and the missing values were handled.

**Country**

Since the data, taken from the UCI Machine Learning repository describes the data to based on transactions for a UK-based and registered non-store online retail, let us check the percentage of orders from each country in the data.

![image](https://user-images.githubusercontent.com/86877457/132364248-5074e4c0-b17d-4b0e-ad34-83de485f3e93.png)

The above graph shows the percentage of orders from the top 10 countries, sorted by the number of orders. This shows that more than 90% of orders are coming from United Kingdom and no other country even makes up 3% of the orders in the data.

**Customers and Products**

The total number of products, transactions, and customers in the data, which correspond to the total unique stock codes was found out

![image](https://user-images.githubusercontent.com/86877457/132364618-7b91ea6c-72d5-4b38-aaac-da5d8a87978e.png)

It can be seen that the data concern 4372 users and that they bought 3958 different products. The total number of transactions carried out is of the order of ∼ 24000.

Many Order were cancelled and across the countries. As per the data, if the invoice number code starts with the letter ‘c’, it indicates a canceled order.

![image](https://user-images.githubusercontent.com/86877457/132365236-c3734aba-7a6f-4f66-8e57-b404449fbfd5.png)

We note that the number of cancellations is quite large ( ∼ 16% of the total number of transactions).
As we can see from the above figure, these cases are the ones where CustomerID values are NaNs and the decription mentioned check, lost, missing, smashed got these itmems. These cases were also removed from the data.

**Stock Code**

several types of peculiar transactions, connected i.e., port charges, bank fee, discount, free gifts, Carry bags, Samples, Amazon Fee,transaction fees which needed to be analysed.

![image](https://user-images.githubusercontent.com/86877457/132366457-8f51877b-885b-47c5-a684-de5e3fd30a3a.png)

**Net Amount/Invoice**

![image](https://user-images.githubusercontent.com/86877457/132366788-11d3cdef-3a24-43ff-a031-ed2648e03867.png)

*Amount shown are in UK Dollars

It can be seen that the vast majority of orders concern purcheses of low value ∼ 78% of purchases give prices in excess of £200.

![image](https://user-images.githubusercontent.com/86877457/132367301-c110a9f9-9566-41eb-ac66-628f4221fd69.png)

*UK not included. UK as expected topped the list int this category.

### Cohort Analysis
**What is Cohort Analysis**

A cohort is a set of users who share similar characteristics over time. Cohort analysis groups the users into mutually exclusive groups and their behaviour is measured over time.

There are three types of cohort analysis:

- **Time cohorts:** It groups customers by their purchase behaviour over time.
- **Behaviour cohorts:** It groups customers by the product or service they signed up for.
- **Size cohorts:** Refers to various sizes of customers who purchase company's products or services. This categorization can be based on the amount of spending in some period of time.

In the following analysis, I created 'Time Cohorts'

Checking the date range of our data, we find that it ranges from the start date: 2010–12–01 to the end date: 2011–12–09.

Next, a column called 'CohortMonth' was created to indicate the month of the transaction by taking the first date of the month of InvoiceDate for each transaction. Then, information about the first month of the transaction was extracted, grouped by the CustomerID.

![image](https://user-images.githubusercontent.com/86877457/132368576-33372fc3-6fc8-4e97-814f-f68394a06a2e.png)

Then I found the difference between the InvoiceMonth and the CohortMonth column in terms of the number of months to get the CohartIndex. Choartindex will tell us about the month of repurchase after their initial purchase.

![image](https://user-images.githubusercontent.com/86877457/132369142-a9ecc310-96aa-4347-b51e-8130e7103e3e.png)

After obtaining the above information, I obtained the cohort analysis matrix by grouping the data by CohortMonth and CohortIndex and aggregating on the CustomerID column.

![image](https://user-images.githubusercontent.com/86877457/132369533-f2581a25-31c9-458b-bb22-3a8f51ab6b96.png)

**What does the above table tell us?**

Consider CohortMonth 2010–12–01: For CohortIndex 0, this tells us that 948 unique customers made transactions during CohortMonth 2010–12–01. For CohortIndex 1, this tells that there are 341 customers out of 948 who made their first transaction during CohortMonth 2010–12–01 and they also made transactions during the next month. That is, they remained active.

Now I calculated the Retention Rate. It is defined as the percentage of active customers out of total customers.

![image](https://user-images.githubusercontent.com/86877457/132369804-450dd1c8-c907-45e4-b8c4-ef3646ca1efc.png)

From the above retention rate heatmap, we can see that there is an average retention of ~35% for the CohortMonth 2010–12–01, with the highest retention rate occurring after 11 months (49.3%). For all the other CohortMonths, the average retention rates are around 18–25%

### RFM Segmentation
RFM stands for **Recency, Frequency, and Monetary.**

RFM analysis is a commonly used technique to generate and assign a score to each customer based on how recent their last transaction was (Recency), how many transactions they have made in the last year (Frequency), and what the monetary value of their transaction was (Monetary).

RFM analysis helps to answer the following questions: Who was our most recent customer? How many times has he purchased items from our shop? And what is the total value of his trade? All this information can be critical to understanding how good or bad a customer is to the company.

After getting the RFM values, a common practice is to create ‘quartiles’ on each of the metrics and assigning the required order.

I used the net Amount to get the monetary value of each transaction i.e., per CustomerID
For RFM analysis, we need to define a ‘snapshot date’, which is the day on which we are conducting this analysis. Here, I have taken the snapshot date as the highest date in the (last date on data + 1). This is equal to the date 2011–12–10.(YYYY-MM-DD)

![image](https://user-images.githubusercontent.com/86877457/132370764-8099ed38-835c-4557-a320-4b543eb8e17b.png)

![image](https://user-images.githubusercontent.com/86877457/132370812-5db15f56-9c46-4a52-878e-f35818749513.png)

Then, I created 4 quartiles segment on this data with split at [25%ile,50%ile,75%ile] and collate these scores into an RFM_Segment column by assigning a score(1,2,3,4) value [R,F,M] respectively to the attributes.

- For the recency metric, the highest value, 4, will be assigned to the customers with the least recency value (since they are the most recent customers).
- For the frequency and monetary metric, the highest value, 4, will be assigned to the customers with the Top 25% frequency and monetary values, respectively.

After dividing the metrics into quartiles, The RFM_Score is calculated by summing up the RFM quartile metrics and RFM_Segment is calulated by collating them as a string of characters. 

![image](https://user-images.githubusercontent.com/86877457/132371242-2643a720-7499-4923-ad50-75385903f6e8.png)

Now the data can be grouped on the basis of RFM Score and mean attributes value can be compared for varoius RFM scores.

![image](https://user-images.githubusercontent.com/86877457/132385657-6d8acfb4-46b8-4b0f-855b-9a5c3608aead.png)

**MY ANALYSIS FROM RFM SEGMENTATION**

As expected, customers with the lowest RFM scores have the highest recency value and the lowest frequency and monetary value, and the vice-versa is true as well.

This data was clustred in three groups on the basis of RFM Score(3-12):
- **'Low':** RFM Score < 5
- **'Middle':** 5 <= RFM Score < 9
- **'Top':** RFM Score >= 9

![image](https://user-images.githubusercontent.com/86877457/132386098-ef6a5e7d-389e-4802-9dd0-92c6a1cd0248.png)

In many scenarios, this would have been okay. But, if we want to properly find out segments on our RFM values, we can use a clustering algorithm like K-means.

### Clustering
I chose K-Means clustering because the attribute were explicitly labelled. 

**Preprocessing data for Clustering**

Firstly, I prepared the data for Kmeans clustering on RFM Score data so that it can meet the key assumptions of Kmeans algorithm, which are:

1. The varaiables should be distributed symmetrically
2. Variables should have similar average values
3. Variables should have similar standard deviation values

![image](https://user-images.githubusercontent.com/86877457/132389115-97fc0124-9c2d-40c1-8f48-b03d49f2b164.png)

As you can see from the above plots, all the variables do not have a symmetrical distribution. All of them are skewed to the right. To remove the skewness, we can try the following transformations:

- Log transformations
- Box-Cox transformations
- Cube root transformations

The log transformation cannot be used for negative values. However, this data do not have any negative values since it is a customer transactions dataset.

After performming log transform and standardising the date, **Skewness has been removed**

![image](https://user-images.githubusercontent.com/86877457/132389568-138f5fbf-54f0-4bf5-ac73-4fa88c395ff8.png)



**Clustering with K-means Algorithm**

I performed K-means Clsutering on the data and follwed the 'Elbow Method' to get the optimal number of clusters. One can also use silhouette analysis to find the optimal number of clusters. For the purpose of this analysis, I have only used the elbow plot method.

![image](https://user-images.githubusercontent.com/86877457/132389865-11f63b62-b931-4ca4-9813-481ca34abf02.png)

From the above plot, the optimal number can be taken as 3 or 4 or 5.

So, after grouping the data in their clusters, for each value among 3, 4 and 5
For interpreting this segment, I drew Snake Plot fro all three values.

![image](https://user-images.githubusercontent.com/86877457/132392045-e1613880-251c-4532-b431-062e36571fad.png)

From the above snake plot, we can see the distribution of recency, frequency, and monetary metric values across the clusters. The clusters seem to be separate from each other, which indicates a good heterogeneous mix of clusters. Best happens for k=3. But k=4 happens to be more practical.

As the final step in this analysis,I extracted this information now for each customer and grouped them in the clusters formed that can be used to map the customer with their relative importance by the company:

This can be approched by finding the relative importance of each attributes for each clusters so that the company can focus more on the attribute of max importance for a particular cluster of customers. Ths was calculated by dividing the Mean value of attributes of each cluster and dividing it by the overall population average. A heatmap was plotted for the result:

![image](https://user-images.githubusercontent.com/86877457/132393027-28b9f33b-8836-4d2c-ae75-74e7174a2531.png)

<!-- FINAL THOUGHTS -->
## Final Thoughts

From the above analysis, we can see that there should be 4 clusters in our data. To understand what these 4 clusters mean in a business scenario, we should look back the table comparing the clustering performance of RFM Segmentation Model and 4 clusters for the mean values of recency, frequency, and monetary metric. On this basis, let us label the clusters as **‘New customers’, ‘Lost customers’, ‘Loyal customers’, and ‘At Risk customers’.**

**MY INTERPRETATION**

![image](https://user-images.githubusercontent.com/86877457/132398189-8d90880b-6966-480b-944d-a9ffba803595.png)

**FURTHER IMPROVEMENT**
- Product Category could be incorporated into the segmentation. Further product description can be used to derive the product categories with the help of NLP.
- Conducting deeper segmentation on customers based on their geographical location, and demographic and psychographic factors.
- Taking other factors such as geographical location, demographic, psychographic factors and purchase history into consideration we can build a predictive model to predict thier next purchase for them and for the customers with similar attributes.
",0,0,1,0,retail,"[cluster, clustering-algorithm, customer-segmentation, python, retail, rfm-analysis]",44-45
suvkp,Sales-Time-Series-Forecasting,,https://github.com/suvkp/Sales-Time-Series-Forecasting,https://api.github.com/repos/Sales-Time-Series-Forecasting/suvkp,The project aims to compare the forecasting accuracy (RMSE score) of clustered product (based on similar time series properties) vs non-clustered product,"# Comparative analysis of clustered and non-clustered retail product forecasting 

![Screenshot](image.jpeg)

## Context:
In retail business it is a challenge to have the right amount of products in stock. Forecasting can be used to predict how many products need to be in stock to comply with the demand of the product without having a surplus of the products. Product clustering is used for applying the same model to the products in the same cluster. In this analysis we use clustering to try and group the item time-series that have similar structure, which allows us to be prepared to handle items in same groups in a similar way in the future. To evaluate if forecasting is better with clustered products compared to non-clustered products, we perform the comparative analysis.

## Goal:
The goal of the analysis was to compare the forecasting accuracy (RMSE score) of clustered product (based on similar time series properties) vs non-clustered product.
",0,0,1,0,retail,[retail],44-45
easonlai,exploratory_data_analysis_retail_sample,,https://github.com/easonlai/exploratory_data_analysis_retail_sample,https://api.github.com/repos/exploratory_data_analysis_retail_sample/easonlai,"This is a sample code repository that leveraged ""Walmart Dataset (Retail)"" from Kaggle to perform Exploratory Data Analysis (EDA) and Weekly Sales Forecast model development for demonstration purposes.","# Exploratory Data Analysis (Sample for Retail)

This is a sample code repository that leveraged [""Walmart Dataset (Retail)""](https://www.kaggle.com/datasets/rutuspatel/walmart-dataset-retail) from [Kaggle](https://www.kaggle.com/) to perform Exploratory Data Analysis (EDA) and Weekly Sales Forecast model development for demonstration purposes.

About the dataset [""Walmart Dataset (Retail)""](https://www.kaggle.com/datasets/rutuspatel/walmart-dataset-retail):
* Store - The Store ID number.
* Date - The week of sales.
* Weekly_Sales - Sales record in the week of the store.
* Holiday_Flag - Indicate whether the week has Public Holiday or not.
* Temperature - Average temperature of the week.
* Fuel_Price - Average fuel price of the week.
* CPI – Prevailing Consumer Price Index.
* Unemployment - Prevailing Unemployment Rate.

Enjoy!
",0,0,1,0,retail,"[eda, exploratory-data-analysis, forecasting, linear-regression, matplotlib, pandas, pandas-python, python, python3, random-forest, retail, retail-data, retail-sales, seaborn, seaborn-plots, sklearn]",44-45
alessandraapds,Market_Intelligence,,https://github.com/alessandraapds/Market_Intelligence,https://api.github.com/repos/Market_Intelligence/alessandraapds,Market intelligence dashboard developed in Power BI.,# Market_Intelligence,0,0,1,0,retail,"[datavisualization, descriptive-analysis, market, powerbi, retail]",44-45
Avinash793,FPGrowth-and-Apriori-algorithm-Association-Rule-Data-Mining,,https://github.com/Avinash793/FPGrowth-and-Apriori-algorithm-Association-Rule-Data-Mining,https://api.github.com/repos/FPGrowth-and-Apriori-algorithm-Association-Rule-Data-Mining/Avinash793,Implementation of FPTree-Growth and Apriori-Algorithm for finding frequent patterns in Transactional Database.,"# FPTree-Algorithm
Implementation of FPTree Algorithm and Apriori Algorithm using HashTree for finding frequent pattern in Transactional Database. Run the code and enter the filename 
and minimum support count as input. I have also attached two input files of chess-dataset and basket-datset (retail) from the official site http://fimi.ua.ac.be/data.
",27,27,3,1,retail-data,"[apriori-algorithm, association-analysis, association-rules, basket-data, chess-data, data-mining, data-mining-algorithms, data-science, fp-growth, fp-tree, fptree, fptree-algorithm, frequent-pattern-mining, hashtable, hashtree, python3, retail-data, transactional-database]",44-45
klaudia-nazarko,rfm-analysis-python,,https://github.com/klaudia-nazarko/rfm-analysis-python,https://api.github.com/repos/rfm-analysis-python/klaudia-nazarko,This repository contains RFM analysis applied to identify customer segments for global retail company and to understand how those groups differ from each other.,"# RFM Analysis in Python

!['RFM segments'](img/puzzle.jpg ""RFM segments"")



Identifying customer segments is beneficial for selecting profitable customers and developing customer loyalty. **RFM (Recency-Frequency-Monetary) analysis** is a simple technique for behaviour based customer segmentation. It is used to determine quantitatively which customers are the most valuable ones by examining:
* how recently a customer has purchased (recency),
* how often they purchase (frequency),
* how much the customer spends (monetary).



------

[**Customer Segmentation with RFM Analysis**](rfm_analysis_python.ipynb)

In the analysis the dataset of global retail company was examined to identify RFM segments and find patterns in the customer base. The analysis contains:

* Creating customer segments with RFM analysis,

* Evaluating distribution of Recency, Frequency and Monetary,

* Analysis of size and value of RFM segments,

* Demographic analysis of RFM segments,

* Behavioral analysis of RFM segments.

  

------

<a href=""https://www.freepik.com/free-photos-vectors/business"">Business vector created by pikisuperstar - www.freepik.com</a>",18,18,2,0,retail-data,"[customer-segmentation, customer-segments, data-science, python, retail-data, rfm-analysis]",44-45
geffy,retailhero-recommender-solution,,https://github.com/geffy/retailhero-recommender-solution,https://api.github.com/repos/retailhero-recommender-solution/geffy,3rd place solution for RetailHero.ai/#2,"#  Решение задачи [RetailHero.ai/#2](https://retailhero.ai/c/recommender_system/overview) [3-е место]

Собрано на основе [бэйслайна](https://github.com/datagym-ru/retailhero-recomender-baseline)

## Шаги по подготовке:

### Скопировать данные в data/raw
```
cd {REPO_ROOT}
mkdir -p data/raw
cp /path/to/upacked/data/*.csv ./data/raw
```


### Запусить основной скрипт
```bash
./main.sh
```

### Упаковать сабмит
```bash
cd submit
zip -r submit_title.zip solution/*
```

## Результаты: 
```
Check: 0,1403
Public: 0,1320
Private: 0,145728
```


## P.S.
В `submit/submit_v4.3f_noDaily_noDebug.zip` находится оригинальный файл сабмита. 

Данный репозиторий представляет более чистую (по сравнению с оригиналом) версию кода для сбора решения. Точная воспроизводимость результатов не гарантируется.

Версии библиотек перечислены в `requirements.txt`
",16,16,0,0,retail-data,"[collaborative-filtering, contest-solution, recommender-system, retail-data]",44-45
neo4j-graph-examples,northwind,neo4j-graph-examples,https://github.com/neo4j-graph-examples/northwind,https://api.github.com/repos/northwind/neo4j-graph-examples,"From RDBMS to Graph, using a classic dataset",,11,11,4,15,retail-data,"[dataset, example-data, graphdb, neo4j, neo4j-approved, northwind, retail-data]",44-45
joymnyaga,MarketBasketAnalysis-Apriori-Algorithm,,https://github.com/joymnyaga/MarketBasketAnalysis-Apriori-Algorithm,https://api.github.com/repos/MarketBasketAnalysis-Apriori-Algorithm/joymnyaga,A simple Market Basket Analysis that uses the apriori algorithm to find affinities between retail products,"# MarketBasketAnalysis-Apriori
A simple Market Basket Analysis that uses the apriori algorithm to find affinities between retail products. Can be used for product placing or product recommendations. 
",5,5,0,0,retail-data,"[apriori-algorithm, association-rule-mining, graph, marketbasketanalysis, r, retail-data, retailanalytics]",44-45
sawadogosalif,Trade_area_analysis,,https://github.com/sawadogosalif/Trade_area_analysis,https://api.github.com/repos/Trade_area_analysis/sawadogosalif,"In the retail industry a trade area, also known as a catchment area, is the geographic area from where you draw your customers. Here I derive trade area from scratch ","# Technical Documentation for  Trade Area Analysis











# I. Component preparation for trade area analysis

## 1. Prepare a hexagonal grid over a selected territory

- Form a hexagonal grid
- Given a diameter for the hexagons to build and a GeoDataFrame on the territory
- Overlay the grid on top of the original territory GeoDataFrame


## 2. **Component Preparation for Trade Area Analysis**



- Prep hexagonal grid over a selected territory

- Prep hexagonal-based aggregations over the available external data sources

  - Worldpop data: Population data
  - OSM data: Point of interest data
  - Carto data: Consumer spending data & Socio-demographic data
  - Unacast data Pitney Bowes points of interest data


- Construct a single file with all hexagonal-based data(grid + aggregations)

- Visualize the data

# II. Trade Area Analysis Execution

 1. Derive density

 2. Derive urbanicity

 3. Derive trade area distance

 4. Form a dataset for further analysis

 5. Visualize the data
",3,3,1,0,retail-data,"[customer-insights, geomarketing, open-data, retail-data, retail-intelligence, trade-area, urbanicty]",44-45
Dorsa-Arezooji,Retail-Analytics,,https://github.com/Dorsa-Arezooji/Retail-Analytics,https://api.github.com/repos/Retail-Analytics/Dorsa-Arezooji, Walmart sales prediction with Linear Regression and Random Forests + Bayesian Structure Learning,"# Retail-Analytics
Walmart sales prediction with Linear Regression and Random Forests + Bayesian Structure Learning

## Dataset
The dataset is taken from [Kaggle](https://www.kaggle.com/manjeetsingh/retaildataset), and consists of 421,570 records:
* Sales_data-set.csv: sales data (2010-02-05 to 2012-11-01) from departments of all stores
* Store_data-set.csv: type and size of the stores
* Features_dataset.csv: additional data related to the stores and regions

1. **features:**

Store |	Date | Temperature	| Fuel_Price| 	MarkDown1 |	MarkDown2	| MarkDown3	| MarkDown4	| MarkDown5	| CPI	| Unemployment	| IsHoliday |
-----|---------|---------|--------|--------|--------|-----|---------|---------|--------|--------|--------|
1	| 05/02/2010	| 42.31	| 2.572	| NA	| NA	| NA	| NA	| NA	| 211.0963582 | 8.106	| FALSE|
1	|12/02/2010	|38.51	|2.548|	NA	|NA|	NA|	NA|	NA	|211.2421698|	8.106|	TRUE
1	|19/02/2010|	39.93|	2.514|	NA|	NA	|NA|	NA	|NA|	211.2891429	|8.106	|FALSE
1	|26/02/2010	|46.63|	2.56|	NA	|NA|	NA	|NA	|NA	|211.3196429	|8.106	|FALSE
1	|05/03/2010	|46.5|	2.625|	NA|	NA|	NA|	NA	|NA|	211.3501429|8.106	|FALSE

2. **sales:**

Store|	Dept|	Date|	Weekly_Sales|	IsHoliday
-----|---------|---------|--------|--------
1|	1|	05/02/2010|	24924.5|	0
1|	1|	12/02/2010|	46039.49|	1
1|	1|	19/02/2010|	41595.55|	0
1|	1|	26/02/2010|	19403.54|	0
1|	1|	05/03/2010|	21827.9|	0

3. **stores:**

Store|	Type|	Size
---|---|---
1 |A  |151315
2 |A	|202307
3	|B	|37392
4	|A	|205863
5	|B	|34875

## Models
### 1. Linear Regression
A first intuition is that a simple, easily interpretable model like LR is a good starting point for analysis. Also, this would provide a basis for future model comparisons.

<img src=""images/LR_results.png"" width=""800"">

* __*Refer to the [report](https://github.com/Dorsa-Arezooji/Retail-Analytics/blob/master/Retail-Analytics_report.pdf) for details and conclusions*__

### 2. Random Forests
Random forests (RFs) are primarily used for classification, however they can also be used for regression (Breiman, 2001). RFs are ensembles of decision trees (DTs), whose inputs are bootstraps of the training samples. The final RF prediction is the average of all of these DTs’ predictions for a given test sample (bootstrap aggregation). Since each DT has a different bootstrap set, the variance is reduced without affecting the bias. By using this form of aggregation, RFs generally have high accuracy (less overfitting, more robust to noise), but are less interpretable than single DTs (Zhao and Zhang, 2008).

<img src=""images/RF_algorithm.png"" width=""800"">

RFs are extremely flexible and obtain very high accuracies (Pavlov, 2000) while still being prone to overfitting. Furthermore, they have embedded feature selection (Saeys, Abeel and Van de Peer, 2008), hence they can be used for strategizing feature selection. Also, they provide good estimates of the test error without repeated model training associated with cross-validation which is costly.

<img src=""images/RF_results.png"" width=""800"">

* __*Refer to the [report](https://github.com/Dorsa-Arezooji/Retail-Analytics/blob/master/Retail-Analytics_report.pdf) for details and conclusions*__

### Backward Feature Elimination
The feature importances calculated in the previous section (RFs) are used to eliminate features one-by-one using backward feature selection.

<img src=""images/backward_feature_elimination.png"" width=""800"">

* The optimum selection of features is the first 6 features (Dept, Size_log, Store, week, Type, CPI), yielding an accuracy of 95.32%.
* The 2 most important features (Dept and Size_log) contribute to 91.53% of the prediction accuracy.

## **Comparison of the Models**

Model |Accuracy | Pros| Cons
------|---------|-----|----
Linear Regression | 11.56%| +simple and interpretable +trained fast| -inadequate for categorical features  -very low prediction accuracy  -extrapolating beyond the range of data is unreliable
Random Forests| 97.07%| +high prediction accuracy  +embedded feature selection  +consistent with both categorical and continuous features| -less interpretable -longer training time

## Bayesian Network Structure Learning

To explore the Bayesian Structure of the dataset, it needs to be categorized in order to reduce the dimensional complexity. [Bayesys](http://bayesian-ai.eecs.qmul.ac.uk/bayesys/) is used for learning the Bayesian structure of the dataset. The input and output files are availible via [Bayesian Learning](https://github.com/Dorsa-Arezooji/Retail-Analytics/tree/master/Bayesian%20Learning).

<img src=""images/Bayesys.png"" width=""800"">

* __*Refer to the [report](https://github.com/Dorsa-Arezooji/Retail-Analytics/blob/master/Retail-Analytics_report.pdf) for details and conclusions*__


## References
* Breiman, L. (2001) ‘Random forests’, Machine learning, (45), pp. 5–32.
* Pavlov, Y. L. (2000) ‘Random Forests’. doi: 10.1515/9783110941975.
* Saeys, Y., Abeel, T. and Van de Peer, Y. (2008) ‘Robust Feature Selection Using Ensemble Feature Selection Techniques’, Machine Learning and Knowledge Discovery in Databases, pp. 313–325. doi: 10.1007/978-3-540-87481-2_21 .
* Zhao, Y. and Zhang, Y. (2008) ‘Comparison of decision tree methods for finding active objects’, Advances in Space Research, pp. 1955–1959. doi: 10.1016/j.asr.2007.07.020.
",3,3,2,0,retail-data,"[bayesian-learning, data-analysis, linear-regression, prediction, random-forest, retail-data]",44-45
rajat7570,Improving-Consumer-Retailer-Connectivity,,https://github.com/rajat7570/Improving-Consumer-Retailer-Connectivity,https://api.github.com/repos/Improving-Consumer-Retailer-Connectivity/rajat7570,Data mining course project,"# Improving-Consumer-Retailer-Connectivity 
### Description 
Many consumers prefer online shopping. With the growth of e-commerce websites, retailers tend to fail to attract more and more consumers. Retailers are facing a dynamic and competitive environment. With an increase in globalization and competitiveness, retailers are seeking a better market campaign. Retailers are collecting large amounts of data. This data collected requires proper mechanisms to convert it into knowledge; using this knowledge retailer can make a better business decision. Retail industry is looking strategy wherein they can target the right customers who may be profitable to them. 
Our project will deal with sales data and customer data that retailers can collect through online shopping websites. Now we will use that data to analyze and finding knowledge using Data mining concepts and methods. Then use this knowledge to study customer segmentation, customer behavior. They will help retailers keep track of customers behavior towards product, so that their marketing strategies are more oriented towards that. Using knowledge from sales data, provide retailers with when they are buying products, how sale affects the sales of the product, which product provide most overall profit, which product provide least overall profit, which products remain unsell, and product associativity, time-series sale analysis of products etc. that will help management in taking strategic decisions and ultimately will increase the overall profit of the retailer.
",2,2,1,0,retail-data,"[data-mining, data-mining-algorithms, knowledge-extraction, retail-data]",44-45
pareshg18,Customer-Segmentation,,https://github.com/pareshg18/Customer-Segmentation,https://api.github.com/repos/Customer-Segmentation/pareshg18,R-Analysis: Identifying high value customers and low value of customers using RFM modelling,"-- THIS ANALYSIS IS DONE IN R USING JUPYTER NOTEBOOK --

# What is Customer-Segmentation?

As the name suggests, this project is about dividing the customer group into different segments. The idea is to group customers who share similar characteristics. How these groups are formed are based on business objectives and the data available. Through this project we can derive insights into Customer LifeTime Value, Purchase Channel and Product proclivities, so a business can tap into the information to guide future decisions.

Customer segmentation can be achieved using a variety of customer demographics such as age, gender, marital status, etc. However, such information is not easily available. What is easily available is TRANSACTIONAL DATA (Customer Accounts, Invoices, Invoice Dates and Times, etc.) How can the customers, now be segmented?

Although it depends on the business objectives, lets use RFM (Recency, Frequency and Monetary Value) metrics to identify high value and low value customers of the business, so that they can be used for marketing purposes.

# Data
The data was obtained from UCI Machine Learning repository https://archive.ics.uci.edu/ml/datasets/Online+Retail

![data](https://user-images.githubusercontent.com/45079009/84124483-24087280-a9f0-11ea-9b6c-75ac28f26589.PNG)


# RFM (Recency, Frequency and Monetary Value) Variables

As previously mentioned, the data did not include any demographic information of the customers, so using the new metrics to segment!

1. RECENCY -- How recently has the customer made his/her purchase?
2. FREQUENCY -- How frequent is the customer? How many purchases over the given time frame?
3. MONETARY VALUE -- How much amount does each customer bring in?

# Dealing with Outliers - A Pareto Analysis

The rule says that more or less, 80% of the results come from the 20% of the causes! In this context, 80% of sales are caused by 20% of the customers. Meaning, top 20% customers contribute most to the sales -- these are our high value customers!

![outliers](https://user-images.githubusercontent.com/45079009/84260997-dfec9f00-aacf-11ea-840c-109c080541ea.png)

This is a very hard to read, reason being our RFM variables are highly skewed!

### Transformed RFM

![outliers_transformed](https://user-images.githubusercontent.com/45079009/84324040-ca599280-ab2c-11ea-84d7-b58eef9d905a.png)

In this project, outliers are VERY IMPORTANT ! Outliers are customers who are either high value customers or are low value customers! Both of these groups present useful information. Therefore, I will include them in the analysis!


# Modelling - Using k-means!!!

### Why did I use k-means?

1. K-MEANS gives disjoint sets - I wanted each customer to belong to one and only one segment!
2. The data set had around 541,000 customers. Therefore, time complexity could be an issue. K-means has a linear time complexity O(n) as opposed to hierarchical which has a quadratic complexity - O(n^2)!

### Optimal number of clusters?

To get the optimal number of clusters -- we can do a number of things ---
1. Elbow method - Gave me 2 or 3 cluster solution 
2. Silhouette method - Gave me 2 cluster solution
3. Gap - Statistic method - Gave me 6 cluster solution

![elbow](https://user-images.githubusercontent.com/45079009/84262492-918ccf80-aad2-11ea-8d62-5dec4435985e.PNG)![gap-statistic](https://user-images.githubusercontent.com/45079009/84262494-92256600-aad2-11ea-89bd-d6c34f209b2d.PNG)![silhouette](https://user-images.githubusercontent.com/45079009/84262495-92bdfc80-aad2-11ea-85ba-04710547f0b9.PNG)


# Final Cluster Solution

![final solution](https://user-images.githubusercontent.com/45079009/84268346-5bece400-aadc-11ea-95d4-b23007db7bba.PNG)


# My take

The decision should be based upon how the business plans to use the results, and the level of granularity they want to see in the clusters. In my opinion, 4 cluster-solution should be the best, where 1 group is high value customers; 2 groups mid value customers and 1 group being the zero value/low value customers with low frequency and low revenue and who were not very recent.
",2,2,1,0,retail-data,"[customer-analysis-for-retail, customer-analytics, pareto-analysis, r, retail-data, rfm]",44-45
RRighart,Retail,,https://github.com/RRighart/Retail,https://api.github.com/repos/Retail/RRighart,A repo containing code for retail sales analyses,"# Retail

This repository contains a Jupyter notebook with PySpark code, to get started with PySpark in the context of retail.
A blog corresponding to this notebook can be found at https://www.rrighart.com/pyspark .
The online retail data were used and be found at http://archive.ics.uci.edu/ml/machine-learning-databases/00502/ .

Ruthger Righart
Email: rrighart@googlemail.com
",2,2,1,0,retail-data,"[data-science, pyspark, pyspark-notebook, python, retail-data]",44-45
shizakhalidi,Retail-Sales-Data-EDA,,https://github.com/shizakhalidi/Retail-Sales-Data-EDA,https://api.github.com/repos/Retail-Sales-Data-EDA/shizakhalidi,"Using Pandas, scikit learn, Seaborne libraries for data cleaning and exploratory data analysis on retail data","# Retail-Sales-Data-EDA
Using different python libraries to perform data cleaning and exploratory data analysis on dataset for retail data.
",2,2,2,0,retail-data,"[data-analysis, data-science, eda, exploratory-data-analysis, pandas, retail-data, scikit-learn, seaborn]",44-45
Davidglezc,ML-Retail-Business,,https://github.com/Davidglezc/ML-Retail-Business,https://api.github.com/repos/ML-Retail-Business/Davidglezc,"The target in this case, is to create a model of machine learning to know the behaviour of users. What is required is an end-to-end model such as those seen in practice in the module. In this case, you have to apply three models and choose the best one, but the previous part is common to all three.",,2,2,1,0,retail-data,"[ml, retail-data, retail-intelligence, retail-transformation]",44-45
stalhabukhari,ARC,,https://github.com/stalhabukhari/ARC,https://api.github.com/repos/ARC/stalhabukhari,A vision-based Automatic Retail Checkout System,"# ARC: Automatic Retail Checkout via Deep Learning


<!-- Shields-->
[<img src=""https://img.shields.io/badge/Paper-%230077B5.svg?&style=plastic&logo=arxiv&labelColor=ff0000&color=ffffff"" />](https://arxiv.org/abs/2104.02832)
[<img src=""https://img.shields.io/badge/Dataset-%230077B5.svg?&style=plastic&logo=google-drive&labelColor=white&color=blue"" />](https://drive.google.com/drive/folders/1joDBa30_k_TegLDXZ2g5J11iLzNS3Py6?usp=sharing)


<!-- Demo GIF-->
<p align=""center"">
  <img src=""assets/demo.gif"" width=""600"">
</p>


<!-- Citation-->
If you found any part of the project useful, please cite as follows:
```bibtex
@misc{bukhari2021arc,
    title={ARC: A Vision-based Automatic Retail Checkout System},
    author={Syed Talha Bukhari and Abdul Wahab Amin and Muhammad Abdullah Naveed and Muhammad Rzi Abbas},
    year={2021},
    eprint={2104.02832},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    url={https://arxiv.org/abs/2104.02832},
}
```
",2,2,1,0,retail-data,"[automation, checkout, computer-vision, deep-learning, retail-data]",44-45
RobyRiyanto,Machine-Learning-For-Retail,,https://github.com/RobyRiyanto/Machine-Learning-For-Retail,https://api.github.com/repos/Machine-Learning-For-Retail/RobyRiyanto,"Using the Apriori algorithm of the Arules package throughout this project to get insight into the top 10 and bottom 10 of the products sold, get a list of all product package combinations with strong correlations and get a list of all product package combinations with specific items.","# Project Machine Learning for Retail with R

This is a project from DqLab that conducts basic data science on some simple retail data and machine learning to produce product package recommendations that can solve stock problems and increase sales.


![](header.png)


## About Store

DQLab.id Fashion is a fashion shop that sells various products such as jeans, shirts, cosmetics, and others. Although it is quite developed, but with the increasing number of competitors and many products whose stocks are still large, it certainly worries DQLab.id Fashion managers.

One solution is to create innovative packages. Where products that were previously unsold but have market share can even be packaged and sold. 


## What We Need To Do

Several steps were taken to complete this project. First is prepare the library and download the required dataset. Then the raw data is cleaned up and scaled using standardization and normalization prior to modeling. And to realize this, using the `Apriori algorithm` of the `Arules` package throughout this project.

1. Get insight into the top 10 and bottom 10 of the products sold.
2. Get a list of all product package combinations with strong correlations.
3. Get a list of all product package combinations with specific items.

",1,1,0,0,retail-data,"[apriori-algorithm, machine-learning, r, retail-data, transaction-dataset]",44-45
AkhithaBabu,Retail-Sector,,https://github.com/AkhithaBabu/Retail-Sector,https://api.github.com/repos/Retail-Sector/AkhithaBabu,"A sales forecasting problem in Retail Sector solved using time series analysis, machine learning and deep learning methodologies.","# Retail Sector
 A project on Sales Forecasing problem in Retail Sector.
 
[Dataset](https://www.kaggle.com/c/rossmann-store-sales)

References : 

* Ali Fallah Tehrani and Diane Ahrens, [""Improved Forecasting and Purchasing of Fashion Products based on the Use of Big Data Techniques""](https://www.researchgate.net/publication/271130839_Improved_Forecasting_and_Purchasing_of_Fashion_Products_based_on_the_Use_of_Big_Data_Techniques)

* A.L.D. Loureiro [""Exploring the use of deep neural networks for sales forecasting in fashion retail""](https://www.sciencedirect.com/science/article/abs/pii/S0167923618301398)
 
```
├── DATA
│    ├── rossmann-store-sales-dataset
│    │   ├── train.csv
│    │   └── store .csv
│    ├── missing_value_handled.csv
│    └── feature_engineering.csv
└── Notebook
    ├── EDA
    │   ├── EDA.ipynb
    │   └── EDA2.ipynb
    ├── Handling_Null_Values.ipynb
    ├── Feature_Engineering.ipynb
    ├── Model_implimentation.ipynb
    ├── Model_TS.ipynb
    └──config.py
```
",1,1,1,0,retail-data,"[deep-learning, forecasting, machine-learning, python, retail-data, time-series, time-series-analysis]",44-45
hamin123,Market-Basket-Analysis-Example,,https://github.com/hamin123/Market-Basket-Analysis-Example,https://api.github.com/repos/Market-Basket-Analysis-Example/hamin123,Exploring Market Basket Analysis,"# Market Basket Analysis Retail Example
Exploring Market Basket Analysis for online retail dataset.
## References
https://www.kaggle.com/hassanamin/market-basket-analysis-for-online-retail-dataset
",1,1,1,0,retail-data,"[association-rules, market-basket-analysis, python3, retail-data]",44-45
besunny95,Powerbi-project,,https://github.com/besunny95/Powerbi-project,https://api.github.com/repos/Powerbi-project/besunny95,This data visualization is done with powerbi and it explains the exploratory data analysis on dataset samplesuperstore,"# Powerbi-project
This data visualization is done with powerbi and it explains the exploratory data analysis on dataset samplesuperstore.
Power BI is a collection of software services, apps, and connectors that work together to turn your unrelated sources of data into coherent, 
visually immersive, and interactive insights. Your data may be an Excel spreadsheet, or a collection of cloud-based and on-premises hybrid data warehouses.
The Power BI service is a cloud-based business analytics service that enables anyone to visualize and analyze data with greater speed, efficiency, and understanding. 
It connects users to a broad range of cloud-based and on-premises data through easy-to-use dashboards, interactive reports, and compelling visualizations that bring data to life.
",1,1,1,0,retail-data,"[exploratory-data-analysis, powerbi, retail-data, visualizations]",44-45
kumarUjjawal,retail_price_recommendation,,https://github.com/kumarUjjawal/retail_price_recommendation,https://api.github.com/repos/retail_price_recommendation/kumarUjjawal,,"# Retail Price Recommendation
In this machine learning project, we will build a model that automatically suggests the right product prices. 
We are provided of the following information:

train_id — the id of the listing

name — the title of the listing

item_condition_id — the condition of the items provided by the sellers

category_name — category of the listing

brand_name — the name of the brand

price — the price that the item was sold for. This is target variable that we will predict

shipping — 1 if shipping fee is paid by seller and 0 by buyer

item_description — the full description of the item

You can download the data from <a href=""https://www.kaggle.com/saitosean/mercari""> Kaggle </a>
",1,1,2,0,retail-data,"[recommendation-system, retail-data]",44-45
pallavitilloo,Descriptive-Data-Mining,,https://github.com/pallavitilloo/Descriptive-Data-Mining,https://api.github.com/repos/Descriptive-Data-Mining/pallavitilloo,Descriptive Data Mining for UK Retail Dataset,"# Descriptive-Data-Mining

Descriptive Data Mining for UK Retail Dataset

![image](https://user-images.githubusercontent.com/72088607/222609882-0c9a1e25-6bd8-4e7a-8978-bc5c74232da7.png)

",1,1,2,0,retail-data,"[data-mining, descriptive-analysis, descriptive-data-mining, pycharm, python, pytorch, retail-data]",44-45
EconMaett,matthiassp.github.io,,https://github.com/EconMaett/matthiassp.github.io,https://api.github.com/repos/matthiassp.github.io/EconMaett,Using Payment Transaction Data to monitor Turnover in Retail Trade and Services in Switzerland,"# Welcome to retailindex.ch!

This website was created as a one-stop-shop for everyone interested in using payment transaction data to monitoring turnover in the retail trade and services sector.

By following the instructions on this page, you should be able to reproduce and adjuste the Worldline - University of St. Gallen - Retail Trade and Services Index, or WLD-UNISG-RTSI for short.

This project was created as part of my thesis for my Master in Economics at the University of St. Gallen with a specialization in Digitization and Data Analytics.

It is an outgrowth of the project [Monitoring Consumption Switzerland](https://monitoringconsumption.com/), a joint initiative of the University of St. Gallen, the University of Lausanne–E4S, and Novalytica, supported by SIX, Worldline as well as the Swiss Payments Association.

Please note that the opinions expressed herein reflect those of the author and not necessarily those of the University of St. Gallen or of the project Monitoring Consumption Switzerland.

This is a public repository with an [MIT license](https://choosealicense.com/licenses/mit/), allowing anyone to modify and distribute this software and associated documentation files as long as the corresponding copyright notice and permission are included.

I highly encourage anyone to improve my work and I intend to update this site regularly. An interactive applicaition using [Shiny](shiny.rstudio.com/) is currently under construction and should go live by December 2021.

For questions and comments, please do not hesitate to contact me under via [email](mailto:matthias.spichiger@outlook.de?subject=[GitHub]%20Retail%20Index)!

Please find some useful resources below:

### The Economist about the real time revolution in economics
- [What real-time indicators suggest about Omicron’s economic impact](https://www.economist.com/finance-and-economics/2022/01/01/what-real-time-indicators-suggest-about-omicrons-economic-impact)
- [Instant economics - A real-time revolution will up-end the practice of macroeconomics](https://www.economist.com/leaders/2021/10/23/a-real-time-revolution-will-up-end-the-practice-of-macroeconomics)
- [The real-time revolution - How the pandemic reshaped the dismal science](https://www.economist.com/briefing/2021/10/23/enter-third-wave-economics)
- [Real-time danger. Why real-time economic data need to be treated with caution](https://www.economist.com/finance-and-economics/2020/07/23/why-real-time-economic-data-need-to-be-treated-with-caution)

### Nowcasting retail sales using payment transaction data
- [Card spending dynamics in Turkey during the COVID-19 pandemic](https://reader.elsevier.com/reader/sd/pii/S1303070121000226?token=7B31B2E98AFFE6C98CD922F8874FF7B4857CD9B6FF9AFA49CAA7B91A21E085BBFD782E44364BC84F2CD33A026F40DD06&originRegion=eu-west-1&originCreation=20211115151955)
- [Tracking the COVID-19 crisis with high-resolution transaction data](https://royalsocietypublishing.org/doi/pdf/10.1098/rsos.210218)
- [From transactions data to economic statistics: Constructing real-rime, high-frequency, geographic measures of consumer spending](https://www.nber.org/system/files/chapters/c14267/c14267.pdf)
- [Dankort payments as a timely indicator of retail sales in Denmark](https://www.econstor.eu/bitstream/10419/82313/1/621225231.pdf)

### Seasonal adjustment of daily time series
- [Seasonal adjustment with X-13ARIMA-SEATS in R by Christoph Sax and Dirk Eddelbuettel](http://www.seasonal.website/seasonal.html)
- [Seasonal adjustment of daily time series by Daniel Ollech](https://www.bundesbank.de/resource/blob/763892/0d1c33f19a204e2233a6fccc6e802487/mL/2018-10-17-dkp-41-data.pdf)
- [Automatic forecasting procedure by Sean Taylor and Ben Letham](https://facebook.github.io/prophet/)
- [Workday, holiday and calendar adjustment with 21st century data: Monthly aggregates from daily diesel fuel purchases](https://www.nber.org/papers/w16897)
- [Using Daily Payment Processor Data to Determine Existence and Length of Retail Shopping Event Effects](https://www.census.gov/content/dam/Census/library/working-papers/2019/econ/hutchinson-czaplicki-adep-wp.pdf)
- [Modeling of Holiday Effects and Seasonality in Daily Time Series](https://www.census.gov/content/dam/Census/library/working-papers/2018/adrm/rrs2018-01.pdf)

### Payment behavior in Switzerland
- [Project Monitoring Consumption Switzerland](https://monitoringconsumption.com/)
- [Swiss Payment Monitor by Sandro Graf and Tobias Trütsch](https://en.swisspaymentmonitor.ch/)
- [Survey on Payment Methods by the Swiss National Bank](https://www.snb.ch/en/iabout/paytrans/paytrans_surveys/id/paytrans_survey_2020)
- [An examination of the economics of payment card systems by David Maurer](https://www.snb.ch/en/mmr/reference/Zahlungskarten/source/Zahlungskarten.en.pdf)

### Academic writing
- [Academic Phrasebank by the University of Manchester](https://www.phrasebank.manchester.ac.uk/)
- [Writing tips for PhD students by John H. Cochrane](https://www.johnhcochrane.com/research-all/writing-tips-for-phd-studentsnbsp)
- [NZZ Visuals Styleguide](https://nzzdev.github.io/Storytelling-Styleguide/#/)
- [Overleaf templates](https://de.overleaf.com/latex/templates)
- [Better BibTeX for Zotero by Emiliano Heyns](https://retorque.re/zotero-better-bibtex/)

### COVID-19 Data Hub
- [COVID-19 Data Hub by Emanuele Guidotti, David Ardia, and Will Rowe](https://doi.org/10.21105/joss.02376)

### Learning R
- [Mastering Shiny by Hadley Wickham](https://mastering-shiny.org/)
- [Mastering Shiny Solutions by Maya Gans, Harly Gotti, and Howard Baek](https://mastering-shiny-solutions.netlify.app/index.html)
- [Forecasting: Principles and Practice by Rob J Hyndman and George Athanasopoulos](https://otexts.com/fpp3/)
- [Advanced R by Hadley Wickham](https://adv-r.hadley.nz/)
- [Advanced R Solutions by Malte Grosser, Henning Bumann & Hadley Wickham](https://advanced-r-solutions.rbind.io/)
",1,1,1,0,retail-data,"[consumption, r, retail-data, seasonal-adjustment, time-series]",44-45
RahulChugwani,retailSpace,,https://github.com/RahulChugwani/retailSpace,https://api.github.com/repos/retailSpace/RahulChugwani,"Web platform where Retail Shop Owners can signup and manage their daily sales, customer analytics, and shop inventories effectively. They can manage potential customers, generated sales, and can view detailed inventory transactions using smart graphs/tables.","# retailSpace
This was my first full-stack project where I build the application from frontend to backend while using dynamic data of customer and inventories.
Its a web application platform where multiple retail shop owners can signup and manage their daily sales, customer analytics and shop inventories effectively.
User can manage potential customers, generated sales and can view detailed inventory transactions by using smart graphs/tables.
The platform provides them different functionalities from daily sales, inventories to having customer and sales analytics in the app itself.

# Motivation: 
The main motive behind this project was to create such application for MSME retailers where they can manage their all sales,inventory realted work on one platform easily and effectively.
By having the record of all their items and customers on the digital platform, it will be very easy for them to keep track of dfferent items and their sales.
In India, most of the retail owners spend their so much of time on keeping record of their inventory and items instead of utilising that time on increasing sales and understanding customer behaviour.
By saving this time, they can increase their sales and can reach out to more customers.  

# Technology Stack:
Web Technologies: 1.JavaScript  2.HTML  3.CSS  4.Bootstrap 4.0  4.AJAX  5.PHP  6.Handling JSON data
Libraries used: Chart.JS
Databases: SQL
Tools: XAMPP, Visual Studio code.

# Detail Description: 
1. Added the login portal to platform where users can signup and signin.
2. The platform is divided into 3 main sections:

a. Customer Section:
      1. Retail Owner can add new customers and their details while generating the bill. Owner can edit and delete the customers details if required.
      2. In this section, owner can view the analytics of customer and can have analysis of customer segment based on area.
      3. During billing, functionality of Paid or Unpaid amount has been added. (which is most common in Indian reatilers).
      
b. Inventory Section:
      1. Here, Owner can add new inventory/item and can edit their quantity and delete details also.
      2. Inventory analysis is also provided in this section where can he can manage his whole inventory and see which how much items are remaining.
      
c. Sales Section:
      1.In this section, owner will be able to see the sales analytics chart.
      2.Different analytics charts are used here to show the daily sales,Items on high sale, etc.
      
3. As it is a web based portal, owner can login from anywhere and can keep the track of its sales and manage their shop easily.
",1,1,2,0,retail-data,"[ajax, bootstrap4, chartjs, css, html5, javascript, jquery, json, php, retail-data]",44-45
Soumith23,CustomerSegmentationAndTargeting,,https://github.com/Soumith23/CustomerSegmentationAndTargeting,https://api.github.com/repos/CustomerSegmentationAndTargeting/Soumith23,Customer Segmentation and Targeting for Retail Industry,"# Customer Segmentation And Targeting
Designed tailored marketing campaigns to target high value customers based on previous year’s transactional sales data. This model leverages RFM analysis coupled with 3-month CLTV prediction using Lasso Regression with 0.8 R-squared value
",1,1,1,0,retail-data,"[cohort-analysis, lasso-regression, linear-regression, omnichannel, retail-data, rfm-analysis, segmentation, targeting]",44-45
pratikdaga12,DataAnalysis-for-ClothingStoreData,,https://github.com/pratikdaga12/DataAnalysis-for-ClothingStoreData,https://api.github.com/repos/DataAnalysis-for-ClothingStoreData/pratikdaga12,SAS Data Analysis on Retail Clothing Data Set,"# DataAnalysis-for-ClothingStoreData
SAS Data Analysis on Retail Clothing Data Set

SAS program to evaluate a clothing sales file, including statistical mean data, univariate anaysis and correlationship analysis. Data file to be used with the SAS program is Cloting_Store_Data.csv - a comma deliminated data file.

Variables:

The following variables contain the percent of total sales spent by the customer on the respective product category: 


	• PSWEATERS: Sweaters 
	• PKNIT_TOPS: Knit tops 
	• PKNIT_DRES: Knit dresses 
	• PBLOUSES: Blouses 
	• PJACKETS: Jackets
	• PCAR_PNTS: Career pants 
	• PCAS_PNTS: Casual pants
	• PSHIRTS: Shirts 
	• PDRESSES: Dresses 
	• PSUITS: Suits 
	• POUTERWEAR: Outerwear 
	• PJEWELRY: Jewelry
	• PFASHION: Fashionable wear 
	• PLEGWEAR: Leg wear 
	• PCOLLSPEND: Collectibles 
	• GMP: Gross margin percentage 
	• PROMOS: Number of marketing promotions on file 
	• DAYS: Number of days the customer has been on file 
	• MARKDOWN: Markdown percentage on customer purchases 
	• CLUSTYPE: MICROVISION LIFESTYLE CLUSTER TYPE 
	• PERCRET: Percent of Returns 
	• In days between purchase: Number of days between purchases 
	• In lifetime avg time betw visits: Lifetime average time between visits.

Six most common lifestyle cluster types in the dataset:

	1. Cluster 10 Home Sweet Home: families, medium-high income and education, manager/professionals, technical/sales
	2. Cluster 1 Upper Crust: metropolitan families, very high income and education, homeowners, managers/professionals
	3. Cluster 4 Mid-life Success: families, very high education, high income, managers/professionals, technical/ sales
	4. Cluster 16 Country Home Families: large families, rural areas, medium education, medium income, precision/crafts
	5. Cluster 8 Movers and Shakers: singles, couples, students and recent graduates, high education and income, managers/professionals, technical/sales
	6. Cluster 15 Great Beginnings: young, singles and couples, medium-high education, medium income, some renters, managers/professionals, technical/sales
",1,1,1,0,retail-data,"[clothing-store, data-analysis, retail-data, sas]",44-45
phanvinh0526,Data-Warehouse,,https://github.com/phanvinh0526/Data-Warehouse,https://api.github.com/repos/Data-Warehouse/phanvinh0526,Data Warehousing project | Outsourced for Autumn Group | Retail Outlet DW | Melbourne based | Sep - Feb 2018,,1,1,2,0,retail-data,"[data-warehousing, etl-pipeline, fred-apps, retail-data, sqlserver-2014, star-schema]",44-45
limchiahooi,customer-segmentation-rfm,,https://github.com/limchiahooi/customer-segmentation-rfm,https://api.github.com/repos/customer-segmentation-rfm/limchiahooi,This repo contains my customer segmentation project in Python. ,"# Customer Segmentation Project
This repo contains the Customer Segmentation project as part of my Data Science portfolio. The project documents the steps to implement customer segmentation in Python 3 code. The segmentation technique used in this project is known as RFM (Recency, Frequency, Monetary) analysis.  RFM analysis is a behaviour-based approach of grouping customers into segments or clusters, based on their previous purchase transactions i.e. how recently, how often, and how much did a customer buy. 

## Dataset
The dataset is publicly available from the **UCI Machine Learning Repository** website. It is a transactional dataset which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based online retailer.

Source: UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/online+retail)",1,1,1,0,retail-data,"[customer, python, retail-data, segmentation]",44-45
indyfree,tailor,,https://github.com/indyfree/tailor,https://api.github.com/repos/tailor/indyfree, Clustering Algorithm for clustering retail products according to custom requirements.,"[![CircleCI](https://circleci.com/gh/indyfree/tailor.svg?style=svg&circle-token=9ef56e6ccc4ae36b491f9cd438f6921d5b258727)](https://circleci.com/gh/indyfree/tailor)
# tailor - Tailored Data Solution
Clustering Algorithm for clustering retail products according to custom requirements.

## Requirements
- python 3.6
- GNU make

## Installation
This project is intended to run on Mac or Linux. On Windows it should also be runnable via the [Linux Subsystem](https://docs.microsoft.com/en-us/windows/wsl/install-win10).

### Clone the repository
```bash
> git clone https://github.com/indyfree/tailor
```

### Install required packages
Installs dependencies with `pip`:
```bash
> make requirements
```

### Provide server information
Create a file `.env` in the project root:
```bash
> cd tailorit
> touch .env
```
Edit the file with an editor of your choice to provide credentials to the tailorit server. The file should look like this:
```bash
export TAILORIT_SERVER_ADDRESS=[address]
export TAILORIT_USER=[user]
export TAILORIT_PW=[password]
```
Where `[address]`, `[user]` and `[password]` have to be substituted with the respective values.

### Get the data
Download the provided data to `data/raw`.
```bash
> make data
```
## Run the project
This project is set up twofold:
1. Custom functions and algorithms live in the *tailor* python package in `src/tailor`.
2. A walkthrough through the data science process and visualizations are made with jupyter notebooks in `notebooks`. Required functions from *tailor* are imported.

Run the jupyter notebooks with:
```bash
> make jupyter
```

You can access the jupyter notebooks via your webbrowser at `http://localhost:8888/`.
",1,1,3,6,retail-data,"[clustering, clustering-algorithm, retail-data]",44-45
kashamcrash,RFMmodeling,,https://github.com/kashamcrash/RFMmodeling,https://api.github.com/repos/RFMmodeling/kashamcrash,Build a RFM (Recency Frequency Monetary) model for Retail Customers,"# Build a RFM (Recency Frequency Monetary) Model for Retail Customers

## What does RFM Mean?
- Recency means the number of days since a customer made the last purchase. 
- Frequency is the number of purchase in a given period. It could be 3 months, 6 months or 1 year. Monetary is the total amount of money a customer spent in that given period. 
- Therefore, big spenders will be differentiated among other customers such as MVP (Minimum Viable Product) or VIP.

### Task in Hand
1. Perform cohort analysis (a cohort is a group of subjects that share a defining characteristic).
2. Create month cohorts and analyze active customers for each cohort.
3. Analyze the retention rate of customers.
4. Build RFM Segments. Give recency, frequency, and monetary scores individually by dividing them into quartiles.
5. Perform customer segmentation using RFM analysis and Kmeans.


#### What is the Dataset about?
* This is a transnational data set which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail. 
* InvoiceNo: Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation. 
* StockCode: Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product. 
* Description: Product (item) name. Nominal. 
* Quantity: The quantities of each product (item) per transaction. Numeric. 
* InvoiceDate: Invoice Date and time. Numeric, the day and time when each transaction was generated. 
* UnitPrice: Unit price. Numeric, Product price per unit in sterling. 
* CustomerID: Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer. 
* Country: Country name. Nominal, the name of the country where each customer resides.
",0,0,1,0,retail-data,"[cohort-analysis, kmeans-clustering, retail-data, rfm-analysis]",44-45
CosmicRishi,Retail-Data-Analysis,,https://github.com/CosmicRishi/Retail-Data-Analysis,https://api.github.com/repos/Retail-Data-Analysis/CosmicRishi,A complete dashboard of Walmart Retail Data Analysis using Tableau.,"# Retail-Data-Analysis
A complete dashboard of Walmart Retail Data Analysis.

### Devised a dashboard using Tableau which includes the following insights:
• Trend of state wise profit, distribution of average profit by states,

• Age group from which the company is getting minimum profits,

• Regional quarterly average profit, brand logo

![Complete dashboard analysis](https://github.com/CosmicRishi/Retail-Data-Analysis/blob/main/dashboard.png)
",0,0,1,0,retail-data,"[analysis, dashboard, data-visualisation, retail-data, tableau, tableau-public]",44-45
MuhammadEhsan02,Power_BI_Retail-Analysis,,https://github.com/MuhammadEhsan02/Power_BI_Retail-Analysis,https://api.github.com/repos/Power_BI_Retail-Analysis/MuhammadEhsan02,Retail data analysis,,0,0,1,0,retail-data,[retail-data],44-45
sunilnita19,Retail-Data-Analysis,,https://github.com/sunilnita19/Retail-Data-Analysis,https://api.github.com/repos/Retail-Data-Analysis/sunilnita19,A complete dashboard of Walmart Retail Data Analysis using Tableau.,"# Retail-Data-Analysis
A complete dashboard of Walmart Retail Data Analysis.

### Devised a dashboard using Tableau which includes the following insights:
• Trend of state wise profit, distribution of average profit by states,

• Age group from which the company is getting minimum profits,

• Regional quarterly average profit, brand logo

![Complete dashboard analysis](https://github.com/sunilnita19/Retail-Data-Analysis/blob/main/dashboard.png)
",0,0,1,0,retail-data,"[analysis, dashboard, data-visualisation, retail-data, tableau, tableau-public]",44-45
saketgautam,Retail-Data-Analysis,,https://github.com/saketgautam/Retail-Data-Analysis,https://api.github.com/repos/Retail-Data-Analysis/saketgautam,A complete dashboard of Walmart Retail Data Analysis using Tableau.,"# Retail-Data-Analysis
A complete dashboard of Walmart Retail Data Analysis.

### Devised a dashboard using Tableau which includes the following insights:
• Trend of state wise profit, distribution of average profit by states,

• Age group from which the company is getting minimum profits,

• Regional quarterly average profit, brand logo

![Complete dashboard analysis](https://github.com/saketgautam/Retail-Data-Analysis/blob/main/dashboard.png)
",0,0,1,0,retail-data,"[analysis, dashboard, data-visualisation, retail-data, tableau, tableau-public]",44-45
Sumit2514,Retail-sales-forecasting,,https://github.com/Sumit2514/Retail-sales-forecasting,https://api.github.com/repos/Retail-sales-forecasting/Sumit2514,Retail sales forecasting (time series) with Autoregression model in Python ,,0,0,1,0,retail-data,"[autoregressive, forecasting, python, retail-data, stationarity, time-series, trend]",44-45
HeniMasmoudi,workshop,,https://github.com/HeniMasmoudi/workshop,https://api.github.com/repos/workshop/HeniMasmoudi,Unit testing for Data science projects ,"
<!-- ABOUT THE PROJECT -->
## About The workshop

The workshop is aimed at getting started with Unit testing during CI/CD product cycle before taking it to production. We use Pytest as the Unit testing framework and python 3.7.


<!-- GETTING STARTED -->
### Installation

These are instructions to install the repo and running. 

1. Clone the repo
   ```sh
   git clone https://github.com/HeniMasmoudi/workshop.git 
   ```
   
2. Lunch anaconda powershell and create an environment with a specific version of Python:
   ```sh
   conda create -n workshop python=3.7.5
   ```

3. Install requirements

   Change your current directory to ../workshop and then:
   ```sh
   conda activate workshop
   ```
   Install python requirements 
   ```sh
   pip install -r requirements.txt
   ```
   Install local packages
   ```sh
   pip install .
   ```   

<!-- Env variables -->
### defining Env variables

1. Open anaconda Powershell Prompt and write spyder 
```sh
   spyder
   ```
2. In spyder open Tools from the options bar
3. Click Current user environment variables
4. A warning will pop up, tap ok
5. right-click and then tap insert
6. enter your key ""workshop"" and the value ""your local path to the workshop folder""
7. save and close, and then restart anaconda powershell prompt


<!-- LICENSE -->
## License

Distributed under the MIT License. See `LICENSE` for more information.



<!-- CONTACT -->
## Contact

Heni Masmoudi - [Linkedin](https://www.linkedin.com/in/heni-masmoudi-bb0964152/)  

Project Link: [https://github.com/HeniMasmoudi/workshop](https://github.com/HeniMasmoudi/workshop)

",0,0,1,0,retail-data,"[datascience, machine-learning, python, retail-data, unittesting, workshop]",44-45
Kish4869,Excel-Automation,,https://github.com/Kish4869/Excel-Automation,https://api.github.com/repos/Excel-Automation/Kish4869,Useful python tricks to speed up your time consuming excel works.,,0,0,1,1,retail-data,"[excel, management, retail-data, retailer-management-system]",44-45
Barsha27,Retail_Analytics,,https://github.com/Barsha27/Retail_Analytics,https://api.github.com/repos/Retail_Analytics/Barsha27,To conduct analysis on your client's transaction dataset and identify customer purchasing behaviours to generate insights and provide commercial recommendations.,"### Objective Of The Project

To conduct analysis on your client's transaction dataset and identify customer purchasing behaviours to generate insights and provide commercial recommendations.

### Tools, libraries and Languages Used

- VS code
-  Python
-  Pandas, Numpy, plotly, matplotlib

### Insights
- most of out sales come from Budget older families, mainstream young singles/couples and mainstream retires
- The higher sales in case of mainstream category is due to higher customers in retired and young single/couples. But the same is not in case of budget category, as older singles/couples have highest customer proportion in budget but the sales are higher in case of budget Older families
- Older families and Young families buy more chips per customer
- The largest size is 380g and the smallest size is 70g
- The no. of transactions(sales) kept on increasing till 24th december, 2018 and then there is a break in the graph on 25th dec,2018. The break is due to the day being a christmas day on which shops and all remains closed and therefore there were no transactions(sales) on that particular day.

",0,0,1,0,retail-data,"[data-cleaning, data-exploration, data-visualisation, retail-data]",44-45
peter-abel,DATA-SCIENCE-PROJECT,,https://github.com/peter-abel/DATA-SCIENCE-PROJECT,https://api.github.com/repos/DATA-SCIENCE-PROJECT/peter-abel,data-science," # DATA-SCIENCE-PROJECT

# A LOGISTIC REGRESSION MODEL ON WHAT PRODUCT KENYAN RETAILERS SHOULD STOCK IN AN INFLATIONARY MARKET.

# Problem statement.

With high inflationary environment and high cost of goods,  consumers are facing pressure on what retail goods to prioritize over. This makes it even harder for retailers who are also budgeting for what goods to stock up on to prevent massive losses and to hopefully ride out the harsh economic times.


# Objectives.


 * To find relevant data for the study.
 * To identify features necessary for this model
 * To create  the model
 * To test this data on a test dataset

# Aim.

The aim of this project is to create a model for small or large scale retailers to use, so as to identify what products to stock up on based on the products qualities and consumer behavior as the features, and an aggregate score as a target.

# DATA
The data used in the study was generated from mockaroo all with values ranging from 0 to 1 so as to indicate the quantitative value of the features used in probability form.

# Features
Features used in the data include: 'Brand Loyalty',	'Purchase Frequency',	'Profit margin',	'Quality',	'sale opportunities',	'resale value',  'after sale service', 	'competition',  'Tax rate', 	'score'. All being factors that affect the products perfomance in the market and also reflect consumer behaviour.
Products that contain these features include:
 Convenience products
 Shopping products
 Specialty products
 Unsought products


The 'score' column is an aggregate of all the features and subtracting features that are undesirable or would be costly for a retailer eg. 'after sale service', 	'competition',  'Tax rate'. 


 # Logistic Regression Assumptions

 

* The independent variables should be independent of each other. That is, the model should have little or no multicollinearity. 
* Binary logistic regression requires the dependent variable to be binary.
* For a binary regression, the factor level 1 of the dependent variable should represent the desired outcome.
* Only the meaningful variables should be included.
* The independent variables are linearly related to the log odds.


",0,0,1,0,retail-data,"[consumer-products, inflation-dynamics, logistic-regression, retail-data]",44-45
RAVI-CHANDRIKA-05,Customer-Segmentation-Unsupervised-Learning,,https://github.com/RAVI-CHANDRIKA-05/Customer-Segmentation-Unsupervised-Learning,https://api.github.com/repos/Customer-Segmentation-Unsupervised-Learning/RAVI-CHANDRIKA-05,"In this project, I will be performing an unsupervised clustering of customer's records from a Retail Chain.","# Customer-Segmentation-Unsupervised-Learning
Understanding customers is critical for success and survival of a Business. Knowing our customers helps a Business to provide products and service that lead to extreme customer satisfaction and help towards the growth of the Business.<br>
Customer segmentation can help strategies Business effectively. Customer segmentation is a process of dividing customers based on similar characteristics – such as demographics or behaviors into groups called clusters. These Customer Segments/ Groups can help Businesses to cater each segment of customer uniquely.<br>
In this project, I will be performing an unsupervised clustering of customer's records from a Retail Chain.<br>


<div style=""width:100%;text-align: center;""> <img src=""https://www.segmentify.com/wp-content/uploads/2021/08/Top-Customer-Segmentation-Examples-every-Marketer-Needs-to-Know.png"" width=""900px""/> </div>
",0,0,1,0,retail-data,"[clustering-algorithm, customer-segmentation, retail-data]",44-45
SanyaGoyal,Time-Series-Modelling,,https://github.com/SanyaGoyal/Time-Series-Modelling,https://api.github.com/repos/Time-Series-Modelling/SanyaGoyal,Building Time Series Exponential Smoothing models to forecast temperature and ARIMA model to forecast weekly sales for a retail store. ,"# Time-Series-Modelling

TS_Exponential_Smoothing_Models

This project aims at building a time series model to create an hourly forecast of temperature for a retail store. The Store’s analysts believe that extreme outdoor temperatures may affect the sales of the main retail store in Harrisburg,PA; They want a forecast of these temperatures to help them further evaluate this claim.

The R script includes code for the following visualizations and models-
1. Time series decomposition
2. Plotting of the temperature series as a time series object
3. Fitting a trend line on the time series plot
4. Calculation of a seasonally adjusted data
5. Plotting seasonally adjusted values on the time series plot
6. Building a Single Exponential Smoothing Model
7. Building a Linear Expotential Smoothing Model
8. Building a Damped Trend Model
9. Building a Holt-Winters ESM
10. Building a Holt-Winters ESM- Multiplicative
11. Checking Accuracy Statistics on the test dataset

TS_Arima_Modelling

A retail store wants to build a weekly sales forecast model for two of its stores in - Phoenix and Tucson, AZ

The R script includes code for the following visualizations and models-
1. Creating time series object and plotting it
2. Building Linear Expotential Smoothing Model for both Tucson and Pheonix
3. Calculating Mean Absolute Percentage Error
4. Checking Stationarity through plots
5. Building Autoregressive Models for Tucson and Pheonix
6. ACF and PACF plots of residuals
7. White noise Tests and plots
8. Forecasting sales 
9. Checking Accuracy on the validation dataset






 
",0,0,1,0,retail-data,"[arima-model, exponential-smoothing-models, retail-data, time-series]",44-45
haihapham,Supermarket-Sales-Analysis,,https://github.com/haihapham/Supermarket-Sales-Analysis,https://api.github.com/repos/Supermarket-Sales-Analysis/haihapham,"This is the historical data that covers sales of a supermarket, Walmart. In this work, I tried to explore the dataset and create a simple model to predict the sales (Weekly_Sales)","# Supermarket Sales Analysis
This is a sample code repository that leveraged [""Walmart Dataset (Retail)""](https://www.kaggle.com/datasets/rutuspatel/walmart-dataset-retail) from [Kaggle](https://www.kaggle.com/) to explore the dataset, perform EDA and predict the Weekly Sales.

About the dataset [""Walmart Dataset (Retail)""](https://www.kaggle.com/datasets/rutuspatel/walmart-dataset-retail):
* Store - The Store ID number.
* Date - The week of sales.
* Weekly_Sales - Sales record in the week of given store.
* Holiday_Flag - Indicate whether the week has Public Holiday or not. (1 – Holiday week, 0 – Non-holiday week)
* Temperature - Average temperature of the week.
* Fuel_Price - Average fuel price of the week in the region.
* CPI – Prevailing Consumer Price Index.
* Unemployment - Prevailing Unemployment Rate.

<b>Holiday Events:</b>
- Super Bowl: 12-Feb-10, 11-Feb-11, 10-Feb-12, 8-Feb-13
- Labour Day: 10-Sep-10, 9-Sep-11, 7-Sep-12, 6-Sep-13
- Thanksgiving: 26-Nov-10, 25-Nov-11, 23-Nov-12, 29-Nov-13
- Christmas: 31-Dec-10, 30-Dec-11, 28-Dec-12, 27-Dec-13
",0,0,1,0,retail-data,"[eda, lgbmregressor, model-deployment, model-evaluation, model-tuning, pandas, python, random-forest, retail-data, seaborn, sklearn]",44-45
gswarge,pos_data_analysis,,https://github.com/gswarge/pos_data_analysis,https://api.github.com/repos/pos_data_analysis/gswarge,"In this project, I analyzed the point of sale transactional dataset to identify sales trends, suggest menu changes, and the impact of pricing changes.",,0,0,0,0,retail-data,"[marketing-analytics, retail-data, retail-intelligence]",44-45
shristigupta,Retail_Analytics_PySpark,,https://github.com/shristigupta/Retail_Analytics_PySpark,https://api.github.com/repos/Retail_Analytics_PySpark/shristigupta,,"# Retail_Analytics_PySpark

## Introduction 
Data driven decisions have become prominent in multiple sectors over the decades. The benefits linked being manifold, it gained popularity in domains of finance, retail, healthcare, transportation etc. Distributed digital mobility has empowered customers with the accessibility to wide range of markets heterogenous on basis of modes of access, prices, product categories, channels of payment. All this leads to a wide range of data at the hands of industries, directly coming from the end-users. This data is like oil which has to be refined and transformed to fuel the rise in revenue, dip in losses and mitigation of risks. 

One such sector that influences a wide range of population and owns a major share in the market is retail analytics. The way we shop has evolved over the decades from going to markets to purchase what we need to now being persuaded to purchase what we might want. Earlier there were just two stakeholders in the market, the industry and the customer. With time, technology and digital revolution we have seen the emergence of a third stakeholder, “data” 

Retail today is being shaped by empowered customer who demands 
Convinience 
Customization 
Collaboration 
Consistency 
To get this right retailers need to empower every decision maker with relevant and accurate insights instantly 

## Spark

Apache Spark is a lightning fast real-time processing framework. It does in-memory computations to analyze data in real-time. It came into picture as Apache Hadoop MapReduce was performing batch processing only and lacked a real-time processing feature. Hence, Apache Spark was introduced as it can perform stream processing in real-time and can also take care of batch processing.
Apart from real-time and batch processing, Apache Spark supports interactive queries and iterative algorithms also. Apache Spark has its own cluster manager, where it can host its application. It leverages Apache Hadoop for both storage and processing. It uses HDFS (Hadoop Distributed File system) for storage and it can run Spark applications on YARN as well.


# ""The best of Both Worlds :)""

The PySpark API allows data scientists with experience of Python to write programming logic in the language most familiar to them, use it to perform rapid distributed transformations on large sets of data, and get the results back in Python-friendly notation. PySpark is likely to be of particular interest to users of the “pandas” open-source library, which provides highperformance, easy-to-use data structures and data analysis tools. pandas enables an entire data analysis workflow to be created within Python —rather than in an analytics-specific language such as R —but it is usually limited to running locally and with relatively small data sets. If a data scientist has established a set of pandas-based operations but needs to tackle a much larger data set, PySpark makes it easy to take advantage of distributed compute resources in the cloud, and then bring back a smaller subset of data for further local analysis —all within Python. And because PySpark supports all standard Python libraries and even C extensions, existing code can take advantage of the power of Spark with only minimal modifications.

# ""Joining Hands"" 

Let me now talk about the use cases that make retail analytics smart Retail Analytics bolstered by Pyspark. 
-Data lake 
-Inventory management 
-Customer segmentation 
-Geographic revenue analysis 
-Product level analysis 

## ""The profit yielding lake"" : Data Lake 

Data lake :

Harry owns a toy firm. He has a seller’s account in Amazon, Flipkart and Paytm. Moreover, his firm has their own website where purchases can be made. They also own some stores across the country where customer can walk-in, search for what they are looking for and purchase in person. Harry wants to know his customer’s profile so that he can allocate the inventory accordingly to minimize shipping costs. But, Harry is tensed because of the multiple data sources, multiple formats and customers across different locations. 

Solution: 
Using Spark Streaming to dump data from multiple sources in the Hadoop Cluster. Followed by predictive analysis by machine learning models in Python. 

## ""Customer Segmentation"" 

A big e-commerce firm, BuyFromMe wants to know their customers better. This would help them push personalized campaigns, introduce new products while focusing on different customer segments. 

Solution: 
The analyst proposes to seggregate customers on grounds of a recency,frequency, model, wherain the score obtained against the RFM matrix denotes the segment of the customer. 


## “Let me clear my stocks"" : Inventory Management 

Caroline owns a women apparel brand. She has multiple stores located across locations which serve as offline stores and warehouses for online customers. Caroline is facing a dip in sales. Her inventory at some places is falling short and at some places it is surplus. She learns that data science can help her manage the inventory better, driving sales and bringing in profits. She looks to a data analyst for help. 

Solution by Data Analyst : 
Using PySpark she provides with the following Solution : 
-Mapping of type and amount of stock requirement to different locations based on historical data 
-Knowing what stock to put where. 
-Finding out old stock that can be purchased as per promotions and requirement at different locations. 




",0,0,1,0,retail-data,"[analytics, big-data, business-solutions, python, retail-data, retail-industry, spark]",44-45
akashjain04,FPGrowthAlgorithm,,https://github.com/akashjain04/FPGrowthAlgorithm,https://api.github.com/repos/FPGrowthAlgorithm/akashjain04,Usage of FPGrowth Algorithm to find frequent item sets,"# FPGrowthAlgorithm

## Usage of FP growth algorithm to find frequent item sets.
",0,0,2,0,retail-data,"[association-rule-mining, association-rules, datacleaning, datamining, fp-tree, fpgrowth, frequent-itemsets, frequent-pattern-mining, jupyter-notebook, python3, retail-data]",44-45
haerinHong,Spain_Retail_Analysis,,https://github.com/haerinHong/Spain_Retail_Analysis,https://api.github.com/repos/Spain_Retail_Analysis/haerinHong,Spain Retail based on UCI data. Analysis Method : Regression,,0,0,1,1,retail-data,"[artificial-intelligence, data-science, machine-learning, matplotlib, numpy, pandas, pca, pca-analysis, regression, retail-data, seaborn, uci]",44-45
bizon,selling-partner-api-sdk,bizon,https://github.com/bizon/selling-partner-api-sdk,https://api.github.com/repos/selling-partner-api-sdk/bizon,A modularized SDK library for Amazon Selling Partner API (fully typed in TypeScript),"# selling-partner-api-sdk

A modularized SDK library for Amazon Selling Partner API (fully typed in TypeScript)

[![codecov](https://codecov.io/gh/bizon/selling-partner-api-sdk/branch/master/graph/badge.svg?token=tqBs3JHHP2)](https://codecov.io/gh/bizon/selling-partner-api-sdk)
[![XO code style](https://badgen.net/badge/code%20style/XO/cyan)](https://github.com/xojs/xo)

[<img src=""https://files.bizon.solutions/images/logo/bizon-horizontal.png"" alt=""Bizon"" width=""250""/>](https://www.bizon.solutions?utm_source=github&utm_medium=readme&utm_campaign=selling-partner-api-sdk)

## CI

[![Codegen](https://github.com/bizon/selling-partner-api-sdk/actions/workflows/codegen.yml/badge.svg)](https://github.com/bizon/selling-partner-api-sdk/actions/workflows/codegen.yml)
[![Tests](https://github.com/bizon/selling-partner-api-sdk/actions/workflows/tests.yml/badge.svg)](https://github.com/bizon/selling-partner-api-sdk/actions/workflows/tests.yml)
[![Release](https://github.com/bizon/selling-partner-api-sdk/actions/workflows/release.yml/badge.svg)](https://github.com/bizon/selling-partner-api-sdk/actions/workflows/release.yml)

## Documentation

To learn more about the Selling Partner API, visit the [official Amazon documentation](https://developer-docs.amazon.com/sp-api/docs).  
Also, see the [generated documentation](https://bizon.github.io/selling-partner-api-sdk/) for each API client.

## Features

This SDK supports the following features:

- Installing only the API clients you need, versioned independently.
- Passing client configuration through environment variables.
- Authenticating using IAM Roles, Users and STS sessions.
- Retrying rate-limited requests by respecting the documented rate-limts and possibly provided headers.
- Logging (non-auth) API requests, responses and errors.
- Passing restricted data tokens to API clients.

## Clients

This repository contains an API client for each of the available Selling Partner API version:

- [aplus-content-api-2020-11-01](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/aplus-content-api-2020-11-01)
- [authorization-api-v1](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/authorization-api-v1)
- [catalog-items-api-2020-12-01](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/catalog-items-api-2020-12-01)
- [catalog-items-api-2022-04-01](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/catalog-items-api-2022-04-01)
- [catalog-items-api-v0](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/catalog-items-api-v0)
- [easy-ship-2022-03-23](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/easy-ship-2022-03-23)
- [fba-inbound-eligibility-api-v1](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/fba-inbound-eligibility-api-v1)
- [fba-inventory-api-v1](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/fba-inventory-api-v1)
- [fba-small-and-light-api-v1](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/fba-small-and-light-api-v1)
- [feeds-api-2020-09-04](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/feeds-api-2020-09-04)
- [feeds-api-2021-06-30](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/feeds-api-2021-06-30)
- [finances-api-v0](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/finances-api-v0)
- [fulfillment-inbound-api-v0](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/fulfillment-inbound-api-v0)
- [fulfillment-outbound-api-2020-07-01](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/fulfillment-outbound-api-2020-07-01)
- [listings-items-api-2020-09-01](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/listings-items-api-2020-09-01)
- [listings-items-api-2021-08-01](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/listings-items-api-2021-08-01)
- [listings-restrictions-api-2021-08-01](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/listings-restrictions-api-2021-08-01)
- [merchant-fulfillment-api-v0](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/merchant-fulfillment-api-v0)
- [messaging-api-v1](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/messaging-api-v1)
- [notifications-api-v1](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/notifications-api-v1)
- [orders-api-v0](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/orders-api-v0)
- [product-fees-api-v0](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/product-fees-api-v0)
- [product-pricing-api-2022-05-01](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/product-pricing-api-2022-05-01)
- [product-pricing-api-v0](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/product-pricing-api-v0)
- [product-type-definitions-api-2020-09-01](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/product-type-definitions-api-2020-09-01)
- [replenishment-api-2022-11-07](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/replenishment-api-2022-11-07)
- [reports-api-2020-09-04](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/reports-api-2020-09-04)
- [reports-api-2021-06-30](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/reports-api-2021-06-30)
- [sales-api-v1](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/sales-api-v1)
- [sellers-api-v1](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/sellers-api-v1)
- [services-api-v1](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/services-api-v1)
- [shipment-invoicing-api-v0](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/shipment-invoicing-api-v0)
- [shipping-api-v1](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/shipping-api-v1)
- [solicitations-api-v1](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/solicitations-api-v1)
- [tokens-api-2021-03-01](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/tokens-api-2021-03-01)
- [uploads-api-2020-11-01](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/uploads-api-2020-11-01)
- [vendor-direct-fulfillment-inventory-api-v1](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/vendor-direct-fulfillment-inventory-api-v1)
- [vendor-direct-fulfillment-orders-api-2021-12-28](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/vendor-direct-fulfillment-orders-api-2021-12-28)
- [vendor-direct-fulfillment-orders-api-v1](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/vendor-direct-fulfillment-orders-api-v1)
- [vendor-direct-fulfillment-payments-api-v1](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/vendor-direct-fulfillment-payments-api-v1)
- [vendor-direct-fulfillment-sandbox-test-data-api-2021-10-28](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/vendor-direct-fulfillment-sandbox-test-data-api-2021-10-28)
- [vendor-direct-fulfillment-shipping-api-2021-12-28](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/vendor-direct-fulfillment-shipping-api-2021-12-28)
- [vendor-direct-fulfillment-shipping-api-v1](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/vendor-direct-fulfillment-shipping-api-v1)
- [vendor-direct-fulfillment-transactions-api-2021-12-28](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/vendor-direct-fulfillment-transactions-api-2021-12-28)
- [vendor-direct-fulfillment-transactions-api-v1](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/vendor-direct-fulfillment-transactions-api-v1)
- [vendor-invoices-api-v1](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/vendor-invoices-api-v1)
- [vendor-orders-api-v1](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/vendor-orders-api-v1)
- [vendor-shipments-api-v1](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/vendor-shipments-api-v1)
- [vendor-transaction-status-api-v1](https://www.github.com/bizon/selling-partner-api-sdk/tree/master/clients/vendor-transaction-status-api-v1)

The API clients are automatically generated from the Swagger/OpenAPI models from [the official models repository](https://github.com/amzn/selling-partner-api-models).
[A code generation workflow](https://github.com/bizon/selling-partner-api-sdk/actions/workflows/codegen.yml) runs twice a day and will create a PR on this repository whenever there are changes to the models.

## Code generation

Some of the source Open API models are invalid (see the [issues](https://github.com/amzn/selling-partner-api-models/issues) and [PRs](https://github.com/amzn/selling-partner-api-models/pulls) on the Amazon repository), so this SDK defines some patches to account for those errors.

Each patch files are [JSON Patches](http://jsonpatch.com/) operations (1 logical change per file).

### Active patches

You can browse all the active patches in the [patches directory](https://github.com/bizon/selling-partner-api-sdk/tree/master/codegen/patches).

## License

MIT

## Miscellaneous

```
    ╚⊙ ⊙╝
  ╚═(███)═╝
 ╚═(███)═╝
╚═(███)═╝
 ╚═(███)═╝
  ╚═(███)═╝
   ╚═(███)═╝
```
",34,34,0,4,retail-analytics,"[amazon, amazon-marketplace, bizon, brand-analytics, nodejs, retail-analytics, sdk, sellercentral, typescript, vendorcentral]",44-45
chinmayeeguru,Olist-Customer-Purchase-Behaviour-Analysis,,https://github.com/chinmayeeguru/Olist-Customer-Purchase-Behaviour-Analysis,https://api.github.com/repos/Olist-Customer-Purchase-Behaviour-Analysis/chinmayeeguru,Analyse the customer purchase behaviour to optimize inventory cost,"# Introduction

OList is an e-commerce company that has faced some losses recently. It wants to manage its inventory very well so as to reduce any unnecessary costs that it might be bearing. It needs to identify the product categories to get rid of without significantly impacting business.


# Objective

- To identify top products that contribute to the revenue
- Analyse the customer purchase behaviour to estimate what items are more likely to be purchased individually or in combination with some other products
- Provide recommendations for better inventory management so as to reduce any unnecessary costs 


# Assumption

- Only the cases having order status as 'delivered' are to be considered.
- The columns ‘order_approved_at’ and ‘order_delivered_timestamp’ are assumed to be equivalent to/same as the columns ‘order_purchase_timestamp’ and ‘order_estimated_delivery_date’ respectively.
- Only those categories which are ordered more than 5 times are considered for creating category association as part of market basket analysis.


# Data Cleaning

##### Orders:

- Only the orders with order status as ‘delivered’ are considered for this case study since 97% of the records are successfully delivered. 
- he missing values for ‘order_approved_at’ are replaced with respective ‘order_purchase_timestamp’.
- Similarly, the missing values for ‘order_delivered_timestamp’ are replaced with respective ‘order_estimated_delivery_date’.

##### Customer:

- For redundant customer records, only the first occurrence is kept and the duplicated ones are removed.

##### Products:

- The missing product category has been replaced with the mode i.e. ‘toys’ as almost 75% of the records belong to ‘toys’ category.
- For product width, length, height and weight, as the data is right-skewed and there is no significant outlier, instead of using mean, the respective median values are used to replace the missing values.


# EDA and Visualizations

Exploratory data analysis is performed on the OList data to identify various patterns in customer purchase behaviour. The visualizations are plotted using Tableau.

##### Top 20 products based on revenue generated and number of orders placed

![image](https://user-images.githubusercontent.com/103338455/162641460-56a0dff3-acea-4c0d-b9f1-a314629d285c.png)

- The highest revenue generated  is 63,885.
- The product that generated highest revenue belongs to the Toys category.
- Most of the products in the top 20 list belong to the toys category.

![image](https://user-images.githubusercontent.com/103338455/162641468-69bc21ea-2940-4bfa-b8e2-67027d72118d.png)

- The most ordered product has been purchased 467 times.
- Some products are purchased frequently despite of the high price.

##### Number of orders per category

![image](https://user-images.githubusercontent.com/103338455/162641483-ba2974f5-fc58-46d1-b164-4fdc5be4c114.png)

- The Toys category is the most ordered category with a total  74,929 orders.
- The categories here are filtered to show only the top 20 which are ordered more than 5 times.

##### Total number of orders and percentage of total running revenue for each product

![image](https://user-images.githubusercontent.com/103338455/162641492-62a0b28c-e9dd-4b7b-a4f3-9c4238132293.png)

- This Revenue Pareto shows the percentage of total running revenue, revenue generated and number of orders for each product id.
- It can be used to identify the contribution of the products towards total revenue.


# Market Basket Analysis

![image](https://user-images.githubusercontent.com/103338455/162641575-1fb21e24-14eb-446b-8ebf-57081cc4a67c.png)

- Market basket analysis is performed to identify the frequently ordered category association.
- Toys are often purchased with various other categories as shown in this Market Basket Analysis.


# Ideal Category Depth

![image](https://user-images.githubusercontent.com/103338455/162641638-04809218-8db1-4db3-a877-2480533bf75e.png)

- Revenue pareto analysis is performed to understand ideal category depth for each category.


# Inferences

- The category ‘toys’ is the most ordered category as it is ordered 74929 times (76% of the total number of orders)
- Apart from ‘toys’, the categories ‘health_beauty’,’bed_bath_table’,’sports_leisure’, ‘computer_accessories’ and ‘furniture_decor’ are the most frequently ordered categories.The above categories with ‘toys’ or/and with each other are most frequent in customers’ basket.
- It is observed that despite of the high price, some products are frequently purchased by the customers.


# Recommendations

- Target customers who have children to boost up sales as they are most likely to purchase ‘toys’ which is the most ordered category. 
- Offer Promo-codes or discounts on frequently ordered category associations and the most ordered products to attract more customers.
- Consider the ideal category depth to minimize the inventory cost by getting rid of the products which are seldom ordered and/or do not have a significant contribution to the total revenue under each product category. 

",2,2,1,0,retail-analytics,"[dashboard, marketing-analytics, pareto-analysis, purchase-analysis, retail-analytics, storytelling, tableau, visualization]",44-45
whoisksy,data-analysis-on-retail-store-promotion,,https://github.com/whoisksy/data-analysis-on-retail-store-promotion,https://api.github.com/repos/data-analysis-on-retail-store-promotion/whoisksy,,"## Data analysis on retail store promotions
* A Retail company with as chain of stores across US and Canada.
* Data set of daily transaction numbers of a certain product.
* For period of  2 years starting from  Feb 2018 to Jan  2020.

### Data Description
| Column Name | Description |
|---|---|
| SHPG_TRXN_LN_DT | Date of the transactions |
| Special day | These are special days / holidays of the year on which we can observe high traffic to the store and so more number of transaction |
|Clearance|These are days when clearance offers are running and so high number of transactions can be seen |
|Promo_name|These are the different promos which can run from at any point of time in the year . Most of the promos are in the form of a package . For ex 7/28 would mean you can buy 7 units of the product for 28 . The customer has to buy the full package to get the offer.Non-Promo : These are days when the products are sold at regular price. B3G3 :  It means buy 3 units and get 3 units free |
|Brands | The company has 2 different brands A & B .  So if the Brand column is marked with single A or B it means that the promo has run for a single brand . A_B means the promo has run for both the brands . There are days when 2 different packages can run For ex 10/35 - 7/28 : This means that the first package 10/35 has run for brand A and 7/28 has run for brand B. |
|Transactions |This shows the number of transactions that have happened for the product . |
 
 
 ### Tasks
* Performing exploratory analysis and providing insights about the performance of the promos.
* Checking for price sensitivity during promotions.
* Calculating sales lift on each promotion types.

",2,2,1,0,retail-analytics,"[data-analysis, data-visualization, plotly, promotion-analysis, retail-analytics, transaction]",44-45
huzaifakhan04,quantium-data-analytics-virtual-experience-program,,https://github.com/huzaifakhan04/quantium-data-analytics-virtual-experience-program,https://api.github.com/repos/quantium-data-analytics-virtual-experience-program/huzaifakhan04,"This repository contains results of the completed tasks for the Quantium Data Analytics Virtual Experience Program by Forage, designed to replicate life in the Retail Analytics and Strategy team at Quantium, using Python.","# Quantium Data Analytics Virtual Experience Program:

This repository contains results of the completed tasks for the [Quantium Data Analytics Virtual Experience Program](https://www.theforage.com/virtual-internships/prototype/NkaC7knWtjSbi6aYv/Data-Analytics) by Forage, designed to replicate life in the Retail Analytics and Strategy team at Quantium, using Python.

**Certificate of Completion:** https://forage-uploads-prod.s3.amazonaws.com/completion-certificates/Quantium/NkaC7knWtjSbi6aYv_Quantium_3waNJW6o5fGQYFLdF_1674762081501_completion_certificate.pdf

### Dependencies:

* Jupyter Notebook ([install](https://docs.jupyter.org/en/latest/install.html))
* pandas ([install](https://pandas.pydata.org/docs/getting_started/install.html))
* NumPy ([install](https://numpy.org/install/))
* xlrd (install - [Anaconda](https://anaconda.org/anaconda/xlrd) | install - [PyPI](https://pypi.org/project/xlrd/))
* Matplotlib ([install](https://matplotlib.org/stable/users/installing/index.html))
* seaborn ([install](https://seaborn.pydata.org/installing.html))

## Introduction:

Quantium has had a data partnership with a large supermarket brand for the last few years who provide transactional and customer data. You are an analyst within the Quantium analytics team and are responsible for delivering highly valued data analytics and insights to help the business make strategic decisions.

Supermarkets will regularly change their store layouts, product selections, prices and promotions. This is to satisfy their customer’s changing needs and preferences, keep up with the increasing competition in the market or to capitalise on new opportunities. The Quantium analytics team are engaged in these processes to evaluate and analyse the performance of change and recommend whether it has been successful. 

In this program you will learn key analytics skills such as:

* Data wrangling
* Data visualization
* Programming skills
* Statistics
* Critical thinking
* Commercial thinking

#### Task 1

### Data preparation and customer analytics

Conduct analysis on your client's transaction dataset and identify customer purchasing behaviours to generate insights and provide commercial recommendations.

**Here is the background information on your task**

You are part of Quantium’s retail analytics team and have been approached by your client, the Category Manager for Chips, who wants to better understand the types of customers who purchase Chips and their purchasing behaviour within the region.

The insights from your analysis will feed into the supermarket’s strategic plan for the chip category in the next half year.

**Here is your task**

We need to present a strategic recommendation to Julia that is supported by data which she can then use for the upcoming category review however to do so we need to analyse the data to understand the current purchasing trends and behaviours. The client is particularly interested in customer segments and their chip purchasing behaviour. Consider what metrics would help describe the customers’ purchasing behaviour. 

To get started, download the resource csv data files below and begin performing high level data checks such as:

* Creating and interpreting high level summaries of the data
* Finding outliers and removing these (if applicable)
* Checking data formats and correcting (if applicable)

You will also want to derive extra features such as pack size and brand name from the data and define metrics of interest to enable you to draw insights on who spends on chips and what drives spends for each customer segment. Remember our end goal is to form a strategy based on the findings to provide a clear recommendation to Julia the Category Manager so make sure your insights can have a commercial application.

**LIFESTAGE:** Customer attribute that identifies whether a customer has a family or not and what point in life they are at e.g. are their children in pre-school/primary/secondary school.

**PREMIUM_CUSTOMER:** Customer segmentation used to differentiate shoppers by the price point of products they buy and the types of products they buy. It is used to identify whether customers may spend more for quality or brand or whether they will purchase the cheapest options.

#### Task 2

### Experimentation and uplift testing

Extend your analysis from Task 1 to help you identify benchmark stores that allow you to test the impact of the trial store layouts on customer sales.

**Here is the background information on your task**

You are part of Quantium’s retail analytics team and have been approached by your client, the Category Manager for Chips, has asked us to test the impact of the new trial layouts with a data driven recommendation to whether or not the trial layout should be rolled out to all their stores.

**Here is your task**

Julia has asked us to evaluate the performance of a store trial which was performed in stores 77, 86 and 88.

To get started use the QVI_data dataset below or your output from task 1 and consider the monthly sales experience of each store. 

This can be broken down by:
* total sales revenue
* total number of customers
* average number of transactions per customer

Create a measure to compare different control stores to each of the trial stores to do this write a function to reduce having to re-do the analysis for each trial store. Consider using Pearson correlations or a metric such as a magnitude distance e.g. 1- (Observed distance – minimum distance)/(Maximum distance – minimum distance) as a measure.

Once you have selected your control stores, compare each trial and control pair during the trial period. You want to test if total sales are significantly different in the trial period and if so, check if the driver of change is more purchasing customers or more purchases per customers etc.

#### Task 3

### Analytics and commercial application

Use your analytics and insights from Task 1 and 2 to prepare a report for your client, the Category Manager.

**Here is the background information on your task**

Task 3 is targeted specifically at building your ability to recognise commercial, actionable insights from your analysis and displaying it in a clear and concise way for your client, with minimal jargon. At Quantium, our analyst graduates sometimes work as what we like to call ""hybrids"" (a mix of analyst and consultant duties) so developing your presentation skills early is a huge win!

As both technical tasks 1 and 2 were open ended in terms of insights, this model answer will focus on the layout and the order of your inclusions, including where to include graphs, taglines, written insights and recommendations.

As part of Quantium’s retail analytics team, you have been conducting a range of analysis on transaction and purchase behaviour data to provide key recommendations to your client, the Category Manager of chips, who is putting together their strategic plan.

**Here is your task**

With our project coming to an end its time for us to send a report to Julia, based on our analytics from the previous tasks. We want to provide her with insights and recommendations that she can use when developing the strategic plan for the next half year.

As best practice at Quantium, we like to use the ""Pyramid Principles"" framework when putting together a report for our clients. If you are not already familiar with this framework you can find quick introductions on by searching form them on the internet.

For this report, we need to include data visualisations, key callouts, insights as well as recommendations and/or next steps.

Keep in mind the key considerations for a presentation:

* Data literacy level of your audience
* Table of contents / agenda
* Problem statement / purpose
* Overview and context
* Content balance
* Layout and content display
* Summary / next steps
",2,2,1,0,retail-analytics,"[consumer-analytics, customer-analytics, customer-segmentation, data-analysis, data-science, data-visualisation, data-wrangling, eda, exploratory-data-analysis, financial-analytics, quantium-virtual-experience, retail-analytics, sales-analytics]",44-45
vinitshetty16,Quantium-Virtual-Internship,,https://github.com/vinitshetty16/Quantium-Virtual-Internship,https://api.github.com/repos/Quantium-Virtual-Internship/vinitshetty16,Solution to Quantium Virtual Internships on Forage,"# Quantium-Virtual-Internship
This repository contains my solution to the [Quantium Data Analytics Virtual Experience Program](https://www.theforage.com/virtual-internships/prototype/NkaC7knWtjSbi6aYv/Data-Analytics?ref=DsEXFixxovqkRxR2u) on Forage
## Introduction

Quantium, which was established in 2002, has a long history of data science innovation in various spheres of the business. They are committed to building a team of aspirational, varied, and enjoyable people as a fast expanding worldwide leader in data science and AI.

For the past few years, Quantium has shared transactional and consumer data with a significant grocery brand. As a member of the Quantium analytics team, it is our duty to provide the business with valuable data analytics and insights to aid in strategic decision-making.

## Task 1: Data preparation and customer analytics

Conduct analysis on your client's transaction dataset and identify customer purchasing behaviours to generate insights and provide commercial recommendations.

* Examine transaction data – look for inconsistencies, missing data across the data set, outliers, correctly identified category items, numeric data across all tables. If you determine any anomalies make the necessary changes in the dataset and save it. Having clean data will help when it comes to your analysis. 

* Examine customer data – check for similar issues in the customer data, look for nulls and when you are happy merge the transaction and customer data together so it’s ready for the analysis ensuring you save your files along the way.

* Data analysis and customer segments – in your analysis make sure you define the metrics – look at total sales, drivers of sales, where the highest sales are coming from etc. Explore the data, create charts and graphs as well as noting any interesting trends and/or insights you find. These will all form part of our report to Julia. 

* Deep dive into customer segments – define your recommendation from your insights, determine which segments we should be targeting, if packet sizes are relative and form an overall conclusion based on your analysis. 

## Task 2: Experimentation and uplift testing

Extend your analysis from Task 1 to help you identify benchmark stores that allow you to test the impact of the trial store layouts on customer sales.

* Select control stores – explore the data and define metrics for your control store selection – think about what would make them a control store. Look at the drivers and make sure you visualise these in a graph to better determine if they are suited. For this piece it may even be worth creating a function to help you. 

* Assessment of the trial – this one should give you some interesting insights into each of the stores, check each trial store individually in comparison with the control store to get a clear view of its overall performance. We want to know if the trial stores were successful or not. 

* Collate findings – summarise your findings for each store and provide an recommendation that we can share with Julia outlining the impact on sales during the trial period.

## Task 3: Analytics and commercial application

Use your analytics and insights from Task 1 and 2 to prepare a report for your client, the Category Manager.

* With our project coming to an end its time for us to send a report to Julia, based on our analytics from the previous tasks. We want to provide her with insights and recommendations that she can use when developing the strategic plan for the next half year.

* As best practice at Quantium, we like to use the “Pyramid Principles” framework when putting together a report for our clients. If you are not already familiar with this framework you can find quick introductions on by searching form them on the internet.

* For this report, we need to include data visualisations, key callouts, insights as well as recommendations and/or next steps.
",1,1,1,0,retail-analytics,"[data-analysis-python, python, quantium-virtual-experience, retail-analytics]",44-45
Afnan-214,DSMethodology_WalmartAnalysis,,https://github.com/Afnan-214/DSMethodology_WalmartAnalysis,https://api.github.com/repos/DSMethodology_WalmartAnalysis/Afnan-214,,"# DSMethodology_WalmartAnalysis
A group project to anlysis walmart stores data for data methodology course and answer predefined tasks such as:
> Which store has maximum sales? <br>
> Which store has maximum standard deviation?<br>
> Which store/s has good quarterly growth rate?<br>
> Find out holidays which have higher sales than the mean sales in non-holiday season for all stores.<br>
> Provide a monthly and semester view of sales in units and give insights.

## Exploratory analysis
Using visualization and statistics functions to explore the variable distribution and the stores' sales nature.
## Insights 
Providing an observation for each output to draw a conclusion about walmart stores' sales.

",1,1,1,0,retail-analytics,"[data-exploration, python, retail-analytics]",44-45
AlessandraMonaco,Visual-Analytics,,https://github.com/AlessandraMonaco/Visual-Analytics,https://api.github.com/repos/Visual-Analytics/AlessandraMonaco,Implementation of a d3.js Visual Analytics dashboard for Sales Analysis and Customer Segmentation in Retail,"# Implementation of a Visual Analytics dashboard for Sales Analysis and Customer Segmentation in Retail
 
 ## Introduction
 This projects implements a dashboard for Visual Analytics in Retail, using javascript (d3.js) for the front-end and python (Flask, Scikit-learn, Numpy, Pandas) for back-end and analytical computations.
 The goal of the visual analysis is to support retailers' decision making in order to maximize profits, allowing for  better resource allocation (buy from the producers only what is more likely to be purchased by customers), more effective targeted advertising (focusing on groups of customers with similar characteristics), more proficient marketing (treating customers differently depending on their value, loyalty, proficiency), improving cutomer satisfaction (the market is aligned with customers' needs and interests). The dashboard provides a visual and interactive interface for sales analyses from different perspectives (trends, seasonalities, patterns, categorical analyses), market segmentation and segment analysis. Different approaches have been proposed for customer segmentation in literature (rfm, supervised, unsupervised, mixed, rule-based, demographic,...), and there is not always a best choice: it depends on what data are available and on business needs. This project proposes 2 types of segmentation: (i) Recency-Frequency-Monetary (RFM) model and (ii) Unsupervised Clustering (K-Means) on principal components obtained running PCA on categorical sales features. The first is about splitting customers depending on their purchase behavior (how frequent and recent they spend, how much they spend, so, HOW they buy), the latter is focused on customer interests, creating clusters of customers that purchased products belonging to the same subset of subcategories (so, WHAT they buy). The end-user can easily interact with categories, clusters or rfm segments to filter the visualizations and exploit the power of Visual Analytics for quick insights.
 
A detailed description of the project can be found in [VA_ProjectReport.pdf](https://github.com/AlessandraMonaco/Visual-Analytics/blob/master/VA_ProjectReport.pdf)

 ## Dataset
 The dataset used in this project is from Kaggle and describes sales of a fictional multiple retailer. It includes 3 csv files:
 * ```Customers.csv``` stores demographic information about customers, such as id, day of birth, gender and the code of the city in which he is located;
 * ```prod_cat_info.csv``` stores information regarding inclusion relationships between product categories;
 * ```Transactions.csv``` stores purchases made by customers for certain product categories, including the product quantity, the date, the total amount spent, the store type and other less relevant information.
 
The dataset contains 6 product macro-categories (Bags, Books, Clothing, Electronics,Footwear, Home and Kitchen), 23 053 transactions and 5 647 customers (but only 5 506 of them have at least one associated transaction). To limit file access and build a quickly responsive system, the original dataset is merged into a single (and more redundant) csv file (```full_data.csv```) with shape (23 053, 15), storing the following information: transaction id, customer id, transaction date, code and name of the product subcategory, code and name of the product category, product quantity, rate, tax, total amount spent in this transaction, store type, customer day of birth,customer gender, customer city code.
 
 ## How to run this project
 To try the interactive dashboard on your local machine, run the server file ```/src/app.py ``` and reach http://127.0.0.1:5000/ through your browser.

 ## Snapshots of the system
 ### Dashboard snapshot
 ![alt text](https://github.com/AlessandraMonaco/Visual-Analytics/blob/master/screenshots/dashboard_screenshot.PNG)

 ### Dashboard snapshot with RFM filtering (and mouse over one RFM segment)
 ![alt text](https://github.com/AlessandraMonaco/Visual-Analytics/blob/master/screenshots/dashboard_filter_rfm.png)

 ### Dashboard snapshot with Cluster filtering (and mouse over calendar heatmap day)
 ![alt text](https://github.com/AlessandraMonaco/Visual-Analytics/blob/master/screenshots/dashboard_filter_cluster.png)

 ### Dashboard snapshot with Electronics products filtering (and mouse over subcategory)
 ![alt text](https://github.com/AlessandraMonaco/Visual-Analytics/blob/master/screenshots/dashboard_filter_category.png)



 ## References
 * Dataset : [Kaggle Retail dataset](https://www.kaggle.com/darpan25bajaj/retail-case-study-data)
 * Paper that inspired this project : [Mohammad Hafiz Hersyah Miftahul Jannah Ricky Akbar, Meza Silvana. **Implementation of Business Intelligence for Sales Data Management Using Interactive Dashboard Visualization in XYZ Stores.** *International Conference on Information Technology Systems and Innovation (ICITSI)*, 2020.](https://ieeexplore.ieee.org/document/9264984)
 * Papers on Customer Segmentation : 
    * [Rajesh Parekh Ron Kohavi. **Visualizing RFM Segmentation.** *Proceedings of the 2004 SIAM international conference*, 2004.](https://www.researchgate.net/publication/220906727_Visualizing_RFM_Segmentation)
    * [A. S. M. Shahadat Hossain. **Customer Segmentation using Centroid Based and Density Based Clustering Algorithms**. *3rd International Conference on Electrical Information and Communication Technology (EICT)*, 2017.](https://ieeexplore.ieee.org/document/8275249)
    * [Saraswati Jadhav Rahul Shirole, Laxmiputra Salokhe. **Customer Segmentation using RFM Model and K-Means Clustering.** *International Journal of Scientific Research in Science and Technology (IJSRST), 8(3)*, 2021.](https://www.researchgate.net/publication/352393770_Customer_Segmentation_using_RFM_Model_and_K-Means_Clustering)
 * Paper to improve parallel coordinates visualization : [Bertjan Broeksema Julian Heinrich. **Big Data Visual Analytics with Parallel Coordinates.** *Big Data Visual Analytics (BDVA)*, 2015](https://ieeexplore.ieee.org/abstract/document/7314286)
",0,0,1,0,retail-analytics,"[analytics, customer-segmentation, d3, d3js, dashboard, market-analysis, market-segmentation, python, retail-analytics, rfm-analysis, segment-analytics, unsupervised-clustering, unsupervised-segmentation, visual-analytics, visualization]",44-45
ishitaagl20,Retailyst,,https://github.com/ishitaagl20/Retailyst,https://api.github.com/repos/Retailyst/ishitaagl20,Exploring Market Basket Analysis and Using Data Driven Insights to Make store layouts,"# Retailyst

##### Exploring Market Basket Analysis and Using Data Driven Insights to Make store layouts

### Market Basket Analysis Results

<a href=""https://github.com/ishitaagl20/Retailyst/README.md"">
<p align = ""center""><img alt=""Logo"" src=""https://github.com/ishitaagl20/Retailyst/blob/master/Images/MBA.jpeg"" height = 300px class=""center""></p>
</a>

### Floor Plan - Backed with Market Basket Analysis and Sales insights


<a href=""https://github.com/ishitaagl20/Retailyst/README.md"">
<p align = ""center""><img alt=""Logo"" src=""https://github.com/ishitaagl20/Retailyst/blob/master/Images/Layout.jpeg"" height = 350px class=""center""></p>
</a>
",0,0,1,0,retail-analytics,"[apriori-algorithm, data-analytics, data-visualization, floorplan, html-css-javascript, market-basket-analysis, mongodb, nodejs, python, retail, retail-analytics, store-layout]",44-45
samisaud,retail-customer-segmentation,,https://github.com/samisaud/retail-customer-segmentation,https://api.github.com/repos/retail-customer-segmentation/samisaud,"Leveraging K-Means clustering, our project categorizes retail customers based on purchasing behaviors and demographics. This provides businesses with actionable insights to tailor marketing efforts, enhancing customer experience and boosting sales.","# retail-customer-segmentation

Welcome to the retail-customer-segmentation repository. In today's competitive retail sector, it's paramount for businesses to grasp the diverse behaviors and needs of their customers. This project aims to help businesses with just that.

By harnessing the power of machine learning, particularly the K-Means clustering algorithm, we've developed a robust method to segment customers into distinct categories. These segments are based on variables like purchasing patterns and demographics.

Why is this important?
With our Retail Customer Segmentation:

Retailers can design targeted marketing campaigns for each customer segment, increasing ROI.
They can provide tailored service offerings, enhancing customer satisfaction and loyalty.
Businesses can make data-driven decisions to optimize sales and understand areas for growth.
We invite you to explore the repository, check out the datasets, the algorithms, and the insights we've derived. Any feedback, contributions, or insights of your own are most welcome!

# Contributors
Ehtesham: Expertise in Database management, Python programming, and ML model creation. Responsible for the initial data preprocessing and setting up the database structure.
Contact: https://github.com/esana1

Sami: Specialized in ML models, Python coding, and Data Visualization. Played a pivotal role in model selection, tuning, and visual representation of data.
Contact: https://github.com/samisaud

Ayush: Keen expertise in Visualization techniques, Business understanding, and problem-solving. Provided valuable business insights to align the project with market needs.
Contact: https://github.com/asriv106
",0,0,1,0,retail-analytics,"[customer-insights, data-visualization, data-visualizations, kmeans-clustering, machine-learning, retail-analytics]",44-45
P1210,GRIP-Internship,,https://github.com/P1210/GRIP-Internship,https://api.github.com/repos/GRIP-Internship/P1210,Tasks performed under Data Science and Business Analytics internship by Sparks Foundation.,"# Internship tasks

Data Science and Business Analytics Internship at the Sparks Foundation.

 - Task 3 - EDA on Retail Data 
 - Task 1 - Supervised ML 
 - Task 4 - EDA on Terrorism Data 
",0,0,1,0,retail-analytics,"[modelling, retail-analytics]",44-45
shrutibalan4591,Retail-Store-Customer-Segment-and-Sales-Analysis,,https://github.com/shrutibalan4591/Retail-Store-Customer-Segment-and-Sales-Analysis,https://api.github.com/repos/Retail-Store-Customer-Segment-and-Sales-Analysis/shrutibalan4591,"This project looks at the sales pattern of a product category in a retail store, using the store’s transaction dataset and identifying customer purchase behavior, to generate insights and recommendations.","# Retail-Store-Customer-Segment-and-Sales-Analysis

## About the Project:
This project looks at the sales pattern of a product category in a retail store, using the store’s transaction dataset and identifying customer purchase behavior, to generate insights and recommendations.

This project comprises of 3 tasks:
### Task 1: Data preparation and customer analytics
We analyze the store’s transaction dataset and identify customer purchasing behaviors to generate insights and provide commercial recommendations.

There are two important datasets in this task, chips sales and customer data. The task is to carry out exploratory data analysis in order to extract insights about the purchasing behavior of different customer groups. We want to then use those insights to formulate strategies to help the store increase chip sales.

Below is the outline of our main tasks along with what we should be looking for in the data for each. 

**Examine transaction data** – look for inconsistencies, missing data across the data set, outliers, correctly identified category items, numeric data across all tables. If we determine any anomalies we make the necessary changes in the dataset and save it. Having clean data will help when it comes to the analysis. 
Examine customer data – check for similar issues in the customer data, look for nulls and when we are happy we merge the transaction and customer data together so it’s ready for the analysis.

**Data analysis and customer segments** – in the analysis we make sure we define the metrics – look at total sales, drivers of sales, where the highest sales are coming from etc. Explore the data, create charts and graphs as well as noting any interesting trends and/or insights you find. These will all form part of the report. 

**Deep dive into customer segments** – define recommendation from the insights, determine which segments should be targeted, if packet sizes are relative and form an overall conclusion based on the analysis. 


### Task 2: Experimentation and uplift testing
We extend our analysis from Task 1 to help us identify benchmark stores that allow us to test the trial store layouts' impact on customer sales.

For this part of the project we will be examining the performance in trial vs control stores to provide a recommendation for each location based on our insight. There are 3 trial stores that have gone through a 3-month trial period with modified store layouts.

Below are some of the areas we focus on:

**Select control stores** – explore the data and define metrics for the control store selection – think about what would make them a control store. Look at the drivers and make sure we visualise these in a graph to better determine if they are suited. For this piece it may even be worth creating a function to help it. 

**Assessment of the trial** – this one should give us  some interesting insights into each of the stores, check each trial store individually in comparison with the control store to get a clear view of its overall performance. We want to know if the trial stores were successful or not. 

**Collate findings** – summarise the findings for each store and provide an recommendation that we can share with the store,  outlining the impact on sales during the trial period.

### Task 3: Analytics and commercial application
Use analytics and insights from Task 1 and 2 to prepare a report for the client(the retail store).(This document is not included in this repo.)



",0,0,1,0,retail-analytics,"[customer-segmentation, data, data-analysis, data-analytics, data-visualization, presentation, python, retail-analytics, sales-analysis]",44-45
vijayrangvani,Olist-Customer-Purchase-Behaviour-Analysis,,https://github.com/vijayrangvani/Olist-Customer-Purchase-Behaviour-Analysis,https://api.github.com/repos/Olist-Customer-Purchase-Behaviour-Analysis/vijayrangvani,Analyse the customer purchase behaviour to optimize inventory cost,"# Olist-Customer-Purchase-Behaviour-Analysis

Introduction
OList is an e-commerce company that has faced some losses recently. It wants to manage its inventory very well so as to reduce any unnecessary costs that it might be bearing. It needs to identify the product categories to get rid of without significantly impacting business.

Objective
To identify top products that contribute to the revenue
Analyse the customer purchase behaviour to estimate what items are more likely to be purchased individually or in combination with some other products
Provide recommendations for better inventory management so as to reduce any unnecessary costs

https://public.tableau.com/views/MarketBasketAnalysis_16697000937350/MarketingAnalytics?:language=en-US&publish=yes&:display_count=n&:origin=viz_share_link
",0,0,1,0,retail-analytics,"[dashboard, market-analysis, pareto-analysis, purchase-analysis, retail-analytics, storytelling, tableau, visulization]",44-45
esana1,retail-customer-segmentation,,https://github.com/esana1/retail-customer-segmentation,https://api.github.com/repos/retail-customer-segmentation/esana1,"Leveraging K-Means clustering, our project categorizes retail customers based on purchasing behaviors and demographics. This provides businesses with actionable insights to tailor marketing efforts, enhancing customer experience and boosting sales.","# retail-customer-segmentation

Welcome to the `retail-customer-segmentation` repository. In today's competitive retail sector, it's paramount for businesses to grasp the diverse behaviors and needs of their customers. This project aims to help businesses with just that.

By harnessing the power of **machine learning**, particularly the **K-Means clustering algorithm**, we've developed a robust method to segment customers into distinct categories. These segments are based on variables like purchasing patterns and demographics.

## Why is this important? 

With our Retail Customer Segmentation:

- **Retailers** can design targeted marketing campaigns for each customer segment, increasing ROI.
- They can provide tailored service offerings, enhancing customer satisfaction and loyalty.
- **Businesses** can make data-driven decisions to optimize sales and understand areas for growth.

We invite you to explore the repository, check out the datasets, the algorithms, and the insights we've derived. Any feedback, contributions, or insights of your own are most welcome!

## Contributors

- **Ehtesham**: Expertise in Database management, Python programming, and ML model creation. Responsible for the initial data preprocessing and setting up the database structure. [Contact Ehtesham](https://github.com/esana1)

- **Sami**: Specialized in ML models, Python coding, and Data Visualization. Played a pivotal role in model selection, tuning, and visual representation of data. [Contact Sami](https://github.com/samisaud)

- **Ayush**: Keen expertise in Visualization techniques, Business understanding, and problem-solving. Provided valuable business insights to align the project with market needs. [Contact Ayush](https://github.com/asriv106)
",0,0,1,0,retail-analytics,"[customer-behavior, customer-insights, customer-segmentation, data-analysis, data-visualization, kmeans-clustering, machine-learning, marketing-strategy, retail-analytics, targeted-marketing]",44-45
sav1nbrave4code,RetailAnalistycsDB,,https://github.com/sav1nbrave4code/RetailAnalistycsDB,https://api.github.com/repos/RetailAnalistycsDB/sav1nbrave4code,Data base structure for retail analyzes project,"# RetailAnalistycsDB

Data base structure for retail analyzes project

## Content

The project implements a database structure with various functions, procedures and triggers.
There is also import and export to tsv files.
In addition to standard tables, presentation tables are created containing the necessary
information (their structure is indicated in the diagram).

## Structure

[diagram](https://dbdiagram.io/d/64f8113d02bd1c4a5e0c34af)

<img src=""./images/diagram.png"">

Description of procedures, functions and triggers - is in the comments in the code itself.

1. To move importing tsv files `make move`
2. To create a database `make create`
3. To drop a database `make drop`
4. To move tsv files and create a database `make`
",0,0,1,0,retail-analytics,"[database, plpgsql, postgresql, retail-analytics, sql]",44-45
dmeoli,OnlineRetail,,https://github.com/dmeoli/OnlineRetail,https://api.github.com/repos/OnlineRetail/dmeoli,Data Mining project 2020/2021 @ University of Pisa,"# Data Mining project 2020/2021 [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/dmeoli/DataMiningUniPi/master)

This project was developed during the Data Mining course @ 
[Department of Computer Science](https://www.di.unipi.it/en/) @ [University of Pisa](https://www.unipi.it/index.php/english) 
under the supervision of prof. 
[Anna Monreale](http://pages.di.unipi.it/amonreale/).

The project consists in data analysis based on the use of data mining tools.
The project has to be performed by a team of 2/3 students. It has to be performed by
using Python. The guidelines require to address specific tasks and results must be
reported in a unique paper. The total length of this paper must be max 20 pages of text
including figures. The students must deliver both: paper and well commented Python
notebooks.

## Task 1: Data Understanding and Preparation

### Task 1.1: Data Understanding
Explore the dataset with the analytical tools
studied and write a concise “data understanding” report describing data
semantics, assessing data quality, the distribution of the variables and the
pairwise correlations.

Subtasks of DU:
- Data semantics
- Distribution of the variables and statistics
- Assessing data quality (missing values, outliers)
- Variables transformations & generation
- Pairwise correlations and eventual elimination of redundant variables

### Task 1.2: Data Preparation 
Improve the quality of your data and prepare it by extracting new features interesting for describing the customer profile and his
purchasing behavior. These indicators have to be extracted for each customer.
Indicators to be computed are:
- I: the total number of items purchased by a customer during the period of
observation
- Iu: the number of distinct items bought by a customer in the period of
observation
- Imax: the maximum number of items purchased by a customer during a
shopping session
- E: the Shannon entropy on the purchasing behavior of the customer

It is mandatory that each team defines additional indicators leading to the
construction of a customer profile that can lead to an interesting analysis of
customer segmentation.

Once, the set of indicators will be computed the team has to explore the new
features for a statistical analysis (distributions, outliers, visualizations,
correlations).

## Task 2: Clustering Analysis
Based on the customer’s profile explore the dataset using various clustering techniques.
Carefully describe your decisions for each algorithm and which are the advantages
provided by the different approaches.

Subtasks:
- Clustering analysis by K-means:
    - Identification of the best value of k
    - Characterization of the obtained clusters by using both analysis of
the k centroids and comparison of the distribution of variables within
the clusters and that in the whole dataset
    - Evaluation of the clustering results
- Analysis by density-based clustering:
    - Study of the clustering parameters
    - Characterization and interpretation of the obtained clusters
- Analysis by hierarchical clustering:
    - Compare different clustering results got by using different version of
the algorithm
    - Show and discuss different dendrograms using different algorithms
- Final evaluation of the best clustering approach and comparison of the clustering
obtained

Explore the opportunity to use alternative clustering
techniques in the library: https://github.com/annoviko/pyclustering/

## Task 3: Predictive Analysis
Consider the problem of predicting for each customer a label that defines if (s)he is a
*high-spending* customer, *medium-spending* customer or *low-spending* customer.
The students need to:
- Define a customer profile that enables the above customer classification. Please,
reason on the suitability of the customer profile, defined for the clustering
analysis. In case this profile is not suitable for the above prediction problem you
can also change the indicators.
- Compute the label for any customer. Note that, the class to be predicted must be
nominal.
- Perform the predictive analysis comparing the performance of different models
discussing the results and discussing the possible preprocessing that they
applied to the data for managing possible problems identified that can make the
prediction hard. Note that the evaluation should be performed on both training
and test set.
  
## Task 4: Sequential Pattern Mining
Consider the problem of mining frequent sequential patterns. To address the task:
- Model the customer as a sequence of baskets
- Apply the sequential pattern mining algorithm
- Discuss the resulting patterns

Note that you can decide to filter out some customers.

The algorithm presented during the Python lecture on sequential
pattern mining does not consider the opportunity to define temporal constraints.
As an optional point, the students can extend the algorithm and analysis considering one
or more constraints.",9,9,3,0,online-retail,"[assessing-data-quality, clustering, correlations, customer-profile, data-mining, data-science, doc2vec, embeddings, gsp-algorithm, hdbscan, nltk, online-retail, pattern-discovery, predictive-analysis, prefixspan, rfm-analysis, sequential-pattern-mining, spm, time-constraints, umap]",44-45
easonlai,online_retail_product_recommendation_samples,,https://github.com/easonlai/online_retail_product_recommendation_samples,https://api.github.com/repos/online_retail_product_recommendation_samples/easonlai,"This is a code sample repository for online retail product recommendations using Collaborative Filtering (Memory-Based, aka History-Based). The source data used the famous Online Retail Data Set from UCI Machine Learning Repository.","﻿# Online Retail Product Recommendation Samples

This is a code sample repository for online retail product recommendations using Collaborative Filtering (Memory-Based, aka History-Based). The source data used the famous [Online Retail Data Set from UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/online+retail).

* /data/Online Retail.xlsx <-- [Online Retail Data Set from UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/online+retail)
* item_to_item_by_collaborative_filtering.ipynb <-- Product-Based Filtering, Product-to-Product product recommendations.
* user_to_user_by_collaborative_filtering.ipynb <-- User-Based Filtering, User-to-User product recommendations.

Below is a list of categories of Recommendation Systems to achieve different objectives.
* Collaborative Filtering
    *  Memory Based (aka History Based)
        * Product-Based Filtering
        * User-Based Filtering
    * Model Based (e.g., Alternating Least Square (ALS))
    * Hybrid
* Content-Based Filtering
* Hybrid

Enjoy!
",6,6,1,0,online-retail,"[collaborative-filtering, cosine-similarity, online-retail, product-recommendation, python, python3]",44-45
alifialistu,RFMAnalysis-KMeans,,https://github.com/alifialistu/RFMAnalysis-KMeans,https://api.github.com/repos/RFMAnalysis-KMeans/alifialistu,"RFM (Recency, Frequency, Monetary) Analysis on an Online Retail Customers using K-Means Clustering with Python","# RFM (Recency, Frequency, Monetary) Analysis on an Online Retail Customers using K-Means Clustering with Python

The Online Retail data contains customer purchasing history. Including CustomerID, Stock Code, Item Description, Quantity, Unit Price, Invoice Date, and Country.

Dataset : https://www.kaggle.com/vijayuv/onlineretail

# Business Understanding
**PROBLEM** : Only have basic customer data but want to know customer behaviour to plan marketing strategy accordingly.

**QUESTION** : How to understand customer behaviour to plan marketing strategy accordingly?

**MEASURE** : A well clustered customer basen on basic data that we have.

# RFM Analysis for Customer Segmentation
RFM analysis is a data-driven customer behavior segmentation methodology in which RFM stands for recency, frequency, and monetary value. The concept is to segment customers based on when their last purchase was (Recency), how many times they have purchased in the past (Frequency), and how much they have spent (Monetary).

RFM analysis enables personalized marketing, improves satisfaction and helps marketers to develop unique, appropriate deals for the right customer groups.

# Conclusion
Most customers are rookie customers and regular customers. There are very few loyal and core customers in this online retail. This may be because the online retail store has not been around for a long time and is still progressing towards success.

# References
* “RFM Analysis: An Effective Customer Segmentation technique using Python” by Anand Singh

https://link.medium.com/oyaAaPRtRab

* “Customer Segmentation with RFM Analysis & Kmeans Clustering” by Anugrah Nurhamid

https://link.medium.com/lnqM7AVtRab

* “Find Your Best Customers with Customer Segmentation in Python” by Susan Li

https://link.medium.com/0GA3V7StRab

* Pareto Chart

https://tylermarrs.com/posts/pareto-plot-with-matplotlib/
",3,3,1,0,online-retail,"[customer-segmentation, kmeans-clustering, online-retail, rfm-analysis]",44-45
HermawanHermawan,Optimizing-Retail-Revenue-in-SQL,,https://github.com/HermawanHermawan/Optimizing-Retail-Revenue-in-SQL,https://api.github.com/repos/Optimizing-Retail-Revenue-in-SQL/HermawanHermawan,Analyze product data for an online sports retail company to optimize revenue.,"PROJECT DESCRIPTION

This projects aims to make use of SQL to analyze product data for an online sports retail company. For such purpose, several common techniques in SQL are applied, including aggregation, cleaning, labeling, Common Table Expressions, and correlation to produce recommendations on how the company can maximize revenue. The dataset used contains several columns such as pricing and revenue, ratings, reviews, descriptions, and website traffic. Each column contains observations in various format, such as numeric, string, and timestamp. Those observations.

PROJECT TASK

1. Counting missing values
2. Nike vs Adidas pricing
3. Labeling price ranges
4. Average discount by brand
5. Correlation between revenue and reviews
6. Ratings and reviews by product description length
7. Reviews by month and brand
8. Footwear product performance
9. Clothing product performance

",2,2,1,1,online-retail,"[aggregation, cleaning, common-table-expression, online-retail, product-data, revenue-optimization, sql]",44-45
Otniel113,OnlineRetail,,https://github.com/Otniel113/OnlineRetail,https://api.github.com/repos/OnlineRetail/Otniel113,Mini Portfolio Analisis Penjualan pada sebuah Ritel Online,"# OnlineRetail
Mini Portfolio Analisis Penjualan pada sebuah Ritel Online yang merupakan tugas individu dari Rakamin Trial Class

## Dataset
Dataset didapat dari pihak Rakamin berupa CSV dengan ukuran 1067371x8 dengan penjelasan setiap kolom sebagai berikut: <br>
● Invoice : Nomor invoice 6 digit yang ditetapkan secara unik untuk setiap transaksi. Jika kode ini dimulai dengan huruf 'C', itu menunjukkan pembatalan. <br>
● StockCode : Kode produk (barang). Angka 5 digit yang ditetapkan secara unik untuk setiap produk yang berbeda. <br>
● Description : Nama produk. <br>
● Quantity : Jumlah kuantitas setiap produk per transaksi. <br>
● InvoiceDate : Tanggal dan waktu invoice, yakni hari dan waktu saat transaksi dibuat. <br>
● UnitPrice : Harga satuan atau harga produk per unit dalam sterling (£). <br>
● CustomerID : Nomor 5 digit yang ditetapkan secara unik untuk setiap pelanggan. <br>
● Country : Nama negara tempat tinggal pelanggan <br>

## Hal yang dilakukan
### Section 1
#### 1.1. Create New Feature: Year
#### 1.2. Filtering Data
#### 1.3. Create New Feature: Revenue
#### 1.4. Average of Revenue per Year
![image](https://user-images.githubusercontent.com/57952404/148207783-c4b18d91-8072-40f5-b569-788279712b21.png)
<br>
### Section 2
#### 2.1. Filtering Data - Customers who finished their purchases
#### 2.2. Filtering Data - Customers who canceled their purchases
#### 2.3. Number of Finished and Canceled Transactions Each Year
#### 2.4. Cancellation Rate
![image](https://user-images.githubusercontent.com/57952404/148207825-cb58444d-ea87-4196-9b9e-e4232f2060e3.png)

",1,1,2,0,online-retail,"[data-analysis, data-science, data-visualization, jupyter-notebook, online-retail]",44-45
Vzenun,DBMS-Online-Retail-Store-Project,,https://github.com/Vzenun/DBMS-Online-Retail-Store-Project,https://api.github.com/repos/DBMS-Online-Retail-Store-Project/Vzenun,An online retail store end to end database application,,1,1,1,0,online-retail,"[ecommerce-application, olap-database, online-retail, python3, sql, workbench]",44-45
Fedorov-Artem,kaggle_OTTO,,https://github.com/Fedorov-Artem/kaggle_OTTO,https://api.github.com/repos/kaggle_OTTO/Fedorov-Artem,code for kaggle OTTO competition (got silver medal),"# kaggle_OTTO
## Task and solution overview.
This is the code for kaggle competition [""OTTO – Multi-Objective Recommender System""](https://www.kaggle.com/competitions/otto-recommender-system). OTTO is Germany's largest online retailer. The task is to predict which exact next item user is going to click next and which items user is going to add to cart or order before the end of the test period. The competition data is real life new users' sessions on OTTO website. The test dataset includes one week of user sessions, truncated at a random point. Organizers also provide participants with history of full new user's sessions for four weeks, preceding the test period. No metadata is available for all the items, that show up in both datasets. Participants only have item ID's, that are called AIDs, and there are about 1.8 mln AIDs showing up in the competition data, including both full and truncated user sessions.

From the beginning I have decided to use Jupyter notebooks ran on kaggle website to produce the solution. However, this decision turned out to have a number of complications. Firstly, kaggle notebooks without GPU support at a time of competition had RAM limit of 30 Gb. That caused several problems, for example, I had to spend some time writing code that would merge two dataframes chunk by chunk, as using a simple merge would cause a memory error. Kaggle notebooks with GPU available have RAM limit of just 13 Gb, that made me choose between some features, instead of using all of them. Then, Jupyter notebooks are not that useful when dealing with projects that require a complicated data pipeline. The total number of notebooks used to produce the final solution is 28, and that number does not include a few more notebooks I created during the competition to test some approaches that brought no fruit.

The solution's pipeline includes the following major stages:
* creating a cross-validation dataset from the last week of known full sessions, similar to the test dataset;
* calculating co-visitation matrixes, word2vec models and making some other calculations aside of the main pipeline;
* generating candidates;
* engineering features;
* training the GBDT re-ranking models on the cross-validation dataset and using those models to select most relevant candidates, generated for the test sessions;
* some final formatting and submitting the results.
Generating candidates, engineering features, training models and making final prediction stages have separate notebooks for clicks, carts and orders. For each of these predictions, a dedicated re-ranking model is trained, on a different dataset, for different candidates,  with different features, although not all the features are different.
![solution's pipeline](https://raw.githubusercontent.com/Fedorov-Artem/kaggle_OTTO/136617a2eaea74884cc1f1750fdf093d243f3765/OTTO_schema.png)


## A closer look at input data and metric.
Each session data consists of a session ID and sequence of events, each event includes AID, a timestamp, and event type, which could be either click, cart or order. There are total 12.9 mln full sessions, and a median full session has 6 events, while there are 1.7 mln truncated sessions in the test dataset and a median truncated session has just 2 events. More than 90% of all events are clicks, most sessions are short and only include clicks. 

For every session in the test dataset, competitors predict 20 clicks, 20 carts and 20 orders. Here is the formula used to score the predictions:

score = 0.1*R<sub>clicks</sub> + 0.3*R<sub>carts</sub> + 0.6*R<sub>orders</sub> , 

and each of the R values is a recall that could take values between 0 and 1. So, the coefficients are set in a way that makes predicting orders more important than predicting carts, and predicting carts more important than predicting clicks.

## All project notebooks
Here is the full list of notebooks, used in the project pipeline:
* notebooks with common code
  * OTTO common (otto-common.ipynb)
  * OTTO common feature engineering (otto-common-fe.ipynb)
* creating a cross-validation dataset
  * Prepare cross-validation (otto-prepare-cv.ipynb)
* calculations aside of the main pipeline
  * ""Regular"" co-visitation click2click matrix (otto_click2click_regular.ipynb)
  * ""Experimental"" co-visitation click2click matrix (otto-click2click-experiment.ipynb)
  * Click2buy and buy2buy co-visitation matrixes (otto-click2buy-buy2buy.ipynb)
  * Click2buy short co-visitation matrix (otto-click2buy-short.ipynb)
  * W2vec model for clicks (otto-word2vec-clicks.ipynb)
  * W2vec model for carts and orders (otto-word2vec-carts-orders.ipynb)
  * Calculations for clicks (create-counts-for-clicks.ipynb)
  * Calculations for buys (create-counts-buys.ipynb)
* generating candidates
  * Generate candidates for clicks (otto-click-candidates-generation.ipynb)
  * Generate candidates for carts (otto-generate-candidates-carts.ipynb)
  * Generate candidates for orders (otto-generate-candidates-orders.ipynb)
* engineering features
  * Feature engineering for clicks model (otto-feature-engineering-clicks.ipynb)
  * Feature engineering for carts model (otto-feature-engineering-carts.ipynb)
  * Feature engineering for orders model (otto-feature-engineering-orders.ipynb)
  * W2vec features for clicks (otto-clicks-w2vec.ipynb)
  * W2vec features for carts (otto-carts-w2vec.ipynb)
  * W2vec features for carts (part_1) (otto-carts-w2vec-part1.ipynb)
  * W2vec features for orders (otto-orders-w2vec.ipynb)
  * W2vec features for orders (part_1) (otto-orders-w2vec-part1.ipynb)
* training and predicting
  * Clicks Model and Prediction (otto-model-clicks.ipynb)
  * Carts Model (otto-model-carts.ipynb)
  * Orders Model (otto-model-orders.ipynb)
  * Carts Prediction (otto-carts-prediction.ipynb)
  * Making and combining predictions for orders (otto-orders-predict-combine.ipynb)
* final formatting and submitting the results
  * OTTO Upload (otto-upload.ipynb)

## Cross-validation datasets
Organizers have published the code they have used to produce the test dataset. It actually cuts all the sessions that started before the test period and continue into test period. Then, it selects sessions that have started during test period and filters out sessions with aids not met in any session before test period (short sessions, of course, are more likely to pass through that filter).  After that the unfiltered sessions are truncated at a random point, leaving at least one known and at least one unknown event. As an output we have a shortened file of full sessions, a cross-validation file of truncated sessions and a file with labels.

I used this code to produce 2 different cross-validation datasets with different random seeds. The intention was to check at some point whether results for different cross-validation sets differ and probably to try using features, generated for two datasets to train two models and then take average prediction. While working on the project, I have compared several times intermediate results for the two datasets, and there never was a significant difference. I have also tried using two models build upon different cross-validation datasets to predict orders, but this improved results just a little bit. So, building all the features for both cross-validation datasets probably was not worth the effort.

In the same notebook I also converted all the data, including inputs and cross-validation sets, from json to parquet, changed datatypes from int64 to int32, and mapped event types to integers (0 - clicks, 1 - carts, 2 - orders) to reduce memory usage.

With cross-validation datasets ready, it is possible to get additional insights on the test dataset and on the events I am going to predict.
The number of full sessions in history has reduced to 10.6 mln, and the cross-validation datasets have 1.8 mln truncated sessions. Out of that number only about 300,000 sessions or 17% have at least one aid carted and about half of that number, about 150,000 sessions or 8% have at least one aid ordered. The vast majority of sessions do not have neither cart nor order labels. This means, cart and order predictions for most short sessions do not matter, as only predictions for sessions with some actual carts or orders give points. At the same time, almost all the sessions do have a single ground truth value for clicks, while very few have no ground truth values.

## Calculations aside of the main pipeline
These notebooks include notebooks calculating the co-visitation matrixes, w2vec models and multiple other calculations combined into 2 notebooks ""create counts for clicks"" and ""create counts for buys"" (buys means any non-click event, i.e. either cart or click). All the calculations in those notebooks are repeated at least twice: once for the cross-validation dataset and then for the full data, in a few cases the calculations need to be made separately for each cross-validation dataset.

Code to calculate the co-visitation matrix is 90% the same for all the matrixes I have tried for this project. So, I've written a class and moved it to the OTTO common notebook, so, that it could be used to create a child class for each case to count different types of co-occurring events.  These notebooks have very little code, and they take 2 to 4 hours to run.

Here is the full list of co-visitation matrixes, used to produce the final result:
* **""Regular"" co-visitation click2click matrix** (otto_click2click_regular.ipynb) counts two events of any type in a session, if time between them is less than 5 minutes. Weight coefficient is calculated in a way that makes later events to have higher weights. The ordering of events does not count for this matrix, this means that if two events are close to each other in the same session, it does not matter which one of them comes first. This matrix is used both for click candidates generation and to calculate a feature (wgt_matrix) for the clicks model.
* **""Experimental"" co-visitation click2click matrix** (otto-click2click-experiment.ipynb) counts two events of any type in a session, if time difference between them is less than 5 minutes and there are no more than 20 events between them. Weight coefficient is calculated in a way that makes later events to have higher weights. The ordering of events does count for this matrix, this means that aid_y is only counted if it comes after aid_x. I tried to use this matrix to generate click candidates, but ""regular"" matrix showed better result. In the final pipeline, this matrix is used to calculate a feature (wgt_exp) for the clicks model.
* **Click2buy co-visitation matrix** (otto-click2buy-buy2buy.ipynb) counts events in a session, if the later event is a buy (either cart or order) and time difference between them is less than 10 hours. The weight value is calculated in a way that makes pairs of events with smaller time difference more important. This matrix is used both for carts and orders candidate generation and to calculate 2 features (wgt_c2buy_full and wgt_c2buy_6_from_full) for carts and orders models.
* **Buy2buy co-visitation matrix** (otto-click2buy-buy2buy.ipynb) counts events in a session, if both of them are buys and time difference between them is less than 5 days. The weight value is always equal to, so each pair of events is equally important. This matrix is used to calculate a feature (wgt_buy2buy) for carts and orders models.
* **Click2buy short co-visitation matrix** (otto-click2buy-short.ipynb) counts events in a session, if the later event is a buy (either cart or order) and time difference between them is less than 2 hours. The weight value is calculated in a way that makes pairs of events with smaller time difference more important. Comparing to previously mentioned click2buy co-visitation matrix, in this one time difference is limited to much shorter time and the weight value declines much faster as time between the events increases. This matrix is used to build a feature (wgt_c2buy_short) for carts and orders models.
* **Exact next click-to-click co-visitation matrix** (create-counts-for-clicks.ipynb) counts only exact next, regardless of their type or of time passed between them. This is the fastest matrix to calculate, so it doesn't have a dedicated notebook, and is calculated in the same notebook with some other side calculations for the clicks model. This matrix is used to calculate two features for the clicks model: 'wgt_last' and 'wgt_before_last'.

Notebooks with w2vec model generation both have very few of rows with code. Information about event type and event time is removed from full sessions, so, the sequence of aids is the only information kept. That information is used then to the w2vec model, using sequences of aids as ""sentences"" and aids as ""words"". These notebooks take significant time to run: it takes about 3 hours to train models for cross-validation and test datasets, using the first 3 weeks of full sessions or all the known full sessions correspondingly.

I had an intuition, that a w2vec model with a longer window would be more useful for carts and orders models, while a model with shorter window would produce better results for the clicks model. I've made the checks during the competition, trying both w2vec models to produce features for each of GBDT models, and confirmed that this is true. So, I kept using two different w2vec models trained with slightly different parameters. However, the difference in performance between the two w2vec models was relatively small, so I choose not to make any additional experiments with changing the models' parameters and tried some other ideas instead.

List of side calculations made in ""Calculations for buys"" notebook:
* conversion rate - means conversion from click to either cart or order;
* conversion to carts - conversion from either clicks, previously carted aids or previously ordered aids to carts;
* conversion to orders - conversion from either clicks, carts or previously ordered aids to new orders;
* average per aid clicks before buy;
* daily total carts/orders per aid;
* average w2vec similarity between the last one aid in session and 5 aids before it.

List of side calculations made in ""Calculations for clicks"" notebook:
* median time users view aid;
* average per day clicks per aid;
* return rate, counting how often users return for a new click or other actions with the same aid;
* exact next click-to-click co-visitation matrix, that has been already mentioned earlier.

## Generating candidates
The generation of candidates is rules-based for all the three pipelines. I've spent significant time trying to improve the candidate generation process, probably put too much effort in it. 

I generally use three sources of candidates: session history aids, co-visitation matrixes applied to session history aids and daily most popular aids. Depending on the number of candidates, I use different hand-picked coefficients that define limits to the number of candidates coming from each source. I've started with using 50 candidates for all the three models, then moved to 75 candidates both for carts and orders. Before the very end of the competition, I've planned to start using 75 candidates for the clicks model as well. But this model has the lowest coefficient in the competition metric, and at the same time it is the most demanding model in terms of memory usage. So, I kept using only 50 candidates for the clicks model.

For clicks model I use the lowest number of aids from session history, as the model is aimed at guessing the exact next aid clicked, and aids clicked some time ago are  usually less relevant. So, I take latest aids from session history as candidates, then add aids from the co-visitation matrix for the exact last aid, then add to the list most common aids suggested by the co-visitation matrix for a few last aids in session history. Then I remove duplicates from the list and cut it to get the desired number of candidates. If after removing all the duplicates there are fewer aids in the list than the desired number of candidates, then I add aids from daily top of most popular aids (after checking for each one that it not present in the list already). For 20 click candidates, my best result was 52.68% percent guessed, while for 50 candidates it was 60.43%.

For cart and order candidates I also take latest aids from session history, first latest buys, then all the latest aids, then add most common aids suggested by the co-visitation matrix for the last buys and then add to the list most common aids suggested by the co-visitation matrix for all the last aids in session. All the constants, like maximum buys to take from session history, maximum aids to take from session history, maximum number of aids, suggested from buys e.t.c. vary for carts/orders and depending on number of candidates, but the logic is mostly the same. Then, like when generating candidates for clicks, I remove duplicates from the list and cut it to get the desired number of candidates. If after removing all the duplicates, there are fewer aids in the list than the desired number of candidates, then I add aids from daily top of most carted/ordered aids (after checking for each one that it not in the list already). For 20 cart candidates, my best result was 40.68% percent guessed, while for 75 candidates it was 47.12%. For orders, the best result for 20 candidates was 64.84%, while for 75 candidates it was 68.95%.

We can see that percent of correctly guessed orders is much higher, than percent of guessed carts. The main reason is that all the carted aids have a very high chance to be ordered, and it is possible to make an obvious move and suggest all previously carted aids are going to be ordered, and get a good percent of guessed items. But it is harder to guess carts, as recently viewed aids have a much lower chance to be added to cart.

Note: this code was written for kaggle competition and to produce good competition result I did a few things, that wouldn't make sense in creating a real life recommendation system. When generating candidates, some information from the future is used, as daily top clicks/carts/orders are not known until the day is over. At the beginning of the competition, organizers answered questions and explained that it would be ok to use forward-looking features, and I understood that other participants would use such features. So, I've also used some information from the future for generating candidates and for creating features for the re-ranking models.

## Feature engineering.
The three feature engineering notebooks take time to run and were the longest notebooks in terms of lines of code. I had to move some calculations to ""Calculations for clicks"" and ""Calculations for buys"" notebooks, and also moved definitions of functions, common to several feature engineering notebooks, to a dedicated notebook ""OTTO common feature engineering"". To further speed up the notebooks, I had to rewrite some code using polars library instead of pandas. All of this made the notebooks manageable in terms of run time and complexity.

Notebooks that calculate the w2vec features for carts and orders take even more time to run, than the corresponding notebooks that calculate all the other features. So, I decided to split each of those notebooks into two notebooks, each processing its chunk of test data. Already after the competition I tried several improvements that increased feature calculation speed, but even with that improvement it takes about 3 hours to calculate the w2vec features for each chunk of the test data.

As many features are common between the notebooks, I will now provide features used at least in one of the models in a single list.
* Session history features (value is equal to some constant if candidate aid is not present in the session):
  * **n** - 0 for the last viewed aid, 1 for aid last viewed before, e.t.c, 125 for aids never viewed;
  * **time_delta** - time in seconds from a moment when aid was last viewed to the last action in session;
  * **type_last** - 0 if no buys for the aid in the session, 1 if the last buy is a cart, 2 if the last buy is an order;
  * **count_views** - number of interactions with aid in the session;
  * **time_viewed** - time from user's click on candidate aid until next event, clipped to 180 seconds and then summed for all interactions with the aid.
* Other session features (features, that only depend on session):
  * **ts_diff** - time in seconds between last event and event before last;
  * **session_time** - time in seconds from first to last event in the session (used in carts and orders models only);
  * **events_last_3hours** - total number of events last 3 hours of session (used in carts and orders models only);
  * **buys_this_session** - total number of cart/order events in session (used in carts and orders models only);
  * **history_mean** - w2vec mean similarity between last aid and previous 4 aid before that (used in carts and orders models only);
  * **buys_in_session** - 0 if no buys, 1 if only carts, 2 if at least 1 order is present in session (used in orders model only).
* Global per aid average counts:
  * **daily_aid_count** - normalized count of events with aid for the previous day;
  * **same_day_aid_count** - normalized count of events with aid for the day;
  * **aid_count_weekly** - normalized count of events/carts/orders with aid for the week;
  * **aid_counts** - total interactions with candidate aid in full sessions (used in clicks model only);
  * **aid_counts_buys** - total buys for candidate aid in full sessions (used in orders model only);
  * **aid_counts_orders**, **aid_counts_carts** - total orders/carts for candidate aid in full sessions;
  * **conv** - simple conversion rate, number of sessions with aid bought divided by total number of sessions with any event with aid (used in carts model only);
  * **total_2order_conv**, **total_2cart_conv** - feature depending on type_last feature. If aid was has no buys, here is conversion rate from views to orders, else - conversion rate from carts to orders or from an order to a second order. Similar feature was constructed for carts, with click2cart, cart2cart and order2cart conversion rates (used in carts and orders models only);
  * **clicks_before_buy** - how many times on average, aid is clicked before first buy (used in carts and orders models only);
  * **time_viewed_clipped** - for how long on average aid is viewed before first buy, before averaging values clipped to 180. This feature has low importance and I thought about removing it, but experiment showed results go a bit down in that case.
* Features built using co-visitation matrixes and w2vec for the clicks model:
  * **wgt_matrix** - sum of co-visitation matrix weights for the last 5 aids, using ""regular"" co-visitation click2click matrix (same co-visitation matrix that was used for candidate generation);
  * **wgt_exp** - sum of co-visitation matrix weights for the last 10 aids normalized by n (divided weight by 1 for the last aid, by 2 for aid before it, then by 3 and so on), using ""experimental"" co-visitation click2click matrix;
  * **wgt_last** - exact next click-to-click co-visitation matrix values for the last aid in the session;
  * **wgt_before_last** - exact next click-to-click co-visitation matrix values for the last aid in the session;
  * **similarity_first** - w2vec similarity between candidate and last aid;
  * **similarity_second** - w2vec similarity between candidate and aid before last.
* Features built using co-visitation matrixes and w2vec for carts and orders models:
  * **wgt_buy2buy** - co-visitation buy2order/buy2cart matrix feature;
  * **wgt_c2buy_short** - co-visitation click2buy matrix feature (matrix counts only cases when there is 1 hour or less between click and buy event);
  * **wgt_c2buy_full** - co-visitation click2buy matrix feature for 30 last aids (if they are within 3 hours from the last event);
  * **wgt_c2buy_6_from_full** - sum of co-visitaion matrix weights for the last 5 aids, using ""regular"" co-visitation click2click matrix;
  * **w2v_20_mean** - average w2vec similarity between candidate and last 20 aids (3 hours from last event);
  * **w2v_20_min** - minimal w2vec similarity between candidate and last 20 aids (3 hours from last event);
  * **w2v_5_max** - maximal w2vec similarity between candidate and last 5 aids;
  * **w2v_5_min** - minimal w2vec similarity between candidate and last 5 aids (this feature also has low importance, but its removal decreased result a bit).

Average importance per 4 folds for all the features in LGBM models (importance_type='gain').
| Feature                | Clicks  | Carts   | Orders  |
|------------------------|---------|---------|---------|
| wgt_matrix             | 2.6e+05 | X       | X       |
| wgt_exp                | 2.4e+06 | X       | X       |
| n                      | 8.0e+06 | 5.2e+06 | 1.7e+06 |
| time_delta             | 8.8e+04 | 1.8e+05 | 3.5e+06 |
| count_views            | 5.6e+04 | 1.0e+04 | 1.3e+04 |
| ts_diff                | 1.2e+05 | 3.3e+04 | 2.0e+04 |
| time_viewed            | 1.3e+05 | 1.1e+05 | 2.7e+04 |
| daily_aid_count        | 4.1e+04 | 1.5e+04 | 9.8e+03 |
| same_day_aid_count     | 2.3e+05 | 6.5e+04 | 2.1e+04 |
| aid_count_weekly       | 2.5e+04 | 2.0e+04 | 1.6e+04 |
| wgt_last               | 2.3e+06 | X       | X       |
| wgt_before_last        | 9.7e+04 | X       | X       |
| time_viewed_clipped    | 2.9e+04 | 1.5e+04 | 1.2e+04 |
| aid_counts             | 2.2e+05 | 3.3e+04 | 1.9e+04 |
| type_last              | 4.7e+04 | 1.0e+04 | 5.4e+04 |
| similarity_first       | 1.6e+05 | X       | X       |
| similarity_second      | 7.3e+04 | X       | X       |
| wgt_buy2buy            | X       | 2.6e+04 | 2.7e+04 |
| wgt_c2buy_short        | X       | 1.2e+05 | 3.4e+04 |
| wgt_c2buy_full         | X       | 5.7e+05 | 1.7e+05 |
| wgt_c2buy_6_from_full  | X       | 5.2e+04 | 3.6e+04 |
| conv                   | X       | 1.6e+04 | X       |
| clicks_before_buy      | X       | 5.8e+04 | 2.6e+04 |
| session_time           | X       | 2.8e+04 | 2.2e+04 |
| events_last_3hours     | X       | 1.1e+05 | 6.1e+04 |
| history_mean           | X       | 3.9e+04 | 2.5e+04 |
| total_2cart/order_conv | X       | 2.0e+04 | 2.4e+05 |
| buys_this_session      | X       | 1.6e+04 | 1.2e+05 |
| w2v_20_mean            | X       | 3.0e+04 | 6.9e+04 |
| w2v_20_min             | X       | 1.8e+04 | 2.7e+04 |
| w2v_5_min              | X       | 1.2e+04 | 8.6e+03 |
| w2v_5_max              | X       | 4.6e+05 | 1.8e+05 |
| aid_counts_buys        | X       | X       | 1.8e+04 |
| buys_in_session        | X       | X       | 6.6e+04 |

## Training the GBDT models and predicting
Notebooks training the GBDT models are the only ones that use GPU. I tried both catboost and LGBM models, and LGBM showed better results. To produce the final prediction for clicks and carts only LGBM predictions are used, while for orders I've built two cross validation datasets and used one of them to train LGBM model, and another one - to train catboost model. Then I combined predictions made by two models, and found out that it makes a slightly better prediction than a single prediction made by LGBM model. I kept that pipeline with two models and two cross-validation datasets for orders, but decided against implementing a similar pipeline for carts.

The same notebooks were used both for cross-validation and for training. I only ran Jupyter cells manually one-by-one for cross-validation, and to get the models trained on all the data I ran the entire notebook using ""save and run"" option. A boolean variable CROSS_VALIDATE was used to prevent notebook from running some cells, only needed for cross-validation.

I only used sessions with at least one positive candidate generated. As a result, the number of sessions used to train carts or orders models was relatively low, as for these models more than 80% of sessions do not have any positive candidates, and even fewer sessions have at least a single candidate guessed at candidate generation stage. So, for these two models, I didn't have significant memory problems. I did decrease the number of negative examples for those models, but that was done to increase their performance.

The situation with clicks model was different. Almost all the sessions have positive targets, and about 60% of them have correct candidates selected at candidate generation stage, so the clicks model has times more data compared to the carts and orders models. I kept removing more negative candidates even after this started decreasing the model's performance. I converted all the float variables to float16 after loading the parquet file. I wrote a custom function to split the cross-validation dataset into folds. But anyway, low memory available for kaggle notebooks with GPU was a limitation for the model.

Close to competition's final days, I have increased the number of candidates for carts and orders models from 50 to 75. This turned out to be a disappointment, more candidates didn't improve the cross-validation score, while it took more time for the notebooks to run. After increasing number of candidates, I had to move prediction for carts and orders models to a separate notebook, as 13 Gb available for a notebook with GPU support was not enough. But I didn't remove additional candidates from the pipeline, as I thought there was a chance some of those additional candidates could be selected successfully after adding more features to the model.

At the end of the competition, the best cross-validation recalls were 54.54% for clicks, 42.08% for carts and 65.80% for orders. You can see that even with the re-ranking GBDT models, the results were not that much better than results of rules-based candidate generation, adding just about 1-2%.

## Final formatting and submitting the results
Little can be said about this notebook. I wrote the code in a way that it was possible to upload results for a single model or results for all the three models, to track improvements for each model on leaderboard. In case of uploading results for a single model, slots for other model's results are filled with data from sample submission file, provided by competition organizers.

## Summary and what can be improved
I started working on the project about a month after the competition was launched and still managed to get into top 3% participants, in the middle of the silver zone. That should be considered to be a fairly good result. After the competition, I've read all the posts of top teams members and understood that all of them used servers with way more RAM and GPU available. I worked solo and lost the competition to people who mostly worked in teams and had way more computational resource. If all the time I spent trying to fit all the data into available memory could be spent on running additional experiments, I would have been able to produce a better result.

After the competition end with all the stories of top teams being published, it is easy to say what can be improved. Top teams used more features and more types of co-visitation matrixes, including really original ones, like a matrix that only counts the first few aids in a session, a matrix that counts only sessions of the last week, matrixes that separately count events before or after 2 PM. No doubt that even with the resource available, I could have done better. There was enough memory to double the number of features for carts and orders models. I could have run more notebooks in parallel to speed up the calculation and could have tried even more features, more types of co-visitation matrixes, more versions of w2vec models. Furthermore, I could have improved my carts and orders models by using some features I've only used for clicks model. For example, I haven't even tried features built for exact last aid in session for carts and orders models, and I haven't tried features built with any of click2click models for carts/orders. Then, there were several bugs in code that also a bit decreased the result. But while all these little improvements would bring me to a higher position, they wouldn't put me on the very top. To win this competition, one needed to have more computational resource available.

## March 2023 upload to github.
In March 2023, about a month after the competition, I decided to review the code, add some comments, delete commented unused code and upload the notebooks to github. At this point further score improvement was not my goal anymore. But while adding comments and reviewing the code I couldn't help making some changes. I fixed a few bugs, moved addidional fucntions to otto_common notebook, created a separate notebook with functions common to feature engineering, checked for ways to speed up the word2vec feature notebook, removed a few features that actually decreased model's performance, e.t.c. As a result not just the code became shorter and clearer, but also the result have improved. After implementing those changes, best cross-validation results improved to 54.71% for clicks, 42.21% for carts and 65.87% for orders, these simple improvements would be enough to move me from 59th to 53rd position.
",1,1,2,0,online-retail,"[catboostranker, jupyter-notebook, kaggle-competition, lgbmranker, machine-learning, online-retail, pandas, polars, recommender-system]",44-45
SandraMoses,RFM-Analysis-for-Customer-Segmentation,,https://github.com/SandraMoses/RFM-Analysis-for-Customer-Segmentation,https://api.github.com/repos/RFM-Analysis-for-Customer-Segmentation/SandraMoses,Analytical SQL Project where we had to analyze the dataset and come up with 5+ insights and divide the customers into segments using analytical functions.,"# RFM-Analysis-for-Customer-Segmentation
Analytical SQL Project where we had to analyze the dataset and come up with 5+ insights and divide the customers into segments using analytical functions.

## Question 1: Insights

Story 1 --> Grand Total By Year

Story 2 --> Most Selling Product

Story 3 --> Top 5 paying Customers

Story 4 --> Group Data By Year and Quarter

Story 5 --> Rush Hour

Story 6 --> Running Totals in 2010

Story 7 --> Time Series Analysis


## Question 2:

#sol 1: The Lazy way	

#sol 2: Greater and Smaller than


### Files contain:
1- The Database Schema

2- .sql file having all the queries

3- .pdf containing screenshots of the output
",0,0,1,0,online-retail,"[analyticalsql, online-retail, segmentation, sql]",44-45
HermawanHermawan,optimize-retail-revenue,,https://github.com/HermawanHermawan/optimize-retail-revenue,https://api.github.com/repos/optimize-retail-revenue/HermawanHermawan,Analyze product data for an online sports retail company to optimize revenue by using SQL,"PROJECT DESCRIPTION


Sports clothing is a booming sector! This project will make use of SQLto analyze product data for an online sports retail company. In this case, it will deal with numeric, string, and timestamp data on pricing and revenue, ratings, reviews, descriptions, and website traffic. For the analysis, the project will use techniques such as aggregation, cleaning, labeling, Common Table Expressions, and correlation to produce recommendations on how the company can maximize revenue!


PROJECT DETAIL


1. Counting missing values
2. Nike vs Adidas pricing
3. Labeling price ranges
4. Average discount by brand
5. Correlation between revenue and reviews
6. Ratings and reviews by product description length
7. Reviews by month and brand
8. Footwear product performance
9. Clothing product performance",0,0,1,0,online-retail,"[online-retail, optimization, revenue, sql]",44-45
aditiisaxena,FastFruits,,https://github.com/aditiisaxena/FastFruits,https://api.github.com/repos/FastFruits/aditiisaxena,Online Retail Store DBMS made using MySQL and Flask,"# FastFruits Online Retail Store
FastFruits is an online retail store that sells fruits and vegetables. We have tried to design the database and business logic layer for a Quick Commerce retail store specializing in fruits and vegetables. Fruits and vegetables are the most difficult Stock Keeping Unit category for most grocery companies owing to a standard wastage rate of 30%. Our system aims to minimize this wastage through algorithmic end-to-end lifecycle management of fruits and vegetables. <br>
This project is a database management system (DBMS) that has been built using MySQL and Flask to manage the data for FastFruits.

![Alt text](Screenshot%202023-05-07%20150838.png ""a title"")
## Features
1. Customers can register and login to purchase fruits and vegetables.
2. Customers can add items to their cart and place an order.
Customers can view their order history.
3. Admins can manage the inventory, update stock and prices of fruits and vegetables.
4. Admins can view order history of all customers and mark orders as fulfilled.
5. Admins can add, update, and delete products from the inventory.
## Dependencies
Python 3.6 or higher<br>
Flask 2.0 or higher<br>
MySQL 8.0 or higher<br>
Flask-MySQLdb 0.2.0 or higher
",0,0,1,0,online-retail,"[css, dbms, flask, html, javascript, mysql, online-retail, python3, queries]",44-45
Otniel113,OnlineSalesBusiness,,https://github.com/Otniel113/OnlineSalesBusiness,https://api.github.com/repos/OnlineSalesBusiness/Otniel113,"Analisis 3 Point (Keuntungan per Item, Total Diskon, Total Retur) dari Bisnis Penjualan Online","# OnlineSalesBusiness
Analisis 3 Point (Keuntungan per Item, Total Diskon, Total Retur) dari Bisnis Penjualan Online yang dikerjakan secara individu.

## Dataset
Dataset didapatkan dari RevoU berupa Google Sheet (Excel) dengan ukuran (1775x6) dan memiliki penjelasan setiap kolom

<br><b>Produt Type</b>     = Tipe Produk (Ada 18 jenis)</br>
<br><b>Net Quantity</b>    = Banyaknya item barang terjual</br>
<br><b>Gross Sales</b>     = Keuntungan Kotor</br>
<br><b>Discounts</b>       = Harga Diskon</br>
<br><b>Returns</b>         = Biaya Retur</br>
<br><b>Total Net Sales</b> = Keuntungan Bersih</br>

## Preprocessing
1. Drop Missing Value
2. Mengganti bilangan real menjadi 2 digit di belakang koma
3. Validasi nilai Total Net Sales

## Data Aggregation (Grouping)
Data dikelompokkan berdasarkan Product Type (Tipe Produk) nya. Dicari juga banyaknya total keuntungan dan juga banyak item terjual, dengan visualisasi berikut:
![Banyak Item](https://drive.google.com/uc?id=119JIamILH8nDeo62svhkBwTdNy9JvQb9)
![Keuntungan Penjualan](https://drive.google.com/uc?id=1F3IRh3T2Jlbu9ZMs1M1H9UllQPjN0zEX)

Basket, Art & Sculpture, Jewelry merupakan 3 produk yang paling berpengaruh karena ketiganya memiliki banyak item penjualan terbanyak dan keuntungan penjualan terbanyak

## Analisis 3 Point
1. Keuntungan per Item
![KeuntunganPerItem](https://drive.google.com/uc?id=1rERNs-NcWMw1Qf1u8Vk0tqdd3WOzaXC-)
2. Total Diskon
![TotalDiskon](https://drive.google.com/uc?id=1r4AaEj3wrGDuB2e7lNGPWPOAPTtEvts7)
3. Total Retur
![TotalRetur](https://drive.google.com/uc?id=1ew6jZfnQTU-pB7-yDbKC8Qktb38PRG4v)

## Lebih Lanjut
Untuk analisis lebih lanjut dan lengkap dapat dilihat di file Online Sales Business.pptx
",0,0,2,0,online-retail,"[data-analysis, data-visualization, online-retail]",44-45
abeltavares,online_retail_pyspark_analysis,,https://github.com/abeltavares/online_retail_pyspark_analysis,https://api.github.com/repos/online_retail_pyspark_analysis/abeltavares,PySpark data analysis of the Online Retail Data Set,"[![PySpark](https://img.shields.io/badge/PySpark-3.3.2-orange.svg)](https://spark.apache.org/docs/latest/api/python/index.html)

# Online Retail Data Analysis

This repository contains an analysis of the Online Retail dataset, which includes transactional data from a UK-based online retailer. The analysis is performed using PySpark in Jupyter Notebooks.

## Dataset

The dataset used in this analysis can be found in the `data` folder. The dataset contains information about customer purchases, including product descriptions, quantities, and prices.

## Notebooks

The analysis is divided into several Jupyter Notebooks, each focusing on a specific aspect of the data:

- `Exploratory_Data_Analysis.ipynb`: Exploratory data analysis to understand the structure and distribution of the data.
- `RFM_Analysis.ipynb`: RFM analysis to segment customers based on their purchasing behavior.
- `KMeans_Clustering.ipynb`: K-means clustering to segment customers based on their order history.
- `Product_Affinity_Analysis.ipynb`: Product affinity analysis to identify which products tend to be purchased together.
- `Market_Basket_Analysis.ipynb`: Market basket analysis to analyze which products tend to be purchased together at different times of day, week, or year.
- `Churn_Analysis.ipynb`: Churn analysis to identify customers who are likely to churn based on their past behavior.

## Requirements

The analysis requires PySpark and Jupyter Notebook. The necessary Python libraries can be installed using the `requirements.txt` file.

## Usage

To run the analysis, clone the repository and open the Jupyter Notebooks in order.

## Contributions

This project is open to contributions. If you have any suggestions or improvements, please feel free to create a pull request.

## Copyright
© 2023 Abel Tavares
",0,0,2,0,online-retail,"[business-intelligence, churn-analysis, customer-segmentation, data-analysis, data-visualization, jupyter-notebook, machine-learning, market-basket-analysis, online-retail, product-affinity-analysis, pyspark]",44-45
Sjoerdbijlsb,Thesis-project-price-premiums,,https://github.com/Sjoerdbijlsb/Thesis-project-price-premiums,https://api.github.com/repos/Thesis-project-price-premiums/Sjoerdbijlsb,This project focuses on exploring product recommendations within the realm of luxury fashion resale. It serves as a companion to my thesis of the MSc Marketing Analytics program at TiU.,"# Online Boutique stores: what factors drive product recommendations in limited-edition resale markets?

This repository accompanies my thesis which investigates product recommendations in the thriving fashion resale market. The study uses product recommendations and recent 
product purchases from [GOAT](https://www.goat.com/), one of the biggest resale platforms in this industry. Because product recommendations are often said to present the 
most popular and revenue-generating items due to popularity bias, this is worth to be examined in this industry where rare and exclusive items play an important role for consumers. The main constructs used in the study are product revenue an item attains through sales, and recommendation count (the number of times a product is recommended on the platform). 



# Repository overview
- The setup of the files in this project is as follows:
```
├── img
├── data
└──  gen
   ├── output
└── src
   ├── analysis
   ├── data-collection
   ├── data-preparation
├── paper
├── .gitignore
├── README.md
├── makefile

```

# Data 

![Product recommendations on GOAT](img/facts_example_2.png)


## Dataset desription [(direct link to data)](https://drive.google.com/drive/u/0/folders/1HfVG22n4h4il92tmDk6Abf5qtocoBxyZ)

```
(1) recinfo_2023-05-02.csv
-Product recommendations(rec_id) per size and product condition (new, used, etc.)
-Relevant product info on brand, release date, color etc.
-The variable display_order refers to the ranking in one of the recommendation lists (each have up to 8 per product).
-For variables that have a display_order of 0, the product info refers to the product that was sampled and where the recommendations are shown(id).

(2) recent_purchases_2023-05-02.csv
-Recent purchases with timestamp for all recommendations in file 1.
-Location of sale included.

(3) productlist20230501.csv
-The top 10,000 products from GOAT's search [list](https://www.goat.com/search) for apparel and sneaker categories, with some product information such as retail price, last sold price.
-Product info here is not based on size per product.

(4) productlist20230425.csv
-Similar to file 3, with a wider selection outside the top 20,000 (100,000 + products)

(5) output_list_counts_all20230508.csv
- A short summary list of brands and the number of times they occur on the platform in total

* Note: file 3 and 4 are first used to draw a sample from in this analysis. File 1 and 2 are used directly in preparing the data and the analysis.
```


# Run study with the same data
If you want to replicate the study I did, it is suggested you run the same R files. This can be done easily if you have installed [make](https://gnuwin32.sourceforge.net/packages/make.htm). To set up make, this is a helpful [guide](https://tilburgsciencehub.com/building-blocks/configure-your-computer/automation-and-workflows/make/)

## Check dependencies first

First install the following packages for R:
```
install.packages(""tidyverse"")
install.packages(""googledrive"")
install.packages(""anytime"")
install.packages(""stringr"")
install.packages(""lubridate"")
install.packages(""xtable"")
install.packages(""vtable"")
install.packages(""car"")
install.packages(""stargazer"")
install.packages(""broom"")
```
To install and set up R this is a helpful [guide](https://tilburgsciencehub.com/building-blocks/configure-your-computer/statistics-and-computation/r/)

**If make and R are set up properly, you should be able to run the project in the following manner:**

- Clone the forked repository onto your local machine using the following command:
```
git clone https://github.com/<your-username>/Thesis-project-price-premiums.git
```
- In your local repository navigate to the base level of this repository
```
cd <yourpath/Thesis-project-price-premiums>
```
- Run make
```
make
```

# Gather new data
In case you want to gather the data from scratch, you can make use of the scrapers in the repository based on Python. The scrapers contact the API endpoints and either make direct requests or use a webdriver to obtain primarily product recommendations and  recent purchase data from GOAT. Unfortunately, not everything can be run in one scraper. The biggest reason is that during the project, more endpoints were needed to obtain more data. 

- Descriptions of each file:
```
(1) Goat_assortment_api.py
-Grabs the number of products per brand (quick summary only) (output corresponds to file 5 in dataset description)

(2) Goat_search_group_counts.py
-Obtains the product pages where the recommendations are found, or to collect items from top 10,000 products (output corresponds to file 3 & 4 in dataset description)

(3) Goat_sample.py
- (optional) is used to draw a sample from the collected product pages.

(4) Goat_recommendation_scraper.py
- Main scraper for recommendations (output corresponds to file 1 in dataset description)

(5) Goat_recent_purchases.py
- Collects recent purchases of recommended items (output corresponds to file 2 in dataset description)

```
- To arrive at the datasets as in the datafile, the scrapers should be run in the above order. When given the right input for a scraper (usually product ID) it is of course also possible to run them and use the outputs independently. 

*Note: For scraper 4 you need a key matching your browser sesssion. Send me a msg

# Contact
sjoerdbijl.sb@gmail.com
",0,0,1,0,online-retail,"[analytics, marketing, online-retail, product-recommendations, recommender-system, web-scraping]",44-45
ravicosoftltd,FreePOS,ravicosoftltd,https://github.com/ravicosoftltd/FreePOS,https://api.github.com/repos/FreePOS/ravicosoftltd,"Window based open source Point of sales built with c#, wpf and mysql","# FreePOS - Point Of Sales
Point of Sales software for retailers, whole salers, products seller, service sellers, groccers, restaurents, barber shops and any kind of business 
Window based open source Point of Sales software, built with c#, wpf and mysql.


## Installation

Install mysql database, then download the [freepos installer](https://github.com/ravicosoftltd/FreePOS/releases/latest/download/FreePOS.Setup.msi) and run

### Features

* All standard features for Point of sales
* Inventory
* Accounting
* Customers and Vendors Ledger
* Multiple users
* Invoice printing
* Reporting
* Send Branded SMS from software
* No Data limits
* Easy to learn and use
* C# WPF and MySql based latest stack
* 100% free and open source
* And much more

### Some Sreenshots

![login](screenshots/1-login.png)
![dashboard](screenshots/2-dashboard.png)
![sale window](screenshots/3-sale.png)
![ledered sales](screenshots/3-salelegered.png)
![ledered sales](screenshots/3-salelist.png)
![product add](screenshots/4-productadd.png)
![product list](screenshots/4-productlist.png)
![product report](screenshots/4-productreport.png)
![user add](screenshots/5-personadd.png)
![user list](screenshots/5-personlist.png)
![transactions](screenshots/6-transactions.png)
![user list](screenshots/7-setting.png)
![user list](screenshots/7-settingdatabase.png)

### Requirements
* .Net framework 4.8
* Mysql >= 5.6

## Contributing / Reporting issues
We are looking for contributors to take part

Create pull request or raise issues and start contributing.

## License

[Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0.html)

More information is available at [freepos.pk](https://freepos.pk).
",17,17,3,4,retailer,"[desktop-application, mysql, point-of-sale, pointofsales, pos, retailer, shops, wholesaler, wpf]",44-45
Mascerade,upm,,https://github.com/Mascerade/upm,https://api.github.com/repos/upm/Mascerade,Unsupervised Product Matching; A product classifier algorithm that groups products into clusters of similarity based on their title using combinations.,"# Unsupervised Product Matching Using Combinations and Permutations

A product matching algorithm that groups products into clusters of similarity based on their title using combinations and permutations.

Based on the paper ""Unsupervised Product Matching Using Combinations and Permutations"" by Leonidas Akritidis, Athanasios Fevgas, Panayiotis Bozanis and Christos Makris
https://arxiv.org/pdf/1903.04276.pdf

# Structure of Project
`bin` - Contains the executables <br>
`build` - Contains the object files (`.o`) <br>
`include` - Contains the header files <br>
`src` - Contains the C++ code <br>
`build.bat` - Builds the project and creates the `main.exe` file in `bin` **(for Windows)** <br>
`build.sh` - Builds the project and creates the `main.exe` file in `bin` **(for Ubuntu)** <br>
\* Change the directory to the build directory `build.bat` or `build.sh` for your system


# How to Use

Simply create a Product object that takes the vendor (the retailer to which the product is associated). The vendor parameter is a **string**. The next parameter is the title of the product, which is also a **string**. See `main.cpp` to see the test code.

# How it Works

For each product, all of the combinations of the tokens (words) of the title are created up to n choose 6. Each token has its own properties, like its frequency and semantic value (item model, attribute or normal token) and based on this, they get different weights. The combinations also have their own properties, such as their Euclidean distance average and the frequency of the combination among the products. From this, using the equations outlined in the paper, each product is assigned a ""dominating cluster"", that is the combination that best represents what the product is. The products with the same cluster represent the same product.

# Uses

This algorithm aims to solve the issue of how to deal with discrepincies among products across different retailers. Using this algorithm, it should be able identify the correct matching product among a wide range of products.

# Reality

In contrast to the aim of this algorithm, it doesn't actually do its job. For example, running the algorithm on the set of data provided by the authors, it is very inaccurate, and it is not my implementation. I acheived similar, if not identical results using the author's algorithm, which can be found at https://github.com/lakritidis/UPM-full
",10,10,2,0,retailer,"[algorithm, matching-algorithm, paper, product, product-matching, retailer]",44-45
SignatureBeef,AusStockChecker,,https://github.com/SignatureBeef/AusStockChecker,https://api.github.com/repos/AusStockChecker/SignatureBeef,"A tool to monitor stock availability and notify you of items you wish to purchase from a collection of Australian retailers, such as a NVIDIA GPU or AMD GPU/CPU, or Xbox or PS5.","# Aus Stock Checker
A tool to monitor stock availability and notify you of items you wish to purchase from a collection of Australian retailers, such as NVIDIA and AMD GPUs/CPUs which are hard to find and when you do you only have minutes to act - every second up your sleeve counts.

Given a list of product urls from these retailers the tool will scan the urls every 30 seconds and it will determine if the item is in stock. If the status has changed to something other than out of stock (in stock, pre order etc) the application will sound a beep and send you an email to purchase your item.

This project was for personal use intended for me to snag one of the new NVIDIA GPU's asap (before miners and other bots!), however it can also be used for nearly all of the items on the supported retailer websites, such as a Xbox Series X, Sony PS5 or even just a keyboard. 

Given that this could be abused and that it is not intended for anything else i've decided i'm not providing a compiled binary, and any use of this is at your own risk and i accept no responsibility at all whatsoever. If the tool works for you, great, if not then feel free to PR with a fix or create a issue to log the problem - i may or may not have the time to fix it for you though.

![Demo image](https://github.com/DeathCradle/AusStockChecker/blob/main/demo.png?raw=true)

## Requirements
 - Visual Studio / IDE / compiler with .NET 5 support
 - An email address to send with
 - An email address to receive notifications
 - Windows or OSX with .NET 5 (Linux not yet tested)

## How to use
 - Open the project in Visual Studio and compile.
 - Modify UserDetails.yaml with your details as required (and recompile each time you make a change, or update bin/Debug/net5.0/UserDetails.yaml directly)
 - Add in a test item that is in stock to confirm the notifications work
 - Use as required

 ## Supported Retailers
 - [Umart](https://www.umart.com.au)
 - [Mwave](https://www.mwave.com.au)
 - [PC Case Gear](https://www.pccasegear.com)
 - [Computer Alliance](https://www.computeralliance.com.au)
 - [PLE](https://www.ple.com.au)
 - [Scorptec](https://www.scorptec.com.au)
 - [CPL](https://www.cplonline.com.au)
 - [MSY](https://www.msy.com.au)
 - [Big W](https://www.bigw.com.au)
 
## Recommendations
 - Sending via gmail
   - You might need to allow legacy applications or create an app password using the following [link](https://support.google.com/accounts/answer/185833?hl=en).
 - Receiving via gmail
   - You can set the sending email address as priority so you get push notifications to your phones home screen.
 - If you are setting this up for the first time, I suggest you use a random in-stock item from the supported retailer of your choice to check to make sure your notifications are working and the PC Beep sound can be heard.

### Remember: no warranty, responsibility on my behalf or support is provided - use at your own risk!

----

### If you like this project feel free to star, fork, share on [reddit](https://reddit.com) etc so i know it's in use.

Bitcoin donations are welcomed via address [3PRfyMh1brjCqzkw9az2aT7yNjbfkwFZqo](bitcoin:3PRfyMh1brjCqzkw9az2aT7yNjbfkwFZqo)

![QR](btc_donations.png)
",7,7,3,0,retailer,"[amd, australian-retailers, bigw, computer-alliance, cplonline, monitor-stock-availability, msy, mwave, nvidia, pccg, playstation, ps5, retailer, scorptec, stock-alert, stock-notification, umart, user-bot, xbox]",44-45
alexandre-k,square-booking-app,,https://github.com/alexandre-k/square-booking-app,https://api.github.com/repos/square-booking-app/alexandre-k,App for Square Unboxed Hackathon,"# square-booking-app
App for Square Unboxed Hackathon

[![Docker Image CI](https://github.com/alexandre-k/square-booking-app/actions/workflows/docker-image.yml/badge.svg?branch=master&event=push)](https://github.com/alexandre-k/square-booking-app/actions/workflows/docker-image.yml)

## Prepare
### Square API
Go to https://developer.squareup.com/apps/
Get your `Sandbox Application ID` and `Sandbox Access Token`.
Set your environment variables as exportable variables in a file called `.env`.

### OAuth
Under development but at the moment Auth0 is being used. Creating an account is needed, but a custom authentication server will likely be used.

### Database
MongoDB is used as a database. A password should be randomly created. We suggest reusing passwords generated in Bitwarden, `pwgen` or any other password generator.

## First run

```
docker-compose build
docker-compose up
```

## Next time run

```
docker-compose up
```

## Run after installing a new package in package.json

```
 docker-compose down
 docker-compose up --build
```

## Access the React app

Go to your browser and enter `http://localhost:5000` in the URL. Any other port can be used instead if 5000 is already being used. See the default in `docker-compose.yml`.

## Clone repository 

After cloning the main repository we need to clone the backend configured as a submodule:
```
git clone git@github.com:alexandre-k/square-booking-app.git
cd square-booking-app
git submodule update --init
```

## Deployment

Example:

```
docker build -t booking-server:[TAG] -f Dockerfile.serve .
docker build -t booking-api:[TAG] -f Dockerfile .
docker run -d -p 8000:8000 booking-server:[TAG]
docker run -d -p 80:80 -p 443:443 booking-api:[TAG]
```
",6,6,2,1,retailer,"[booking, box, react, retailer, square, typescript]",44-45
SaiJeevanPuchakayala,Cashier,,https://github.com/SaiJeevanPuchakayala/Cashier,https://api.github.com/repos/Cashier/SaiJeevanPuchakayala,"Cashier is a Payment book application designed to maintain customers, payments, and their purchases. A retailer will be an admin of the application and each customer of the retailer’s shop will be the user. ","# Cashier

#### Cashier is a Payment book application designed to maintain customers, payments, and their purchases. A retailer will be an admin of the application and each customer of the retailer’s shop will be the user. Customers can create their account in Cashier and they will be able to see their purchase history, pending payments, and also if the user is having any doubt or complaint they can contact the retailer by using the contact our service. Admin will maintain data about purchases made by the customers & can see payment details and pending payment of the customers and will send alerts to the customers if there is any due for payment or payment is pending for a long time.


#### This Web App is live at : [E Cashier.](https://e-cashier.herokuapp.com/)

## Tech Stack

**Frontend:** [semantic-ui](https://semantic-ui.com/)

**Backend:** [Flask](https://flask.palletsprojects.com/en/2.0.x/)

**Database:** [MySQL](https://remotemysql.com/)

## Have a look at Admin Panel By SigningIn Using Credentials Below
**Email:** xebip78501@hyprhost.com

**Password:** cashieradmin

## Environment Variables
To run this project, you will need to add the following variables to your app.py

```bash
db_name = ""Your DB_NAME""
db_password = ""Your DB_PASSWORD""
email = ""Your MAIL_ID""
password = ""Your MAIL_ID_PASSWORD""
```


### Run Locally
Clone the project

```bash
>_ git clone https://github.com/SaiJeevanPuchakayala/Cashier.git
```

Install dependencies

```bash
 >_ pip install -r requirements.txt 
```

Start the server
```bash
>_ python app.py 
```
",3,3,1,0,retailer,"[ecommerce, flask-application, mysql, mysql-database, retail-stores-visualisation, retailer, semantic-ui, sematic-html, stripe]",44-45
NiranjanStack,Customer-Behavior-Analysis-in-R-Shiny,,https://github.com/NiranjanStack/Customer-Behavior-Analysis-in-R-Shiny,https://api.github.com/repos/Customer-Behavior-Analysis-in-R-Shiny/NiranjanStack,Analyzing transactions of a retailer to predict promotional items.,"# Customer-Behavior-Analysis-in-R-Shiny
Analyzing transactions of a retailer to predict promotional items.

The dataset has previous five years purchase transactions of customers. Predictive analysis is done by applying machine learning
algorithms to find the most frequent item sets purchased.

Aprioir Algorithms : To generate associatin rules and find the most frequent item sets.
Support Vector Machines : To predict the month of the year in which the sales of a particular item is maximum.

Shiny (Library in R) is used to display the results.
Shiny Dashboard: https://niranjanrshiny.shinyapps.io/Prediction_App/

",3,3,2,0,retailer,"[analysis, analyzing-transactions, data-science, machinelearning-r, predictive-analytics, retailer, shiny, shiny-apps, svm-model]",44-45
MarksSoftwareGmbH,itk-shop,,https://github.com/MarksSoftwareGmbH/itk-shop,https://api.github.com/repos/itk-shop/MarksSoftwareGmbH,ITK Channel - Reseller Shop / eCommerce Plugin for CakePHP3 in PHP7,"# CakePHP Application Skeleton

[![Build Status](https://img.shields.io/travis/cakephp/app/master.svg?style=flat-square)](https://travis-ci.org/cakephp/app)
[![Total Downloads](https://img.shields.io/packagist/dt/cakephp/app.svg?style=flat-square)](https://packagist.org/packages/cakephp/app)

A skeleton for creating applications with [CakePHP](https://cakephp.org) 3.x.

The framework source code can be found here: [cakephp/cakephp](https://github.com/cakephp/cakephp).

## Installation

1. Download [Composer](https://getcomposer.org/doc/00-intro.md) or update `composer self-update`.
2. Run `php composer.phar create-project --prefer-dist cakephp/app [app_name]`.

If Composer is installed globally, run

```bash
composer create-project --prefer-dist cakephp/app
```

In case you want to use a custom app dir name (e.g. `/myapp/`):

```bash
composer create-project --prefer-dist cakephp/app myapp
```

You can now either use your machine's webserver to view the default home page, or start
up the built-in webserver with:

```bash
bin/cake server -p 8765
```

Then visit `http://localhost:8765` to see the welcome page.

## Update

Since this skeleton is a starting point for your application and various files
would have been modified as per your needs, there isn't a way to provide
automated upgrades, so you have to do any updates manually.

## Configuration

Read and edit `config/app.php` and setup the `'Datasources'` and any other
configuration relevant for your application.

## Layout

The app skeleton uses a subset of [Foundation](http://foundation.zurb.com/) (v5) CSS
framework by default. You can, however, replace it with any other library or
custom styles.
",1,1,0,0,retailer,"[cakephp, cakephp-application, cakephp-plugin, cakephp3, checkout, ebusiness, ecommerce, ecommerce-application, ecommerce-platform, ecommerce-website, itk, onlineshop, reseller, reseller-platform, rest, restful-api, retailer, retailerportal, retailerwebsite, shop]",44-45
aleksejhoffaerber,SalesIntelligence,,https://github.com/aleksejhoffaerber/SalesIntelligence,https://api.github.com/repos/SalesIntelligence/aleksejhoffaerber,"Applying product segmentation, demand forecasting, and revenue optimization to increase online retailer revenue","**Live version available here:**  [https://karoronty.shinyapps.io/SalesIntelligence/](https://karoronty.shinyapps.io/SalesIntelligence/)

![ui](https://github.com/aleksejhoffaerber/SalesIntelligence/blob/master/ui_screenshot.png)

## Goal
The goal of “Sales Intelligence” is to apply product classification, forecasting, and revenue optimization for the next month ahead to maximize revenue. The end goal is to build interactive visualizations in a dashboard that appear natural to non-technical business end users.

What makes this project unique in addition to adding optimization is the access to price data: usually, modern data sets from online retailers that are only available for confidential use include more predictors, especially on marketing, sales campaigns, inventory levels etc. In this project, the goal is to fulfill the listed guidelines based on the predictor limitation but still creating accurate product profiles to identify revenue optimization potential.

## Usage
In the dashboard, the user selects a single product where the price needs to be optimized. The user can filter the products by a segment created by the product level RFM analysis if desired. Initially, there are two visible tabs for the user to analyze the entire set of products with. The main tab, RFM analysis, contains a heat map based on the products available for price optimization. The second tab, Segments, contains the monetary values of the different segments the user can filter the products on. After selecting a product and running the optimization, results of the optimization are presented. The top bar contains the optimal price, the forecasted revenue with that price and the revenue increasement. The plots show the effect on sales in three different ways, the effect on revenue over time, the effect on quantity over time and the resulting revenues from different prices. The blue values indicate the sales without optimization, while the white dot indicates the optimized sales with the optimal price. It is important to keep in mind that the forecasted sales with the optimized prices are only indicative and may not reflect the actual future sales in the most accurate way.

## Data
The UCI Machine Learning Repository offers [a 2-year (time series) multivariate dataset](https://archive.ics.uci.edu/ml/datasets/Online+Retail+II) that was already used in some research articles especially for data mining research purposes. The data is from an internationally exporting British online retailer of unique all-occasion giftware. To our knowledge, revenue optimization has not been done on this data set before, at least publicly. The structure of the dataset is as follows:

- **Invoice**: Invoice level, indicating the basket of a customer
- **StockCode**: Unique product identifier, with 4,631 unique products
- **Description**: Product description in text form 
- **Quantity**: Sales quantity, with returns as negative values
- **InvoiceDate**: Datetime in the minute level, from 2009-12-01 10:06 to 2011-12-09 9:57
- **Price**: Unit price in pounds
- **Customer ID**: Unique ID for each customer, with 23% have a missing value
- **Country**: Country of the customer, 43 unique values

## Code Architecture 
In order to facilitate deployment, code readability, and further maintenance the code base is segmented into four different scripts. The data flows and code interdependencies between the scripts can be depicted as follows:

### 1. EDA.ipynb
Containst the exploratory data analysis that acts as the basis for several decisions made in dashboard. 

### 2. modeling.R
modeling.R includes the data loading, cleansing, and modeling of the application. Most of the basic data cleansing refers to product name harmonization, deletion of invalid products and invoice positions. Orders with a negative price or quantity or products with less than 24 months of order history were deleted as well. Additionally, empirical fluctuation process analysis was applied in order to delete products with significant breaks in their data. This is essential for the demand forecasts based on ARIMA.

Then, a product classification is implemented based on the Recency, Frequency, Monetary (RFM) analysis. The resulting product level RFM segments were calculated using sensible values.

Lastly and because of 701 resulting products, model training is carried out using parallelization. The ARIMA models use the monthly historic mean of the price in order to predict quantity one month ahead, with the parameters for the models being chosen automatically using AICc.

The resulting data, segments, and models are saved as R objects for usage in app.R. This script is not run inside the Shiny app to save time but should instead be run periodically to obtain latest models with the latest data. In this application the exchange of data happens via .RDS files, unlike in the optimal scenario where the data would be stored in databases and read from there by the Shiny app.

### 3. functions.R
functions.R is the functional backbone of the application. It includes functions for demand forecasts, revenue optimization, necessary plots, and data translations to allow for an easy understandable user interface. Unlike in a typical R program, all the functions also include the data and variables needed inside the functions for the deployment as required by Shiny.

##### translate_input()
Translates the product name used in the input to a product id.

##### get_forecasts()
Creates product prices, based on the app input, that are allowed to vary by ±30% percent, using £0.01 steps from the original price. Based on the ARIMA demand prediction model, new demand quantities will be predicted for the different, varying prices. Original and predicted prices will be joined into the resulting tibble.

##### get_optimal_forecast()
Identifies the best price point based on the best revenue result by performing a simple sorting on the forecasted revenue and saves it into a tibble.

##### plot_revenue_forecasts()
Takes the results from get_optimal_forecast() and plots the revenue time-series for the given product. Also compares the forecasted revenue without an optimized price (based on the ARIMA quantity forecast and the historical mean price (t+1)), with the optimized revenue (based on the forecast and optimization).

##### plot_quantity_forecasts() 
Similar to plot_revenue_forecasts(), but just for the quantity. It compares forecasted quantity without price optimization with quantity based on the optimization.

##### plot_revenue_price()
Compares the price and revenue between the forecasted and optimized scenario. The plot is computed based on the complete product price variation from the resulting tibble of get_forecasts(). 

##### create_segments()
Computes the RFM segments

### 4. app.R
app.R is the application backbone and includes the necessary libraries, connects to modeling.R and functions.R, imports the aforementioned data and models, includes the product segmentation plots and the UI and server functions for Shiny. 

##### ui()
The shiny front-end is based on a simple dashboard page. This page consists of:
- Two dynamic input fields for product segment (RFM) and product name
- A button to initiate the optimization for a single selected product
- A menu consisting of RFM, Segments, and Results for the optimization results
- For RFM and Segments the respective classification graphs are shown in the corresponding menu tabs
- The graphs for revenue, quantity and revenue-price comparisons using a 2x1 (patchwork) grid that are shown after optimization computation 
- InfoBoxes summarizing relevant information for optimized price, and absolute and relative revenue increase 
- A notification to notify the user of the ongoing optimization process

##### server()
The back end mainly takes care of the dynamic event triggers, plot renders, info-box messages and notifications:
- The initial menu bar only includes RFM and Segments. The final item “Results” only appears after the final optimization has finished computing
- Input that can be filtered in the product name depends on the selected input from the previous RFM segment filter
- InfoBox outputs are calculated based on the data from get_forecasts()
- Demand and optimization related plots are rendered as a single plot using a patchwork grid 
- The RFM and segment plots are static and therefore rendered only when the app is launched, and updated only when the underlying data is updated, and the models are run again

## Tags
r, retailer, optimization, segmentation, forecasting, ARIMA, time series, visualization, interactive, git, shiny, dashboard
",1,1,2,0,retailer,"[forecasting, retailer, revenue-optimization]",44-45
dilijev,guest-list-generator,,https://github.com/dilijev/guest-list-generator,https://api.github.com/repos/guest-list-generator/dilijev,Generate a Guest List from multiple ticket sales and guest list CSV files.,"# Guest List Generator
[![Licensed under the MIT License](https://img.shields.io/badge/License-MIT-blue.svg)](https://github.com/dilijev/guest-list-generator/blob/master/LICENSE.txt)

A merchant might list events on multiple online ticketing retailers to reach the widest audience possible.
Printed tickets could be easily spoofed, so a merchant would want to have a check-in at
at ticketed events where staff compares a ticket or ID with a known guest list
to confirm that a guest has indeed purchased a ticket ahead of time.

For example, an orchestra which has two shows in a single weekend every 2-3 months may list tickets for sale
on Brown Paper Tickets, Gold Star, Groupon, and possibly other retailers.
Additionally, they may offer a season pass for sale towards the beginning of the season,
which would need to be applied to all events.
Finally, a list of reserved or VIP tickets may be maintained separately.

Creating the list of guests is fairly straightforward and mechanical,
but because of the differences in formats, it is difficult and errors or omissions may result.
Ticket sales might stay open as late as the night before an event,
and events may be early in the morning.
Different retailers will all have a different format for tables of tickets purchased.
Additionally, the lists from each retailer may not be in a compact or easy-to-consult format,
may contain duplicate entries instead of a quantity of tickets, etc.

This program solves all these problems by simply taking in all of the source lists and producing a single,
compact table that can be used to check in guests at the door.

# Using This Software

* Get [Node.js](https://nodejs.org/).
* `git clone https://github.com/dilijev/guest-list-generator`
* `cd guest-list-generator`
* `npm install` (will install necessary libraries, e.g. `argparse`)
* Manually download your ticket sales lists from the retailers supported by this software and convert to CSV if necessary.
* Run e.g. `generator.js --bpt bpt.csv --bpt-season bpt-season.csv --gs gs.csv --groupon groupon.csv --extra extra.csv [--out list.csv]`
* Run `generator.js --help` for more information

# Roadmap

## Work Items
* Add tests with anonymized names and ticket numbers.
* Use an actual CSV parser instead of hacking it (support commas within cells and escaped quotes (`""`)).

## Minor Bugs
* For retailers which provide Full Name (instead of First, Last), improve logic for splitting out first and last names.

## Quality of Life Improvements
* Add ability to have multiple files from the same retailers
(handles possibility of similar listings for the same event which involve different offers or packages and are therefore priced differently).
* Improve the ability to easily add more retailers or input list types.
* Add ability to specify output table layout (give more semantic meaning to the input formats to allow this).
* General goal: Require less ahead-of-time work on the part of the operator of this program.

## Difficult / Low Value / Probably Not Worthwhile
* Download the sources directly from the retailers instead of manually downloading.

# License

Licensed under the MIT License.
See [LICENSE.txt](https://github.com/dilijev/guest-list-generator/blob/master/LICENSE.txt) for details.

This software was created specifically for use by the Seattle Festival Orchestra,
but should be applicable to any similar group or merchant with a need to create guest lists from sales reports
from multiple online retailers.
",1,1,3,0,retailer,"[brown-paper-tickets, compact, csv, goldstar, groupon, guest, node, node-js, nodejs, orchestra, retailer, sales, ticket, ticket-sales]",44-45
ancor1369,fe-management-system,,https://github.com/ancor1369/fe-management-system,https://api.github.com/repos/fe-management-system/ancor1369,Project containing the web page of the central management system for the tags solution,"# Management web page

Prifast administrates price changes due to market fluctuations. The whole system reduces the cost of implementing new prices due to the fact that it does not need to deploy labor to obtain results.

## View of product listing

![alt text](Images/ProductManagement.PNG)

## View of Store to product relationship

![alt text](Images/storeToProduct.PNG)
",0,0,0,4,retailer,"[retail-automation, retailer]",44-45
sadrulhossain,d-retail,,https://github.com/sadrulhossain/d-retail,https://api.github.com/repos/d-retail/sadrulhossain,A distribution channel and inventory management software with retailer site developed for Arroz Group,"<p align=""center""><img src=""https://res.cloudinary.com/dtfbvvkyp/image/upload/v1566331377/laravel-logolockup-cmyk-red.svg"" width=""400""></p>

<p align=""center"">
<a href=""https://travis-ci.org/laravel/framework""><img src=""https://travis-ci.org/laravel/framework.svg"" alt=""Build Status""></a>
<a href=""https://packagist.org/packages/laravel/framework""><img src=""https://poser.pugx.org/laravel/framework/d/total.svg"" alt=""Total Downloads""></a>
<a href=""https://packagist.org/packages/laravel/framework""><img src=""https://poser.pugx.org/laravel/framework/v/stable.svg"" alt=""Latest Stable Version""></a>
<a href=""https://packagist.org/packages/laravel/framework""><img src=""https://poser.pugx.org/laravel/framework/license.svg"" alt=""License""></a>
</p>

## About Laravel

Laravel is a web application framework with expressive, elegant syntax. We believe development must be an enjoyable and creative experience to be truly fulfilling. Laravel takes the pain out of development by easing common tasks used in many web projects, such as:

- [Simple, fast routing engine](https://laravel.com/docs/routing).
- [Powerful dependency injection container](https://laravel.com/docs/container).
- Multiple back-ends for [session](https://laravel.com/docs/session) and [cache](https://laravel.com/docs/cache) storage.
- Expressive, intuitive [database ORM](https://laravel.com/docs/eloquent).
- Database agnostic [schema migrations](https://laravel.com/docs/migrations).
- [Robust background job processing](https://laravel.com/docs/queues).
- [Real-time event broadcasting](https://laravel.com/docs/broadcasting).

Laravel is accessible, powerful, and provides tools required for large, robust applications.

## Learning Laravel

Laravel has the most extensive and thorough [documentation](https://laravel.com/docs) and video tutorial library of all modern web application frameworks, making it a breeze to get started with the framework.

If you don't feel like reading, [Laracasts](https://laracasts.com) can help. Laracasts contains over 1500 video tutorials on a range of topics including Laravel, modern PHP, unit testing, and JavaScript. Boost your skills by digging into our comprehensive video library.

## Laravel Sponsors

We would like to extend our thanks to the following sponsors for funding Laravel development. If you are interested in becoming a sponsor, please visit the Laravel [Patreon page](https://patreon.com/taylorotwell).

### Premium Partners

- **[Vehikl](https://vehikl.com/)**
- **[Tighten Co.](https://tighten.co)**
- **[Kirschbaum Development Group](https://kirschbaumdevelopment.com)**
- **[64 Robots](https://64robots.com)**
- **[Cubet Techno Labs](https://cubettech.com)**
- **[Cyber-Duck](https://cyber-duck.co.uk)**
- **[Many](https://www.many.co.uk)**
- **[Webdock, Fast VPS Hosting](https://www.webdock.io/en)**
- **[DevSquad](https://devsquad.com)**
- **[OP.GG](https://op.gg)**

## Contributing

Thank you for considering contributing to the Laravel framework! The contribution guide can be found in the [Laravel documentation](https://laravel.com/docs/contributions).

## Code of Conduct

In order to ensure that the Laravel community is welcoming to all, please review and abide by the [Code of Conduct](https://laravel.com/docs/contributions#code-of-conduct).

## Security Vulnerabilities

If you discover a security vulnerability within Laravel, please send an e-mail to Taylor Otwell via [taylor@laravel.com](mailto:taylor@laravel.com). All security vulnerabilities will be promptly addressed.

## License

The Laravel framework is open-sourced software licensed under the [MIT license](https://opensource.org/licenses/MIT).
",0,0,2,0,retailer,"[distribution-channel, inventory, laravel, php, retailer]",44-45
Adithyan-R-K,My_Retailer,,https://github.com/Adithyan-R-K/My_Retailer,https://api.github.com/repos/My_Retailer/Adithyan-R-K,"My Retailer is a PHP based web application which provide an effective way of goods delivery between Stockiest, Distributor and Retailer","# MY REATILER
### My Retailer  is a PHP based web application which provide an effective way of goods delivery between Stockiest, Distributor and Retailer
",0,0,1,0,retailer,"[delivery, mysql, php, retailer]",44-45
chuayaocong,e-commerce,,https://github.com/chuayaocong/e-commerce,https://api.github.com/repos/e-commerce/chuayaocong,Group project using Python and data visualisation to analyse e-commerce data from an UK online retailer.,,0,0,1,0,retailer,"[cpi, e-commerce, retailer]",44-45
ArupDas15,MajorProject,,https://github.com/ArupDas15/MajorProject,https://api.github.com/repos/MajorProject/ArupDas15,"The project ""Transforming Transactional Marketing of Retailers"" was done during my Bachelor's in computer Science & Engineering in Siddaganga Institute of Technology, Tumkur, Karnataka, India as a final year major  project.","Transforming Transactional Marketing of Retailers
-----------------------------------------------------
Our main idea is to make a blockchain of customers for a supermarket which is shared amongst its various outlets along with the accumulation of credit. Our proposed system includes the customer credits provided by the retailer upon shopping above a speciﬁed amount, to be accumulated in the customer's block. Collectively, such blocks form a blockchain of the customer accounts altogether for the company, which is shared amongst the network of the company’s outlet present in various cities to maintain a properly distributed ledger. The system will allow us to:
1. Make credits called Value-coins with a ﬁxed limit. This limit of how many Value-coins are to be generated in total is decided by the retailer based on the proﬁt margin.
2. Create an autonomous private Blockchain with rules on how many Value-coins will be issued to the customer on spending how much money.
3. Transactions are validated and details of transactions are added to the customer block.

Our project also aims at simplifying the customer experience when it comes to shopping. The customers today have to stand for several minutes in the cash counter to pay their bills. Such a situation can be avoided by enabling the customer to pay soon after they have ﬁnished adding items to their cart. This can be done by enabling the customer to scan the barcodes of the product using their phones and ﬁnally pay the total amount.

Project Partners:
-----------------------
1. Ashmani Singh: https://www.linkedin.com/in/ashmani-singh-28a485128/
2. Akash Prasad: https://www.linkedin.com/in/akash-prasad-225723148/
3. Harsh Raj: https://www.linkedin.com/in/harsh-raj-b4089153/
4. Arup Das: https://www.linkedin.com/in/arup-das-90033a153/

Papers Published based on this work:
------------------------------------------
1. Enhancing retail business and customer experience using blockchain approach, DOI: 10.1504/IJBC.2020.111573
   International Journal of Blockchains and Cryptocurrencies, 2020 Vol.1 No.3, pp.273 - 285
   Accessible at: https://www.inderscience.com/info/inarticle.php?artid=111573 </br>
   **Free Copy Available [Here](https://github.com/ArupDas15/MajorProject/blob/62e3f10f66a53f9acab629c4747a777010265c8f/Enhancing%20Retail%20business%20and%20customer%20experience%20using%20blockchain%20approach.pdf)**.
 
2. Transforming Transactional Marketing of Retailers Using Blockchain Approach.
   Paper presented and published in International Conference on Sustainable Computing, Technology and Management (SUSCOM 2019) held in February 26-28, 2019 at Amity University, Rajasthan, India.
   Accessible at: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3350886
   
Link to Report : 
-----------------
The final report can be found [here](https://github.com/ArupDas15/MajorProject/blob/8e7f5ce665621c8bfdc02bcda6e9cb0dbbbf577c/report.pdf).
",0,0,1,0,retailer,"[blockchain, cryptocurrency, retailer, shopping, transactions]",44-45
pedroroccon,in-stock,,https://github.com/pedroroccon/in-stock,https://api.github.com/repos/in-stock/pedroroccon,"A simple application built in Laravel to check the availability of a given product in different retailer, also using automated tests to validate the results. The main idea of that project is build an app that we could refactor later using clean code knowledges and design patterns like Strategy and Factory.","# In Stock
In Stock is an application built with Laravel + Vue that tracks the availability and price of certain products across multiple retailers.
Each product might be avaliable from certain retailers, but we have to check if it's in stock.

## Checking availability and prices
If you want to check the availability and prices for a given product, you should add products and retailers in database.
Then, you can run the following command:

```
php artisan instock:track
```

The command above will returns all the products in database and updates the availability and prices for each retailer.

**This application is under development**",0,0,1,0,retailer,"[laravel, prices, retailer, stock]",44-45
Jamie-GiHu,ETL-Project,,https://github.com/Jamie-GiHu/ETL-Project,https://api.github.com/repos/ETL-Project/Jamie-GiHu,"The project objective is to scrape multiple web sources with product and price information from three major electronic retailers, complete ERD and store it in a database. Purpose of this database is to help consumers decide where to purchase TV’s, once we display it in a customized website.","### PROJECT BRIEF:

To compare prices at three major retailers to obtain the data to where consumers should purchase TV’s.  An additional retailer (Harvey Norman) was added after there were problems with JB Hifi (explained in the report).

The initial data sources that will be sourced:
* The Good Guys
* JB Hifi
* Bing Lee
* Harvey Norman (additional from the  original brief)

IDENTIFIED PROCESSES: 

A.    EXTRACT: 
Scrape the data from the retail outlets from their online stores obtaining the following information:
* Brand
* Size
* Product Name
* Model Number
* Currency
* Price
* Link to image
* Category

B.    TRANSFORM: 
Complete ERD to explain the setup of the data table.  
Clean and store the data into a SQL database for querying.
Amend and column types as required.
Additional column names as required will be “retailer”.

C.    LOAD: 
We will be putting the information into a SQL database, because we believe the structural integrity will be the same for the scraped and transformed data.

ADDITIONAL - TIME PENDING

D    WEBSITE:
* Build a basic website from a template to demonstrate who the data may be utilised by a consumer.
* Query the pricing sql database to extract the lowest price and display on a website that queries the database.

FINAL: 
Report - As below.

https://docs.google.com/document/d/1l2jfcqFcHQY6tJFci6olXn_G2GmE8IN77Cu7BaS0mSg/edit?usp=sharing

### FILES SUBMITTED:

<strong>1. Extract</strong>
* Extract_Bing_Lee.ipynb
* Extract_Good_Guys.ipynb
* Extract_Harvey_Norman.ipynb

<strong>2. Transform & Load</strong>
* Final_Transform.ipynb
* Final_Schema.sql

<strong>3. Report</strong>
* ETL_Project_Report_14_08_20.docx (which includes ERD)

<strong>4. Web Proposal and Template</strong>
* Some templates for our mock website 

",0,0,3,0,retailer,"[consumer, e-commerce, etl, extract, price, retailer, sql-database]",44-45
FFnewbeees,music-player,,https://github.com/FFnewbeees/music-player,https://api.github.com/repos/music-player/FFnewbeees,React Native Retailer List ,"# Retailer List

### Built by React Native


## Getting Started

First, run the development expo server:

```bash
npm start
# or
yarn start
# or
pnpm start
```

Second, press following keys to open the app in simulators to see the result.

```bash
› Press a │ open Android
› Press i │ open iOS simulator
› Press w │ open web

# or

npm / yarn android
# or
npm / yarn ios
# or
npm / yarn web


```




",0,0,1,0,retailer,"[list, react-native, retailer]",44-45
Nachtalb,day-deals,,https://github.com/Nachtalb/day-deals,https://api.github.com/repos/day-deals/Nachtalb,Bot that posts the best daily retail store sale offers to a Telegram channel for easy access. ,"# Day Deal Telegram Bot

This is the bot used for the ""[Angebote des Tages (Schweiz)][channel]"" channel.
Said channel posts daily and weekly spacial offers of various swiss online
retailers such as [digitec][digitec].

[![Angebote des Tages (Schweiz)](https://img.shields.io/badge/-Telegram-0088CC?logo=telegram&logoColor=white)][channel]

## Supported Sites

There are not many retailers with specifically day / week deals that I know
of so the list is a bit small. But these are basically all the ones the swiss
population as a whole really cares about.

| Retailer | Type | Time | Method |
| --- | --- | --- | --- |
| [digitec][digitec] | Daily | 00:00 | Via a [graphql endpoint][digitec-code] |
| [Galaxus][galaxus] | Daily | 00:00 | Via a [graphql endpoint][digitec-code] |
| [20min][20min] | Daily | 00:00 | Scraping the [website][20min-code] |
| [20min][20min-weekly] | Weekly | Mo 00:00 | Scraping the [website][20min-code] |
| [Brack / daydeal.ch][daydeal] | Daily | 09:00 | Scraping the [website][daydeal-code] |
| [Brack / daydeal.ch][daydeal-week] | Weekly | Mo 09:00 | Scraping the [website][daydeal-code] |

## Setup

```bash
# Clone repo
git clone https://github.com/Nachtalb/day-deals
cd day-deals

# Install dependencies
poetry install

# Copy sample config and adjust values inside
cp config.sample.json config.json
```

You can get a bot `token` from [BotFather][botfather]. The `chat_id` of the
chat the deals are posted to can be retrieved by using an unofficial telegram
client such as [64gram][64gram] (desktop), [Plus][plus] or via another bot like
[IDBot][idbot].

## Usage

Just run the bot:

```bash
poetry run python day.py
```

To run it periodically you can easily create a cronjob:

```bash
5 * * * * /path/to/python /path/to/day-deals/day.py 2>&1 | /usr/bin/logger -t day-deals
```

The `2>1 | /urs/bin/logger -t day-deals` is optional. It just makes sure that
all outputs are correctly logged to the crontab logs.

[channel]: https://t.me/angebote_des_tages_schweiz
[digitec]: https://www.digitec.ch/
[digitec-code]: https://github.com/Nachtalb/day-deals/blob/fc20d5b33ceba6a1a289479c7c76c19a66af82b6/day.py#L12-L80
[galaxus]: https://www.galaxus.ch/
[20min]: https://myshop.20min.ch/de_DE/category/angebot-des-tages
[20min-weekly]: https://myshop.20min.ch/de_DE/category/wochenangebot
[20min-code]: https://github.com/Nachtalb/day-deals/blob/fc20d5b33ceba6a1a289479c7c76c19a66af82b6/day.py#L129-L169
[daydeal]: https://daydeal.ch
[daydeal-week]: https://www.daydeal.ch/deal-of-the-week
[daydeal-code]: https://github.com/Nachtalb/day-deals/blob/fc20d5b33ceba6a1a289479c7c76c19a66af82b6/day.py#L83-L126
[botfather]: https://t.me/BotFather
[64gram]: https://github.com/TDesktop-x64/tdesktop
[plus]: https://play.google.com/store/apps/details?id=org.telegram.plus&hl=en&gl=US
[idbot]: https://t.me/myidbot
",0,0,1,0,retailer,"[20minutes, brack, digitec, galaxus, retailer, telegram-bot]",44-45
Luisginan,pos-2011,,https://github.com/Luisginan/pos-2011,https://api.github.com/repos/pos-2011/Luisginan,"The POS application helps retailers handle their operations, including purchase orders, delivery orders, cashier systems, stock management, and comprehensive reporting capabilities.","# pos-2011
The POS  application to help retailer handle operation 
",0,0,2,0,retailer,"[cashier-app, pos-app, retailer]",44-45
erictam96,E-commerceRetailerFYP,,https://github.com/erictam96/E-commerceRetailerFYP,https://api.github.com/repos/E-commerceRetailerFYP/erictam96,"Android E-commerce Platform. Allow retailer to post product, manage order, chat and view report","# E-commerce Retailer Android App FYP
This Android App is for my university final year project. This Merchant app allow merchant to sell their products. This project has 3 app (Customer, Runner and Merchant). You can refer to my github repo and give me your github ID for invitations for backend PHP code. Thank you

# Login
Default
- ID : r@r.com
- PASSWORD: 123456

# Youtube Link
Source Code Explanation
https://youtu.be/mXxaPKRWa6I
</br>App Demo
https://youtu.be/BiTefBBHnVQ

# Funtional Module
- Register product with serial number, variant, quantity, discount
- Confirm stock availablity and order packing  
- Manage Product
- View sales report(Daily, Monthly,Cancellation)
- Boost sales & Advertising (with payment module)
- Chat with customer 
- Push notification and notification list 
- Manage user profile 

# Usage
- Firebase Authentication, Realtime Database 
- Fabric for crashlytic and beta deploy

# Sample Screenshot

![alt text](https://s15.postimg.cc/d3g0oqmxn/register_product.jpg) Figure 1: Register Product </br></br>
![alt text](https://s15.postimg.cc/llpgt3gln/manage_product.png) Figure 2: Manage Product </br></br>
![alt text](https://s15.postimg.cc/q7ll1ecej/add_serial_number.jpg) Figure 3: Add Product Serial Number </br></br>
![alt text](https://s15.postimg.cc/i23j38vvf/confirm_stock.jpg) Figure 4: Confirm Stock Availablity</br></br>
![alt text](https://s15.postimg.cc/jtwhy6ht7/navigation_panel.png) Figure 5: Navigation Panel</br></br>
![alt text](https://s15.postimg.cc/jtwhy5my3/pack_order.jpg) Figure 6: Pack Order </br></br>
![alt text](https://s15.postimg.cc/xnkun7n97/monthly_report.jpg) Figure 7: View Monthly Report </br></br>
![alt text](https://s15.postimg.cc/3vns82q63/sale_report.png) Figure 8: View Daily Report</br></br>
![alt text](https://s15.postimg.cc/yd3mzm5t7/view_cancel_report.jpg) Figure 9: View Cancellation Report</br></br>
![alt text](https://s15.postimg.cc/mb895g9ff/boost_sale.png) Figure 10: Boost Sales & Advertising</br></br>
![alt text](https://s15.postimg.cc/f80dptguj/check_delivery_status.jpg) Figure 11: Check Delivery Status</br></br>
![alt text](https://s15.postimg.cc/hckqqxl23/setting.jpg) Figure 12: Setting UI</br></br>
",31,31,0,1,retailers,"[android, android-application, chat, ecommerce, ecommerce-platform, java, merchants, recyclerview-adapter, reporting, retailers, seller, store]",44-45
abdulazeem-tk4vr,LIVEMart,,https://github.com/abdulazeem-tk4vr/LIVEMart,https://api.github.com/repos/LIVEMart/abdulazeem-tk4vr,Grocery Shop ecommerce Mobile App with Firebase Realtime Database built in Java using Android Studio.,"
# Grocery Shop eCommerce Mobile Android Application </br>( Java | Firebase )

An e-commerce android application that handles the dynamics of shopping groceries online for customers, retailers as well as wholesalers, hereby easing the user experience and catering to the needs of modern day businesses. Please see the report [here](https://drive.google.com/file/d/1vJVX2_IoBgb7cZaiypGeQTA6cj_qEzAo/view?usp=sharing) for details on how this project was implemeneted
<br/><br/>
Built using Android studio primarily in Java with Firebase as the backend and deployed a Realtime database.<br/>
<!-- <br/>Completed the application in 25 days with over 150 hours of dedication and efforts.<br/> -->
# Tables of Contents
* [Introduction](#introduction)
* [Usage](#usage)
* [Description and Screenshots](#Description)
* [Contributions](#contributions)

<!-- * [Overview](#overview) -->


# Introduction
As a part of the course requirements for Object Oriented Programming, we developed an application
(Android/Web based/Stand-alone) for e-marketing that connects customers (individuals who shop
for home purpose) to retailers (people dealing with multiple items who stores items in large
quantities) and retailers to wholesalers (warehouse maintaining people) with the
mandatory functionalities mentioned in the problem statement. You can read the problem statement [here](https://drive.google.com/file/d/1VPLrlsWz1bdHlz52VRiq1TQR8rMw6TF0/view?usp=sharing)
<br/>
<br/>
Though we were given the freedom to choose any language that supports OOP with any framework, we decided to build an Android Mobile Application as we wanted to try something new. Some of us had experience in HTML , but the idea of creating a mobile android application was rather more appealing to us. The reason why we chose Java over Kotlin was because of the abundance of resources available online and it was the language chosen for the course content. 
<br/>
<br/>
This application provides an interface for users to sign-up, login, browse through categories of products from various stores, filter the store's products based on the location and quantity, have the ability to place an order online/offline in the near future , maintain records of carts ,orders ,transactions and also provide the feedback for a specific product from a specific store.
<br/>
<br/>
If the purchase is to be made online, the delivery details are displayed on the appropriate user's dashboard. For an offline transaction, the user is allowed to create a memo and a calendar notification for a specific date and time as a reminder.
<br/>
<br/>
Stores also have the feature of updating existing products and adding new products onto the application. To keep track of products that are added, I have also integrated an approval module which allows the admin to decide which products get to be displayed on the application.
<br/>
<br/>

# Usage
Now that you've reached this heading, you must be interested in developing an android application with a similar problem statement or you're just curious to see a demo of the app.
<br/>
<br/>
To check out the code for the app, navigate in this manner [ app -> src -> main -> java/com/example/oop_project/Main ]
<br/>
For layouts and drawables, navigate to [ app -> src -> main -> res]
<br/>
<br/>
If you'd like to start with Android development, you can check out these links :<br/>
https://github.com/codepath/android_guides
<br/>
https://github.com/coder2hacker/Android-Development-RoadMap<br/>
<br/>
You should also check the courses on Udacity which are the visual equivalents :
https://classroom.udacity.com/courses/ud9012
<br/>
<br/>
To download the app, install android studio, download the zip folder with respect to this project and import it. Then run the app on the emulator or on your phone.
<br/>

Kindly watch the following links before you start with the emulation.

https://www.youtube.com/watch?v=FeKfIWJyQMs
</br>
https://www.youtube.com/watch?v=13DPpfuP1Zs
</br>
</br>

<!-- Credentials :
</br>
</br>
Customer :
</br>
Username : Macha
</br>
Password : 123
</br>
</br>
Retailer : </br>
Username : Fgretailer </br>
Password : ty 
</br>
</br>
Wholesaler :
</br>
Username : Babuwhole</br>
Password : 99</br>
</br> -->


<!-- Can this project be done in kotlin?
<br/><br/>
Yes, as we came closer towards completing the project, we realized that with a quick few searches, we could've developed the application in Kotlin too. It would've taken us a few more days to understand the syntax and replicate the ideas implemented in Java but given the time constraints, Java was the more viable option. -->

# Description and Screenshots

Module 1: Registration and Sign-Up
<br/>
Module 2: Dashboards for every type of user
<br/>
Module 3: Navigation Module
<br/>
Module 4: Placing order and status of order
<br/>
Module 5: Feedback and Queries
<br/>
<br/>
Please access the Project Report [here](https://drive.google.com/file/d/1vJVX2_IoBgb7cZaiypGeQTA6cj_qEzAo/view?usp=sharing) to see in-depth the functionalities included in this Android App <br/><br/>


# Contributions
Abdul Azeem https://github.com/abdulazeem-tk4vr
<br/>
Anirudh Sundar https://github.com/Anirudh-Sundar
<br/>
Karthik Suresh https://github.com/karths8
<br/><br/>
Special Mention : <br/>
Aryan Arora https://github.com/aryanarora180
<br/><br/>
Graph :
https://github.com/abdulazeem-tk4vr/LIVEMart/graphs/contributors




<!-- # Overview

If you have any questions with respect to the project, you can e-mail me at sabdulazeem01@gmail.com -->



",13,13,1,3,retailers,"[android, android-application, android-studio, college-project, crud-application, customer, firebase, firebase-auth, firebase-realtime-database, firebase-storage, grocery, gui-application, java, language, object-oriented-programming, problem-statement, retailers, shop, xml]",44-45
diivyya,Agro-Tech,,https://github.com/diivyya/Agro-Tech,https://api.github.com/repos/Agro-Tech/diivyya,Overall app for farmers,"# AgroTech

![logo](https://user-images.githubusercontent.com/68610804/112413250-e4740280-8d45-11eb-9515-bcdab5b3cad8.jpeg)


## Features

![Capture](https://user-images.githubusercontent.com/68610804/112470806-1b233a80-8d91-11eb-9731-39c97da3b9c9.PNG)


 In this project, we are providing a farmer friendly web-based app that gives complete information about what crop to grow, what fertilizers and pesticides to use depending upon land condition and weather conditions.
 
 It also provides a platform that connects farmers directly to consumers or retailers thus, eliminates middlemen. This web application can be used by both farmers as well as retailers/consumers accordingly.
 
 ![Capture](https://user-images.githubusercontent.com/68610804/112406730-b806b900-8d3a-11eb-80b6-ef70f6d40278.PNG)

 
 ## Installation Steps:
 
 1. git clone 
 2. pip install virtualenv
 3. python -m venv ve
 4. source ve/Scripts/activate
 5. pip install django pillow joblib sklearn
 6. python manage.py runserver

## And you are all set! <3
",6,6,1,0,retailers,"[agriculture, crops-dataset, django, farmers, machine-learning, python, python3, random-forest, retailers, sqlite3]",44-45
VincentBernet,Tirico-ShopCameraAnalytics,,https://github.com/VincentBernet/Tirico-ShopCameraAnalytics,https://api.github.com/repos/Tirico-ShopCameraAnalytics/VincentBernet,Shopper Flow Solution built on Electron  💰📈,"# Tirico-ShopCameraAnalytics

## Elevator pitch 

<p align=""justify""> Tirico is a web-based decision support application for all store managers. It allows, from the analysis of camera images, to obtain valuable information, statistics and graphs on customer behavior, allowing your store managers to make the best decisions on the layout of the shelves. Notably on the customer journey in your shelves, via for example heat maps, or graphs on the relationship between the time spent in a department and the money spent at the cash register for the products of this same department. All our studies aim to put into perspective the correlation between the customer behavior analyzed via our camera and the analysis of the receipts. Finally, our AI-generated tips help you to better decipher our graphs and suggest possible initiatives.
 </p>

We help you to better understand your customers behaviors, in order to optimize your sales!

Solution realized with :
- **Electron** for the front-end part (html, css, js)
- **Tenserflow** for the image analysis and the machine learning part (with some python for the IA part)

<p align=""center""><img  width=""5000"" height=""auto"" src=""Tirico_Client/ressource/demo/Tirico_Gif.gif""\></p>

---

## Table of Contents 

- **[Application](#application)**
- **[Installation](#installation)**
- **[Functionalities](#features)**
- **[Team's Contact](#team)**
- **[Licence](#Licences)**

---
<a name='application'></a>
## Application

***Find all the data in the form of graphs :***
<p align=""center""><img src=""Tirico_Client/ressource/demo/dash_commwhite.JPG""\></p>

***Select your sales area and indicate the types of products sold :***
<p align=""center""><img src=""Tirico_Client/ressource/demo/selection_zone.gif""\></p>

***Find your heat maps of the different zones according to their popularity and the correlation with their specifics sells :***
<p align=""center""><img src=""Tirico_Client/ressource/demo/heatmap.JPG""\></p>

---
<a name='installation'></a>
## Installation

In your installation folder, create your git and connect it to this repository : 
``` python
git init
git remote add origin https://github.com/VincentBernet/Tirico-ShopCameraAnalitics
git pull origin master
```
 
To then install all the components necessary for the application :
``` python
npm install
Installer dans Tirico_Server/code_python/data le fichier suivant : 
https://drive.google.com/open?id=1cewMfusmPjYWbrnuJRuKhPMwRe_b9PaT (yolov4.weights)
```

To start the web-app :
``` javascript
npm start
```
 
For the python part (allowing generation of heat map, AI generated tips etc.) :  
Install pip or anaconda to install libraries more easily :

``` javascript
pip install pandas                         (to create graphs)
pip install plotly                         (to create graphs)
pip install -c plotly plotly-orca          (to save in .png those graphs)
pip install seaborn                        (to create HeatMaps)
```
Sample login to test Tirico : 
- Identifiant : **vb@gmail.com** | Mot de Passe : 123

---
<a name='features'></a>
## Functionalities
On this application we have implemented multiple features such as :
 - Login/Register/Logout/ForgottenPassword to access your account
 - Define the number of shops you own, to compare their result and have all the diagrams on one board
 - Several analytics such as heat maps, affluence graphs, purchase conversion in relation to the time spent by the customer in each department, department analysis etc. All of those analytics are for each shops, you have aswell combined analytics to compare them directly
 
---
<a name='team'></a>
## Team

> The whole application was realized by a team of seven students during their engineering course at EFREI PARIS : <br> 

 - **[Jean Hecke](https://www.linkedin.com/in/jean-hecke-92060015b/)** & **[Louis Gailhac](https://www.linkedin.com/in/gailhac-louis/)** & **[Serge Nicolas Excoffier](https://www.linkedin.com/in/serge-excoffier/)**
 - **[Hélène Boersma](https://www.linkedin.com/in/h%C3%A9l%C3%A8ne-boersma-a0a16b17b/)** & **[Emeline Bagoris](https://www.linkedin.com/in/emeline-bagoris-116905142/)**
 - **[Sébastien Friedberg](https://www.linkedin.com/in/sebastien-friedberg/)** & **[Vincent Bernet](https://www.linkedin.com/in/vincent-bernet/)**

> Feel free to contact us on Github or on Linkedin

---
<a name='Licences'></a>
## Licence

[![License](http://img.shields.io/:license-mit-blue.svg?style=flat-square)](http://badges.mit-license.org)

- **[MIT license](http://opensource.org/licenses/mit-license.php)**
- Copyright 2021 © **[Tirico's Team](#team)**.
",2,2,1,8,retailers,"[analytics, retailers, shopper, shopper-flow-solution]",44-45
DanzTim,Graphics-Card-Website-Scraper---Australia,,https://github.com/DanzTim/Graphics-Card-Website-Scraper---Australia,https://api.github.com/repos/Graphics-Card-Website-Scraper---Australia/DanzTim,Scraper for Western Australian graphics card retailer from their website,"# Graphics-Card-Website-Scraper---Australia
Scraper for Western Australian graphics card retailer for their website.
MSY is the only one that can be used for outside WA, just change the URL to your location.

The python scripts uses Beautiful Soup. So you will need BeautifulSoup installed. Get here: https://www.crummy.com/software/BeautifulSoup/

Currently only works for the stated retailers.
Will add new scripts for different website/retailers.

Please feel free to make changes or leave comment.
",1,1,1,2,retailers,"[pricelist, product, retailers, scraper, webscraper]",44-45
evelinesurbakti,Optimizing-Retail-Strategy-based-on-the-Pattern-of-Products-in-Groceries-Transactions,,https://github.com/evelinesurbakti/Optimizing-Retail-Strategy-based-on-the-Pattern-of-Products-in-Groceries-Transactions,https://api.github.com/repos/Optimizing-Retail-Strategy-based-on-the-Pattern-of-Products-in-Groceries-Transactions/evelinesurbakti,What kind of relationship of product that we have in the UK and Ireland wholesalers? How can that information be used to help business decisions making? You can find the answers in this repo.,"# Optimizing Retail Strategy based on the Pattern of Products in Groceries Transaction

### Introduction

A simple groceries list or a receipt provides a pattern of buying behavior of customers. It shows the relationship between products and the chance to inform business decisions such as product placement or even combination packages in order to maximize profits. So, what kind of relationship of product that we have in UK and Ireland wholesalers? How can that information be used to help business decisions making?

This project has the following goal:
Find a strong relationship between products that leads to the most profitable product combination. 

### About the Data

Description:
The <a href=""https://archive.ics.uci.edu/ml/datasets/online+retail""> data set </a> contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail.The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.
  
Before transforming the data set into one transaction for each InvoiceNo and building the association rules model, I undertook exploratory data analysis and preprocessing to see if I could uncover any suspicious pattern in the data as the initial step of data cleaning. 

## Exploratory Data Analysis & Preprocessing

In this part, I implemented the following steps:
- Checking the head of the data set

As you can see from the first 5 observations below, we have a clean description of products with capital letters including its InvoiceNo. 
![](./image/Originaldataset.JPG)

- Checking the tail of the data set, it has invalid invoice with negative quantity value.
![](./image/InvalidInvoice.JPG)

- After removing the observation with negative quantity value and make sure all of the data is completed. Here is the final condition of the clean and ready to use data, the final data count ended up being 397942.

![](./image/DropNA.png)

- Check the country with the highest number of observations
<p align=""center"">
<img src=""./image/Countrylist.JPG"" width=""250"" height=""200""/>
</p>

The data set of UK has a highest number of observations and also we will use Ireland data set. Both countries are situated next to each another and we want to see whether they have the same pattern or not. In the following explanation we will see the difference between them and how to model the association rules of products for both countries. But before that, we will have a brief description about the Apriori algorithm, why we use it and how to use it. 

## Apriori Algorithm
**Apriori algorithm** or we also can call it association rules mining, will take the data as the transaction object on which mining is to be applied. Before we discuss deeper about the algorithm, there are three terms that we should know: Support, Lift and Confidence. 

The main reason we use Apriori Algorithm is because it uses a breadth-first search strategy to count the support of itemsets and uses *a candidate generation function* which exploits the downward closure property of support. It considers every combination and all length of transactions (while we also can set it arbitrarily).

**Support** indicates the probability that a randomly chosen transaction contains both item A and B. The higher support gives more benefit into business when they use it to make a product package or combination.

<p align=""center"">
<img width=""330"" height=""60"" src=""./image/support.JPG"">
</p>

From the formula, support of A and B is the probability of the frequencies of product A and B together in one transaction compare to the whole number of transactions. More frequent the transaction, the higher its support would be.

**Confidence** is the probability that an product B is purchased in a randomly chosen transaction given that product A has been purchased.

<p align=""center"">
<img width=""350"" height=""80"" src=""./image/confidence.JPG"">
</p>

The confidence indicates the direction for cross-selling, it shows how ‘sure’ we are that product A and B be bought together by comparing the number of transactions those have product A and B with the number of transactions whose product A.

**Lift** indicates the strength of the relationship between the products. When the value of lift is greater than 1, it indicates a higher relationship.

<p align=""center"">
<img width=""330"" height=""80"" src=""./image/lift.JPG"">
</p>

## Association Rules Analysis of UK Data Set

First of all, subset the data for UK. We can see the data below:

![](./image/transaction_data_UK5.JPG)

This analysis requires that all the data for a transaction be included in 1 row (like a receipt, we have one receipt number and also the groceries list) while the items should be 1-hot encoded. So, we transform the data into a matrix with the list of products, we give a 'flag' 1 when the product is exist and 0 when it is not on the transaction.

![](./image/finaltransaction_UK_hotencoded1.JPG)

Then, what is the most frequent product in UK? The answer is ""WHITE HANGING HEART T-LIGHT HOLDER""
<p align=""center"">
<img align=""center"" width=""430"" height=""380"" src=""./image/frequent_itemsets_UK.JPG"">
</p>

### The Customers in UK tend to buy the Teacup with Various Colours

In constructing the Apriori algorithm, we have to set the minimum support and also the minimum confidence into the apriori function from **MLxtend**.
We set the confidence as 70% as we have a large number of observations for UK, and 2% for the support. 

![](./image/rules_UK_code.JPG)

The result is sorted based on lift in descending order, means the strength of relationship is sorted from the strongest and weaker as the lift is getting smaller.  
![](./image/rules_UK.JPG)

Association rules are normally written like this: {ROSES REGENCY TEACUP AND SAUCER, GREEN REGENCY TEACUP AND SAUCER} -> {PINK REGENCY TEACUP AND SAUCER} which means that there is a strong relationship between products: ""ROSES REGENCY TEACUP AND SAUCER, GREEN REGENCY TEACUP AND SAUCER"" and ""PINK REGENCY TEACUP AND SAUCER"" since customers purchased them in the same transaction.

In the above example, the {ROSES REGENCY TEACUP AND SAUCER , GREEN REGEN...	} is the antecedent and the {PINK REGENCY TEACUP AND SAUCER} is the consequent. Both antecedents and consequents can have multiple items. In other words, {ROSES REGENCY TEACUP AND SAUCER, GREEN REGENCY TEACUP AND SAUCER, etc} -> {PINK REGENCY TEACUP AND SAUCER, etc} is still a valid rule.

*Support* of (ROSES REGENCY TEACUP AND SAUCER, GREEN REGENCY TEACUP AND SAUCER) and PINK REGENCY TEACUP AND SAUCER is 0.022177, it means they appear together in 2.2177% of transactions. In other words, on 10000 transactions there are on average 221.77 transactions with both together. 

*Confidence* is the percentage of transactions that contain the two products together, out of the transactions containing one of the two products. As a consequence, there are always two confidence numbers for each couple of products.

Confidence is useful because it gives the direction of the cross-selling. We can make the hypothesis that it’s easier to sell ‘ROSES REGENCY TEACUP AND SAUCER’ to someone buying ‘GREEN REGENCY TEACUP AND SAUCER’, with confidence up tp 77.78% compared to the opposite (only 70.5%).

However, we should keep in mind that the high-confidence rules can be misleading, because the confidence measure ignores the support of the product appearing in B. Lift measure overcomes this problem, as it considers the support of both products A and B in the ratio.

*Lift* gives the strength of the relationship between two products. The rule of thumbs:
- When lift is 0 - 1, there is no relationship.
- When lift is more than 1, the products have a positive relationship, the customer buy the products more frequently than it would happen.
- When lift is lower than 0, the customers buy the products less frequently than it would happen.

Lift measures the extent to which A and B are not independent and despite having the same value, the interpretation of the two lift values should not be the same because the maximum attainable value is different

As an example, the first rule shows when ""ROSES REGENCY TEACUP AND SAUCER, GREEN REGENCY TEACUP AND SAUCER"" are together with ""PINK REGENCY TEACUP AND SAUCER"" , it has a Lift of 22.53, meaning that consumers buy them together about ~22.5 more times than it would happen by chance. This is a very strong relationship.

As for the visualization, here is top 4 rules based on their value of lift. 

![](./image/scatterplot.gif)

<img width=""470"" height=""380"" src=""./image/parallelplot.JPG"">

And also a graph to show the relationship between the products where the arrows and rules with darker red color has higher lift. 
![](./image/graph.gif)

## Association Rules Analysis of Ireland (EIRE) Data Set

After the same process is done with the Ireland data, we have the result of the most frequent items in Ireland. 

<p align=""center"">
<img align=""center"" width=""500"" height=""380"" src=""./image/frequent_itemsets_IE.JPG"">
</p>

Irish love carriage as it is the first rank of most frequent item, follows by ""REGENCY CAKESTAND 3 TIER"" which is the third rank on most frequent list in UK data set. Both countries seems to share the same interest with the cakestand. 

### The Customers in Ireland tend to buy the Cakestand
For Ireland data set, the setting of minimum support and minimum confidence are changed since the number of observations is smaller rather than the UK data set. 

We set the confidence as 95% (before is 70%) as we have smaller number of observations rather UK, and increase minimum support from 2% into 7%. 

![](./image/rules_IE_code.JPG)

![](./image/rules_IE.JPG)

There are three rules with confidence value equal to 1.0. These rules are very important and the items should have a high priorities in the marketing strategy. 

## Conclusion
We have done Association Rules Analysis using an actual online retail transaction data from UK and Ireland. The result of this market basket analysis could be used for the optimization of retail strategy. There are marketing insights that we can optimize, here is the principle of Marketing Mix for the items with a strong relationship:
- Place: We can put the items with strong relationship close to each other, in this case we can design their image close to each other on the webpage,
- Promotion: We can put them together with additional discount or create a new promotion strategy, and
- Price: We can calculate the pricing strategy for the products. 

For UK wholesalers, I recommend to put these types of teacup and saucer: ROSES REGENCY, GREEN REGENCY and PINK REGENCY next to one another. Another recommendation, they can give a discount price for ROSES REGENCY or GREEN REGENCY when the customers plan to buy the PINK REGENCY. Best of all, the customers would be happy to pay a lower price for the product bundling consist of these teacups and saucers. 

For Ireland wholesalers, the analysis of association rules show that both ROSES REGENCY and REGENCY CAKESTAND 3 TIER has confidence value = 1.0. The marketing strategy should combine both items in product bundling and discount promotion. 
",1,1,1,0,retailers,"[apriori-algorithm, association-rule-mining, association-rules, retailers]",44-45
davidmensahedem,Rubix_R2cubid,,https://github.com/davidmensahedem/Rubix_R2cubid,https://api.github.com/repos/Rubix_R2cubid/davidmensahedem,"A PHP Application for managing orders, retailers and marketers.","﻿# Rubix_R2cubid
<p align=""center"">
    <a href=""http://instrumentshop.42web.io/Rubix"">
        <p align=""center"">R2cubid_Rubix (version 1.0.0)</p>
    </a>
    
</p>

<p align=""center"">
    A PHP Application for managing orders, retailers and marketers.
</p>

<p align=""center"">
    This repository consists of a <strong>PHP Application</strong> built with <strong>JQUERY, APEX CHARTS, HTML, CSS AND MySQL.</strong> 
</p>

## Application features

- `Admin Home Page` - Dashboard.

- `Marketers Home Page` - To add a retailer and orders.

- `Real-time rendering` 

- `Geolocation API Integration`

- `Effective MVC Architecture`

- `Responsive Design`

- `Informative charts (Sales Bar chart and Orders Pie Chart)`

## Done

<p>1- Geolocation API Integration ✅</p>
<p>2- Add/Edit/Delete Marketer ✅</p>
<p>3- Add/Edit/Delete Retailer. ✅</p>
<p>4- Take picutre and record retailer's shop. ✅</p>
<p>5- Add another shop.✅</p>
<p>6- Order products.✅</p>
<p>6- Real time rendering.  ✅   </p>
<p>7- Effective searching of marketers and retailers.  ✅   </p>

## Future

<p>1- Make it a desktop app with.❌ </p>

",1,1,1,0,retailers,"[ajax, apexcharts, css, javascript, jqeury, marketers, mysql, orders, pdo, pdo-mysql, php, real-time, retailers]",44-45
gowthamrajk,Farmer-And-Retailer-Application,,https://github.com/gowthamrajk/Farmer-And-Retailer-Application,https://api.github.com/repos/Farmer-And-Retailer-Application/gowthamrajk,It is a basic console application using Object Oriented Principles. Farmers can directly deal with the retailers without having a moderator. Farmers can bid their product and the good product can be taken by the retailers with certain affordable base price.,"# Farmer-And-Retailer-Application

It is a basic console application using Object Oriented Principles.
Farmers can directly deal with the retailers wihout having a moderator.
Farmers can bid their product and the good product can be taken by the retailers with certain affordable base price.
",1,1,2,0,retailers,"[classes-and-objects, console-application, exception-handling, farmers, inheritance, java-8, object-oriented-programming, polymorphism, retailers]",44-45
EagleDangar,BlackFriday-prediction,,https://github.com/EagleDangar/BlackFriday-prediction,https://api.github.com/repos/BlackFriday-prediction/EagleDangar,Predict how much the customer is going to spend money on next Black Friday by analyzing Black Friday's purchase history.,"# Black Friday Insights and Prediction

------------

![](https://raw.githubusercontent.com/EagleDangar/BlackFriday-prediction/master/images/blackfridaycanceled.jpg)

## I also released my article on [Medium](https://medium.com/@mayur11061997/black-friday-insights-and-prediction-16933b8a8bfb).


## Table Of Contents

  - [What is Black Friday ?](#what-is-black-friday-?)
  - [Python and Libraries](#python-and-libraries)
  - [File Structure](#file-structure)
  - [Summary](#summary)
  - [Acknowledgements](#acknowledgements)

## What is Black Friday ?
Black Friday is the day after Thanksgiving. Retailers typically offer steep discounts on Black Friday to kick off the holiday season.Black friday is the one of the most profitable days for all the retailers and for the buyers too. It's common for retailers to offer special promotions and open their doors during the pre-dawn hours on Black Friday to attract customers.

To keep up with the competition, retailers need some insights and cluster of customers to segmantize and to target them. So, retailers can use different marketing strategies to attract different types of customers.

Data scientist can help retailers with proper insights based on the historical data of black friday and also give proper data driven solutions by using Data science methods and by following the **CRISP-DM Process (Cross Industry Process for Data Mining)**. Thus, they can achieve thier target/goal.This project is all about this.

below are the steps of CRISP-DM Process ,

1. Business Understanding
2. Data Understanding
3. Prepare Data
4. Data Modeling
5. Evaluate the Results
6. Deploy


------------

## Python and Libraries
- Python 3.7+
- pandas , scikit-learn, seaborn, XGBoost, jupyter notebook.

## File Structure

        ├── README.md
        ├── blackfriday-insights-and-model.ipynb
        ├── images
        │   ├── blackfriday-1.jpg
        │   ├── blackfriday-2.jpg
        │   └── blackfridaycanceled.jpg
        ├── kaggle
        │   └── input
        │       └── black-friday
        │           ├── test.csv
        │           └── train.csv
        └── model
            ├── BFriday_XGB_Model_V1.pkl
            └── BFriday_XGB_Model_V1_details.json

- Directories
	-  kaggle/input/black-friday/ :-  Contains all the data-sets
	-  images/ :- Contains all the required images
	-  model/ :- Contains all the files related to saved model


- Files
	- blackfriday-insights-and-model.ipynb :-  Main notebook of this project
	- train.csv :-  Training Data-set
	- test.csv :- Test Data-set
	- BFriday_XGB_Model_V1.pkl  :- saved xgboost model
	- BFriday_XGB_Model_V1_details.json  :-  saved xgboost model's definition 


## Summary

In summary, We went through each and every steps of **CRISP_DM**, did different types of analysis on the data, visualized the data with different types of charts and trained the model with different types of machine learning models to predict the purchase amount that user might spend on next black friday.


## Acknowledgements

- [Kaggle - Black Friday ](https://www.kaggle.com/sdolezel/black-friday )
",0,0,2,0,retailers,"[acknowledgements, blackfriday, case-study, datascience, insights, purchase, retailers]",44-45
COMP231W21-G5,Mechanic-Checker,COMP231W21-G5,https://github.com/COMP231W21-G5/Mechanic-Checker,https://api.github.com/repos/Mechanic-Checker/COMP231W21-G5,"The Mechanic Checker Web Application allows Canadian users located in the Greater Toronto Area (GTA) to compare auto part-related items and automotive-related services, from local stores and major retailers, i.e. Ebay, Amazon, and Alibaba, with plans for adding support for more major retailers, e.g. Walmart.","# Mechanic Checker

The Mechanic Checker Web Application is live on this URL: http://mechanicchecker-env-1.eba-q734qnyr.us-east-1.elasticbeanstalk.com/

For additional documentation please refer to our [Mechanic Checker Wiki](https://github.com/COMP231W21-G5/Mechanic-Checker/wiki)!

The Mechanic Checker Web Application allows Canadian users located in the Greater Toronto Area (GTA) to compare auto part-related items and automotive-related services, from local stores and major retailers, i.e. Ebay, Amazon, and Alibaba, with plans for adding support for more major retailers, e.g. Walmart. Local stores can create an account to post their auto part-related items and automotive-related services listings. Additionally for local stores users can filter search results based on local stores near their location via the Google Maps API. For the major retailers, the Mechanic Checker uses product APIs provided by said retailers to obtain their auto part-related items listings. 

![image](https://user-images.githubusercontent.com/30096267/114967002-ac8e4580-9e41-11eb-85ee-0f0a96682e61.png)

*The Mechanic Checker Home Page.*

The Mechanic Checker website is built on [.NET Core 2.1](https://docs.microsoft.com/en-us/aspnet/core/release-notes/aspnetcore-2.1?view=aspnetcore-5.0).

This is Phase 1 of a 2-Phase [Centennial College](https://www.centennialcollege.ca/) capstone project for created for [Software Development Project 1 (COMP-231)](https://www.centennialcollege.ca/programs-courses/full-time/course/software-development-project-i/), for the professor Jake Nesovic. Phase 2 of the Mechanic Checker project will be done in [Software Development Project 2 (COMP-313)](https://www.centennialcollege.ca/programs-courses/full-time/course/software-development-project-2-COMP-313/).

## Mechanic Checker Team

Ibrahim Jomaa and Emmanuel Ajayi are the Tech Leads for this project. They have admin permission on this repository, and are listed in the `CODEOWNERS` file. Their tasks include reviewing and approving [pull requests](https://github.com/COMP231W21-G5/Mechanic-Checker/pulls).

The Mechanic Checker team is as follows:
- Michael Asemota:
    - Role: Scrum Master / Customer
    - Program: [Software Engineering Technology (Co-op)](https://www.centennialcollege.ca/programs-courses/full-time/software-engineering-technology/)
    - [GitHub](https://github.com/Asemota33)
    - [LinkedIn](https://www.linkedin.com/in/michaelasemota)
- Ibrahim Jomaa: 
    - Role: Software Developer | Tech Lead
    - Program: [Software Engineering Technology (Co-op)](https://www.centennialcollege.ca/programs-courses/full-time/software-engineering-technology/)
    - [GitHub](https://github.com/Function-0)
    - [LinkedIn](https://www.linkedin.com/in/ibrahim-jomaa/)
- Sanjib Saha: 
    - Role: Software Developer
    - Program: [Software Engineering Technology](https://www.centennialcollege.ca/programs-courses/full-time/software-engineering-technology/)
    - [GitHub](https://github.com/SanjibSaha27)
    - [LinkedIn](https://www.linkedin.com/in/sanjib-saha-79914b1bb/)
- Shaminda Abeysekara: 
    - Role: Software Developer
    - Program: [Software Engineering Technology](https://www.centennialcollege.ca/programs-courses/full-time/software-engineering-technology/)
    - [GitHub](https://github.com/Shaminda1017)
    - [LinkedIn](https://www.linkedin.com/in/shamindaabeysekara)
- Emmanuel Ajayi: 
    - Role: Software Developer | Tech Lead
    - Program: [Game Programming (Co-op)](https://www.centennialcollege.ca/programs-courses/full-time/game-programming/)
    - [GitHub](https://github.com/Dami908)
    - [LinkedIn](https://www.linkedin.com/in/emmalare)
- Nusrat Jahan: 
    - Role: Software Developer
    - Program: [Software Engineering Technician](https://www.centennialcollege.ca/programs-courses/full-time/software-engineering-technician/)
    - [GitHub](https://github.com/nusratjt)
    - [LinkedIn](https://www.linkedin.com/in/nusrat-jahan-6047aa171/)
- Shaniquo McKenzie: 
    - Role: Software Developer
    - Program: [Health Informatics Technology (Co-op)](https://www.centennialcollege.ca/programs-courses/full-time/health-informatics-technology/)
    - [GitHub](https://github.com/shaniquo)
    - [LinkedIn](https://www.linkedin.com/in/shaniquo-mckenzie)

## Running Locally

The Mechanic Checker source code requires the [Visual Studio IDE](https://visualstudio.microsoft.com/) to access the `MechanicChecker.sln` solution file. This is located under the path: 
```
MechanicChecker/MechanicChecker.sln
```

Additional dependencies may be required to install using [NuGet](https://docs.microsoft.com/en-us/nuget/what-is-nuget).

Unit testing folders can also be found where the `MechanicChecker.sln` file is located. The folders are as follows:
- MechanicCheckerCoreUnitTests
- mechanicCheckerTest

## Deployment Architecture

The Mechanic Checker is hosted on [Amazon Web Services (AWS)](https://aws.amazon.com/), at region us-east-1. Using [GitHub Actions](https://github.com/features/actions), a CI/CD pipeline is used to push code from GitHub to [AWS Elastic Beanstalk](https://aws.amazon.com/elasticbeanstalk/). An [AWS MySQL relational database (Amazon RDS)](https://aws.amazon.com/rds/), and a [AWS Simple Storage Service (Amazon S3)](https://aws.amazon.com/s3/) has also been provisioned on AWS. The 1 year [AWS Free Tier](https://aws.amazon.com/free/) membership is being used to fund AWS's Cloud Services.

### Deployment Diagram

![deployment-diagram-v5](https://user-images.githubusercontent.com/30096267/114967191-fa0ab280-9e41-11eb-9c50-f368ed9d8a47.png)

## License

The Mechanic Checker Web Application is licensed under the MIT License.

See the [LICENSE](https://github.com/COMP231W21-G5/Mechanic-Checker/blob/develop/LICENSE) file for more information.


",0,0,3,18,retailers,"[alibaba, amazon, automotive, autoparts, ebay, gta, local-stores, price-comparison, product-comparison, retailers, services]",44-45
pskliff,vtb-data-fusion,,https://github.com/pskliff/vtb-data-fusion,https://api.github.com/repos/vtb-data-fusion/pskliff,This repository provides code solution for Data Fusion Contest task 1,"# vtb-data-fusion

This repository provides code solution for [Data Fusion Contest](https://boosters.pro/championship/data_fusion/overview) task 1  
  
**Short description:** `Single distilbert`  
**Place: *7/265 (top 3%)***  
**Public LB = *0.8683***  
**Private LB = *0.8674***   

## Requirements

To install requirements:

```setup
pip install -r requirements.txt
```

## Datasets
[Boosters](https://boosters.pro/championship/data_fusion/data)
### Data description
Task is to predict the predefined category of the item in a receipt based on its name


## Solution description
- Baseline — [Russian Part of Multilingual Distillbert](https://huggingface.co/Geotrend/bert-base-ru-cased) as is (spoiler - it was *Cased*): Public = `0.7875`
- **+** Pretraining on **masked language modeling** task: Public = `0.8261`
- **+** *Label Smoothing*: Public = `0.8323`
- **+** *Custom Model Arch* (Weighted sum of hidden states + multisample dropout): Public = `0.8354`
- **+** *Lowercase*: Public = `0.8459`
- **+** *Increase number* of training *epochs* to 50: Public = `0.8532`
- **+** *Pseudolabeling* (distilbert-distilbert): Public = `0.8626`
- **+** *Pseudolabeling* (RuBERT-distilbert): Public = `0.8683`


## How to run
- Pretrain [RuBERT](https://huggingface.co/DeepPavlov/rubert-base-cased) and [distilbert](https://huggingface.co/Geotrend/bert-base-ru-cased) on all unique texts using **masked language modeling** task: `train_mlm_base_tokenizer.ipynb`
- Finetune pretrained **RuBERT** on the texts with labels (~40k unique texts): `rubert_base.ipynb`
- Create pseudolabels (~1M unique texts) for all unique texts using finetuned **RuBERT**: `pseudo_label.ipynb`
- Finetune **distilbert** on these pseudolabels: `pseudo_label.ipynb`
- Create submission *.zip* with finetuned **distilbert**: `pseudo_label.ipynb`
",8,8,2,0,retail,"[bert, classification, distilbert, fine-tuning, huggingface, nlp, pretrain, receipts, retail, rubert]",44-45
ciellosinc,FSC-PS,ciellosinc,https://github.com/ciellosinc/FSC-PS,https://api.github.com/repos/FSC-PS/ciellosinc,D365 FSC DevOps setup&maintenance tools for GitHub,"# FSC-PS for GitHub
:rocket: FSC-PS for GitHub is a set of GitHub templates and actions, which can be used to setup and maintain professional DevOps processes for your Dynamics 365 FSC, Retail or ECommerce  projects.

The goal is that people who have created their GitHub repositories based on the FSC-PS templates, can maintain these repositories and stay current just by running a workflow, which updates their repositories. This includes necessary changes to scripts and workflows to cope with new features and functions.

The template repository to use as starting point are:
- https://github.com/ciellosinc/FSC-PS.FSC is the GitHub repository template for D365 FSC Extenstions. This is your starting point.
- https://github.com/ciellosinc/FSC-PS.Retail is the GitHub repository template for D365 Legacy Retail Extenstions. This is your starting point.
- https://github.com/ciellosinc/FSC-PS.Commerce is the GitHub repository template for D365 Commerce Extenstions. This is your starting point.
- https://github.com/ciellosinc/FSC-PS.ECommerce is the GitHub repository template for D365 ECommerce Extenstions. This is your starting point.

The below usage scenarios takes you through how to get started and how to perform the most common tasks.

Usage scenarios:
1. [Set up repository](Scenarios/SetupRepo.md)
2. [Set up CI](Scenarios/SetupCI.md)
3. [Set up CD](Scenarios/SetupCD.md)
4. [Set up your own GitHub runner to increase build performance](Scenarios/SelfHostedGitHubRunner.md)
5. [Update FSC-PS files](Scenarios/UpdateFSC-PS.md)
6. [Add environment from the different tenant](Scenarios/AddEnvironmentFromTheDifferentTenant.md)
7. [D365FSC. Include Test model into the deployable package ](Scenarios/IncludeTestModel.md)
8. [D365FSC. Build a specific model(`s) ](Scenarios/DeploySpecificModel.md)
9. [D365FSC. Deploy the code to the environment ](Scenarios/DeployCode.md)

**Note:** Please refer to [this description](Scenarios/settings.md) to learn about the settings file and how you can modify default behaviors.
# This project
This project in the main source repository for FSC-PS for GitHub. This project is deployed on every release to a branch in the following repositories:

- https://github.com/ciellosinc/FSC-PS.FSC is the GitHub repository template for D365 FSC Extenstions. This is your starting point.
- https://github.com/ciellosinc/FSC-PS.Retail is the GitHub repository template for D365 Legacy Retail Extenstions. This is your starting point.
- https://github.com/ciellosinc/FSC-PS.Commerce is the GitHub repository template for D365 Commerce Extenstions. This is your starting point.
- https://github.com/ciellosinc/FSC-PS.ECommerce is the GitHub repository template for D365 ECommerce Extenstions. This is your starting point.
- https://github.com/ciellosinc/FSC-PS-Actions is the GitHub repository containing the GitHub Actions used by the template above.
",8,8,2,0,retail,"[365, d365, d365actions, d365fo, d365tools, devops, devops-tools, dynamics, dynamics-365, fsc, powershell, retail]",44-45
skribl5000,ParserA_ru,,https://github.com/skribl5000/ParserA_ru,https://api.github.com/repos/ParserA_ru/skribl5000,Parser + analyzer for cars just by one link,"# ParserA_ru
Parser + analyzer for cars just by one link

Project contains two modules: 


1)Parser.py:
Parcer for your link from autoru.
Will write the infromation to pandas DataFrame and clone to local excel file.
Information: Title, Price, km, year, location. 

2)Analyze.ipynb:
will analyze our data from parcer and show some usefull information about data.
Visualization(will be continue..): 
	boxplot with prices by year, age and km_category(40000 km for each)
	lineplot with trend line and correlation calculation
	...
",0,0,2,0,retail-data,"[data-science, parsing, retail-data]",44-45
JamesN93,Excel-Data-Cleaning,,https://github.com/JamesN93/Excel-Data-Cleaning,https://api.github.com/repos/Excel-Data-Cleaning/JamesN93,A data cleaning exercise on Excel where I clean data and calculate lifetime values of different customer cohorts. Lifetime value means the average value of purchase by a customer in a specific time-based cohort.,,0,0,1,0,retail-data,"[calculations, cleaning-data, data-analysis, excel, retail-data, visualization]",44-45
ltp111,customers_segment,,https://github.com/ltp111/customers_segment,https://api.github.com/repos/customers_segment/ltp111,Analyse an online retail dataset for customers segmentation,"# customers_segment
Analyse an online retail dataset for customers segmentation.
Link to the data: https://archive.ics.uci.edu/ml/datasets/online+retail

If the file cannot be renderred in github (sometimes it happens!), please use this link instead: https://nbviewer.jupyter.org/github/linh279/customers_segment/blob/master/Clustering_Retail_Data.ipynb

# Summary of project:
We want to use this data to study the problem of customers segmentation, which is a common task that businesses, especially retailers, require in order to undertake marketing activities, pricing plans, customer services and other business strategies.

We are going to do the followings:
- Data preparation: the dataset has different issues in terms of duplicates, problematic entries, missing
entries and outliers that need to be dealt with
- Feature creation: derive extra features from the data (frequency and equally-sized price bins)
- Exploratory analysis
- Data transformation: transform the data to be suitable for clustering algorithms and dimension reduction.
- Dimension reduction for feature selection
- Clustering using different techniques
- Evaluation of clustering results
",0,0,1,0,retail-data,"[clustering-methods, python, retail-data]",44-45
stephaniebernhard,retail_analytics,,https://github.com/stephaniebernhard/retail_analytics,https://api.github.com/repos/retail_analytics/stephaniebernhard,Analysis of retail sales and time series forecast using exponential smoothing,"# Retail_Analytics / Time Series Forecast
## Goal
Analyse data from a retailer and predict departement-wide sales for each store.

## Data Source
The data can be downloaded from [Kaggle](https://www.kaggle.com/manjeetsingh/retaildataset).

If you want to run the code, please note that the files have been renamed (Features data set -> features, sales data-set -> sales, stores data-set -> stores)

## Approach
1. Screen data
2. Transform data (cast datatypes, clean up, select relevant)
3. Join data
4. Plot data for feature correlation analysis
5. Choose apropriate model
6. Evaluate predictions

## Results
After cleaning the data and casting correct datatypes, I decided to plot distinct features in order to see whether there's a correlation as this would indicate which model to use for the predictions.

Most of the features did not correlate with the weekly sales as shown in the following figure which compares weekly sales to the unemployment rate:

![png](out/unemployment.png)

There seems to be a seasonal trend, though as illustrated in the following figure which compares the weekly sales over the given years:

![png](out/years.png)

As there are no straightforward features which could be used for a feature-based model, I decided to use a time-series approach and implemented exponential smoothing (Holt Winters) using a statistical model. 

![png](out/prediction.png)

Using a ratio of 100 entries for training and the remaining 43 entries for testing, the mean absolute percentage error is 2.66%.
Allowing more samples in the training data, i.e. 130 and using the remaining 13 entries for testing resulted in an error of 1.48%. 

",0,0,2,0,retail-data,"[data-analysis, data-science, forecast, retail-data, time-series-analysis]",44-45
saivivek7495,Retail-Price-Prediction-Hackathon-MachineHack-Competition,,https://github.com/saivivek7495/Retail-Price-Prediction-Hackathon-MachineHack-Competition,https://api.github.com/repos/Retail-Price-Prediction-Hackathon-MachineHack-Competition/saivivek7495,Machine Hack - Great Indian Hiring Hackathon  - In this competition we were challenged to come up with an algorithm foretelling Retail prices. ,"# Retail-Price-Prediction-Hackathon-MachineHack-Competition

OBJECTIVE: In this Competition we were challenged to come up with an algorithm foretelling Retail prices. 

PROJECT DESCRIPTION : We were given by past transaction data along with their unit price, here attributes are invoice number, invoice date, description of
product, unit price, stock code etc. 

STRATEGY : Performed Multivariate Regression Analysis & developed a model with RMSE metric to generalize well on unseen data.

",0,0,1,0,retail-data,"[hackathon-project, knn-regression, linear-regression, machine-learning, pandas, python, retail-data]",44-45
samirsaci,inventory-deterministic,,https://github.com/samirsaci/inventory-deterministic,https://api.github.com/repos/inventory-deterministic/samirsaci,Inventory Management for Retail — Deterministic Demand,"## Inventory Management for Retail — Deterministic Demand 📈
*Build a simple model to simulate the impact of several replenishment rules (Basic, EOQ) on the inventory costs and ordering costs*

<p align=""center"">
  <img align=""center"" src=""https://miro.medium.com/max/1280/1*GrAHw2d9Sa5qShf-cKhYYA.png"">
</p>

For most retailers, inventory management systems take a fixed, rule-based approach to forecast and replenishment orders management.

Considering the distribution of the demand, the objective is to build a replenishment policy that will minimize your ordering, holding and shortage costs.
-Ordering Costs: fixed cost to place an order due to administrative costs, system maintenance or manufacturing costs in (Euros/Order)
- Holding Costs: all the costs required to hold your inventory (storage, insurance, and capital costs) in (Euros/unit x time)
- Shortage/Stock-out Costs: the costs of not having enough inventory to meet the customer demand (Lost Sales, Penalty) in (Euros/Unit)

### Article
In this [Article](https://www.samirsaci.com/inventory-management-for-retail-deterministic-demand/), we will present a simple methodology using a discrete simulation model built with Python to test several inventory management rules based assuming:
- Deterministic Constant Demand: D (Units/Year)
- Lead Time between ordering and replenishment (Days)
- Cost of shortage and storage (Euros/Unit)

### Problem Statement
As an Inventory Manager of a mid-size retail chain, you are in charge of setting the replenishment quantity in the ERP.

Based on the feedbacks of the store manager, you start to doubt that the replenishment rules of the ERP are the most optimal especially for the fast runners because your stores are facing lost sales due to stock-outs.

For each SKU, you would like to build a simple simulation model to test several inventory rules and estimate the impact on:
- Total Costs: how much does it cost to receive, store and sells this product?
- Shortages: what is the % of lost sales due to stock-out?

### Objective
1. Visualize the current rule used by the store's manager
2. Calculate the Economic Order Quantity and simulate the impact
3. Visualize the impact of lead time between ordering and receiving
4. Real-Time Visualization of COGS for each rule

### Data set
This analysis will be based on the M5 Forecasting dataset of Walmart stores sales records ([Link](
https://www.kaggle.com/c/m5-forecasting-accuracy)).

## Code
This repository code you will find all the code used to explain the concepts presented in the article.

## About me 🤓
Senior Supply Chain Engineer with an international experience working on Logistics and Transportation operations. \
Have a look at my portfolio: [Data Science for Supply Chain Portfolio](https://samirsaci.com) \
Data Science for Warehousing📦, Transportation 🚚 and Demand Forecasting 📈 

",10,10,1,0,retail,"[inventory, inventory-management, python, retail, supply-chain, supply-chain-management]",44-45
samirsaci,supply-planning,,https://github.com/samirsaci/supply-planning,https://api.github.com/repos/supply-planning/samirsaci,Supply Planning using Linear Programming with Python,"## Supply Planning using Linear Programming with Python 🚛
*Where do you need to allocate your stock to meet customers demand and reduce your transportation costs?*


<p align=""center"">
  <img align=""center"" src=""https://miro.medium.com/max/1280/1*y4AHwh75uQ771dEdO6sxJg.png"">
</p>

Supply planning is the process of managing the inventory produced by manufacturing to fulfil the requirements created from the demand plan.

Your target is to balance supply and demand in a manner to ensure the best service level at the lowest cost.

### Article
In this [Article](https://www.samirsaci.com/supply-planning-using-linear-programming-with-python/), we will present a simple methodology to use Integer Linear Programming to answer a complex Supply Planning Problem considering:
- Inbound Transportation Costs from the Plants to the Distribution Centers (DC) ($/Carton)
- Outbound Transportation Costs from the DCs to the final customer ($/Carton)
- Customer Demand (Carton)

### Problem Statement
As a Supply Planning manager of a mid-size manufacturing company, you received the feedback that the distribution costs are too high.
Based on the analysis of the Transportation Manager this is mainly due to the stock allocation rules.

In some cases, your customers are not shipped by the closest distribution centre, which impacts your freight costs.

### Your Distribution Network
- 2 plants producing products with infinite capacity
*Note: we’ll see later how we can improve this assumption easily*
- 2 distribution centres that receive finished goods from the two plants and deliver them to the final customers
*Note: we will consider that these warehouses operate X-Docking to avoid considering the concept of stock capacity in our model
200 stores (delivery points)*

### Question
Which Plant i and Distribution n should I chose to produce and deliver 100 units to Store p at the lowest cost?

## Code
This repository code you will find all the code used to explain the concepts presented in the article.

## About me 🤓
Senior Supply Chain Engineer with an international experience working on Logistics and Transportation operations. \
Have a look at my portfolio: [Data Science for Supply Chain Portfolio](https://samirsaci.com) \
Data Science for Warehousing📦, Transportation 🚚 and Demand Forecasting 📈 
",9,9,1,0,retail,"[linear-programming, optimization, python, retail, supply-chain, supply-chain-management, supply-planning]",44-45
samirsaci,sap-automation-po,,https://github.com/samirsaci/sap-automation-po,https://api.github.com/repos/sap-automation-po/samirsaci,Orders Creation Automation with SAP for Retail Using VB,"## Orders Creation Automation with SAP for Retail Using VB 🏪
*Automatically creating Purchase Order(s) based on information stored in an Excel spreadsheet.*

<p align=""center"">
  <img align=""center"" src=""https://miro.medium.com/max/875/1*dWGaJwzyTD4La3nApuUn3A.png"" style=""width: 75%; height: 75%""/>
</p>

### What is a Purchase Order (PO)?
In SAP Retail, a purchase order is a commercial document indicating types, quantities, agreed prices and delivery information (locations, time) for products or services.

### Youtube Video
Find in the link below a short animated explained video to understand the concept behind this solution
<div align=""center"">
  <a href=""https://www.youtube.com/watch?v=EY9yt0BTr2M""><img src=""https://github.com/samirsaci/sap-automation-po/blob/main/thumbnail.webp"" alt=""Explainer Video Link""></a>
</div>

### Scenario
> You want to order 10 Pcs of an article (SAP Code: 145654789 ) at the agreed price of 200$ to be delivered 20.09.2020 in WH01 by Supplier XXX (Vendor Code: 15487). 
You are working in a purchase organization (Code: PORG ) of Retail Company (Code: RTCP) and you’re part of the purchase group (Code: PRGP).

### How to do PO Creation in SAP?
ME21N transaction can be used to operate PO Creation

### Article
In this [Article](https://www.samirsaci.com/sap-automation-of-orders-creation-for-retail/), we will build a VBA script to
automate PO creation in SAP.

## Code
This repository code you will find all the code used to explain the concepts presented in the article.

## About me 🤓
Senior Supply Chain Engineer with an international experience working on Logistics and Transportation operations. \
Have a look at my portfolio: [Data Science for Supply Chain Portfolio](https://samirsaci.com) \
Data Science for Warehousing📦, Transportation 🚚 and Demand Forecasting 📈 
",2,2,1,0,retail,"[procurement, retail, rpa, sap, supply-chain, supply-chain-management, vba]",44-45
samirsaci,product-segmentation,,https://github.com/samirsaci/product-segmentation,https://api.github.com/repos/product-segmentation/samirsaci,Product Segmentation for Retail with Python,"## Product Segmentation for Retail with Python 📈
*A statistical methodology to segment your products based on turnover and demand variability*

<p align=""center"">
  <img align=""center"" src=""https://miro.medium.com/max/1280/1*JhUhKtJdNQR2QA4IkRTtEw.png"">
</p>

Product segmentation refers to the activity of grouping products that have similar characteristics and serve a similar market. It is usually related to marketing (Sales Categories) or manufacturing (Production Processes).
However, as a Logistics Manager, you rarely care about the product itself when managing goods flows; except for the dangerous and oversized products.

Your attention is mainly focused on the sales volumes distribution (fast/slow movers), demand variability and delivery lead time.

You want to put efforts into managing products that have:
- The highest contribution to your total turnover: ABC Analysis
- The most unstable demand: Demand Variability

### Article
In this [Article](https://www.samirsaci.com/product-segmentation-for-retail-with-python/), we will introduce simple statistical tools to combine ABC Analysis and Demand 
Variability to perform products segmentation.

### Problem Statement
You are the Operational Director of a local Distribution Center (DC) that delivers 10 Hypermarkets.

In your scope you the responsibility of
- Preparation and delivery of replenishment orders from stores
* Demand Planning and Inventory Management

### Question
What does impact your logistic performance?

### Data set
This analysis will be based on the M5 Forecasting dataset of Walmart stores sales records ([Link](
https://www.kaggle.com/c/m5-forecasting-accuracy)).

## Code
This repository code you will find all the code used to explain the concepts presented in the article.

## About me 🤓
Senior Supply Chain Engineer with an international experience working on Logistics and Transportation operations. \
Have a look at my portfolio: [Data Science for Supply Chain Portfolio](https://samirsaci.com) \
Data Science for Warehousing📦, Transportation 🚚 and Demand Forecasting 📈 

",6,6,1,0,retail,"[inventory-management, product-management, python, retail, supply-chain, supply-chain-management]",44-45
samirsaci,sap-automation,,https://github.com/samirsaci/sap-automation,https://api.github.com/repos/sap-automation/samirsaci,Product Listing Automation with SAP for Retail Using VB,"## Product Listing Automation with SAP for Retail Using VB 🏪
*Automated Listing Articles Excel-VB and SAP GUI Scripting Tool.*

<p align=""center"">
  <img align=""center"" src=""https://miro.medium.com/max/875/1*dWGaJwzyTD4La3nApuUn3A.png"" style=""width: 75%; height: 75%""/>
</p>

In SAP Retail, the **Material listing** is the operation that links one article to one assortment.

*Definition: An assortment is an SAP Retail object to which materials are assigned (a process known as “listing”).*

**Example:** SAP is used by your Retail Company with a scope including 20 stores named {ST01, ST02, … ST20}. These stores are delivered by Trucks from two Distribution Centers (DC) named {WH01, WH02}.

> Scenario — you want to buy a new reference (SAP Code: 145654789) that will be delivered by the supplier in WH02 and only sold in ST04.

#### Listing Process
- Listing 145654789 in WH02: this reference can be received in this DC
- Listing 145654789 in ST04: this reference can be received and sold in ST04

### Youtube Video
Find in the link below a short animated explained video to understand the concept behind this solution
<div align=""center"">
  <a href=""https://www.youtube.com/watch?v=yodNWnf7PQ0""><img src=""https://github.com/samirsaci/sap-automation/blob/main/thumbnail.webp"" alt=""Explainer Video Link""></a>
</div>

### Article
In this [Article](https://www.samirsaci.com/sap-automation-of-product-listing-for-retail/), we will build a VBA script to
automate product listing in SAP.

## Code
This repository code you will find all the code used to explain the concepts presented in the article.

## About me 🤓
Senior Supply Chain Engineer with an international experience working on Logistics and Transportation operations. \
Have a look at my portfolio: [Data Science for Supply Chain Portfolio](https://samirsaci.com) \
Data Science for Warehousing📦, Transportation 🚚 and Demand Forecasting 📈 


",1,1,1,0,retail,"[retail, rpa, sap, supply-chain, supply-chain-management, vba]",44-45
samirsaci,procurement-management,,https://github.com/samirsaci/procurement-management,https://api.github.com/repos/procurement-management/samirsaci,Procurement Process Optimization with Python,"## Procurement Process Optimization with Python 🚛
*Use non-linear programming to find the optimal ordering policy that minimizes capital, transportation and storage costs*

<p align=""center"">
  <img align=""center"" src=""https://miro.medium.com/max/1280/1*LlLZcqtUGUdqwLHSfF_N3g.png"">
</p>

Procurement management is a strategic approach to acquiring goods or services from preferred vendors, within your determined budget, either on or before a specific deadline.

Your target is to balance supply and demand in a manner to ensure a minimum level of inventory to meet your store demand.

### Article
In this [Article](https://www.samirsaci.com/procurement-process-optimization-with-python/), will present a simple methodology using Non-Linear Programming to design an optimal inventory replenishment strategy for a mid-size retail store considering:
- Transportation Costs from the Supplier Warehouse to the Store Reserve ($/Carton)
- Costs to finance your inventory (% of inventory value in $)
- Reserve (Store’s Warehouse) Rental Costs for storage ($/Carton)

### Problem Statement
As a Store Manager of a mid-size retail location, you are in charge of setting the replenishment quantity in the ERP.

For each SKU, when the inventory level is below a certain threshold your ERP is sending an automatic Purchase Order (PO) to your supplier.

You need to balance with the constraints of stock capacity, transportation costs and cost of inventory to fix the right quantity for your PO.

### Question
Which Quantity per replenishment Qi should you set in the ERP to minimize the total costs?

## Code
This repository code you will find all the code used to explain the concepts presented in the article.

## About me 🤓
Senior Supply Chain Engineer with an international experience working on Logistics and Transportation operations. \
Have a look at my portfolio: [Data Science for Supply Chain Portfolio](https://samirsaci.com) \
Data Science for Warehousing📦, Transportation 🚚 and Demand Forecasting 📈 
",3,3,1,0,retail,"[procurement, python, retail, supply-chain, supply-chain-management]",44-45
databricks-industry-solutions,als-recommender,databricks-industry-solutions,https://github.com/databricks-industry-solutions/als-recommender,https://api.github.com/repos/als-recommender/databricks-industry-solutions,Products We Think You Might Like: Generating Personalized Recommendations Using Matrix Factorization,"## ALS Recommender System Intro

Recommender systems are becoming increasing important as companies seek better ways to select products to present to end users. In this solution accelerator, we will explore a form of collaborative filter referred to as a matrix factorization.  

Matrix factorization works by assembling a set of ratings for various products made by a set of users.  The large user x products matrix is decomposed into smaller user and product submatrices associated with some developer-specified number of latent factors.  In many ways, a matrix factorization is a dimension reduction technique but one where missing values in the original matrix are allowed.

When examining ratings for a large number of user and product combinations, most users will engage with a very smaller percentage of products.  This causes us to have a user x products matrix that is highly sparse. When we decompose this matrix into the submatrices, the two can be combined to *recreate* the original matrix in a manner that provides ratings estimates for all products, including those a user has not yet engaged.  This ability to fill-in the missing ratings forms the basis for recommending new products to a user.

Matrix factorization recommenders are frequently used in scenarios where we wish to suggest new and repeat purchase items to a user.  *People like you also bought ...*, *Products we think you'll like ...*, and *Based on your purchase history ...* styled recommendations are frequently delivered through this type of recommender.

The challenge in developing a matrix factorization recommender is the large amount of computational horsepower required to calculate the submatrices.  Alternating Least Squares (ALS) is one approach that decomposes the process into a series of incremental steps that can be implemented in a distributed manner. In this solution accelerator, we will train and deploy an ALS-based matrix factorization recommender using the ALS capabilities in Apache Spark to demonstrate how this is done.

&copy; 2022 Databricks, Inc. All rights reserved. The source in this notebook is provided subject to the Databricks License [https://databricks.com/db-license-source].  All included or referenced third party libraries are subject to the licenses set forth below.

| library                                | description             | license    | source                                              |
|----------------------------------------|-------------------------|------------|-----------------------------------------------------|
| PyYAML                                 | Reading Yaml files      | MIT        | https://github.com/yaml/pyyaml                      |

## Instruction

To run this accelerator, clone this repo into a Databricks workspace. Attach the `RUNME` notebook to any cluster running a DBR 11.0 or later runtime, and execute the notebook via Run-All. A multi-step-job describing the accelerator pipeline will be created, and the link will be provided. Execute the multi-step-job to see how the pipeline runs. The job configuration is written in the RUNME notebook in json format. The cost associated with running the accelerator is the user's responsibility.
",1,1,2,1,retail,"[databricks, databricks-industry-solutions, recommendation-engine, recommendation-system, recommender-system, retail]",44-45
navassherif98,Dash_4U-Analysis,,https://github.com/navassherif98/Dash_4U-Analysis,https://api.github.com/repos/Dash_4U-Analysis/navassherif98,This Repository contains a Simple Dash App for Stock and Monthly Report Analysis of Shop(4U),"![ViewCount](https://views.whatilearened.today/views/github/navassherif98/Dash_4U-Analysis.svg?cache=remove)

# Dash_4U-Analysis

### Simple Dash App for Stock and Monthly Report Analysis of My Retail Shop(4U)


![ezgif com-gif-maker](https://user-images.githubusercontent.com/55757415/115176464-c8684480-a0ea-11eb-9f8a-d7dfac7f8106.gif)
",1,1,0,0,retail,"[analysis, api, application, business, business-analytics, dash, dash-apps, dashboard, development, financial-analysis, graph, python, retail, sqlalchemy]",44-45
samirsaci,inventory-stochastic,,https://github.com/samirsaci/inventory-stochastic,https://api.github.com/repos/inventory-stochastic/samirsaci,Inventory Management for Retail — Stochastic Demand,"## Inventory Management for Retail — Stochastic Demand 📈
*Simulate the impact of safety stock level on inventory management performance metrics assuming a normal distribution of your demand*

<p align=""center"">
  <img align=""center"" src=""https://miro.medium.com/max/1280/1*qd_L-_YIZwrgwvt9WHkk-w.png"">
</p>

For most retailers, inventory management systems take a fixed, rule-based approach to forecast and replenishment orders management.

Considering the distribution of the demand, the objective is to build a replenishment policy that will minimize your ordering, holding and shortage costs.

### Article
In this [Article](https://www.samirsaci.com/inventory-management-for-retail-stochastic-demand-2/), we will improve this model and introduce a simple methodology using a discrete simulation model 
built with Python to test several inventory management rules assuming a normal distribution of the customer demand.

### Problem Statement
As an Inventory Manager of a mid-size retail chain, you are in charge of setting the replenishment quantity in the ERP.

Based on the feedbacks of the store manager, you start to doubt that the replenishment rules of the ERP are the most optimal especially for the fast runners because your stores are facing lost sales due to stock-outs.

For each SKU, you would like to build a simple simulation model to test several inventory rules and estimate the impact on:
- Performance Metrics
- Cycle Service Level (CSL): probability to have a stock-out for each cycle (%)
- Item Fill Rate (IFR): percentage of customer demand met without stock-out (%)

### Question
What does impact your logistic performance?

### Data set
This analysis will be based on the M5 Forecasting dataset of Walmart stores sales records ([Link](
https://www.kaggle.com/c/m5-forecasting-accuracy)).

## Code
This repository code you will find all the code used to explain the concepts presented in the article.

## About me 🤓
Senior Supply Chain Engineer with an international experience working on Logistics and Transportation operations. \
Have a look at my portfolio: [Data Science for Supply Chain Portfolio](https://samirsaci.com) \
Data Science for Warehousing📦, Transportation 🚚 and Demand Forecasting 📈 

",8,8,1,1,retail,"[inventory-management, optimization, python, retail, stochastic, supply-chain, supply-chain-management]",44-45
navassherif98,Website-4U_Mens_and_Sports,,https://github.com/navassherif98/Website-4U_Mens_and_Sports,https://api.github.com/repos/Website-4U_Mens_and_Sports/navassherif98,This repository contains a Business website for 4U Mens and Sports Retail Shop and its hosted on Heroku and Github ,"# Website-4U_Mens_and_Sports

A Website for Retail Shop : 4U Mens and Sports , Mundur
<br />
<br />
### Hosted on Github
#### Check it out : https://navassherif98.github.io/Website-4U_Mens_and_Sports/


### Hosted on Heroku
#### check it out : https://mundur4u.herokuapp.com/
",2,2,1,0,retail,"[bootstrap, business, business-website, css, github, heroku, hosted, html, javascript, python, retail, retail-shop, shop, templates, website, website-for-business, website-for-retailshop]",44-45
samirsaci,business-profitability,,https://github.com/samirsaci/business-profitability,https://api.github.com/repos/business-profitability/samirsaci,Maximize your Business Profitability with Python,"## Maximize your Business Profitability with Python 📈
*Use linear programming to help your local bakery to improve its business profitability by selecting the right items to sell*

<p align=""center"">
  <img align=""center"" src=""https://miro.medium.com/max/1280/1*64O1JAsgcvVv1hyE-RnZKQ.png"">
</p>

### Article
In this [Article](https://www.samirsaci.com/maximize-your-business-profitability-with-python/), we will see how to help your local bakery to maximize its profit by producing the right items using Linear Programming with Python.


### Problem Statement
You want to help your local bakery to maximize its profit. They sell several types of product including cakes, pastries and sandwiches.

Available resources
- 4 bakers working 6 hours per day (Total: 24 hours/day)
- 1 assistant for sandwiches 4 hours per day
- 2 ovens that can be used 24 hours per day (Total: 48 hours/day)
- 100 slots available for stock and display

These items have different resource needs and profit level

<p align=""center"">
  <img align=""center"" src=""https://miro.medium.com/max/700/1*ToORI5-OD4XJUrt0kY46Ew.png"">
</p>

### Objective
What do you need to produce to maximize your daily profit?

## Code
This repository code you will find all the code used to explain the concepts presented in the article.

## About me 🤓
Senior Supply Chain Engineer with an international experience working on Logistics and Transportation operations. \
Have a look at my portfolio: [Data Science for Supply Chain Portfolio](https://samirsaci.com) \
Data Science for Warehousing📦, Transportation 🚚 and Demand Forecasting 📈 

",2,2,2,0,retail,"[business-profit, linear-programming, operations-research, optimization, python, retail, supply-chain, supply-chain-management]",44-45
eugenebelieve,marketplace,,https://github.com/eugenebelieve/marketplace,https://api.github.com/repos/marketplace/eugenebelieve,"A Marketplace POV running Microservices on NodeJS & ExpressJS, a Frontend application on ReactJS and a MongoDB Database. You can quickly import a retail dataset and have a fully functional Marketplace with login, users, products, catalogs, review, orders, payment and much more.","<img src=""application/public/images/marketplace/marketplace.png"" alt=""dashboard"" height=""400"">

A Marketplace running on a <strong>MongoDB Atlas database</strong> with Microservices on <strong>Nodejs & Expressjs</strong> and a <strong>frontend on Reactjs</strong>

# Usage

## Retail Marketplace (MongoDB, NodeJS, ExpressJS, ReactJS & JWT)

### Clone Repositorie

Clone this Repositorie to your local machine

```
git clone https://github.com/eugenebelieve/marketplace-retail.git
```


## Usage

### Env Variables

Create or modify the .env file in then root and add the following

```
NODE_ENV = 'development'
PORT = '5000'
MONGO_URI = ""YOUR_MONGODB_URI_HERE""
JWT_SECRET = 'random_secret_key'
PAYPAL_CLIENT_ID = 'your paypal client id'
```

### Install Dependencies (frontend & backend)

```
npm install
cd frontend
npm install
```

### Run

```
# Run frontend Application (:3000) & Microservices (:5000)
npm run dev

# Run backend only
npm run server
```

### Seed Database

You can use the following commands to seed the database with some sample users and products as well as destroy all data

```
# Import General data
npm run data:general

# Import Automotive data
npm run data:automotive

# Import Retail data
npm run data:retail

# Destroy data
npm run data:destroy
```

```
Sample User Logins

admin@example.com (Admin)
123456

john@example.com (Customer)
123456

jane@example.com (Customer)
123456
```
",2,2,1,1,retail,"[express, marketplace, mern, mongodb, node, react, retail]",44-45
samirsaci,segmentation,,https://github.com/samirsaci/segmentation,https://api.github.com/repos/segmentation/samirsaci,Streamlit Application for ABC Analysis & Product Segmentation ,"# Automate ABC Analysis & Product Segmentation with Streamlit 📈
*A statistical methodology to segment your products based on turnover and demand variability using an automated solution with a web application designed with the framework Streamlit*

<p align=""center"">
  <img align=""center"" src=""images/streamlit_capture.PNG"" width=75%>
</p>
<p align=""center""><b>streamlit Application UI</b></p>

Product segmentation refers to the activity of grouping products that have similar characteristics and serve a similar market. It is usually related to marketing _(Sales Categories)_ or manufacturing _(Production Processes)_. However as a **Supply Chaine Engineer** your focus is not on the product itself but more on the complexity of managing its flow.

Your want to understand the sales volumes distribution (fast/slow movers) and demand variability to optimize your production, storage and delivery operations to ensure the best service level by considering: 
- The highest contribution to your total volume: ABC Analysis
- The most unstable demand: Demand Variability

I have designed this **Streamlit App** to provide a tool to **Supply Chain Engineers** for Product Segmentation, with a focus on retail products, of their portofolio considering the complexity of the demand and the volumes contribution of each item.

### Understand the theory behind 📜
In this [Article](https://www.samirsaci.com/product-segmentation-for-retail-with-python/), you can find details about the theory used to build this tool. 

# Access the application 🖥️ 
> Access it here: [Product Segmentation for Retail](https://share.streamlit.io/samirsaci/segmentation/main/segmentation.py)

## **Step 0: Why should you use it?**
This Streamlit Web Application has been designed for Supply Chain Engineers to support them in their Inventory Management. It will help you to automate product segmentation using statistics.

## **Step 1: What do you want to do?**
You have two ways to use this application:
- 🖥️ Look at the results computed by the model using the pre-loaded dataset: in that case you just need to scroll to see the visuals and the analyses
OR
- 💾 Upload your dataset of sales records that includes columns related to:
  - **Item master data**
  _For example: SKU ID, Category, Sub-Category, Store ID_
  - **Date of the sales**:
  _For example: Day, Week, Month, Year_
  - **Quantity or value**: this measure will be used for the ABC analysis
  _For example: units, cartons, pallets or euros/dollars/your local currency_

## **Step 2: Prepare the analysis**

### **1. 💾 Upload your dataset of sales records**
<p align=""center"">
  <img align=""center"" src=""images/step_1.PNG"" width=40%>
</p>
<p align=""center""><b>Step 1:</b> upload your dataset of sales records</p>


💡 _Please make sure that you dataset format is csv with a file size lower than 200MB. If you want to increase the size, you'd better copy this repository and deploy the app locally following the instructions below._

### **2. 📅 [Parameters] select the columns for the date (day, week, year) and the values (quantity, $)**
<p align=""center"">
  <img align=""center"" src=""images/step_2.PNG"" width=75%>
</p>
<p align=""center""><b>Step 2:</b> select the columns for the date (day, week, year) and the values (quantity, $)</p>


💡 _If you have several columns for the date (day, week, month) and for the values (quantity, amount) you can use only one column per category for each run of calculation._

### **3. 📉 [Parameters] select all the columns you want to keep in the analysis**
<p align=""center"">
  <img align=""center"" src=""images/step_3.PNG"" width=75%>
</p>
<p align=""center""><b>Step 3:</b> select the columns for the date (day, week, year)</p>


💡 _This step will basically help you to remove the columns that you do not need for your analysis to increase the speed of computation and reduce the usage of ressources._

### **4. 🏬 [Parameters] select all the related to product master data (SKU ID, FAMILIY, CATEGORY, STORE LOCATION)**
<p align=""center"">
  <img align=""center"" src=""images/step_4.PNG"" width=75%>
</p>
<p align=""center""><b>Step 4:</b> select all the related to product master data (SKU ID, FAMILIY, CATEGORY, STORE LOCATION)</p>


💡 _In this step you will show at what granularity you want to do your analysis. For example it can be at:_
  - _Item, Store level: that means the same item in two stores will represent two SKU_
  - _Item ID level: that means you group the sales of your item in all stores_

### **5. 🛍️ [Parameters] select one feature you want to use for analysis by family**
<p align=""center"">
  <img align=""center"" src=""images/step_5.PNG"" width=75%>
</p>
<p align=""center""><b>Step 5:</b> select one feature you want to use for analysis by family</p>


💡 _This feature will be used to plot the repartition of (A, B, C) product by family_

### **6. 🖱️ Click on Start Calculation? to launch the analysis**
<p align=""center"">
  <img align=""center"" src=""images/step_6.PNG"" width=75%>
</p>
<p align=""center""><b>Step 6:</b> Start Calculation</p>


💡 _This feature will be used to plot the repartition of (A, B, C) product by family_

# Get insights about your sales records 💡

### **Pareto Analysis**

<p align=""center"">
  <img align=""center"" src=""images/pareto.PNG"" width=75%>
</p>
<p align=""center""><b>Concept</b> Pareto Analysis</p>


**INSIGHTS:** 
1. How many SKU represent 80% of your total sales?
2. How much sales represent 20% of your SKUs?

_For more information about the theory behind the pareto law and its application in Supply Chain Management: [Pareto Principle for Warehouse Layout Optimization](https://www.samirsaci.com/reduce-warehouse-space-with-the-pareto-principle-using-python/)_

### **ABC Analysis with Demand Variability**

<p align=""center"">
  <img align=""center"" src=""images/abc_analysis.PNG"" width=75%>
</p>
<p align=""center""><b>Streamlit App Screenshot:</b> ABC Analysis plot</p>


**QUESTIONS: WHAT IS THE PROPORTION OF?** 
1. **LOW IMPORTANCE SKUS**: C references
2. **STABLE DEMAND SKUS**: A and B SKUs with a coefficient of variation below 1 
3. **HIGH IMPORTANCE SKUS**: A and B SKUS with a high coefficient of variation

Your inventory management strategies will be impacted by this split:
- A minimum effort should be put in **LOW IMPORTANCE SKUS**
- Automated rules with a moderate attention for **STABLE SKUS**
- Complex replenishment rules and careful attention for **HIGH IMPORTANCE SKUS**


_For more information: [Article](https://www.samirsaci.com/product-segmentation-for-retail-with-python/)_

<p align=""center"">
  <img align=""center"" src=""images/split_category.PNG"" width=75%>
</p>
<p align=""center""><b>Streamlit App Screenshot:</b> ABC SKU split for each family/category</p>


**QUESTIONS:** 
1. What is the split of SKUS by FAMILY?
2. What is the split of SKUS by ABC class in each FAMILY?


### **Normality Test**

<p align=""center"">
  <img align=""center"" src=""images/normality.PNG"" width=75%>
</p>
<p align=""center""><b>Streamlit App Screenshot:</b> Normality test</p>


**QUESTION:** 
- Which SKUs have a sales distribution that follows a normal distribution?

Many inventory rules and safety stock formula can be used only if the sales distribution of your item is following a normal distribution. Thefore, it's better to know the % of your portofolio that can be managed easily.

_For more information: [Inventory Management for Retail — Stochastic Demand](https://www.samirsaci.com/inventory-management-for-retail-stochastic-demand-2/)_


# Build the application locally 🏗️ 

## **Build a python local environment (recommanded)** 

### Then install **virtualenv** using pip3

    sudo pip3 install virtualenv 

### Now create a virtual environment 

    virtualenv venv 
  
### Active your virtual environment    
    
    source venv/bin/activate
  
## Launch Streamlit 🚀

### Install all dependencies needed using requirements.txt

     pip install -r requirements.txt 

### Run the application  

    streamlit run segmentation.py 

### Click on the Network URL in the shell   
  <p align=""center"">
    <img align=""center"" src=""images/network.PNG"" width=50%>
  </p>
  
> -> Enjoy!
# About me 🤓
Senior Supply Chain Engineer with an international experience working on Logistics and Transportation operations. \
Have a look at my portfolio: [Data Science for Supply Chain Portfolio](https://samirsaci.com) \
Data Science for Warehousing📦, Transportation 🚚 and Demand Forecasting 📈 
",13,13,1,0,retail,"[inventory-management, product-segmentation, retail, streamlit, supply-chain, supply-chain-management]",44-45
