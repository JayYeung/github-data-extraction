{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1PqMPI-nJmD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from github import Github\n",
        "import base64\n",
        "import os\n",
        "import json\n",
        "import requests\n",
        "from requests.structures import CaseInsensitiveDict\n",
        "import sys\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "from random import shuffle, randint, choice\n",
        "from functools import cache \n",
        "from local_secrets import API_KEYS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wga656b3PSe6"
      },
      "outputs": [],
      "source": [
        "def get_topic_list(main_topic):\n",
        "    naics = pd.read_csv(\"NAICS Topics.csv\")\n",
        "    lst_of_topics = naics[naics[\"Definition\"] ==\n",
        "                          main_topic][\"Related Github Topics\"].tolist()\n",
        "    return lst_of_topics[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghtmEwVVH-5L"
      },
      "outputs": [],
      "source": [
        "g = Github(\"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\")\n",
        "\n",
        "\n",
        "def get_name_repo_txt(repo_topic, base_folder):\n",
        "    text = repo_topic\n",
        "    pattern = r'full_name=\"([^\"]+)\"'\n",
        "    match = re.search(pattern, text)\n",
        "    if match:\n",
        "        full_name = match.group(1)\n",
        "\n",
        "    text = full_name\n",
        "    pattern = r'(.+)/'\n",
        "    match = re.search(pattern, text)\n",
        "\n",
        "    if match:\n",
        "        owner = match.group(1)\n",
        "\n",
        "    text = full_name\n",
        "    pattern = r'/(.+)'\n",
        "\n",
        "    match = re.search(pattern, text)\n",
        "\n",
        "    if match:\n",
        "        repo = match.group(1)\n",
        "\n",
        "    data = [f\"{owner}\", f\"{repo}\"]\n",
        "    return data\n",
        "\n",
        "def put_txt_in_folder(topic_input_list, base_folder_path, Main_Topic, limit=1000):\n",
        "    new_file_path = f\"data/repo_by_topic/{Main_Topic}/topics_{Main_Topic}.csv\"\n",
        "    \n",
        "    if os.path.exists(new_file_path):\n",
        "        print(f\"{new_file_path} already exists. Skipping...\")\n",
        "        return\n",
        "\n",
        "    topics = topic_input_list.split(\", \")\n",
        "\n",
        "    list_of_repos = []\n",
        "    repo_topics = []\n",
        "\n",
        "    for topic in topics:\n",
        "        print(f\"Fetching repositories for topic {topic}...\")\n",
        "        try: \n",
        "            repos = g.search_repositories(query=f'topic:{topic}')\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching repositories for topic {topic}: {e}\")\n",
        "            time.sleep(5)  \n",
        "            continue\n",
        "\n",
        "        for repo in repos:\n",
        "            repo_string = str(repo)\n",
        "            list_of_repos.append(repo_string)\n",
        "            repo_topics.append(topic)\n",
        "\n",
        "    data = []\n",
        "\n",
        "    for repo, topic in zip(list_of_repos, repo_topics):\n",
        "        try:\n",
        "            new_entry = get_name_repo_txt(str(repo), base_folder_path)\n",
        "            new_entry.append(topic)\n",
        "            data.append(new_entry)\n",
        "        except:\n",
        "            print(\"Failure\")\n",
        "\n",
        "    if not os.path.exists(f\"data/repo_by_topic/{Main_Topic}\"):\n",
        "        os.makedirs(f\"data/repo_by_topic/{Main_Topic}\")\n",
        "\n",
        "    with open(new_file_path, \"w\") as file:\n",
        "        file.write(f\"{Main_Topic}\\n\")\n",
        "        file.write(\"User, Repo, Topic\\n\")\n",
        "        for entry in data:\n",
        "            file.write(f\"{entry[0]}, {entry[1]}, {entry[2]}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyVIrNF1I-r7"
      },
      "outputs": [],
      "source": [
        "def get_url(user, repo, path=\"\"):\n",
        "    if path != \"\":\n",
        "        return \"https://api.github.com/repos/\" + user + \"/\" + repo\n",
        "    else:\n",
        "        return \"https://api.github.com/repos/\" + user + \"/\" + repo + \"/\" + path\n",
        "\n",
        "\n",
        "def file_to_list_dict(filepath):\n",
        "    with open(filepath) as file:\n",
        "        content = file.read()\n",
        "    result = []\n",
        "    i = 0\n",
        "    while i < len(content):\n",
        "        if content[i] == '{':\n",
        "            start = i\n",
        "            cb1 = 1\n",
        "            cb2 = 0\n",
        "            while cb1 != cb2:\n",
        "                i += 1\n",
        "                if content[i] == \"{\":\n",
        "                    cb1 += 1\n",
        "                elif content[i] == '}':\n",
        "                    cb2 += 1\n",
        "            result.append(json.loads(content[start:i+1]))\n",
        "        i += 1\n",
        "    return result\n",
        "\n",
        "\n",
        "def get(user, repo, category, nested=False):\n",
        "    rdict = get_content(user, repo)\n",
        "    if category in rdict:\n",
        "        return rdict[category]\n",
        "    else:\n",
        "        return \"N/A\"\n",
        "\n",
        "\n",
        "def get_readme(user, repo):\n",
        "    rdict = get_content(user, repo, path=\"contents/README.md\")\n",
        "    if rdict and 'content' in rdict:\n",
        "        decoded = base64.b64decode(rdict['content']).decode('utf-8')\n",
        "        return decoded\n",
        "    else:\n",
        "        return rdict\n",
        "\n",
        "working_keys = API_KEYS.copy()\n",
        "\n",
        "def get_content(user, repo, path=\"\", max_retries=3, backoff_time=60):\n",
        "    global working_keys\n",
        "\n",
        "    url = f\"https://api.github.com/repos/{repo}/{user}\"\n",
        "\n",
        "    if path != \"\":\n",
        "        url += f\"/{path}\"\n",
        "\n",
        "    headers = CaseInsensitiveDict()\n",
        "    headers[\"Accept\"] = \"application/vnd.github+json\"\n",
        "    headers[\"X-Github-Api-Version\"] = \"2022-11-28\"\n",
        "\n",
        "    retries = 0\n",
        "    while retries <= max_retries:\n",
        "        try: \n",
        "            if not working_keys:\n",
        "                print(\"Resetting working keys...\")\n",
        "                working_keys = API_KEYS.copy()\n",
        "            current_key = choice(working_keys)\n",
        "            headers[\"Authorization\"] = f\"Bearer {current_key}\"\n",
        "\n",
        "            resp = requests.get(url, headers=headers)\n",
        "            resp_json = json.loads(resp.text)\n",
        "\n",
        "            if resp.status_code == 200:\n",
        "                return resp_json\n",
        "            elif \"message\" in resp_json and \"rate limit exceeded for\" in resp_json[\"message\"].lower():\n",
        "                print(f\"Rate limit reached for a key. Removing key...\")\n",
        "                working_keys.remove(current_key)\n",
        "                time.sleep(randint(0, backoff_time))\n",
        "            elif resp_json.get(\"message\") == \"Not Found\":\n",
        "                print(f\"Repository {url} not found. Giving up.\")\n",
        "                return None\n",
        "            else:\n",
        "                print(f\"Request failed with status code {resp.status_code}. Retrying in {backoff_time} seconds...\")\n",
        "                print(f\"Response: {resp_json}\")\n",
        "                time.sleep(randint(0, backoff_time))\n",
        "                retries += 1\n",
        "            backoff_time *= 3\n",
        "        except Exception as e:\n",
        "            print(f\"SOMETHING CRASHED IT: {e}. Retrying in {backoff_time} seconds...\")\n",
        "            time.sleep(backoff_time)\n",
        "\n",
        "    print(f\"Max retries reached. Could not fetch data for {url}.\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def get_repos(filepath):\n",
        "    lst = []\n",
        "    f = open(filepath, 'r')\n",
        "    f.readline()\n",
        "    f.readline()\n",
        "    curr = f.readline().split(\",\")\n",
        "    while len(curr) > 1:\n",
        "        lst.append((curr[0].strip(), curr[1].strip(), curr[2][:-1].strip()))\n",
        "        curr = f.readline().split(\",\")\n",
        "    f.close()\n",
        "    return lst\n",
        "\n",
        "def get_and_save_data(topic, repos):\n",
        "    data_path = \"./data/repo_by_topic/\" + topic + \"/\"\n",
        "    csv_file_path = data_path + \"data_\" + topic + \".csv\"\n",
        "    txt_file_path = data_path + \"repos_\" + topic + \".txt\"\n",
        "    \n",
        "    if not os.path.exists(data_path):\n",
        "        os.makedirs(data_path)\n",
        "    \n",
        "    existing_data = pd.DataFrame()\n",
        "    reposlst = []\n",
        "    \n",
        "    if os.path.exists(csv_file_path):\n",
        "        existing_data = pd.read_csv(csv_file_path)\n",
        "    \n",
        "    data_list = []\n",
        "    count = 0\n",
        "\n",
        "    shuffle(repos)\n",
        "\n",
        "    for rname, ruser, rtopic in repos:\n",
        "        if not existing_data.empty:\n",
        "            temp = existing_data[(existing_data['repo'] == rname) & (existing_data['user'] == ruser)]\n",
        "            if not temp.empty:\n",
        "                continue\n",
        "\n",
        "        url_content = get_content(ruser, rname)\n",
        "        if not url_content:\n",
        "            print('skipping', ruser, rname)\n",
        "            continue\n",
        "        \n",
        "        reposlst.append(url_content)\n",
        "        \n",
        "        organization = get(ruser, rname, \"organization\")\n",
        "        org_login = organization['login'] if organization != \"N/A\" else organization\n",
        "\n",
        "        topics = get(ruser, rname, \"topics\")\n",
        "        topics_str = '[' + ', '.join(topics) + ']' if topics else \"N/A\"\n",
        "\n",
        "        row_dict = {\n",
        "            \"repo\": rname,\n",
        "            \"user\": ruser,\n",
        "            \"organization\": org_login,\n",
        "            \"url (HTML)\": get(ruser, rname, \"html_url\"),\n",
        "            \"url (API)\": get_url(ruser, rname)[:-1],\n",
        "            \"description\": get(ruser, rname, \"description\"),\n",
        "            \"readme\": get_readme(ruser, rname),\n",
        "            \"stargazer count\": get(ruser, rname, \"stargazers_count\"),\n",
        "            \"watcher count\": get(ruser, rname, \"watchers_count\"),\n",
        "            \"subscriber count\": get(ruser, rname, \"subscribers_count\"),\n",
        "            \"open issue count\": get(ruser, rname, \"open_issues_count\"),\n",
        "            \"topic (search)\": rtopic,\n",
        "            \"topics\": topics_str,\n",
        "            \"NAICS Code\": \"00000\"\n",
        "        }        \n",
        "        data_list.append(row_dict)\n",
        "        count += 1\n",
        "\n",
        "        if count % 1 == 0:\n",
        "            print(f'savign data for {topic}, it has {count} repos')\n",
        "            temp_data = pd.DataFrame(data_list)\n",
        "            temp_data.to_csv(csv_file_path, mode='a', header=not os.path.exists(csv_file_path), index=False)\n",
        "            \n",
        "            # Save to text file\n",
        "            with open(txt_file_path, \"a\") as f:\n",
        "                for repo in reposlst:\n",
        "                    f.write(f\"{repo}\\n\")\n",
        "            \n",
        "            # Clear temporary lists\n",
        "            data_list = []\n",
        "            reposlst = []\n",
        "\n",
        "    # Save remaining data\n",
        "    if data_list:\n",
        "        temp_data = pd.DataFrame(data_list)\n",
        "        temp_data.to_csv(csv_file_path, mode='a', header=not os.path.exists(csv_file_path), index=False)\n",
        "        \n",
        "    if reposlst:\n",
        "        with open(txt_file_path, \"a\") as f:\n",
        "            for repo in reposlst:\n",
        "                f.write(f\"{repo}\\n\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "3e02r63jkjcs",
        "outputId": "751d5f97-5985-46b9-9dfb-69917e947c51"
      },
      "outputs": [],
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import pandas as pd\n",
        "\n",
        "def process_main_topic(main_topic):\n",
        "    print(f'Working on {main_topic}')\n",
        "    main_topic_list = get_topic_list(main_topic)\n",
        "    \n",
        "    base_folder_path = '/content/drive/My Drive/Finance and Insurance Repo (Data)'\n",
        "    put_txt_in_folder(main_topic_list, base_folder_path, main_topic)\n",
        "\n",
        "    print(f'Finished putting txt in folder {main_topic}')\n",
        "\n",
        "    file = f\"data/repo_by_topic/{main_topic}/topics_{main_topic}.csv\"\n",
        "    repos = get_repos(file)\n",
        "    get_and_save_data(main_topic, repos)\n",
        "\n",
        "naics = pd.read_csv(\"NAICS Topics.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "    executor.map(process_main_topic, naics['Definition']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
